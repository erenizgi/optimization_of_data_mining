{
    "author": "vasqu",
    "message": "ðŸ”´[`Attention`] Bert-based Models Attention Refactor (#38301)\n\n* clean start to bert refactor\n\n* some test fixes\n\n* style\n\n* fix last tests\n\n* be strict on positional embeddings, fixup according tests\n\n* cache support\n\n* more cache fixes, new causal API\n\n* simplify masks, fix tests for gen\n\n* flex attn, static cache support, round of fixes\n\n* ?\n\n* this time\n\n* style\n\n* fix flash attention tests, flex attention requires torch 2.7.x to work with multiple classes (as recompile strats force a size call which is wrongly interpreted before)\n\n* roberta\n\n* fixup sdpa remains\n\n* attention split, simplify args and kwargs, better typing\n\n* fix encoder decoder\n\n* fix test\n\n* modular roberta\n\n* albert\n\n* data2vectext, making it modular tomorrow\n\n* modular data2vec text\n\n* tmp disable\n\n* xmod + cache position fixes\n\n* whoops\n\n* electra + markuplm, small fixes\n\n* remove wrong copy\n\n* xlm_roberta + some embedding fixes\n\n* roberta prelayernorm\n\n* RemBert: remove copy, maybe doing it later\n\n* ernie\n\n* fix roberta offloading\n\n* camembert\n\n* copy fixes\n\n* bert generation + fixes on eager\n\n* xlm roberta xl\n\n* bridgetower (text) + seamlessv2 copy fixes\n\n* rocbert + small fixes\n\n* whoops\n\n* small round of fixups\n\n* NOTE: kernels didnt load with an earlier version, some fixup (needs another look bc cross deps)\n\n* the end of the tunnel?\n\n* fixup nllbmoe + style\n\n* we dont need this anymore\n\n* megatron bert is barely used, low prio skip for now\n\n* Modernize bert (template for others)\n\nNOTE: trying to push this through, might be overdue if not in time possible\n\n* check inputs for all others (if checkmarked)\n\n* fix bridgetower\n\n* style\n\n* fix encoder decoder (partially but cause found and fix also, just needs to be done for everything else)\n\n* proper fix for bert to force intermediate dict outputs\n\n* propagate to others\n\n* style\n\n* xlm roberta xl investigation, its the layernorm...\n\n* mobile bert\n\n* revert this, might cause issues with composed models\n\n* review\n\n* style",
    "sha": "155f7e2e6237bc4f623aecf9bfd97442d5723963",
    "files": [
        {
            "sha": "31caa335bb64d93f3792b93e52dd279ea571a0b6",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 196,
            "deletions": 308,
            "changes": 504,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -14,16 +14,15 @@\n # limitations under the License.\n \"\"\"PyTorch ALBERT model.\"\"\"\n \n-import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -33,16 +32,22 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import (\n     apply_chunking_to_forward,\n     find_pruneable_heads_and_indices,\n     prune_linear_layer,\n )\n-from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_albert import AlbertConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -69,33 +74,32 @@ def __init__(self, config: AlbertConfig):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        past_key_values_length: int = 0,\n     ) -> torch.Tensor:\n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n-            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n+            position_ids = self.position_ids[:, :seq_length]\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -112,6 +116,65 @@ def forward(\n         return embeddings\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class AlbertAttention(nn.Module):\n     def __init__(self, config: AlbertConfig):\n         super().__init__()\n@@ -120,19 +183,22 @@ def __init__(self, config: AlbertConfig):\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads}\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.hidden_size = config.hidden_size\n         self.attention_head_size = config.hidden_size // config.num_attention_heads\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n+\n+        self.attention_dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size)\n-\n-        self.attention_dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-        self.output_dropout = nn.Dropout(config.hidden_dropout_prob)\n         self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.pruned_heads = set()\n \n@@ -141,6 +207,8 @@ def __init__(self, config: AlbertConfig):\n             self.max_position_embeddings = config.max_position_embeddings\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n+        self.is_causal = False\n+\n     def prune_heads(self, heads: list[int]) -> None:\n         if len(heads) == 0:\n             return\n@@ -164,125 +232,45 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        key_layer = self.key(hidden_states)\n-        value_layer = self.value(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-        key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        value_layer = value_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            seq_length = hidden_states.size()[1]\n-            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.attention_dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-        context_layer = context_layer.transpose(2, 1).flatten(2)\n-\n-        projected_context_layer = self.dense(context_layer)\n-        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n-        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n-        return (layernormed_context_layer, attention_probs) if output_attentions else (layernormed_context_layer,)\n-\n-\n-class AlbertSdpaAttention(AlbertAttention):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions:\n-            logger.warning(\n-                \"AlbertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` . Falling back to \"\n-                \"the eager attention implementation, but specifying the eager implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(hidden_states, attention_mask, output_attentions=output_attentions)\n-\n-        batch_size, seq_len, _ = hidden_states.size()\n-        query_layer = (\n-            self.query(hidden_states)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-        key_layer = (\n-            self.key(hidden_states)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-        value_layer = (\n-            self.value(hidden_states)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-\n-        attention_output = torch.nn.functional.scaled_dot_product_attention(\n-            query=query_layer,\n-            key=key_layer,\n-            value=value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=False,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=False,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n \n-        attention_output = attention_output.transpose(1, 2)\n-        attention_output = attention_output.reshape(batch_size, seq_len, self.all_head_size)\n-\n-        projected_context_layer = self.dense(attention_output)\n-        projected_context_layer_dropout = self.output_dropout(projected_context_layer)\n-        layernormed_context_layer = self.LayerNorm(hidden_states + projected_context_layer_dropout)\n-        return (layernormed_context_layer,)\n-\n+        attn_output = self.dense(attn_output)\n+        attn_output = self.output_dropout(attn_output)\n+        attn_output = self.LayerNorm(hidden_states + attn_output)\n \n-ALBERT_ATTENTION_CLASSES = {\n-    \"eager\": AlbertAttention,\n-    \"sdpa\": AlbertSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class AlbertLayer(nn.Module):\n@@ -293,7 +281,7 @@ def __init__(self, config: AlbertConfig):\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n         self.full_layer_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attention = ALBERT_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = AlbertAttention(config)\n         self.ffn = nn.Linear(config.hidden_size, config.intermediate_size)\n         self.ffn_output = nn.Linear(config.intermediate_size, config.hidden_size)\n         self.activation = ACT2FN[config.hidden_act]\n@@ -304,20 +292,18 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n-        attention_output = self.attention(hidden_states, attention_mask, head_mask, output_attentions)\n+        attention_output, _ = self.attention(hidden_states, attention_mask, head_mask, **kwargs)\n \n         ffn_output = apply_chunking_to_forward(\n             self.ff_chunk,\n             self.chunk_size_feed_forward,\n             self.seq_len_dim,\n-            attention_output[0],\n+            attention_output,\n         )\n-        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output[0])\n-\n-        return (hidden_states,) + attention_output[1:]  # add attentions if we output them\n+        hidden_states = self.full_layer_layer_norm(ffn_output + attention_output)\n+        return hidden_states\n \n     def ff_chunk(self, attention_output: torch.Tensor) -> torch.Tensor:\n         ffn_output = self.ffn(attention_output)\n@@ -337,28 +323,11 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n-        layer_hidden_states = ()\n-        layer_attentions = ()\n-\n         for layer_index, albert_layer in enumerate(self.albert_layers):\n-            layer_output = albert_layer(hidden_states, attention_mask, head_mask[layer_index], output_attentions)\n-            hidden_states = layer_output[0]\n-\n-            if output_attentions:\n-                layer_attentions = layer_attentions + (layer_output[1],)\n-\n-            if output_hidden_states:\n-                layer_hidden_states = layer_hidden_states + (hidden_states,)\n-\n-        outputs = (hidden_states,)\n-        if output_hidden_states:\n-            outputs = outputs + (layer_hidden_states,)\n-        if output_attentions:\n-            outputs = outputs + (layer_attentions,)\n-        return outputs  # last-layer hidden state, (layer hidden states), (layer attentions)\n+            hidden_states = albert_layer(hidden_states, attention_mask, head_mask[layer_index], **kwargs)\n+        return hidden_states\n \n \n class AlbertTransformer(nn.Module):\n@@ -374,15 +343,10 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n-        return_dict: bool = True,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[BaseModelOutput, tuple]:\n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n \n-        all_hidden_states = (hidden_states,) if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         head_mask = [None] * self.config.num_hidden_layers if head_mask is None else head_mask\n \n         for i in range(self.config.num_hidden_layers):\n@@ -392,33 +356,28 @@ def forward(\n             # Index of the hidden group\n             group_idx = int(i / (self.config.num_hidden_layers / self.config.num_hidden_groups))\n \n-            layer_group_output = self.albert_layer_groups[group_idx](\n+            hidden_states = self.albert_layer_groups[group_idx](\n                 hidden_states,\n                 attention_mask,\n                 head_mask[group_idx * layers_per_group : (group_idx + 1) * layers_per_group],\n-                output_attentions,\n-                output_hidden_states,\n+                **kwargs,\n             )\n-            hidden_states = layer_group_output[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + layer_group_output[-1]\n-\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n @auto_docstring\n class AlbertPreTrainedModel(PreTrainedModel):\n-    config: AlbertConfig\n+    config_class = AlbertConfig\n     base_model_prefix = \"albert\"\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": AlbertLayer,\n+        \"attentions\": AlbertAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n@@ -464,7 +423,7 @@ class AlbertForPreTrainingOutput(ModelOutput):\n \n @auto_docstring\n class AlbertModel(AlbertPreTrainedModel):\n-    config: AlbertConfig\n+    config_class = AlbertConfig\n     base_model_prefix = \"albert\"\n \n     def __init__(self, config: AlbertConfig, add_pooling_layer: bool = True):\n@@ -513,6 +472,7 @@ def _prune_heads(self, heads_to_prune: dict[int, list[int]]) -> None:\n             inner_group_idx = int(layer - group_idx * self.config.inner_group_num)\n             self.encoder.albert_layer_groups[group_idx].albert_layers[inner_group_idx].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -522,84 +482,59 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[BaseModelOutputWithPooling, tuple]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n         embedding_output = self.embeddings(\n             input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n         )\n \n-        use_sdpa_attention_mask = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n-        )\n-\n-        if use_sdpa_attention_mask:\n-            extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                attention_mask, embedding_output.dtype, tgt_len=seq_length\n-            )\n-        else:\n-            extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n-            extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-            extended_attention_mask = (1.0 - extended_attention_mask) * torch.finfo(self.dtype).min\n+        attention_mask = self._update_full_mask(attention_mask, embedding_output)\n \n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            extended_attention_mask,\n+            attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n \n         sequence_output = encoder_outputs[0]\n \n         pooled_output = self.pooler_activation(self.pooler(sequence_output[:, 0])) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -629,6 +564,7 @@ def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n     def get_input_embeddings(self) -> nn.Embedding:\n         return self.albert.embeddings.word_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -640,9 +576,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         sentence_order_label: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[AlbertForPreTrainingOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -670,18 +604,15 @@ def forward(\n         >>> prediction_logits = outputs.prediction_logits\n         >>> sop_logits = outputs.sop_logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.albert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output, pooled_output = outputs[:2]\n@@ -696,10 +627,6 @@ def forward(\n             sentence_order_loss = loss_fct(sop_scores.view(-1, 2), sentence_order_label.view(-1))\n             total_loss = masked_lm_loss + sentence_order_loss\n \n-        if not return_dict:\n-            output = (prediction_scores, sop_scores) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return AlbertForPreTrainingOutput(\n             loss=total_loss,\n             prediction_logits=prediction_scores,\n@@ -775,6 +702,7 @@ def set_output_embeddings(self, new_embeddings: nn.Linear) -> None:\n     def get_input_embeddings(self) -> nn.Embedding:\n         return self.albert.embeddings.word_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -785,9 +713,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[MaskedLMOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -824,18 +750,15 @@ def forward(\n         0.81\n         ```\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_outputs = outputs[0]\n \n@@ -846,10 +769,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -877,6 +796,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -887,28 +807,23 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[SequenceClassifierOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -939,10 +854,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -969,6 +880,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -979,26 +891,21 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[TokenClassifierOutput, tuple]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.albert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1011,10 +918,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1035,6 +938,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1046,22 +950,17 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[AlbertForPreTrainingOutput, tuple]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.albert(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1088,10 +987,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1113,6 +1008,7 @@ def __init__(self, config: AlbertConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1123,9 +1019,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[AlbertForPreTrainingOutput, tuple]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1157,7 +1051,6 @@ def forward(\n             num_choices-1]` where *num_choices* is the size of the second dimension of the input tensors. (see\n             *input_ids* above)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1176,9 +1069,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1192,10 +1084,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,"
        },
        {
            "sha": "ec8031507d50bdbcc447cb5933ea81b7020ca2f8",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 36,
            "deletions": 33,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -90,15 +90,11 @@ def to_tuple(self) -> tuple[Any]:\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->AltRoberta\n class AltRobertaEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -112,37 +108,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -158,7 +161,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -171,10 +175,26 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n class AltRobertaSelfAttention(nn.Module):\n     def __init__(self, config, position_embedding_type=None):\n@@ -1366,21 +1386,4 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n __all__ = [\"AltCLIPPreTrainedModel\", \"AltCLIPVisionModel\", \"AltCLIPTextModel\", \"AltCLIPModel\"]"
        },
        {
            "sha": "fc1f57aec0e7872c3f2c64823447cad132afa7d0",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -873,7 +873,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "97e736520fe6138e0b052c7ff1d17004d0889eb3",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -530,7 +530,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -567,7 +567,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -687,7 +687,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "384e34351ea78ef6ac43a49ec1cb31700b273a43",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 431,
            "deletions": 480,
            "changes": 911,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -15,19 +15,19 @@\n # limitations under the License.\n \"\"\"PyTorch BERT model.\"\"\"\n \n-import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -40,13 +40,18 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_bert import BertConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -83,7 +88,7 @@ def forward(\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n@@ -93,9 +98,10 @@ def forward(\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -112,18 +118,78 @@ def forward(\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class BertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -138,215 +204,156 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only bert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n                 )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+class BertCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+        self.config = config\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        return context_layer, attention_probs\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-class BertSdpaSelfAttention(BertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-    # Adapted from BertSelfAttention\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n-            logger.warning_once(\n-                \"BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n-                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                past_key_values,\n-                output_attentions,\n-                cache_position,\n-            )\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        query_layer = (\n-            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n         else:\n-            key_layer = (\n-                self.key(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n-            value_layer = (\n-                self.value(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n-        # a causal mask in case tgt_len == 1.\n-        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n-\n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n-\n-        return attn_output, None\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class BertSelfOutput(nn.Module):\n@@ -363,19 +370,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-BERT_SELF_ATTENTION_CLASSES = {\n-    \"eager\": BertSelfAttention,\n-    \"sdpa\": BertSdpaSelfAttention,\n-}\n-\n-\n class BertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = BERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = BertCrossAttention if is_cross_attention else BertSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = BertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -398,29 +401,29 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n class BertIntermediate(nn.Module):\n@@ -457,38 +460,42 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BertAttention(config, layer_idx=layer_idx)\n+        self.attention = BertAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = BertAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = BertIntermediate(config)\n         self.output = BertOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -497,24 +504,21 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -523,11 +527,10 @@ def feed_forward_chunk(self, attention_output):\n \n \n class BertEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([BertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -538,77 +541,26 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -701,10 +653,18 @@ def forward(self, sequence_output, pooled_output):\n \n @auto_docstring\n class BertPreTrainedModel(PreTrainedModel):\n-    config: BertConfig\n+    config_class = BertConfig\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": BertLayer,\n+        \"attentions\": BertSelfAttention,\n+        \"cross_attentions\": BertCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -770,13 +730,13 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = BertEmbeddings(config)\n         self.encoder = BertEncoder(config)\n \n         self.pooler = BertPooler(config) if add_pooling_layer else None\n \n-        self.attn_implementation = config._attn_implementation\n         self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n@@ -796,6 +756,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -807,52 +768,40 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n+        else:\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n+\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -862,55 +811,16 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n-\n-        use_sdpa_attention_masks = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n-        # Expand the attention mask\n-        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n-            # Expand the attention mask for SDPA.\n-            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-            if self.config.is_decoder:\n-                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                    attention_mask,\n-                    input_shape,\n-                    embedding_output,\n-                    past_key_values_length,\n-                )\n-            else:\n-                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-\n-            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n-                # Expand the attention mask for SDPA.\n-                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-            else:\n-                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n         # attention_probs has shape bsz x n_heads x N x N\n@@ -920,32 +830,137 @@ def forward(\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -972,6 +987,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -983,9 +999,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BertForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1015,18 +1029,15 @@ def forward(\n         >>> seq_relationship_logits = outputs.seq_relationship_logits\n         ```\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output, pooled_output = outputs[:2]\n@@ -1039,10 +1050,6 @@ def forward(\n             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n             total_loss = masked_lm_loss + next_sentence_loss\n \n-        if not return_dict:\n-            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return BertForPreTrainingOutput(\n             loss=total_loss,\n             prediction_logits=prediction_scores,\n@@ -1079,6 +1086,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1091,21 +1099,17 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-        **loss_kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -1120,22 +1124,17 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n         prediction_scores = self.cls(sequence_output)\n \n         lm_loss = None\n         if labels is not None:\n-            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **loss_kwargs)\n-\n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n+            lm_loss = self.loss_function(prediction_scores, labels, self.config.vocab_size, **kwargs)\n \n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n@@ -1173,6 +1172,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1185,19 +1185,14 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1207,9 +1202,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1220,10 +1214,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()  # -100 index = padding token\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -1271,6 +1261,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1281,10 +1272,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1321,18 +1309,15 @@ def forward(\n             )\n             labels = kwargs.pop(\"next_sentence_label\")\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1344,10 +1329,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n \n-        if not return_dict:\n-            output = (seq_relationship_scores,) + outputs[2:]\n-            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n-\n         return NextSentencePredictorOutput(\n             loss=next_sentence_loss,\n             logits=seq_relationship_scores,\n@@ -1378,6 +1359,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1388,28 +1370,23 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1439,9 +1416,6 @@ def forward(\n             elif self.config.problem_type == \"multi_label_classification\":\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n \n         return SequenceClassifierOutput(\n             loss=loss,\n@@ -1466,6 +1440,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1476,9 +1451,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1510,7 +1483,6 @@ def forward(\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1530,9 +1502,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1546,10 +1517,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1574,6 +1541,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1584,26 +1552,21 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1616,10 +1579,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1640,6 +1599,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1651,22 +1611,17 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.bert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1693,10 +1648,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "8966adc1eb26f1f7b0d7636b23418f2beab3908b",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 432,
            "deletions": 268,
            "changes": 700,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -14,24 +14,35 @@\n # limitations under the License.\n \"\"\"PyTorch BERT model specific for generation.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import (\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_bert_generation import BertGenerationConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -50,19 +61,80 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->BertGeneration\n class BertGenerationSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -77,126 +149,169 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only bert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+# Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->BertGeneration\n+class BertGenerationCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in BertGenerationModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        return context_layer, attention_probs\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-BERT_GENERATION_SELF_ATTENTION_CLASSES = {\n-    \"eager\": BertGenerationSelfAttention,\n-}\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BertGeneration,BERT->BERT_GENERATION\n class BertGenerationAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = BERT_GENERATION_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = BertGenerationCrossAttention if is_cross_attention else BertGenerationSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = BertGenerationSelfOutput(config)\n         self.pruned_heads = set()\n@@ -219,29 +334,29 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->BertGeneration\n@@ -281,40 +396,42 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BertGenerationAttention(config, layer_idx=layer_idx)\n+        self.attention = BertGenerationAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = BertGenerationAttention(\n-                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n             )\n         self.intermediate = BertGenerationIntermediate(config)\n         self.output = BertGenerationOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -323,37 +440,35 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output)\n         return layer_output\n \n \n+# Copied from transformers.models.bert.modeling_bert.BertEncoder\n class BertEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n+        # Ignore copy\n         self.layer = nn.ModuleList([BertGenerationLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -364,76 +479,26 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-        if use_cache and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -475,9 +540,18 @@ def forward(self, input_ids=None, position_ids=None, inputs_embeds=None, past_ke\n \n @auto_docstring\n class BertGenerationPreTrainedModel(PreTrainedModel):\n-    config: BertGenerationConfig\n+    config_class = BertGenerationConfig\n     base_model_prefix = \"bert\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": BertGenerationLayer,\n+        \"attentions\": BertGenerationSelfAttention,\n+        \"cross_attentions\": BertGenerationCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -521,6 +595,7 @@ class BertGenerationEncoder(BertGenerationPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = BertGenerationEmbeddings(config)\n         self.encoder = BertEncoder(config)\n@@ -542,6 +617,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -552,69 +628,40 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,  # NOOP kwargs, for now\n-    ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n         else:\n-            encoder_extended_attention_mask = None\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -623,31 +670,155 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n \n-        if not return_dict:\n-            return (sequence_output,) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n class BertGenerationOnlyLMHead(nn.Module):\n     def __init__(self, config):\n@@ -696,6 +867,7 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n         self.lm_head.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -707,12 +879,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -738,7 +908,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -752,9 +921,8 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n             **kwargs,\n         )\n \n@@ -770,10 +938,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,"
        },
        {
            "sha": "04cc28e56bf92e5acd45e2065ddef51664880a4e",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1617,7 +1617,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -1738,7 +1738,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "348bf2707584bf57484876912de9044d21a03319",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -375,7 +375,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None"
        },
        {
            "sha": "accc1bdc75592ec544dbca786982cb3ee6bb5b96",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -197,7 +197,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None"
        },
        {
            "sha": "5cd138fe3180004b3b87fca1fac6b7b919ef4331",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -495,7 +495,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -533,7 +533,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -654,7 +654,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "1c1cf379d03203eb5ea13932298201646035f08a",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -488,7 +488,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -526,7 +526,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -647,7 +647,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "ff88a0a087d1eab37dbeae7d6e3225cc1a3c3b91",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 483,
            "deletions": 291,
            "changes": 774,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -14,17 +14,18 @@\n # limitations under the License.\n \"\"\"PyTorch BridgeTower Model\"\"\"\n \n-import math\n from collections import OrderedDict\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, QuickGELUActivation\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -33,13 +34,18 @@\n     ModelOutput,\n     SequenceClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging, torch_int\n+from ...utils.generic import can_return_tuple\n from .configuration_bridgetower import BridgeTowerConfig, BridgeTowerTextConfig, BridgeTowerVisionConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _TOKENIZER_FOR_DOC = \"RobertaTokenizer\"\n@@ -400,19 +406,80 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->BridgeTower\n class BridgeTowerSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -427,126 +494,169 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only roberta can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+# Copied from transformers.models.roberta.modeling_roberta.RobertaCrossAttention with Roberta->BridgeTower\n+class BridgeTowerCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in BridgeTowerModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        return context_layer, attention_probs\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-BRIDGE_TOWER_SELF_ATTENTION_CLASSES = {\n-    \"eager\": BridgeTowerSelfAttention,\n-}\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->BridgeTower,BERT->BRIDGE_TOWER\n class BridgeTowerAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = BRIDGE_TOWER_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = BridgeTowerCrossAttention if is_cross_attention else BridgeTowerSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = BridgeTowerSelfOutput(config)\n         self.pruned_heads = set()\n@@ -569,88 +679,87 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n class BridgeTowerBertCrossLayer(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BridgeTowerAttention(config, layer_idx=layer_idx)\n+        self.attention = BridgeTowerAttention(config, is_causal=True, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n-        self.crossattention = BridgeTowerAttention(config, layer_idx=layer_idx)\n+        self.crossattention = BridgeTowerAttention(\n+            config,\n+            position_embedding_type=\"absolute\",\n+            is_causal=False,\n+            layer_idx=layer_idx,\n+            is_cross_attention=True,\n+        )\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n         encoder_hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_attention_mask=None,\n-        past_key_values=None,\n-        output_attentions=False,\n-        cache_position=None,\n+        past_key_value=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attention_outputs = self.attention(\n+        self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             head_mask=None,\n-            output_attentions=output_attentions,\n-            past_key_values=None,\n+            past_key_value=None,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        # add self attentions if we output attention weights\n-        outputs = self_attention_outputs[1:]\n+        attention_output = self_attention_output\n \n-        cross_attention_outputs = self.crossattention(\n+        cross_attention_output, cross_attn_weights = self.crossattention(\n             attention_output,\n-            attention_mask=encoder_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            cache_position=cache_position,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_value=past_key_value,\n+            **kwargs,\n         )\n-        attention_output = cross_attention_outputs[0]\n-        # add cross attentions if we output attention weights\n-        outputs = outputs + cross_attention_outputs[1:]\n+        attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return (\n+            layer_output,\n+            self_attn_weights,\n+            cross_attn_weights,\n+        )\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -663,44 +772,44 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = BridgeTowerAttention(config, layer_idx=layer_idx)\n+        self.attention = BridgeTowerAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = BridgeTowerAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = BridgeTowerAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = BridgeTowerIntermediate(config)\n         self.output = BridgeTowerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    # copied from transformers.models.bert.modeling_bert.BertLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attention_outputs = self.attention(\n+        outputs = ()\n+        self_attention_output, self_attn_weights = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        if self.is_decoder:\n-            outputs = self_attention_outputs[1:-1]\n-        else:\n-            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -709,38 +818,40 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, cross_attn_weights = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n+            outputs = (cross_attn_weights,)\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        return (layer_output,) + outputs\n+        return outputs + (\n+            layer_output,\n+            self_attn_weights,\n+        )\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output)\n         return layer_output\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->BridgeTowerText\n+# copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->BridgeTowerText\n class BridgeTowerTextEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList(\n             [BridgeTowerTextLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n         )\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -753,35 +864,14 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             layer_outputs = layer_module(\n@@ -790,9 +880,9 @@ def forward(\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]\n@@ -804,21 +894,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -827,15 +905,11 @@ def forward(\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->BridgeTowerText\n class BridgeTowerTextEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -849,37 +923,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -895,7 +976,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -908,26 +990,25 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n+        Args:\n+            x: torch.Tensor x:\n \n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n \n \n @auto_docstring\n@@ -1004,6 +1085,7 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = BridgeTowerTextEmbeddings(config)\n         self.encoder = BridgeTowerTextEncoder(config)\n@@ -1027,7 +1109,11 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @can_return_tuple\n     @auto_docstring\n+    # NOTE: bridgetower with its multimodality has a more complicated scheme making records harder\n+    # for now we skip the copies from bert but stay close to the original\n+    # copied from transformers.models.bert.modeling_bert.BertModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -1038,77 +1124,54 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n \n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n         else:\n-            encoder_extended_attention_mask = None\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -1117,24 +1180,43 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             past_key_values_length=past_key_values_length,\n         )\n+\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n@@ -1145,6 +1227,116 @@ def forward(\n             cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1181,10 +1373,10 @@ def __init__(self, config):\n                 ln.bias.data = self.vision_model.visual.ln_post.bias.data\n \n         self.cross_modal_image_layers = nn.ModuleList(\n-            [BridgeTowerBertCrossLayer(text_config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+            [BridgeTowerBertCrossLayer(text_config) for _ in range(config.num_hidden_layers)]\n         )\n         self.cross_modal_text_layers = nn.ModuleList(\n-            [BridgeTowerBertCrossLayer(text_config, layer_idx=i) for i in range(config.num_hidden_layers)]\n+            [BridgeTowerBertCrossLayer(text_config) for _ in range(config.num_hidden_layers)]\n         )\n \n         # Class token => Linear => Tanh"
        },
        {
            "sha": "dd882e60096f7194c5509a11bff65d85540b6048",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 627,
            "deletions": 677,
            "changes": 1304,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/camembert/modular_camembert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_camembert.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n@@ -13,19 +19,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch CamemBERT model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -37,117 +42,93 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_camembert import CamembertConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Camembert\n-class CamembertEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n-    def __init__(self, config):\n-        super().__init__()\n-        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n-        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-        self.register_buffer(\n-            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n-        )\n \n-        # End copy\n-        self.padding_idx = config.pad_token_id\n-        self.position_embeddings = nn.Embedding(\n-            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n-        )\n+logger = logging.get_logger(__name__)\n \n-    def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n-        if position_ids is None:\n-            if input_ids is not None:\n-                # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n-            else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n \n-        if input_ids is not None:\n-            input_shape = input_ids.size()\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n         else:\n-            input_shape = inputs_embeds.size()[:-1]\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n \n-        seq_length = input_shape[1]\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n \n-        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n-        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n-        # issue #5664\n-        if token_type_ids is None:\n-            if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.word_embeddings(input_ids)\n-        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n \n-        embeddings = inputs_embeds + token_type_embeddings\n-        if self.position_embedding_type == \"absolute\":\n-            position_embeddings = self.position_embeddings(position_ids)\n-            embeddings += position_embeddings\n-        embeddings = self.LayerNorm(embeddings)\n-        embeddings = self.dropout(embeddings)\n-        return embeddings\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n-        \"\"\"\n-        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n \n-        Args:\n-            inputs_embeds: torch.Tensor\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n \n-        Returns: torch.Tensor\n-        \"\"\"\n-        input_shape = inputs_embeds.size()[:-1]\n-        sequence_length = input_shape[1]\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n \n-        position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n-        )\n-        return position_ids.unsqueeze(0).expand(input_shape)\n+    return attn_output, attn_weights\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Camembert\n class CamembertSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -162,219 +143,158 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only camembert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n \n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n                 )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in CamembertModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+class CamembertCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+        self.config = config\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        return context_layer, attention_probs\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaSdpaSelfAttention with Roberta->Camembert\n-class CamembertSdpaSelfAttention(CamembertSelfAttention):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-    # Adapted from CamembertSelfAttention\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n-            logger.warning_once(\n-                \"CamembertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n-                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                past_key_values,\n-                output_attentions,\n-                cache_position,\n-            )\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        bsz, tgt_len, _ = hidden_states.size()\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        query_layer = (\n-            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n         else:\n-            key_layer = (\n-                self.key(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n-            value_layer = (\n-                self.value(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n-        # a causal mask in case tgt_len == 1.\n-        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n-\n-        return attn_output, None\n \n-\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaSelfOutput with Roberta->Camembert\n class CamembertSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -389,20 +309,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-CAMEMBERT_SELF_ATTENTION_CLASSES = {\n-    \"eager\": CamembertSelfAttention,\n-    \"sdpa\": CamembertSdpaSelfAttention,\n-}\n-\n-\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaAttention with Roberta->Camembert,ROBERTA->CAMEMBERT\n class CamembertAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = CAMEMBERT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = CamembertCrossAttention if is_cross_attention else CamembertSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = CamembertSelfOutput(config)\n         self.pruned_heads = set()\n@@ -425,32 +340,31 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Roberta->Camembert\n class CamembertIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -466,7 +380,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Roberta->Camembert\n class CamembertOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -481,84 +394,252 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaLayer with Roberta->Camembert\n class CamembertLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = CamembertAttention(config, layer_idx=layer_idx)\n+        self.attention = CamembertAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = CamembertAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = CamembertAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = CamembertIntermediate(config)\n         self.output = CamembertOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n+        )\n+        attention_output = self_attention_output\n+\n+        if self.is_decoder and encoder_hidden_states is not None:\n+            if not hasattr(self, \"crossattention\"):\n+                raise ValueError(\n+                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n+                    \" by setting `config.add_cross_attention=True`\"\n+                )\n+\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n+            )\n+            attention_output = cross_attention_output\n+\n+        layer_output = apply_chunking_to_forward(\n+            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        return layer_output\n+\n+    def feed_forward_chunk(self, attention_output):\n+        intermediate_output = self.intermediate(attention_output)\n+        layer_output = self.output(intermediate_output, attention_output)\n+        return layer_output\n+\n+\n+class CamembertLMHead(nn.Module):\n+    \"\"\"Camembert Head for masked language modeling.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n+        self.decoder.bias = self.bias\n+\n+    def forward(self, features, **kwargs):\n+        x = self.dense(features)\n+        x = gelu(x)\n+        x = self.layer_norm(x)\n+\n+        # project back to size of vocabulary with bias\n+        x = self.decoder(x)\n+\n+        return x\n+\n+    def _tie_weights(self):\n+        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n+        # For accelerate compatibility and to not break backward compatibility\n+        if self.decoder.bias.device.type == \"meta\":\n+            self.decoder.bias = self.bias\n+        else:\n+            self.bias = self.decoder.bias\n+\n+\n+@auto_docstring\n+class CamembertPreTrainedModel(PreTrainedModel):\n+    config_class = CamembertConfig\n+    base_model_prefix = \"roberta\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": CamembertLayer,\n+        \"attentions\": CamembertSelfAttention,\n+        \"cross_attentions\": CamembertCrossAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, CamembertLMHead):\n+            module.bias.data.zero_()\n+\n+\n+class CamembertEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n+\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+        self.register_buffer(\n+            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n+        )\n+\n+        self.padding_idx = config.pad_token_id\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n+        if position_ids is None:\n+            if input_ids is not None:\n+                # Create the position ids from the input token ids. Any padded tokens remain padded.\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n+            else:\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n+\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        batch_size, seq_length = input_shape\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+\n+        embeddings = inputs_embeds + token_type_embeddings\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n+        \"\"\"\n+        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n-        if self.is_decoder and encoder_hidden_states is not None:\n-            if not hasattr(self, \"crossattention\"):\n-                raise ValueError(\n-                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n-                    \" by setting `config.add_cross_attention=True`\"\n-                )\n+        Args:\n+            inputs_embeds: torch.Tensor\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+        Returns: torch.Tensor\n+        \"\"\"\n+        input_shape = inputs_embeds.size()[:-1]\n+        sequence_length = input_shape[1]\n \n-        layer_output = apply_chunking_to_forward(\n-            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n+        position_ids = torch.arange(\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n-        outputs = (layer_output,) + outputs\n+        return position_ids.unsqueeze(0).expand(input_shape)\n \n-        return outputs\n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n \n-    def feed_forward_chunk(self, attention_output):\n-        intermediate_output = self.intermediate(attention_output)\n-        layer_output = self.output(intermediate_output, attention_output)\n-        return layer_output\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaEncoder with Roberta->Camembert\n class CamembertEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([CamembertLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -569,81 +650,29 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertPooler\n class CamembertPooler(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -659,120 +688,35 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-@auto_docstring\n-class CamembertPreTrainedModel(PreTrainedModel):\n-    config: CamembertConfig\n-    base_model_prefix = \"roberta\"\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->CamembertLMHead\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, CamembertLMHead):\n-            module.bias.data.zero_()\n-\n-\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->Camembert\n-class CamembertClassificationHead(nn.Module):\n-    \"\"\"Head for sentence-level classification tasks.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n-\n-    def forward(self, features, **kwargs):\n-        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n-        x = self.dropout(x)\n-        x = self.dense(x)\n-        x = torch.tanh(x)\n-        x = self.dropout(x)\n-        x = self.out_proj(x)\n-        return x\n-\n-\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead with Roberta->Camembert\n-class CamembertLMHead(nn.Module):\n-    \"\"\"Camembert Head for masked language modeling.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n-        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n-\n-    def forward(self, features, **kwargs):\n-        x = self.dense(features)\n-        x = gelu(x)\n-        x = self.layer_norm(x)\n-\n-        # project back to size of vocabulary with bias\n-        x = self.decoder(x)\n-\n-        return x\n-\n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n-\n-@auto_docstring\n-class CamembertModel(CamembertPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n-    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n-    Kaiser and Illia Polosukhin.\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://huggingface.co/papers/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n \n-    To behave as a decoder the model needs to be initialized with the `is_decoder` argument of the configuration set to\n-    `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n+    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n+    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n-\n-    .. _*Attention is all you need*: https://huggingface.co/papers/1706.03762\n-\n     \"\"\"\n+)\n+class CamembertModel(CamembertPreTrainedModel):\n+    _no_split_modules = [\"CamembertEmbeddings\", \"CamembertLayer\"]\n \n-    _no_split_modules = []\n-\n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.__init__ with Roberta->Camembert\n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\"\n         add_pooling_layer (bool, *optional*, defaults to `True`):\n             Whether to add a pooling layer\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = CamembertEmbeddings(config)\n         self.encoder = CamembertEncoder(config)\n \n         self.pooler = CamembertPooler(config) if add_pooling_layer else None\n \n-        self.attn_implementation = config._attn_implementation\n         self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n@@ -792,8 +736,8 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n-    # Copied from transformers.models.roberta.modeling_roberta.RobertaModel.forward\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -804,52 +748,40 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n+        else:\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n+\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -859,55 +791,16 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n-\n-        use_sdpa_attention_masks = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n-        # Expand the attention mask\n-        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n-            # Expand the attention mask for SDPA.\n-            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-            if self.config.is_decoder:\n-                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                    attention_mask,\n-                    input_shape,\n-                    embedding_output,\n-                    past_key_values_length,\n-                )\n-            else:\n-                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-\n-            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n-                # Expand the attention mask for SDPA.\n-                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-            else:\n-                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n         # attention_probs has shape bsz x n_heads x N x N\n@@ -917,35 +810,137 @@ def forward(\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForMaskedLM(CamembertPreTrainedModel):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n@@ -957,9 +952,9 @@ def __init__(self, config):\n                 \"If you want to use `CamembertForMaskedLM` make sure `config.is_decoder=False` for \"\n                 \"bi-directional self-attention.\"\n             )\n+        self.lm_head = CamembertLMHead(config)\n \n         self.roberta = CamembertModel(config, add_pooling_layer=False)\n-        self.lm_head = CamembertLMHead(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -970,6 +965,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -982,9 +978,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1001,8 +995,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1012,9 +1004,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n@@ -1026,10 +1017,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -1038,25 +1025,47 @@ def forward(\n         )\n \n \n+class CamembertClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        classifier_dropout = (\n+            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n+        )\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n+\n+    def forward(self, features, **kwargs):\n+        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n+        x = self.dropout(x)\n+        x = self.dense(x)\n+        x = torch.tanh(x)\n+        x = self.dropout(x)\n+        x = self.out_proj(x)\n+        return x\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n-    CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n+    Camembert Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n     pooled output) e.g. for GLUE tasks.\n     \"\"\"\n )\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForSequenceClassification(CamembertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n         self.config = config\n+        self.classifier = CamembertClassificationHead(config)\n \n         self.roberta = CamembertModel(config, add_pooling_layer=False)\n-        self.classifier = CamembertClassificationHead(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1067,9 +1076,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1086,18 +1093,15 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         logits = self.classifier(sequence_output)\n@@ -1127,10 +1131,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1140,18 +1140,18 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForMultipleChoice with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForMultipleChoice(CamembertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n-\n-        self.roberta = CamembertModel(config)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         self.classifier = nn.Linear(config.hidden_size, 1)\n \n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1162,9 +1162,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1197,7 +1195,6 @@ def forward(\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1217,9 +1214,8 @@ def forward(\n             attention_mask=flat_attention_mask,\n             head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         pooled_output = outputs[1]\n \n@@ -1234,10 +1230,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1247,22 +1239,22 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForTokenClassification(CamembertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n-\n-        self.roberta = CamembertModel(config, add_pooling_layer=False)\n         classifier_dropout = (\n             config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n         )\n         self.dropout = nn.Dropout(classifier_dropout)\n         self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n \n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1273,9 +1265,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1290,18 +1280,15 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1316,10 +1303,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1329,18 +1312,18 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering with Roberta->Camembert, ROBERTA->CAMEMBERT\n class CamembertForQuestionAnswering(CamembertPreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n+        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n \n         self.roberta = CamembertModel(config, add_pooling_layer=False)\n-        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1352,9 +1335,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1367,18 +1348,15 @@ def forward(\n \n             [What are token type IDs?](../glossary#token-type-ids)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1405,10 +1383,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1420,10 +1394,9 @@ def forward(\n \n @auto_docstring(\n     custom_intro=\"\"\"\n-    CamemBERT Model with a `language modeling` head on top for CLM fine-tuning.\n+    Camembert Model with a `language modeling` head on top for CLM fine-tuning.\n     \"\"\"\n )\n-# Copied from transformers.models.roberta.modeling_roberta.RobertaForCausalLM with Roberta->Camembert, ROBERTA->CAMEMBERT, FacebookAI/roberta-base->almanach/camembert-base\n class CamembertForCausalLM(CamembertPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n \n@@ -1432,9 +1405,9 @@ def __init__(self, config):\n \n         if not config.is_decoder:\n             logger.warning(\"If you want to use `CamembertLMHeadModel` as a standalone, add `is_decoder=True.`\")\n+        self.lm_head = CamembertLMHead(config)\n \n         self.roberta = CamembertModel(config, add_pooling_layer=False)\n-        self.lm_head = CamembertLMHead(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1445,6 +1418,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1457,12 +1431,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1495,7 +1467,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -1510,9 +1481,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1529,10 +1500,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,\n@@ -1543,23 +1510,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n __all__ = [\n     \"CamembertForCausalLM\",\n     \"CamembertForMaskedLM\","
        },
        {
            "sha": "49b640a6468037b9ec168a4c728085bed188ab58",
            "filename": "src/transformers/models/camembert/modular_camembert.py",
            "status": "added",
            "additions": 544,
            "deletions": 0,
            "changes": 544,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodular_camembert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -0,0 +1,544 @@\n+# coding=utf-8\n+# Copyright 2019 Inria, Facebook AI Research and the HuggingFace Inc. team.\n+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch CamemBERT model.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...modeling_outputs import (\n+    CausalLMOutputWithCrossAttentions,\n+    MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import can_return_tuple\n+from ..roberta.modeling_roberta import (\n+    RobertaForCausalLM,\n+    RobertaForMaskedLM,\n+    RobertaForMultipleChoice,\n+    RobertaForQuestionAnswering,\n+    RobertaForSequenceClassification,\n+    RobertaForTokenClassification,\n+    RobertaModel,\n+    RobertaPreTrainedModel,\n+)\n+\n+\n+class CamembertPreTrainedModel(RobertaPreTrainedModel):\n+    base_model_prefix = \"roberta\"\n+\n+\n+class CamembertModel(RobertaModel):\n+    pass\n+\n+\n+class CamembertForMaskedLM(RobertaForMaskedLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        masked_lm_loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(prediction_scores.device)\n+            loss_fct = CrossEntropyLoss()\n+            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+\n+        return MaskedLMOutput(\n+            loss=masked_lm_loss,\n+            logits=prediction_scores,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CamembertForSequenceClassification(RobertaForSequenceClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CamembertForMultipleChoice(RobertaForMultipleChoice):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n+            `input_ids` above)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        \"\"\"\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n+        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        flat_inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        outputs = self.roberta(\n+            flat_input_ids,\n+            position_ids=flat_position_ids,\n+            token_type_ids=flat_token_type_ids,\n+            attention_mask=flat_attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=flat_inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        pooled_output = outputs[1]\n+\n+        pooled_output = self.dropout(pooled_output)\n+        logits = self.classifier(pooled_output)\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(reshaped_logits.device)\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CamembertForTokenClassification(RobertaForTokenClassification):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CamembertForQuestionAnswering(RobertaForQuestionAnswering):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class CamembertForCausalLM(RobertaForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.camembert\n+\n+        self.roberta = CamembertModel(config, add_pooling_layer=False)\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n+            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n+            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, CamembertForCausalLM, AutoConfig\n+        >>> import torch\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"almanach/camembert-base\")\n+        >>> config = AutoConfig.from_pretrained(\"almanach/camembert-base\")\n+        >>> config.is_decoder = True\n+        >>> model = CamembertForCausalLM.from_pretrained(\"almanach/camembert-base\", config=config)\n+\n+        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> prediction_logits = outputs.logits\n+        ```\"\"\"\n+        if labels is not None:\n+            use_cache = False\n+\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        lm_loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(prediction_scores.device)\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return CausalLMOutputWithCrossAttentions(\n+            loss=lm_loss,\n+            logits=prediction_scores,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            cross_attentions=outputs.cross_attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"CamembertForCausalLM\",\n+    \"CamembertForMaskedLM\",\n+    \"CamembertForMultipleChoice\",\n+    \"CamembertForQuestionAnswering\",\n+    \"CamembertForSequenceClassification\",\n+    \"CamembertForTokenClassification\",\n+    \"CamembertModel\",\n+    \"CamembertPreTrainedModel\",\n+]"
        },
        {
            "sha": "33ad9463ff243ee91699315ee8dd1285e152543e",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 36,
            "deletions": 33,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -97,23 +97,6 @@ def window_reverse(windows, window_size, height, width):\n     return windows\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n # contrastive loss function, adapted from\n # https://sachinruk.github.io/blog/pytorch/pytorch%20lightning/loss%20function/gpu/2021/03/07/CLIP.html#CLIP-loss-function\n def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n@@ -997,15 +980,11 @@ def forward(self, hidden_states):\n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->ClapText, persistent=False->persistent=True\n class ClapTextEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -1019,37 +998,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=True\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -1065,7 +1051,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -1078,10 +1065,26 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n # Copied from transformers.models.align.modeling_align.eager_attention_forward\n def eager_attention_forward("
        },
        {
            "sha": "60128f882dd3d3d018c3f10888ee1528f17bf074",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -430,7 +430,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "0cba1f8940039e42451d248a9655f7b88f2694c2",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 563,
            "deletions": 474,
            "changes": 1037,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/data2vec/modular_data2vec_text.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_data2vec_text.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n@@ -12,18 +18,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Data2VecText model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -35,30 +41,27 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_data2vec_text import Data2VecTextConfig\n \n \n-logger = logging.get_logger(__name__)\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-_HIDDEN_STATES_START_POSITION = 2\n+logger = logging.get_logger(__name__)\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->Data2VecText\n-class Data2VecTextForTextEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+class Data2VecTextEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -72,37 +75,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -118,7 +128,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -131,24 +142,99 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaSelfAttention with Roberta->Data2VecText\n class Data2VecTextSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -163,114 +249,158 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only data2vec_text can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+class Data2VecTextCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in Data2VecTextModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-        return context_layer, attention_probs\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n class Data2VecTextSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -285,19 +415,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-DATA2VEC_TEXT_SELF_ATTENTION_CLASSES = {\n-    \"eager\": Data2VecTextSelfAttention,\n-}\n-\n-\n-# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Data2VecText,BERT->DATA2VEC_TEXT\n class Data2VecTextAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = DATA2VEC_TEXT_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = Data2VecTextCrossAttention if is_cross_attention else Data2VecTextSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = Data2VecTextSelfOutput(config)\n         self.pruned_heads = set()\n@@ -320,32 +446,31 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n class Data2VecTextIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -361,7 +486,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOutput\n class Data2VecTextOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -376,46 +500,47 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Data2VecText\n class Data2VecTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = Data2VecTextAttention(config, layer_idx=layer_idx)\n+        self.attention = Data2VecTextAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = Data2VecTextAttention(\n-                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n             )\n         self.intermediate = Data2VecTextIntermediate(config)\n         self.output = Data2VecTextOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -424,38 +549,68 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output)\n         return layer_output\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Data2VecText\n+@auto_docstring\n+class Data2VecTextPreTrainedModel(PreTrainedModel):\n+    config_class = Data2VecTextConfig\n+    base_model_prefix = \"data2vec_text\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Data2VecTextForTextEmbeddings\", \"Data2VecTextLayer\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Data2VecTextLayer,\n+        \"attentions\": Data2VecTextSelfAttention,\n+        \"cross_attentions\": Data2VecTextCrossAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n+            if hasattr(module, \"weight\") and module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+\n+\n class Data2VecTextEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([Data2VecTextLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -466,81 +621,29 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertPooler\n class Data2VecTextPooler(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -556,46 +659,9 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-@auto_docstring\n-class Data2VecTextPreTrainedModel(PreTrainedModel):\n-    config: Data2VecTextConfig\n-    base_model_prefix = \"data2vec_text\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Data2VecTextForTextEmbeddings\", \"Data2VecTextLayer\"]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            if hasattr(module, \"bias\") and module.bias is not None:\n-                module.bias.data.zero_()\n-            if hasattr(module, \"weight\") and module.weight is not None:\n-                module.weight.data.fill_(1.0)\n-\n-\n @auto_docstring\n class Data2VecTextModel(Data2VecTextPreTrainedModel):\n-    \"\"\"\n-\n-    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n-    cross-attention is added between the self-attention layers, following the architecture described in *Attention is\n-    all you need*_ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\n-    Kaiser and Illia Polosukhin.\n-\n-    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n-    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n-    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n-\n-    .. _*Attention is all you need*: https://huggingface.co/papers/1706.03762\n-\n-    \"\"\"\n+    _no_split_modules = [\"Data2VecTextEmbeddings\", \"Data2VecTextLayer\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\"\n@@ -604,12 +670,15 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n-        self.embeddings = Data2VecTextForTextEmbeddings(config)\n+        self.embeddings = Data2VecTextEmbeddings(config)\n         self.encoder = Data2VecTextEncoder(config)\n \n         self.pooler = Data2VecTextPooler(config) if add_pooling_layer else None\n \n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -627,6 +696,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -638,77 +708,40 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n         else:\n-            encoder_extended_attention_mask = None\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -717,34 +750,208 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             past_key_values_length=past_key_values_length,\n         )\n+\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+class Data2VecTextLMHead(nn.Module):\n+    \"\"\"Data2VecText Head for masked language modeling.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n+        self.decoder.bias = self.bias\n+\n+    def forward(self, features, **kwargs):\n+        x = self.dense(features)\n+        x = gelu(x)\n+        x = self.layer_norm(x)\n+\n+        # project back to size of vocabulary with bias\n+        x = self.decoder(x)\n+\n+        return x\n+\n+    def _tie_weights(self):\n+        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n+        # For accelerate compatibility and to not break backward compatibility\n+        if self.decoder.bias.device.type == \"meta\":\n+            self.decoder.bias = self.bias\n+        else:\n+            self.bias = self.decoder.bias\n+\n+\n+class Data2VecTextClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        classifier_dropout = (\n+            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n+        )\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n+\n+    def forward(self, features, **kwargs):\n+        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n+        x = self.dropout(x)\n+        x = self.dense(x)\n+        x = torch.tanh(x)\n+        x = self.dropout(x)\n+        x = self.out_proj(x)\n+        return x\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -772,6 +979,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -784,13 +992,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -814,7 +1019,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -829,10 +1033,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -847,10 +1050,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,\n@@ -886,6 +1085,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -898,18 +1098,14 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -919,9 +1115,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n@@ -933,10 +1128,6 @@ def forward(\n             labels = labels.to(prediction_scores.device)\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -945,38 +1136,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaLMHead with Roberta->Data2VecText\n-class Data2VecTextLMHead(nn.Module):\n-    \"\"\"Data2VecText Head for masked language modeling.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-\n-        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n-        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n-        self.decoder.bias = self.bias\n-\n-    def forward(self, features, **kwargs):\n-        x = self.dense(features)\n-        x = gelu(x)\n-        x = self.layer_norm(x)\n-\n-        # project back to size of vocabulary with bias\n-        x = self.decoder(x)\n-\n-        return x\n-\n-    def _tie_weights(self):\n-        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n-        # For accelerate compatibility and to not break backward compatibility\n-        if self.decoder.bias.device.type == \"meta\":\n-            self.decoder.bias = self.bias\n-        else:\n-            self.bias = self.decoder.bias\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     Data2VecText Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n@@ -995,6 +1154,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1005,28 +1165,23 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         logits = self.classifier(sequence_output)\n@@ -1056,10 +1211,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1080,6 +1231,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1090,9 +1242,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1124,7 +1274,6 @@ def forward(\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1144,9 +1293,8 @@ def forward(\n             attention_mask=flat_attention_mask,\n             head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         pooled_output = outputs[1]\n \n@@ -1161,10 +1309,6 @@ def forward(\n             labels = labels.to(reshaped_logits.device)\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1189,6 +1333,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1199,26 +1344,21 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1233,10 +1373,6 @@ def forward(\n             labels = labels.to(logits.device)\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1245,29 +1381,6 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.RobertaClassificationHead with Roberta->Data2VecText\n-class Data2VecTextClassificationHead(nn.Module):\n-    \"\"\"Head for sentence-level classification tasks.\"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n-        classifier_dropout = (\n-            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n-        )\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n-\n-    def forward(self, features, **kwargs):\n-        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n-        x = self.dropout(x)\n-        x = self.dense(x)\n-        x = torch.tanh(x)\n-        x = self.dropout(x)\n-        x = self.out_proj(x)\n-        return x\n-\n-\n @auto_docstring\n class Data2VecTextForQuestionAnswering(Data2VecTextPreTrainedModel):\n     def __init__(self, config):\n@@ -1280,6 +1393,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1291,22 +1405,17 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, QuestionAnsweringModelOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.data2vec_text(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1333,10 +1442,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1346,22 +1451,6 @@ def forward(\n         )\n \n \n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n __all__ = [\n     \"Data2VecTextForCausalLM\",\n     \"Data2VecTextForMaskedLM\","
        },
        {
            "sha": "76a75671ecf8e32630b15cff5236e12f8c4beabd",
            "filename": "src/transformers/models/data2vec/modular_data2vec_text.py",
            "status": "added",
            "additions": 622,
            "deletions": 0,
            "changes": 622,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_text.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -0,0 +1,622 @@\n+# coding=utf-8\n+# Copyright 2022 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Data2VecText model.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import (\n+    CausalLMOutputWithCrossAttentions,\n+    MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import can_return_tuple\n+from ..roberta.modeling_roberta import (\n+    RobertaClassificationHead,\n+    RobertaCrossAttention,\n+    RobertaEmbeddings,\n+    RobertaLayer,\n+    RobertaLMHead,\n+    RobertaModel,\n+    RobertaSelfAttention,\n+)\n+from .configuration_data2vec_text import Data2VecTextConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Data2VecTextEmbeddings(RobertaEmbeddings):\n+    pass\n+\n+\n+class Data2VecTextSelfAttention(RobertaSelfAttention):\n+    pass\n+\n+\n+class Data2VecTextCrossAttention(RobertaCrossAttention):\n+    pass\n+\n+\n+class Data2VecTextLayer(RobertaLayer):\n+    pass\n+\n+\n+@auto_docstring\n+class Data2VecTextPreTrainedModel(PreTrainedModel):\n+    config_class = Data2VecTextConfig\n+    base_model_prefix = \"data2vec_text\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Data2VecTextForTextEmbeddings\", \"Data2VecTextLayer\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Data2VecTextLayer,\n+        \"attentions\": Data2VecTextSelfAttention,\n+        \"cross_attentions\": Data2VecTextCrossAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            if hasattr(module, \"bias\") and module.bias is not None:\n+                module.bias.data.zero_()\n+            if hasattr(module, \"weight\") and module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+\n+\n+@auto_docstring\n+class Data2VecTextModel(RobertaModel):\n+    pass\n+\n+\n+class Data2VecTextLMHead(RobertaLMHead):\n+    pass\n+\n+\n+class Data2VecTextClassificationHead(RobertaClassificationHead):\n+    pass\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Data2VecText Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n+)\n+class Data2VecTextForCausalLM(Data2VecTextPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        if not config.is_decoder:\n+            logger.warning(\"If you want to use `Data2VecTextLMHeadModel` as a standalone, add `is_decoder=True.`\")\n+\n+        self.data2vec_text = Data2VecTextModel(config, add_pooling_layer=False)\n+        self.lm_head = Data2VecTextLMHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.decoder\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.decoder = new_embeddings\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n+            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n+            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Data2VecTextForCausalLM, Data2VecTextConfig\n+        >>> import torch\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"facebook/data2vec-text-base\")\n+        >>> config = Data2VecTextConfig.from_pretrained(\"facebook/data2vec-text-base\")\n+        >>> config.is_decoder = True\n+        >>> model = Data2VecTextForCausalLM.from_pretrained(\"facebook/data2vec-text-base\", config=config)\n+\n+        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> prediction_logits = outputs.logits\n+        ```\"\"\"\n+        if labels is not None:\n+            use_cache = False\n+\n+        outputs = self.data2vec_text(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        lm_loss = None\n+        if labels is not None:\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return CausalLMOutputWithCrossAttentions(\n+            loss=lm_loss,\n+            logits=prediction_scores,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            cross_attentions=outputs.cross_attentions,\n+        )\n+\n+\n+@auto_docstring\n+class Data2VecTextForMaskedLM(Data2VecTextPreTrainedModel):\n+    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        if config.is_decoder:\n+            logger.warning(\n+                \"If you want to use `Data2VecTextForMaskedLM` make sure `config.is_decoder=False` for \"\n+                \"bi-directional self-attention.\"\n+            )\n+\n+        self.data2vec_text = Data2VecTextModel(config, add_pooling_layer=False)\n+        self.lm_head = Data2VecTextLMHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.decoder\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.decoder = new_embeddings\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, MaskedLMOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        outputs = self.data2vec_text(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        masked_lm_loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+\n+            labels = labels.to(prediction_scores.device)\n+            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+\n+        return MaskedLMOutput(\n+            loss=masked_lm_loss,\n+            logits=prediction_scores,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Data2VecText Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n+    pooled output) e.g. for GLUE tasks.\n+    \"\"\"\n+)\n+class Data2VecTextForSequenceClassification(Data2VecTextPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.config = config\n+\n+        self.data2vec_text = Data2VecTextModel(config, add_pooling_layer=False)\n+        self.classifier = Data2VecTextClassificationHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, SequenceClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        outputs = self.data2vec_text(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            labels = labels.to(logits.device)\n+\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class Data2VecTextForMultipleChoice(Data2VecTextPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.data2vec_text = Data2VecTextModel(config)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.classifier = nn.Linear(config.hidden_size, 1)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n+            `input_ids` above)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        \"\"\"\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n+        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        flat_inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        outputs = self.data2vec_text(\n+            flat_input_ids,\n+            position_ids=flat_position_ids,\n+            token_type_ids=flat_token_type_ids,\n+            attention_mask=flat_attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=flat_inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        pooled_output = outputs[1]\n+\n+        pooled_output = self.dropout(pooled_output)\n+        logits = self.classifier(pooled_output)\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+\n+            labels = labels.to(reshaped_logits.device)\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class Data2VecTextForTokenClassification(Data2VecTextPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.data2vec_text = Data2VecTextModel(config, add_pooling_layer=False)\n+        classifier_dropout = (\n+            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n+        )\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, TokenClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        outputs = self.data2vec_text(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+\n+            labels = labels.to(logits.device)\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class Data2VecTextForQuestionAnswering(Data2VecTextPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.data2vec_text = Data2VecTextModel(config, add_pooling_layer=False)\n+        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, QuestionAnsweringModelOutput]:\n+        outputs = self.data2vec_text(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"Data2VecTextForCausalLM\",\n+    \"Data2VecTextForMaskedLM\",\n+    \"Data2VecTextForMultipleChoice\",\n+    \"Data2VecTextForQuestionAnswering\",\n+    \"Data2VecTextForSequenceClassification\",\n+    \"Data2VecTextForTokenClassification\",\n+    \"Data2VecTextModel\",\n+    \"Data2VecTextPreTrainedModel\",\n+]"
        },
        {
            "sha": "4626a37750c1a63909bdaa9483270736233dccc4",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -490,7 +490,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -684,7 +684,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "398514cafe3f6b3a669e0ec44d52a9754a3baf87",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -305,7 +305,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -499,7 +499,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "100e48034abb2ad56fb1f0b3dfec15b2576d7d74",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 473,
            "deletions": 343,
            "changes": 816,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch ELECTRA model.\"\"\"\n \n-import math\n from dataclasses import dataclass\n from typing import Callable, Optional, Union\n \n@@ -23,8 +22,10 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithCrossAttentions,\n@@ -36,13 +37,24 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_electra import ElectraConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -81,7 +93,7 @@ def forward(\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n@@ -91,9 +103,10 @@ def forward(\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -110,19 +123,80 @@ def forward(\n         return embeddings\n \n \n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Electra\n class ElectraSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -137,111 +211,157 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only bert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+# Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->Electra\n+class ElectraCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in ElectraModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        return context_layer, attention_probs\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n@@ -259,19 +379,16 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-ELECTRA_SELF_ATTENTION_CLASSES = {\n-    \"eager\": ElectraSelfAttention,\n-}\n-\n-\n # Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Electra,BERT->ELECTRA\n class ElectraAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = ELECTRA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = ElectraCrossAttention if is_cross_attention else ElectraSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = ElectraSelfOutput(config)\n         self.pruned_heads = set()\n@@ -294,29 +411,29 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate\n@@ -356,38 +473,42 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = ElectraAttention(config, layer_idx=layer_idx)\n+        self.attention = ElectraAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = ElectraAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = ElectraAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = ElectraIntermediate(config)\n         self.output = ElectraOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -396,24 +517,21 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -423,11 +541,10 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Electra\n class ElectraEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([ElectraLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -438,77 +555,26 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -551,9 +617,18 @@ def forward(self, generator_hidden_states):\n \n @auto_docstring\n class ElectraPreTrainedModel(PreTrainedModel):\n-    config: ElectraConfig\n+    config_class = ElectraConfig\n     base_model_prefix = \"electra\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": ElectraLayer,\n+        \"attentions\": ElectraSelfAttention,\n+        \"cross_attentions\": ElectraCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -601,6 +676,7 @@ def __init__(self, config):\n \n         self.encoder = ElectraEncoder(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -618,6 +694,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -629,89 +706,197 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n+        if self.config.is_decoder:\n+            use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+            use_cache = False\n \n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n         else:\n-            encoder_extended_attention_mask = None\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n \n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n-        hidden_states = self.embeddings(\n+        embedding_output = self.embeddings(\n             input_ids=input_ids,\n             position_ids=position_ids,\n             token_type_ids=token_type_ids,\n             inputs_embeds=inputs_embeds,\n             past_key_values_length=past_key_values_length,\n         )\n-\n         if hasattr(self, \"embeddings_project\"):\n-            hidden_states = self.embeddings_project(hidden_states)\n+            embedding_output = self.embeddings_project(embedding_output)\n \n-        hidden_states = self.encoder(\n-            hidden_states,\n-            attention_mask=extended_attention_mask,\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n \n-        return hidden_states\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n+\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=encoder_outputs.last_hidden_state,\n+            past_key_values=encoder_outputs.past_key_values,\n+        )\n+\n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n \n \n class ElectraClassificationHead(nn.Module):\n@@ -854,6 +1039,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -864,28 +1050,23 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         discriminator_hidden_states = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = discriminator_hidden_states[0]\n@@ -914,10 +1095,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + discriminator_hidden_states[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -942,6 +1119,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -952,9 +1130,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], ElectraForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -987,18 +1163,15 @@ def forward(\n         >>> predictions.squeeze().tolist()\n         [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         discriminator_hidden_states = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         discriminator_sequence_output = discriminator_hidden_states[0]\n \n@@ -1015,10 +1188,6 @@ def forward(\n             else:\n                 loss = loss_fct(logits.view(-1, discriminator_sequence_output.shape[1]), labels.float())\n \n-        if not return_dict:\n-            output = (logits,) + discriminator_hidden_states[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return ElectraForPreTrainingOutput(\n             loss=loss,\n             logits=logits,\n@@ -1054,6 +1223,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, word_embeddings):\n         self.generator_lm_head = word_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1064,28 +1234,23 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         generator_hidden_states = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         generator_sequence_output = generator_hidden_states[0]\n \n@@ -1098,10 +1263,6 @@ def forward(\n             loss_fct = nn.CrossEntropyLoss()  # -100 index = padding token\n             loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + generator_hidden_states[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=loss,\n             logits=prediction_scores,\n@@ -1131,6 +1292,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1141,26 +1303,21 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         discriminator_hidden_states = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         discriminator_sequence_output = discriminator_hidden_states[0]\n \n@@ -1172,10 +1329,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + discriminator_hidden_states[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1186,7 +1339,7 @@ def forward(\n \n @auto_docstring\n class ElectraForQuestionAnswering(ElectraPreTrainedModel):\n-    config: ElectraConfig\n+    config_class = ElectraConfig\n     base_model_prefix = \"electra\"\n \n     def __init__(self, config):\n@@ -1199,6 +1352,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1210,21 +1364,17 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         discriminator_hidden_states = self.electra(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = discriminator_hidden_states[0]\n@@ -1251,13 +1401,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (\n-                start_logits,\n-                end_logits,\n-            ) + discriminator_hidden_states[1:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1279,6 +1422,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1289,9 +1433,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1323,7 +1465,6 @@ def forward(\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1343,9 +1484,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = discriminator_hidden_states[0]\n@@ -1359,10 +1499,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + discriminator_hidden_states[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1397,6 +1533,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.generator_lm_head = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1411,10 +1548,8 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1438,7 +1573,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -1453,9 +1587,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1470,10 +1604,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[1:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,"
        },
        {
            "sha": "37e3cb7a1d617523a57d5ed2e2a14960e2dcff26",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 12,
            "deletions": 17,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -28,6 +28,7 @@\n from ...modeling_outputs import BaseModelOutput, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.generic import can_return_tuple\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel, AutoModelForCausalLM\n from .configuration_encoder_decoder import EncoderDecoderConfig\n@@ -339,6 +340,7 @@ def from_encoder_decoder_pretrained(\n         config = EncoderDecoderConfig.from_encoder_decoder_configs(encoder.config, decoder.config, **kwargs)\n         return cls(encoder=encoder, decoder=decoder, config=config)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -352,9 +354,7 @@ def forward(\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n@@ -411,24 +411,26 @@ def forward(\n         >>> # generation\n         >>> generated = model.generate(input_ids)\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        # `record outputs` can rely on the absence of the kwarg to retrieve whether the config should be used or not\n+        # Hence, we use this workaround to allow for defaults to work as expected\n+        kwargs_shared = {key: kwargs[key] for key in [\"output_attentions\", \"output_hidden_states\"] if key in kwargs}\n \n         kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n+        kwargs_encoder = kwargs_encoder | kwargs_shared\n \n         kwargs_decoder = {\n             argument[len(\"decoder_\") :]: value for argument, value in kwargs.items() if argument.startswith(\"decoder_\")\n         }\n         if \"num_items_in_batch\" in kwargs_encoder:\n             kwargs_decoder[\"num_items_in_batch\"] = kwargs_encoder.pop(\"num_items_in_batch\", None)\n+        kwargs_decoder = kwargs_decoder | kwargs_shared\n \n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n                 inputs_embeds=inputs_embeds,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n+                return_dict=True,\n                 **kwargs_encoder,\n             )\n         elif isinstance(encoder_outputs, tuple):\n@@ -457,28 +459,21 @@ def forward(\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=attention_mask,\n             inputs_embeds=decoder_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             use_cache=use_cache,\n             past_key_values=past_key_values,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n             **kwargs_decoder,\n         )\n \n         # Compute loss independent from decoder (as some shift the logits inside them)\n         loss = None\n         if labels is not None:\n             warnings.warn(DEPRECATION_WARNING, FutureWarning)\n-            logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n+            logits = decoder_outputs.logits\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            if loss is not None:\n-                return (loss,) + decoder_outputs + encoder_outputs\n-            else:\n-                return decoder_outputs + encoder_outputs\n-\n         return Seq2SeqLMOutput(\n             loss=loss,\n             logits=decoder_outputs.logits,"
        },
        {
            "sha": "3e94cf71d1e61d0fa2603b07532f4e1c6f240b15",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 586,
            "deletions": 473,
            "changes": 1059,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/ernie/modular_ernie.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ernie.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2022 The HuggingFace Inc. team.\n #\n@@ -12,20 +18,20 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch ERNIE model.\"\"\"\n \n-import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -38,13 +44,18 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_ernie import ErnieConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -56,9 +67,6 @@ def __init__(self, config):\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n         self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n-        self.use_task_id = config.use_task_id\n-        if config.use_task_id:\n-            self.task_type_embeddings = nn.Embedding(config.task_type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n@@ -71,6 +79,10 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n+        self.use_task_id = config.use_task_id\n+        if config.use_task_id:\n+            self.task_type_embeddings = nn.Embedding(config.task_type_vocab_size, config.hidden_size)\n+\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -85,7 +97,7 @@ def forward(\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n@@ -95,9 +107,10 @@ def forward(\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -122,19 +135,78 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Ernie\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class ErnieSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -149,114 +221,158 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only ernie can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+class ErnieCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in ErnieModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        return context_layer, attention_probs\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Ernie\n class ErnieSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -271,19 +387,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-ERNIE_SELF_ATTENTION_CLASSES = {\n-    \"eager\": ErnieSelfAttention,\n-}\n-\n-\n-# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Ernie,BERT->ERNIE\n class ErnieAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = ERNIE_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = ErnieCrossAttention if is_cross_attention else ErnieSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = ErnieSelfOutput(config)\n         self.pruned_heads = set()\n@@ -306,32 +418,31 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Ernie\n class ErnieIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -347,7 +458,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOutput with Bert->Ernie\n class ErnieOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -362,44 +472,47 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Ernie\n class ErnieLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = ErnieAttention(config, layer_idx=layer_idx)\n+        self.attention = ErnieAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = ErnieAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = ErnieAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = ErnieIntermediate(config)\n         self.output = ErnieOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -408,123 +521,28 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output)\n         return layer_output\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Ernie\n-class ErnieEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n-        super().__init__()\n-        self.config = config\n-        self.layer = nn.ModuleList([ErnieLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n-        cache_position: Optional[torch.Tensor] = None,\n-    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n-        for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_head_mask = head_mask[i] if head_mask is not None else None\n-\n-            layer_outputs = layer_module(\n-                hidden_states,\n-                attention_mask,\n-                layer_head_mask,\n-                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n-                encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n-            )\n-\n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n-        return BaseModelOutputWithPastAndCrossAttentions(\n-            last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n-        )\n-\n-\n-# Copied from transformers.models.bert.modeling_bert.BertPooler with Bert->Ernie\n class ErniePooler(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -540,7 +558,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertPredictionHeadTransform with Bert->Ernie\n class ErniePredictionHeadTransform(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -558,7 +575,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertLMPredictionHead with Bert->Ernie\n class ErnieLMPredictionHead(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -582,50 +598,64 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOnlyMLMHead with Bert->Ernie\n-class ErnieOnlyMLMHead(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.predictions = ErnieLMPredictionHead(config)\n-\n-    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n-        prediction_scores = self.predictions(sequence_output)\n-        return prediction_scores\n-\n-\n-# Copied from transformers.models.bert.modeling_bert.BertOnlyNSPHead with Bert->Ernie\n-class ErnieOnlyNSPHead(nn.Module):\n+class ErnieEncoder(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n-        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n-\n-    def forward(self, pooled_output):\n-        seq_relationship_score = self.seq_relationship(pooled_output)\n-        return seq_relationship_score\n+        self.config = config\n+        self.layer = nn.ModuleList([ErnieLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n \n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+        for i, layer_module in enumerate(self.layer):\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-# Copied from transformers.models.bert.modeling_bert.BertPreTrainingHeads with Bert->Ernie\n-class ErniePreTrainingHeads(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.predictions = ErnieLMPredictionHead(config)\n-        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n+            hidden_states = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_values,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n \n-    def forward(self, sequence_output, pooled_output):\n-        prediction_scores = self.predictions(sequence_output)\n-        seq_relationship_score = self.seq_relationship(pooled_output)\n-        return prediction_scores, seq_relationship_score\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n \n \n @auto_docstring\n class ErniePreTrainedModel(PreTrainedModel):\n-    config: ErnieConfig\n+    config_class = ErnieConfig\n     base_model_prefix = \"ernie\"\n     supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": ErnieLayer,\n+        \"attentions\": ErnieSelfAttention,\n+        \"cross_attentions\": ErnieCrossAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n@@ -636,32 +666,8 @@ def _init_weights(self, module):\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-\n-\n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Output type of [`ErnieForPreTraining`].\n-    \"\"\"\n-)\n-# Copied from transformers.models.bert.modeling_bert.BertForPreTrainingOutput with Bert->Ernie\n-class ErnieForPreTrainingOutput(ModelOutput):\n-    r\"\"\"\n-    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-        (classification) loss.\n-    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-        before SoftMax).\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: Optional[torch.FloatTensor] = None\n-    seq_relationship_logits: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n+        elif isinstance(module, ErnieLMPredictionHead):\n+            module.bias.data.zero_()\n \n \n @auto_docstring(\n@@ -673,35 +679,37 @@ class ErnieForPreTrainingOutput(ModelOutput):\n \n     To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n+    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n )\n class ErnieModel(ErniePreTrainedModel):\n-    # Copied from transformers.models.clap.modeling_clap.ClapTextModel.__init__ with ClapText->Ernie\n+    _no_split_modules = [\"ErnieLayer\"]\n+\n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\"\n         add_pooling_layer (bool, *optional*, defaults to `True`):\n             Whether to add a pooling layer\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = ErnieEmbeddings(config)\n         self.encoder = ErnieEncoder(config)\n \n         self.pooler = ErniePooler(config) if add_pooling_layer else None\n \n+        self.position_embedding_type = config.position_embedding_type\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.embeddings.word_embeddings\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    # Copied from transformers.models.bert.modeling_bert.BertModel._prune_heads\n     def _prune_heads(self, heads_to_prune):\n         \"\"\"\n         Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n@@ -710,6 +718,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -722,11 +731,10 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -735,17 +743,28 @@ def forward(\n             assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n             config.task_type_vocab_size-1]\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n@@ -759,39 +778,57 @@ def forward(\n         batch_size, seq_length = input_shape\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n-            )\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            # specific to ernie\n+            task_type_ids=task_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n             else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n \n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n@@ -800,41 +837,174 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            token_type_ids=token_type_ids,\n-            task_type_ids=task_type_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`ErnieForPreTraining`].\n+    \"\"\"\n+)\n+class ErnieForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+class ErniePreTrainingHeads(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.predictions = ErnieLMPredictionHead(config)\n+        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n+\n+    def forward(self, sequence_output, pooled_output):\n+        prediction_scores = self.predictions(sequence_output)\n+        seq_relationship_score = self.seq_relationship(pooled_output)\n+        return prediction_scores, seq_relationship_score\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -845,7 +1015,6 @@ def forward(\n class ErnieForPreTraining(ErniePreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForPreTraining.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -855,15 +1024,14 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForPreTraining.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.cls.predictions.decoder\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForPreTraining.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -876,9 +1044,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n         next_sentence_label: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], ErnieForPreTrainingOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -913,8 +1079,6 @@ def forward(\n         >>> seq_relationship_logits = outputs.seq_relationship_logits\n         ```\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -923,9 +1087,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output, pooled_output = outputs[:2]\n@@ -938,10 +1101,6 @@ def forward(\n             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n             total_loss = masked_lm_loss + next_sentence_loss\n \n-        if not return_dict:\n-            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return ErnieForPreTrainingOutput(\n             loss=total_loss,\n             prediction_logits=prediction_scores,\n@@ -951,6 +1110,16 @@ def forward(\n         )\n \n \n+class ErnieOnlyMLMHead(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.predictions = ErnieLMPredictionHead(config)\n+\n+    def forward(self, sequence_output: torch.Tensor) -> torch.Tensor:\n+        prediction_scores = self.predictions(sequence_output)\n+        return prediction_scores\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     Ernie Model with a `language modeling` head on top for CLM fine-tuning.\n@@ -959,7 +1128,6 @@ def forward(\n class ErnieForCausalLM(ErniePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.__init__ with BertLMHeadModel->ErnieForCausalLM,Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -972,15 +1140,14 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.cls.predictions.decoder\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLMHeadModel.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -994,12 +1161,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[list[torch.Tensor]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1012,7 +1177,6 @@ def forward(\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -1028,9 +1192,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1045,10 +1209,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,\n@@ -1063,7 +1223,6 @@ def forward(\n class ErnieForMaskedLM(ErniePreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1079,15 +1238,14 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.cls.predictions.decoder\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.cls.predictions.decoder = new_embeddings\n         self.cls.predictions.bias = new_embeddings.bias\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1101,9 +1259,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1116,9 +1272,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1129,9 +1282,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1142,18 +1294,13 @@ def forward(\n             loss_fct = CrossEntropyLoss()  # -100 index = padding token\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n         )\n \n-    # Copied from transformers.models.bert.modeling_bert.BertForMaskedLM.prepare_inputs_for_generation\n     def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n         input_shape = input_ids.shape\n         effective_batch_size = input_shape[0]\n@@ -1179,13 +1326,22 @@ def can_generate(cls) -> bool:\n         return False\n \n \n+class ErnieOnlyNSPHead(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.seq_relationship = nn.Linear(config.hidden_size, 2)\n+\n+    def forward(self, pooled_output):\n+        seq_relationship_score = self.seq_relationship(pooled_output)\n+        return seq_relationship_score\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     Ernie Model with a `next sentence prediction (classification)` head on top.\n     \"\"\"\n )\n class ErnieForNextSentencePrediction(ErniePreTrainedModel):\n-    # Copied from transformers.models.bert.modeling_bert.BertForNextSentencePrediction.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1195,6 +1351,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1206,10 +1363,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1251,8 +1405,6 @@ def forward(\n             )\n             labels = kwargs.pop(\"next_sentence_label\")\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1261,9 +1413,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1275,10 +1426,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n \n-        if not return_dict:\n-            output = (seq_relationship_scores,) + outputs[2:]\n-            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n-\n         return NextSentencePredictorOutput(\n             loss=next_sentence_loss,\n             logits=seq_relationship_scores,\n@@ -1294,7 +1441,6 @@ def forward(\n     \"\"\"\n )\n class ErnieForSequenceClassification(ErniePreTrainedModel):\n-    # Copied from transformers.models.bert.modeling_bert.BertForSequenceClassification.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n@@ -1310,6 +1456,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1321,9 +1468,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1336,8 +1481,6 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1346,9 +1489,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1378,9 +1520,6 @@ def forward(\n             elif self.config.problem_type == \"multi_label_classification\":\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n \n         return SequenceClassifierOutput(\n             loss=loss,\n@@ -1392,7 +1531,6 @@ def forward(\n \n @auto_docstring\n class ErnieForMultipleChoice(ErniePreTrainedModel):\n-    # Copied from transformers.models.bert.modeling_bert.BertForMultipleChoice.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n \n@@ -1406,6 +1544,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1417,9 +1556,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1456,7 +1593,6 @@ def forward(\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1477,9 +1613,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1493,10 +1628,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1507,7 +1638,6 @@ def forward(\n \n @auto_docstring\n class ErnieForTokenClassification(ErniePreTrainedModel):\n-    # Copied from transformers.models.bert.modeling_bert.BertForTokenClassification.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n@@ -1522,6 +1652,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1533,9 +1664,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1546,8 +1675,6 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1556,9 +1683,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1571,10 +1697,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1585,7 +1707,6 @@ def forward(\n \n @auto_docstring\n class ErnieForQuestionAnswering(ErniePreTrainedModel):\n-    # Copied from transformers.models.bert.modeling_bert.BertForQuestionAnswering.__init__ with Bert->Ernie,bert->ernie\n     def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n@@ -1596,6 +1717,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1608,9 +1730,7 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1619,8 +1739,6 @@ def forward(\n             assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n             config.task_type_vocab_size-1]\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.ernie(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1629,9 +1747,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1658,10 +1775,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,"
        },
        {
            "sha": "30261966b3d0df0081a62d4821e97a1d8faf22c8",
            "filename": "src/transformers/models/ernie/modular_ernie.py",
            "status": "added",
            "additions": 1012,
            "deletions": 0,
            "changes": 1012,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodular_ernie.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -0,0 +1,1012 @@\n+# coding=utf-8\n+# Copyright 2022 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch ERNIE model.\"\"\"\n+\n+import warnings\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPoolingAndCrossAttentions,\n+    CausalLMOutputWithCrossAttentions,\n+    MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n+    NextSentencePredictorOutput,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n+from ..bert.modeling_bert import (\n+    BertCrossAttention,\n+    BertEmbeddings,\n+    BertEncoder,\n+    BertForMaskedLM,\n+    BertForMultipleChoice,\n+    BertForNextSentencePrediction,\n+    BertForPreTraining,\n+    BertForPreTrainingOutput,\n+    BertForQuestionAnswering,\n+    BertForSequenceClassification,\n+    BertForTokenClassification,\n+    BertLayer,\n+    BertLMHeadModel,\n+    BertLMPredictionHead,\n+    BertModel,\n+    BertPooler,\n+    BertSelfAttention,\n+)\n+from .configuration_ernie import ErnieConfig\n+\n+\n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ErnieEmbeddings(BertEmbeddings):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.use_task_id = config.use_task_id\n+        if config.use_task_id:\n+            self.task_type_embeddings = nn.Embedding(config.task_type_vocab_size, config.hidden_size)\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        task_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        batch_size, seq_length = input_shape\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+\n+        embeddings = inputs_embeds + token_type_embeddings\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n+\n+        # add `task_type_id` for ERNIE model\n+        if self.use_task_id:\n+            if task_type_ids is None:\n+                task_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+            task_type_embeddings = self.task_type_embeddings(task_type_ids)\n+            embeddings += task_type_embeddings\n+\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+\n+class ErnieSelfAttention(BertSelfAttention):\n+    pass\n+\n+\n+class ErnieCrossAttention(BertCrossAttention):\n+    pass\n+\n+\n+class ErnieLayer(BertLayer):\n+    pass\n+\n+\n+class ErniePooler(BertPooler):\n+    pass\n+\n+\n+class ErnieLMPredictionHead(BertLMPredictionHead):\n+    pass\n+\n+\n+class ErnieEncoder(BertEncoder):\n+    pass\n+\n+\n+@auto_docstring\n+class ErniePreTrainedModel(PreTrainedModel):\n+    config_class = ErnieConfig\n+    base_model_prefix = \"ernie\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": ErnieLayer,\n+        \"attentions\": ErnieSelfAttention,\n+        \"cross_attentions\": ErnieCrossAttention,\n+    }\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, ErnieLMPredictionHead):\n+            module.bias.data.zero_()\n+\n+\n+class ErnieModel(BertModel):\n+    _no_split_modules = [\"ErnieLayer\"]\n+\n+    def __init__(self, config, add_pooling_layer=True):\n+        super().__init__(self, config)\n+        self.config = config\n+        self.gradient_checkpointing = False\n+\n+        self.embeddings = ErnieEmbeddings(config)\n+        self.encoder = ErnieEncoder(config)\n+\n+        self.pooler = ErniePooler(config) if add_pooling_layer else None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        \"\"\"\n+        if self.config.is_decoder:\n+            use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        else:\n+            use_cache = False\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+            )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+\n+        if input_ids is not None and inputs_embeds is not None:\n+            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n+        elif input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+            input_shape = input_ids.size()\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+        else:\n+            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+\n+        batch_size, seq_length = input_shape\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n+\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            # specific to ernie\n+            task_type_ids=task_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n+\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n+\n+        return BaseModelOutputWithPoolingAndCrossAttentions(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            past_key_values=encoder_outputs.past_key_values,\n+        )\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n+\n+class ErnieForPreTrainingOutput(BertForPreTrainingOutput):\n+    pass\n+\n+\n+class ErnieForPreTraining(BertForPreTraining):\n+    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        next_sentence_label: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], ErnieForPreTrainingOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n+            the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        next_sentence_label (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence\n+            pair (see `input_ids` docstring) Indices should be in `[0, 1]`:\n+\n+            - 0 indicates sequence B is a continuation of sequence A,\n+            - 1 indicates sequence B is a random sequence.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, ErnieForPreTraining\n+        >>> import torch\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n+        >>> model = ErnieForPreTraining.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n+\n+        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> prediction_logits = outputs.prediction_logits\n+        >>> seq_relationship_logits = outputs.seq_relationship_logits\n+        ```\n+        \"\"\"\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output, pooled_output = outputs[:2]\n+        prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n+\n+        total_loss = None\n+        if labels is not None and next_sentence_label is not None:\n+            loss_fct = CrossEntropyLoss()\n+            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+            next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n+            total_loss = masked_lm_loss + next_sentence_loss\n+\n+        return ErnieForPreTrainingOutput(\n+            loss=total_loss,\n+            prediction_logits=prediction_scores,\n+            seq_relationship_logits=seq_relationship_score,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForCausalLM(BertLMHeadModel):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[list[torch.Tensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n+            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n+            ignored (masked), the loss is only computed for the tokens with labels n `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        if labels is not None:\n+            use_cache = False\n+\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.cls(sequence_output)\n+\n+        lm_loss = None\n+        if labels is not None:\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return CausalLMOutputWithCrossAttentions(\n+            loss=lm_loss,\n+            logits=prediction_scores,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            cross_attentions=outputs.cross_attentions,\n+        )\n+\n+\n+class ErnieForMaskedLM(BertForMaskedLM):\n+    _tied_weights_keys = [\"cls.predictions.decoder.bias\", \"cls.predictions.decoder.weight\"]\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.cls(sequence_output)\n+\n+        masked_lm_loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()  # -100 index = padding token\n+            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+\n+        return MaskedLMOutput(\n+            loss=masked_lm_loss,\n+            logits=prediction_scores,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForNextSentencePrediction(BertForNextSentencePrediction):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], NextSentencePredictorOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the next sequence prediction (classification) loss. Input should be a sequence pair\n+            (see `input_ids` docstring). Indices should be in `[0, 1]`:\n+\n+            - 0 indicates sequence B is a continuation of sequence A,\n+            - 1 indicates sequence B is a random sequence.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, ErnieForNextSentencePrediction\n+        >>> import torch\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n+        >>> model = ErnieForNextSentencePrediction.from_pretrained(\"nghuyong/ernie-1.0-base-zh\")\n+\n+        >>> prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n+        >>> next_sentence = \"The sky is blue due to the shorter wavelength of blue light.\"\n+        >>> encoding = tokenizer(prompt, next_sentence, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**encoding, labels=torch.LongTensor([1]))\n+        >>> logits = outputs.logits\n+        >>> assert logits[0, 0] < logits[0, 1]  # next sentence was random\n+        ```\n+        \"\"\"\n+\n+        if \"next_sentence_label\" in kwargs:\n+            warnings.warn(\n+                \"The `next_sentence_label` argument is deprecated and will be removed in a future version, use\"\n+                \" `labels` instead.\",\n+                FutureWarning,\n+            )\n+            labels = kwargs.pop(\"next_sentence_label\")\n+\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        pooled_output = outputs[1]\n+\n+        seq_relationship_scores = self.cls(pooled_output)\n+\n+        next_sentence_loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            next_sentence_loss = loss_fct(seq_relationship_scores.view(-1, 2), labels.view(-1))\n+\n+        return NextSentencePredictorOutput(\n+            loss=next_sentence_loss,\n+            logits=seq_relationship_scores,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForSequenceClassification(BertForSequenceClassification):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        pooled_output = outputs[1]\n+\n+        pooled_output = self.dropout(pooled_output)\n+        logits = self.classifier(pooled_output)\n+\n+        loss = None\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForMultipleChoice(BertForMultipleChoice):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n+            `input_ids` above)\n+        \"\"\"\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n+        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        pooled_output = outputs[1]\n+\n+        pooled_output = self.dropout(pooled_output)\n+        logits = self.classifier(pooled_output)\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForTokenClassification(BertForTokenClassification):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class ErnieForQuestionAnswering(BertForQuestionAnswering):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        task_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        start_positions: Optional[torch.Tensor] = None,\n+        end_positions: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        task_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Task type embedding is a special embedding to represent the characteristic of different tasks, such as\n+            word-aware pre-training task, structure-aware pre-training task and semantic-aware pre-training task. We\n+            assign a `task_type_id` to each task and the `task_type_id` is in the range `[0,\n+            config.task_type_vocab_size-1]\n+        \"\"\"\n+        outputs = self.ernie(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            task_type_ids=task_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"ErnieForCausalLM\",\n+    \"ErnieForMaskedLM\",\n+    \"ErnieForMultipleChoice\",\n+    \"ErnieForNextSentencePrediction\",\n+    \"ErnieForPreTraining\",\n+    \"ErnieForQuestionAnswering\",\n+    \"ErnieForSequenceClassification\",\n+    \"ErnieForTokenClassification\",\n+    \"ErnieModel\",\n+    \"ErniePreTrainedModel\",\n+]"
        },
        {
            "sha": "3588ca78e0d0706d3f9923fd1498ed04b895890e",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -490,7 +490,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -658,7 +658,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "544fed9f1e51c3e75705570c224d59470245bc8b",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -269,7 +269,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "157176c1fd3857a667dc02808efbd07250b6cf62",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -110,7 +110,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "d76107fcfe38a0142faca1ae343a84012d744821",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 21,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -73,23 +73,6 @@ def _make_causal_mask(\n     return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -622,13 +605,15 @@ def forward(\n             bsz, seq_len = input_ids.size()\n             if position_ids is None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(\n+                position_ids = self.create_position_ids_from_input_ids(\n                     input_ids, self.padding_idx, past_key_values_length\n                 ).to(input_ids.device)\n         else:\n             bsz, seq_len = inputs_embeds.size()[:-1]\n             if position_ids is None:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length)\n+                position_ids = self.create_position_ids_from_inputs_embeds(\n+                    inputs_embeds, past_key_values_length, self.padding_idx\n+                )\n \n         # expand embeddings if needed\n         max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n@@ -637,8 +622,9 @@ def forward(\n \n         return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, self.weights.shape[-1]).detach()\n \n+    @staticmethod\n     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds\n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_length):\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -651,10 +637,27 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n \n+    @staticmethod\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n class KosmosTextAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\""
        },
        {
            "sha": "b37233744da8da7e06e224bde45ed601d87873ec",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 25,
            "deletions": 22,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -78,23 +78,6 @@ def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int]\n     return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n KOSMOS2_5_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -685,13 +668,15 @@ def forward(\n             bsz, seq_len = input_ids.size()\n             if position_ids is None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(\n+                position_ids = self.create_position_ids_from_input_ids(\n                     input_ids, self.padding_idx, past_key_values_length\n                 ).to(input_ids.device)\n         else:\n             bsz, seq_len = inputs_embeds.size()[:-1]\n             if position_ids is None:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length)\n+                position_ids = self.create_position_ids_from_inputs_embeds(\n+                    inputs_embeds, past_key_values_length, self.padding_idx\n+                )\n \n         # expand embeddings if needed\n         max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n@@ -700,8 +685,9 @@ def forward(\n \n         return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, self.weights.shape[-1]).detach()\n \n+    @staticmethod\n     # Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding.create_position_ids_from_inputs_embeds\n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_length):\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -714,10 +700,27 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n \n+    @staticmethod\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n # Copied from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextFFN with Kosmos2->Kosmos2_5\n class Kosmos2_5TextFFN(nn.Module):\n@@ -1617,7 +1620,7 @@ def prepare_inputs_for_generation(\n \n         # cut input_ids if past_key_values is used\n         if past_key_values is not None:\n-            position_ids = create_position_ids_from_input_ids(\n+            position_ids = Kosmos2_5TextSinusoidalPositionalEmbedding.create_position_ids_from_input_ids(\n                 input_ids,\n                 padding_idx=self.config.pad_token_id,\n                 past_key_values_length=0,"
        },
        {
            "sha": "ac1e4d0544bf9647d6d8b1ed0638a635aee33685",
            "filename": "src/transformers/models/lilt/modeling_lilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flilt%2Fmodeling_lilt.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -479,12 +479,10 @@ def layout_feed_forward_chunk(self, attention_output):\n \n \n class LiltEncoder(nn.Module):\n-    # Copied from transformers.models.bert.modeling_bert.BertEncoder.__init__ with Bert->Lilt\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n-        self.layer = nn.ModuleList([LiltLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n+        self.layer = nn.ModuleList([LiltLayer(config) for _ in range(config.num_hidden_layers)])\n \n     def forward(\n         self,"
        },
        {
            "sha": "e7b64e50a79ab6f8471f7b25af2e32999f7ccffc",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 29,
            "deletions": 20,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -72,17 +72,6 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n     return shifted_input_ids\n \n \n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n # Copied from transformers.models.bart.modeling_bart.BartScaledWordEmbedding with Bart->M2M100\n class M2M100ScaledWordEmbedding(nn.Embedding):\n     \"\"\"\n@@ -146,12 +135,14 @@ def forward(\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()\n             # Create the position ids from the input token ids. Any padded tokens remain padded.\n-            position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(\n-                input_ids.device\n-            )\n+            position_ids = self.create_position_ids_from_input_ids(\n+                input_ids, self.padding_idx, past_key_values_length\n+            ).to(input_ids.device)\n         else:\n             bsz, seq_len = inputs_embeds.size()[:-1]\n-            position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length)\n+            position_ids = self.create_position_ids_from_inputs_embeds(\n+                inputs_embeds, past_key_values_length, self.padding_idx\n+            )\n \n         # expand embeddings if needed\n         max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n@@ -160,7 +151,8 @@ def forward(\n \n         return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, self.weights.shape[-1]).detach()\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_length):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -173,10 +165,27 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n \n+    @staticmethod\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n # Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n def eager_attention_forward(\n@@ -553,7 +562,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -591,7 +600,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -712,7 +721,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "24d056043feeb049b3ad3d5350df3efe2df26a78",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -507,7 +507,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -545,7 +545,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -666,7 +666,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "a0c6985b3da915adff495ff5994191e98ce8b29e",
            "filename": "src/transformers/models/markuplm/modeling_markuplm.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fmodeling_markuplm.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -90,23 +90,6 @@ def forward(self, xpath_tags_seq=None, xpath_subs_seq=None):\n         return xpath_embeddings\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n class MarkupLMEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -134,8 +117,9 @@ def __init__(self, config):\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n+    @staticmethod\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_inputs_embeds\n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -148,10 +132,27 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n     def forward(\n         self,\n         input_ids=None,\n@@ -160,7 +161,6 @@ def forward(\n         token_type_ids=None,\n         position_ids=None,\n         inputs_embeds=None,\n-        past_key_values_length=0,\n     ):\n         if input_ids is not None:\n             input_shape = input_ids.size()\n@@ -172,9 +172,9 @@ def forward(\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(input_ids, self.padding_idx)\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if token_type_ids is None:\n             token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)"
        },
        {
            "sha": "3a0eff5851033e3d195b47dfa0684c6cf49b5269",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -530,7 +530,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -568,7 +568,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -689,7 +689,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "121ae19850ff537123e3ca54e0d68bb9c083c463",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -104,7 +104,7 @@ def forward(\n         return embeddings\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->MegatronBert\n+# copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->MegatronBert\n class MegatronBertSelfAttention(nn.Module):\n     def __init__(self, config, position_embedding_type=None, layer_idx=None):\n         super().__init__()"
        },
        {
            "sha": "c44a24acbae9f3d00d117c8c292e3d5a96b9aef6",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 165,
            "deletions": 217,
            "changes": 382,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -20,16 +20,17 @@\n # OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n # SOFTWARE.\n \n-import math\n import warnings\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPooling,\n@@ -40,12 +41,18 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import ModelOutput, auto_docstring, logging\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_mobilebert import MobileBertConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -138,12 +145,45 @@ def forward(\n         return embeddings\n \n \n+# Copied from transformers.models.bart.modeling_bart.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class MobileBertSelfAttention(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.true_hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.true_hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.true_hidden_size, self.all_head_size)\n@@ -152,52 +192,42 @@ def __init__(self, config):\n         )\n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n+        self.is_causal = False\n+\n     def forward(\n         self,\n         query_tensor: torch.Tensor,\n         key_tensor: torch.Tensor,\n         value_tensor: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = query_tensor.shape\n-        query_layer = (\n-            self.query(query_tensor)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-        key_layer = (\n-            self.key(key_tensor)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-        value_layer = (\n-            self.value(value_tensor)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n+        input_shape = query_tensor.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(query_tensor).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(key_tensor).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(value_tensor).view(*hidden_shape).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            **kwargs,\n         )\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-        return outputs\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class MobileBertSelfOutput(nn.Module):\n@@ -250,21 +280,20 @@ def forward(\n         layer_input: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_output, attn_weights = self.self(\n             query_tensor,\n             key_tensor,\n             value_tensor,\n             attention_mask,\n             head_mask,\n-            output_attentions,\n+            **kwargs,\n         )\n         # Run a linear projection of `hidden_size` then add a residual\n         # with `layer_input`.\n-        attention_output = self.output(self_outputs[0], layer_input)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, layer_input)\n+        return attention_output, attn_weights\n \n \n class MobileBertIntermediate(nn.Module):\n@@ -392,7 +421,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return layer_outputs\n \n \n-class MobileBertLayer(nn.Module):\n+class MobileBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.use_bottleneck = config.use_bottleneck\n@@ -411,48 +440,31 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         if self.use_bottleneck:\n             query_tensor, key_tensor, value_tensor, layer_input = self.bottleneck(hidden_states)\n         else:\n             query_tensor, key_tensor, value_tensor, layer_input = [hidden_states] * 4\n \n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             query_tensor,\n             key_tensor,\n             value_tensor,\n             layer_input,\n             attention_mask,\n             head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        s = (attention_output,)\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.num_feedforward_networks != 1:\n-            for i, ffn_module in enumerate(self.ffn):\n+            for ffn_module in self.ffn:\n                 attention_output = ffn_module(attention_output)\n-                s += (attention_output,)\n \n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output, hidden_states)\n-        outputs = (\n-            (layer_output,)\n-            + outputs\n-            + (\n-                torch.tensor(1000),\n-                query_tensor,\n-                key_tensor,\n-                value_tensor,\n-                layer_input,\n-                attention_output,\n-                intermediate_output,\n-            )\n-            + s\n-        )\n-        return outputs\n+        return layer_output\n \n \n class MobileBertEncoder(nn.Module):\n@@ -465,36 +477,16 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 head_mask[i],\n-                output_attentions,\n+                **kwargs,\n             )\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        # Add last layer\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, all_hidden_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=all_hidden_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class MobileBertPooler(nn.Module):\n@@ -581,6 +573,15 @@ def forward(self, sequence_output: torch.Tensor, pooled_output: torch.Tensor) ->\n class MobileBertPreTrainedModel(PreTrainedModel):\n     config: MobileBertConfig\n     base_model_prefix = \"mobilebert\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MobileBertLayer,\n+        \"attentions\": MobileBertSelfAttention,\n+    }\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -637,6 +638,8 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n+\n         self.embeddings = MobileBertEmbeddings(config)\n         self.encoder = MobileBertEncoder(config)\n \n@@ -659,6 +662,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -668,36 +672,22 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(input_shape, device=device)\n-        if token_type_ids is None:\n-            token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+        )\n \n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n+        attention_mask = self._update_full_mask(\n+            attention_mask,\n+            embedding_output,\n+        )\n \n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n@@ -706,30 +696,43 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids, position_ids=position_ids, token_type_ids=token_type_ids, inputs_embeds=inputs_embeds\n-        )\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -763,6 +766,7 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Em\n \n         return super().resize_token_embeddings(new_num_tokens=new_num_tokens)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -774,9 +778,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         next_sentence_label: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[torch.FloatTensor] = None,\n-        output_hidden_states: Optional[torch.FloatTensor] = None,\n-        return_dict: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MobileBertForPreTrainingOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -806,18 +808,15 @@ def forward(\n         >>> prediction_logits = outputs.prediction_logits\n         >>> seq_relationship_logits = outputs.seq_relationship_logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output, pooled_output = outputs[:2]\n         prediction_scores, seq_relationship_score = self.cls(sequence_output, pooled_output)\n@@ -829,10 +828,6 @@ def forward(\n             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), next_sentence_label.view(-1))\n             total_loss = masked_lm_loss + next_sentence_loss\n \n-        if not return_dict:\n-            output = (prediction_scores, seq_relationship_score) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return MobileBertForPreTrainingOutput(\n             loss=total_loss,\n             prediction_logits=prediction_scores,\n@@ -869,6 +864,7 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None) -> nn.Em\n         )\n         return super().resize_token_embeddings(new_num_tokens=new_num_tokens)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -879,28 +875,23 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -911,10 +902,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()  # -100 index = padding token\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -948,6 +935,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -958,10 +946,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, NextSentencePredictorOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -997,18 +982,15 @@ def forward(\n             )\n             labels = kwargs.pop(\"next_sentence_label\")\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1019,10 +1001,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             next_sentence_loss = loss_fct(seq_relationship_score.view(-1, 2), labels.view(-1))\n \n-        if not return_dict:\n-            output = (seq_relationship_score,) + outputs[2:]\n-            return ((next_sentence_loss,) + output) if next_sentence_loss is not None else output\n-\n         return NextSentencePredictorOutput(\n             loss=next_sentence_loss,\n             logits=seq_relationship_score,\n@@ -1054,6 +1032,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1064,28 +1043,23 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1115,9 +1089,6 @@ def forward(\n             elif self.config.problem_type == \"multi_label_classification\":\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n \n         return SequenceClassifierOutput(\n             loss=loss,\n@@ -1140,6 +1111,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1151,22 +1123,17 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         start_positions: Optional[torch.Tensor] = None,\n         end_positions: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1193,10 +1160,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1222,6 +1185,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1232,9 +1196,7 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1266,7 +1228,6 @@ def forward(\n             num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n             `input_ids` above)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1286,9 +1247,8 @@ def forward(\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         pooled_output = outputs[1]\n@@ -1302,10 +1262,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1331,6 +1287,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1341,26 +1298,21 @@ def forward(\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.mobilebert(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1373,10 +1325,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,"
        },
        {
            "sha": "3fcbd936af9b6522fb70a2fabef27da3c0c64114",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 28,
            "deletions": 25,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -77,23 +77,6 @@ def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start\n     return shifted_input_ids\n \n \n-# Copied from transformers.models.roberta.modeling_roberta.create_position_ids_from_input_ids\n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n def load_balancing_loss_func(router_probs: torch.Tensor, expert_indices: torch.Tensor) -> float:\n     r\"\"\"\n     Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n@@ -150,7 +133,7 @@ def forward(self, input_ids: torch.Tensor):\n         return super().forward(input_ids) * self.embed_scale\n \n \n-# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding\n+# Copied from transformers.models.m2m_100.modeling_m2m_100.M2M100SinusoidalPositionalEmbedding with M2M100->NllbMoe\n class NllbMoeSinusoidalPositionalEmbedding(nn.Module):\n     \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n \n@@ -200,12 +183,14 @@ def forward(\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()\n             # Create the position ids from the input token ids. Any padded tokens remain padded.\n-            position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length).to(\n-                input_ids.device\n-            )\n+            position_ids = self.create_position_ids_from_input_ids(\n+                input_ids, self.padding_idx, past_key_values_length\n+            ).to(input_ids.device)\n         else:\n             bsz, seq_len = inputs_embeds.size()[:-1]\n-            position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length)\n+            position_ids = self.create_position_ids_from_inputs_embeds(\n+                inputs_embeds, past_key_values_length, self.padding_idx\n+            )\n \n         # expand embeddings if needed\n         max_pos = self.padding_idx + 1 + seq_len + past_key_values_length\n@@ -214,7 +199,8 @@ def forward(\n \n         return self.weights.index_select(0, position_ids.view(-1)).view(bsz, seq_len, self.weights.shape[-1]).detach()\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_length):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, past_key_values_length, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -227,10 +213,27 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds, past_key_values_\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape).contiguous() + past_key_values_length\n \n+    @staticmethod\n+    # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings.create_position_ids_from_input_ids\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n \n class NllbMoeTop2Router(nn.Module):\n     \"\"\"\n@@ -1059,7 +1062,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to"
        },
        {
            "sha": "09ea75c3b1fece2d372a41704d71923afc2a531f",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -486,7 +486,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -524,7 +524,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -645,7 +645,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "b9ba1aca6d287ee1ecf885535f6c560286bff03c",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -779,7 +779,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -817,7 +817,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -938,7 +938,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "5c056be5ae890f1fd6936680fc93b0e595798991",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -88,7 +88,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -126,7 +126,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -247,7 +247,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "9ca406775eaebb2e81b5a5cdf40200d8e170bc54",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -72,7 +72,7 @@ def _update_full_mask(\n         inputs_embeds: torch.Tensor,\n     ):\n         if attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n@@ -110,7 +110,7 @@ def _update_causal_mask(\n                 )\n             return attention_mask\n \n-        if self.config._attn_implementation == \"flash_attention_2\":\n+        if \"flash\" in self.config._attn_implementation:\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n@@ -231,7 +231,7 @@ def _update_cross_attn_mask(\n     ):\n         # expand encoder attention mask\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n-            if self.config._attn_implementation == \"flash_attention_2\":\n+            if \"flash\" in self.config._attn_implementation:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n             elif self.config._attn_implementation == \"sdpa\":\n                 # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on"
        },
        {
            "sha": "17517ca1209d8c6daeb6724829328625498c0083",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1662,6 +1662,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs,\n     ) -> Union[tuple, ProphetNetSeq2SeqLMOutput]:\n         r\"\"\"\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n@@ -1864,6 +1865,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[tuple, ProphetNetDecoderLMOutput]:\n         r\"\"\"\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):"
        },
        {
            "sha": "8e187421f1524714cf9e2a94bb48ff7cbf1b604d",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -262,7 +262,7 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    # Copied from transformers.models.bert.modeling_bert.BertAttention.forward\n+    # copied from transformers.models.bert.modeling_bert.BertAttention.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -333,7 +333,7 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RemBertIntermediate(config)\n         self.output = RemBertOutput(config)\n \n-    # Copied from transformers.models.bert.modeling_bert.BertLayer.forward\n+    # copied from transformers.models.bert.modeling_bert.BertLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8810be00a0d0f33b36edb050167b7b8c23436cbb",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 484,
            "deletions": 519,
            "changes": 1003,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/roberta/modular_roberta.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_roberta.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n # Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n@@ -13,19 +19,18 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch RoBERTa model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n-from torch import nn\n+import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -37,26 +42,27 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta import RobertaConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n class RobertaEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -70,37 +76,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -116,7 +129,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -129,24 +143,99 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n \n-# Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->Roberta\n class RobertaSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -161,219 +250,158 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n-        )\n-\n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n-\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only roberta can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n                 )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+class RobertaCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+        self.config = config\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n-        return context_layer, attention_probs\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n \n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-# Copied from transformers.models.bert.modeling_bert.BertSdpaSelfAttention with Bert->Roberta\n-class RobertaSdpaSelfAttention(RobertaSelfAttention):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n-        super().__init__(config, position_embedding_type=position_embedding_type, layer_idx=layer_idx)\n-        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-    # Adapted from RobertaSelfAttention\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        cache_position: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n-            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n-            logger.warning_once(\n-                \"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n-                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n-                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n-                '`attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                attention_mask,\n-                head_mask,\n-                encoder_hidden_states,\n-                past_key_values,\n-                output_attentions,\n-                cache_position,\n-            )\n-\n-        bsz, tgt_len, _ = hidden_states.size()\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n \n-        query_layer = (\n-            self.query(hidden_states).view(bsz, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_states from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n         else:\n-            key_layer = (\n-                self.key(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n-            value_layer = (\n-                self.value(current_states)\n-                .view(bsz, -1, self.num_attention_heads, self.attention_head_size)\n-                .transpose(1, 2)\n-            )\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n                 )\n                 # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n-        # a causal mask in case tgt_len == 1.\n-        is_causal = self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n-            attn_mask=attention_mask,\n-            dropout_p=self.dropout_prob if self.training else 0.0,\n-            is_causal=is_causal,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        attn_output = attn_output.transpose(1, 2)\n-        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n-\n-        return attn_output, None\n \n-\n-# Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n class RobertaSelfOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -388,20 +416,15 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-ROBERTA_SELF_ATTENTION_CLASSES = {\n-    \"eager\": RobertaSelfAttention,\n-    \"sdpa\": RobertaSdpaSelfAttention,\n-}\n-\n-\n-# Copied from transformers.models.bert.modeling_bert.BertAttention with Bert->Roberta,BERT->ROBERTA\n class RobertaAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n-            config,\n-            position_embedding_type=position_embedding_type,\n-            layer_idx=layer_idx,\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = RobertaCrossAttention if is_cross_attention else RobertaSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = RobertaSelfOutput(config)\n         self.pruned_heads = set()\n@@ -424,32 +447,31 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states,\n+            encoder_hidden_states=encoder_hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertIntermediate\n class RobertaIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -465,7 +487,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertOutput\n class RobertaOutput(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -480,44 +501,47 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertLayer with Bert->Roberta\n class RobertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaAttention(config, layer_idx=layer_idx)\n+        self.attention = RobertaAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n-            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\", layer_idx=layer_idx)\n+            self.crossattention = RobertaAttention(\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n+            )\n         self.intermediate = RobertaIntermediate(config)\n         self.output = RobertaOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -526,38 +550,66 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n         layer_output = self.output(intermediate_output, attention_output)\n         return layer_output\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->Roberta\n+@auto_docstring\n+class RobertaPreTrainedModel(PreTrainedModel):\n+    config_class = RobertaConfig\n+    base_model_prefix = \"roberta\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": RobertaLayer,\n+        \"attentions\": RobertaSelfAttention,\n+        \"cross_attentions\": RobertaCrossAttention,\n+    }\n+\n+    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->RobertaLMHead\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, RobertaLMHead):\n+            module.bias.data.zero_()\n+\n+\n class RobertaEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList([RobertaLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)])\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -568,81 +620,29 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n-# Copied from transformers.models.bert.modeling_bert.BertPooler\n class RobertaPooler(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -658,32 +658,6 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n-@auto_docstring\n-class RobertaPreTrainedModel(PreTrainedModel):\n-    config: RobertaConfig\n-    base_model_prefix = \"roberta\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"RobertaEmbeddings\", \"RobertaSelfAttention\", \"RobertaSdpaSelfAttention\"]\n-    _supports_sdpa = True\n-\n-    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->RobertaLMHead\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.padding_idx is not None:\n-                module.weight.data[module.padding_idx].zero_()\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, RobertaLMHead):\n-            module.bias.data.zero_()\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n@@ -696,7 +670,6 @@ def _init_weights(self, module):\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n )\n-# Copied from transformers.models.bert.modeling_bert.BertModel with Bert->Roberta, BERT->ROBERTA\n class RobertaModel(RobertaPreTrainedModel):\n     _no_split_modules = [\"RobertaEmbeddings\", \"RobertaLayer\"]\n \n@@ -707,13 +680,13 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = RobertaEmbeddings(config)\n         self.encoder = RobertaEncoder(config)\n \n         self.pooler = RobertaPooler(config) if add_pooling_layer else None\n \n-        self.attn_implementation = config._attn_implementation\n         self.position_embedding_type = config.position_embedding_type\n \n         # Initialize weights and apply final processing\n@@ -733,6 +706,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -744,52 +718,40 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[Union[list[torch.FloatTensor], Cache]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n+        else:\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n+\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -799,55 +761,16 @@ def forward(\n             past_key_values_length=past_key_values_length,\n         )\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n-\n-        use_sdpa_attention_masks = (\n-            self.attn_implementation == \"sdpa\"\n-            and self.position_embedding_type == \"absolute\"\n-            and head_mask is None\n-            and not output_attentions\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n-        # Expand the attention mask\n-        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n-            # Expand the attention mask for SDPA.\n-            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-            if self.config.is_decoder:\n-                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                    attention_mask,\n-                    input_shape,\n-                    embedding_output,\n-                    past_key_values_length,\n-                )\n-            else:\n-                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-        else:\n-            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-            # ourselves in which case we just need to make it broadcastable to all heads.\n-            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-\n-            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n-                # Expand the attention mask for SDPA.\n-                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n-                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n-                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n-                )\n-            else:\n-                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n-        else:\n-            encoder_extended_attention_mask = None\n-\n         # Prepare head mask if needed\n         # 1.0 in head_mask indicate we keep the head\n         # attention_probs has shape bsz x n_heads x N x N\n@@ -857,32 +780,135 @@ def forward(\n \n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -910,6 +936,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -922,12 +949,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -960,7 +985,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -975,9 +999,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -994,10 +1018,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,\n@@ -1033,6 +1053,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1045,9 +1066,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1064,8 +1083,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -1075,9 +1092,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n@@ -1089,10 +1105,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -1150,6 +1162,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1160,9 +1173,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1179,18 +1190,15 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         logits = self.classifier(sequence_output)\n@@ -1220,10 +1228,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1244,6 +1248,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1254,9 +1259,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1289,7 +1292,6 @@ def forward(\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1309,9 +1311,8 @@ def forward(\n             attention_mask=flat_attention_mask,\n             head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         pooled_output = outputs[1]\n \n@@ -1326,10 +1327,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1354,6 +1351,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1364,9 +1362,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1381,18 +1377,15 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1407,10 +1400,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1453,6 +1442,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1464,9 +1454,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1479,18 +1467,15 @@ def forward(\n \n             [What are token type IDs?](../glossary#token-type-ids)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1517,10 +1502,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1530,22 +1511,6 @@ def forward(\n         )\n \n \n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n __all__ = [\n     \"RobertaForCausalLM\",\n     \"RobertaForMaskedLM\","
        },
        {
            "sha": "e98eddf99bf5e8d6d8bbf05d6c398b448ed3738a",
            "filename": "src/transformers/models/roberta/modular_roberta.py",
            "status": "added",
            "additions": 800,
            "deletions": 0,
            "changes": 800,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodular_roberta.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -0,0 +1,800 @@\n+# coding=utf-8\n+# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n+# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch RoBERTa model.\"\"\"\n+\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import gelu\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import (\n+    CausalLMOutputWithCrossAttentions,\n+    MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutput,\n+    TokenClassifierOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import can_return_tuple\n+from ..bert.modeling_bert import BertCrossAttention, BertEmbeddings, BertLayer, BertModel, BertSelfAttention\n+from .configuration_roberta import RobertaConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class RobertaEmbeddings(BertEmbeddings):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        del self.pad_token_id\n+        del self.position_embeddings\n+\n+        self.padding_idx = config.pad_token_id\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ):\n+        if position_ids is None:\n+            if input_ids is not None:\n+                # Create the position ids from the input token ids. Any padded tokens remain padded.\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n+            else:\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n+\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        batch_size, seq_length = input_shape\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+\n+        embeddings = inputs_embeds + token_type_embeddings\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n+        \"\"\"\n+        We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n+\n+        Args:\n+            inputs_embeds: torch.Tensor\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        input_shape = inputs_embeds.size()[:-1]\n+        sequence_length = input_shape[1]\n+\n+        position_ids = torch.arange(\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+        )\n+        return position_ids.unsqueeze(0).expand(input_shape)\n+\n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n+\n+class RobertaSelfAttention(BertSelfAttention):\n+    pass\n+\n+\n+class RobertaCrossAttention(BertCrossAttention):\n+    pass\n+\n+\n+class RobertaLayer(BertLayer):\n+    pass\n+\n+\n+@auto_docstring\n+class RobertaPreTrainedModel(PreTrainedModel):\n+    config_class = RobertaConfig\n+    base_model_prefix = \"roberta\"\n+    supports_gradient_checkpointing = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": RobertaLayer,\n+        \"attentions\": RobertaSelfAttention,\n+        \"cross_attentions\": RobertaCrossAttention,\n+    }\n+\n+    # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->RobertaLMHead\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, RobertaLMHead):\n+            module.bias.data.zero_()\n+\n+\n+class RobertaModel(BertModel):\n+    def __init__(self, config, add_pooling_layer=True):\n+        super().__init__(self, config)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    RoBERTa Model with a `language modeling` head on top for CLM fine-tuning.\n+    \"\"\"\n+)\n+class RobertaForCausalLM(RobertaPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        if not config.is_decoder:\n+            logger.warning(\"If you want to use `RobertaLMHeadModel` as a standalone, add `is_decoder=True.`\")\n+\n+        self.roberta = RobertaModel(config, add_pooling_layer=False)\n+        self.lm_head = RobertaLMHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.decoder\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.decoder = new_embeddings\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n+            `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n+            ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, RobertaForCausalLM, AutoConfig\n+        >>> import torch\n+\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/roberta-base\")\n+        >>> config = AutoConfig.from_pretrained(\"FacebookAI/roberta-base\")\n+        >>> config.is_decoder = True\n+        >>> model = RobertaForCausalLM.from_pretrained(\"FacebookAI/roberta-base\", config=config)\n+\n+        >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> prediction_logits = outputs.logits\n+        ```\"\"\"\n+        if labels is not None:\n+            use_cache = False\n+\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        lm_loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(prediction_scores.device)\n+            lm_loss = self.loss_function(\n+                prediction_scores,\n+                labels,\n+                vocab_size=self.config.vocab_size,\n+                **kwargs,\n+            )\n+\n+        return CausalLMOutputWithCrossAttentions(\n+            loss=lm_loss,\n+            logits=prediction_scores,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            cross_attentions=outputs.cross_attentions,\n+        )\n+\n+\n+@auto_docstring\n+class RobertaForMaskedLM(RobertaPreTrainedModel):\n+    _tied_weights_keys = [\"lm_head.decoder.weight\", \"lm_head.decoder.bias\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        if config.is_decoder:\n+            logger.warning(\n+                \"If you want to use `RobertaForMaskedLM` make sure `config.is_decoder=False` for \"\n+                \"bi-directional self-attention.\"\n+            )\n+\n+        self.roberta = RobertaModel(config, add_pooling_layer=False)\n+        self.lm_head = RobertaLMHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head.decoder\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head.decoder = new_embeddings\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should be in `[-100, 0, ...,\n+            config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n+            loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_attention_mask,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        prediction_scores = self.lm_head(sequence_output)\n+\n+        masked_lm_loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(prediction_scores.device)\n+            loss_fct = CrossEntropyLoss()\n+            masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n+\n+        return MaskedLMOutput(\n+            loss=masked_lm_loss,\n+            logits=prediction_scores,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class RobertaLMHead(nn.Module):\n+    \"\"\"Roberta Head for masked language modeling.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.decoder = nn.Linear(config.hidden_size, config.vocab_size)\n+        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n+        self.decoder.bias = self.bias\n+\n+    def forward(self, features, **kwargs):\n+        x = self.dense(features)\n+        x = gelu(x)\n+        x = self.layer_norm(x)\n+\n+        # project back to size of vocabulary with bias\n+        x = self.decoder(x)\n+\n+        return x\n+\n+    def _tie_weights(self):\n+        # To tie those two weights if they get disconnected (on TPU or when the bias is resized)\n+        # For accelerate compatibility and to not break backward compatibility\n+        if self.decoder.bias.device.type == \"meta\":\n+            self.decoder.bias = self.bias\n+        else:\n+            self.bias = self.decoder.bias\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    RoBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the\n+    pooled output) e.g. for GLUE tasks.\n+    \"\"\"\n+)\n+class RobertaForSequenceClassification(RobertaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.config = config\n+\n+        self.roberta = RobertaModel(config, add_pooling_layer=False)\n+        self.classifier = RobertaClassificationHead(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        sequence_output = outputs[0]\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        return SequenceClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class RobertaForMultipleChoice(RobertaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.roberta = RobertaModel(config)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.classifier = nn.Linear(config.hidden_size, 1)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors. (See\n+            `input_ids` above)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        \"\"\"\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        flat_position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        flat_token_type_ids = token_type_ids.view(-1, token_type_ids.size(-1)) if token_type_ids is not None else None\n+        flat_attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        flat_inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        outputs = self.roberta(\n+            flat_input_ids,\n+            position_ids=flat_position_ids,\n+            token_type_ids=flat_token_type_ids,\n+            attention_mask=flat_attention_mask,\n+            head_mask=head_mask,\n+            inputs_embeds=flat_inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+        pooled_output = outputs[1]\n+\n+        pooled_output = self.dropout(pooled_output)\n+        logits = self.classifier(pooled_output)\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(reshaped_logits.device)\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class RobertaForTokenClassification(RobertaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.roberta = RobertaModel(config, add_pooling_layer=False)\n+        classifier_dropout = (\n+            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n+        )\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class RobertaClassificationHead(nn.Module):\n+    \"\"\"Head for sentence-level classification tasks.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        classifier_dropout = (\n+            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n+        )\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n+\n+    def forward(self, features, **kwargs):\n+        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n+        x = self.dropout(x)\n+        x = self.dense(x)\n+        x = torch.tanh(x)\n+        x = self.dropout(x)\n+        x = self.out_proj(x)\n+        return x\n+\n+\n+@auto_docstring\n+class RobertaForQuestionAnswering(RobertaPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+\n+        self.roberta = RobertaModel(config, add_pooling_layer=False)\n+        self.qa_outputs = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n+        r\"\"\"\n+        token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+            This parameter can only be used when the model is initialized with `type_vocab_size` parameter with value\n+            >= 2. All the value in this tensor should be always < type_vocab_size.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        \"\"\"\n+        outputs = self.roberta(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            token_type_ids=token_type_ids,\n+            position_ids=position_ids,\n+            head_mask=head_mask,\n+            inputs_embeds=inputs_embeds,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        total_loss = None\n+        if start_positions is not None and end_positions is not None:\n+            # If we are on multi-GPU, split add a dimension\n+            if len(start_positions.size()) > 1:\n+                start_positions = start_positions.squeeze(-1)\n+            if len(end_positions.size()) > 1:\n+                end_positions = end_positions.squeeze(-1)\n+            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n+            ignored_index = start_logits.size(1)\n+            start_positions = start_positions.clamp(0, ignored_index)\n+            end_positions = end_positions.clamp(0, ignored_index)\n+\n+            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n+            start_loss = loss_fct(start_logits, start_positions)\n+            end_loss = loss_fct(end_logits, end_positions)\n+            total_loss = (start_loss + end_loss) / 2\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=total_loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"RobertaForCausalLM\",\n+    \"RobertaForMaskedLM\",\n+    \"RobertaForMultipleChoice\",\n+    \"RobertaForQuestionAnswering\",\n+    \"RobertaForSequenceClassification\",\n+    \"RobertaForTokenClassification\",\n+    \"RobertaModel\",\n+    \"RobertaPreTrainedModel\",\n+]"
        },
        {
            "sha": "0085992d2a9ae56e3158f78e22d1508f12219023",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 490,
            "deletions": 363,
            "changes": 853,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963",
            "patch": "@@ -15,16 +15,17 @@\n # limitations under the License.\n \"\"\"PyTorch RoBERTa-PreLayerNorm model.\"\"\"\n \n-import math\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n+from ...cache_utils import Cache, EncoderDecoderCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -36,27 +37,28 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n+from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_roberta_prelayernorm import RobertaPreLayerNormConfig\n \n \n+if is_torch_flex_attn_available():\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n # Copied from transformers.models.roberta.modeling_roberta.RobertaEmbeddings with Roberta->RobertaPreLayerNorm\n class RobertaPreLayerNormEmbeddings(nn.Module):\n-    \"\"\"\n-    Same as BertEmbeddings with a tiny tweak for positional embeddings indexing.\n-    \"\"\"\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n-    # Copied from transformers.models.bert.modeling_bert.BertEmbeddings.__init__\n     def __init__(self, config):\n         super().__init__()\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n-        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n         self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n \n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -70,37 +72,44 @@ def __init__(self, config):\n             \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n         )\n \n-        # End copy\n         self.padding_idx = config.pad_token_id\n         self.position_embeddings = nn.Embedding(\n             config.max_position_embeddings, config.hidden_size, padding_idx=self.padding_idx\n         )\n \n     def forward(\n-        self, input_ids=None, token_type_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n-    ):\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n         if position_ids is None:\n             if input_ids is not None:\n                 # Create the position ids from the input token ids. Any padded tokens remain padded.\n-                position_ids = create_position_ids_from_input_ids(input_ids, self.padding_idx, past_key_values_length)\n+                position_ids = self.create_position_ids_from_input_ids(\n+                    input_ids, self.padding_idx, past_key_values_length\n+                )\n             else:\n-                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds)\n+                position_ids = self.create_position_ids_from_inputs_embeds(inputs_embeds, self.padding_idx)\n \n         if input_ids is not None:\n             input_shape = input_ids.size()\n         else:\n             input_shape = inputs_embeds.size()[:-1]\n \n-        seq_length = input_shape[1]\n+        batch_size, seq_length = input_shape\n \n         # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n         # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n         # issue #5664\n         if token_type_ids is None:\n             if hasattr(self, \"token_type_ids\"):\n-                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n+                # NOTE: We assume either pos ids to have bsz == 1 (broadcastable) or bsz == effective bsz (input_shape[0])\n+                buffered_token_type_ids = self.token_type_ids.expand(position_ids.shape[0], -1)\n+                buffered_token_type_ids = torch.gather(buffered_token_type_ids, dim=1, index=position_ids)\n+                token_type_ids = buffered_token_type_ids.expand(batch_size, seq_length)\n             else:\n                 token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n \n@@ -116,7 +125,8 @@ def forward(\n         embeddings = self.dropout(embeddings)\n         return embeddings\n \n-    def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n+    @staticmethod\n+    def create_position_ids_from_inputs_embeds(inputs_embeds, padding_idx):\n         \"\"\"\n         We are provided embeddings directly. We cannot infer which are padded so just generate sequential position ids.\n \n@@ -129,24 +139,101 @@ def create_position_ids_from_inputs_embeds(self, inputs_embeds):\n         sequence_length = input_shape[1]\n \n         position_ids = torch.arange(\n-            self.padding_idx + 1, sequence_length + self.padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n+            padding_idx + 1, sequence_length + padding_idx + 1, dtype=torch.long, device=inputs_embeds.device\n         )\n         return position_ids.unsqueeze(0).expand(input_shape)\n \n+    @staticmethod\n+    def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n+        \"\"\"\n+        Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n+        are ignored. This is modified from fairseq's `utils.make_positions`.\n+\n+        Args:\n+            x: torch.Tensor x:\n+\n+        Returns: torch.Tensor\n+        \"\"\"\n+        # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n+        mask = input_ids.ne(padding_idx).int()\n+        incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n+        return incremental_indices.long() + padding_idx\n+\n+\n+# Copied from transformers.models.bert.modeling_bert.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    use_cache: Optional[bool] = None,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(2, 3))\n+\n+    # Relative positional embeddings\n+    if module.position_embedding_type == \"relative_key\" or module.position_embedding_type == \"relative_key_query\":\n+        query_length, key_length = query.shape[2], key.shape[2]\n+        if use_cache:\n+            position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=query.device).view(-1, 1)\n+        else:\n+            position_ids_l = torch.arange(query_length, dtype=torch.long, device=query.device).view(-1, 1)\n+        position_ids_r = torch.arange(key_length, dtype=torch.long, device=query.device).view(1, -1)\n+        distance = position_ids_l - position_ids_r\n+\n+        positional_embedding = module.distance_embedding(distance + module.max_position_embeddings - 1)\n+        positional_embedding = positional_embedding.to(dtype=query.dtype)  # fp16 compatibility\n+\n+        if module.position_embedding_type == \"relative_key\":\n+            relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores\n+        elif module.position_embedding_type == \"relative_key_query\":\n+            relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query, positional_embedding)\n+            relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key, positional_embedding)\n+            attn_weights = attn_weights + relative_position_scores_query + relative_position_scores_key\n+\n+    # Scaling is shifted in case of embeddings being relative\n+    attn_weights = attn_weights * scaling\n+\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n \n # Copied from transformers.models.bert.modeling_bert.BertSelfAttention with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormSelfAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n                 f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n                 f\"heads ({config.num_attention_heads})\"\n             )\n+        self.config = config\n \n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size)\n@@ -161,111 +248,157 @@ def __init__(self, config, position_embedding_type=None, layer_idx=None):\n             self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n         self.is_decoder = config.is_decoder\n+        self.is_causal = is_causal\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n-        query_layer = self.query(hidden_states)\n-        query_layer = query_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-            1, 2\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.attention_head_size)\n+\n+        # get all proj\n+        query_layer = self.query(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        key_layer = self.key(hidden_states).view(*hidden_shape).transpose(1, 2)\n+        value_layer = self.value(hidden_states).view(*hidden_shape).transpose(1, 2)\n+\n+        if past_key_value is not None:\n+            # decoder-only bert can have a simple dynamic cache for example\n+            current_past_key_value = past_key_value\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                current_past_key_value = past_key_value.self_attention_cache\n+\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            key_layer, value_layer = current_past_key_value.update(\n+                key_layer,\n+                value_layer,\n+                self.layer_idx,\n+                {\"cache_position\": cache_position},\n+            )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n         )\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        return attn_output, attn_weights\n \n-        is_updated = False\n-        is_cross_attention = encoder_hidden_states is not None\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                is_updated = past_key_values.is_updated.get(self.layer_idx)\n-                if is_cross_attention:\n-                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n-                    curr_past_key_value = past_key_values.cross_attention_cache\n-                else:\n-                    curr_past_key_value = past_key_values.self_attention_cache\n-            else:\n-                curr_past_key_value = past_key_values\n \n-        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        if is_cross_attention and past_key_values is not None and is_updated:\n-            # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n-            value_layer = curr_past_key_value.layers[self.layer_idx].values\n-        else:\n-            key_layer = self.key(current_states)\n-            key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n-                1, 2\n+# Copied from transformers.models.bert.modeling_bert.BertCrossAttention with Bert->RobertaPreLayerNorm\n+class RobertaPreLayerNormCrossAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None, is_causal=False, layer_idx=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n             )\n-            value_layer = self.value(current_states)\n-            value_layer = value_layer.view(\n-                batch_size, -1, self.num_attention_heads, self.attention_head_size\n-            ).transpose(1, 2)\n-\n-            if past_key_values is not None:\n-                # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n-                cache_position = cache_position if not is_cross_attention else None\n-                key_layer, value_layer = curr_past_key_value.update(\n-                    key_layer, value_layer, self.layer_idx, {\"cache_position\": cache_position}\n-                )\n-                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n-                if is_cross_attention and isinstance(past_key_values, EncoderDecoderCache):\n-                    past_key_values.is_updated[self.layer_idx] = True\n+        self.config = config\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.scaling = self.attention_head_size**-0.5\n \n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n         if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n-            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n-            if past_key_values is not None:\n-                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n-                    -1, 1\n-                )\n-            else:\n-                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n-            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n-            distance = position_ids_l - position_ids_r\n-\n-            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n-            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n-\n-            if self.position_embedding_type == \"relative_key\":\n-                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores\n-            elif self.position_embedding_type == \"relative_key_query\":\n-                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n-                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n-                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in RobertaPreLayerNormModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n \n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+        self.is_causal = is_causal\n+        self.layer_idx = layer_idx\n \n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[EncoderDecoderCache] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        src_len = encoder_hidden_states.shape[1]\n+\n+        q_input_shape = (bsz, tgt_len, -1, self.attention_head_size)\n+        kv_input_shape = (bsz, src_len, -1, self.attention_head_size)\n \n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        # get query proj\n+        query_layer = self.query(hidden_states).view(*q_input_shape).transpose(1, 2)\n+\n+        is_updated = past_key_value.is_updated.get(self.layer_idx) if past_key_value is not None else False\n+        if past_key_value is not None and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].keys\n+            value_layer = past_key_value.cross_attention_cache.layers[self.layer_idx].values\n+        else:\n+            key_layer = self.key(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n+            value_layer = self.value(encoder_hidden_states).view(*kv_input_shape).transpose(1, 2)\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+            if past_key_value is not None:\n+                # save all states to the cache\n+                key_layer, value_layer = past_key_value.cross_attention_cache.update(\n+                    key_layer, value_layer, self.layer_idx\n+                )\n+                # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+                past_key_value.is_updated[self.layer_idx] = True\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.position_embedding_type != \"absolute\":\n+                raise ValueError(\n+                    f\"You are using {self.config._attn_implementation} as attention type. However, non-absolute \"\n+                    'positional embeddings can not work with them. Please load the model with `attn_implementation=\"eager\"`.'\n+                )\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        return context_layer, attention_probs\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout.p,\n+            scaling=self.scaling,\n+            head_mask=head_mask,\n+            # only for relevant for non-absolute positional embeddings\n+            use_cache=past_key_value is not None,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        return attn_output, attn_weights\n \n \n class RobertaPreLayerNormSelfOutput(nn.Module):\n@@ -282,10 +415,14 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n \n \n class RobertaPreLayerNormAttention(nn.Module):\n-    def __init__(self, config, position_embedding_type=None, layer_idx=None):\n+    def __init__(\n+        self, config, position_embedding_type=None, is_causal=False, layer_idx=None, is_cross_attention=False\n+    ):\n         super().__init__()\n-        self.self = RobertaPreLayerNormSelfAttention(\n-            config, position_embedding_type=position_embedding_type, layer_idx=layer_idx\n+        self.is_cross_attention = is_cross_attention\n+        attention_class = RobertaPreLayerNormCrossAttention if is_cross_attention else RobertaPreLayerNormSelfAttention\n+        self.self = attention_class(\n+            config, position_embedding_type=position_embedding_type, is_causal=is_causal, layer_idx=layer_idx\n         )\n         self.output = RobertaPreLayerNormSelfOutput(config)\n         self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -310,30 +447,30 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n         hidden_states_pre_layer_norm = self.LayerNorm(hidden_states)\n-        self_outputs = self.self(\n+        attention_mask = attention_mask if not self.is_cross_attention else encoder_attention_mask\n+        attention_output, attn_weights = self.self(\n             hidden_states_pre_layer_norm,\n-            attention_mask,\n-            head_mask,\n-            encoder_hidden_states,\n-            past_key_values,\n-            output_attentions,\n-            cache_position,\n+            encoder_hidden_states=encoder_hidden_states,\n+            attention_mask=attention_mask,\n+            head_mask=head_mask,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attention_output, hidden_states)\n+        return attention_output, attn_weights\n \n \n class RobertaPreLayerNormIntermediate(nn.Module):\n@@ -372,40 +509,42 @@ def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = RobertaPreLayerNormAttention(config, layer_idx=layer_idx)\n+        self.attention = RobertaPreLayerNormAttention(config, is_causal=config.is_decoder, layer_idx=layer_idx)\n         self.is_decoder = config.is_decoder\n         self.add_cross_attention = config.add_cross_attention\n         if self.add_cross_attention:\n             if not self.is_decoder:\n                 raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n             self.crossattention = RobertaPreLayerNormAttention(\n-                config, position_embedding_type=\"absolute\", layer_idx=layer_idx\n+                config,\n+                position_embedding_type=\"absolute\",\n+                is_causal=False,\n+                layer_idx=layer_idx,\n+                is_cross_attention=True,\n             )\n         self.intermediate = RobertaPreLayerNormIntermediate(config)\n         self.output = RobertaPreLayerNormOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n+        past_key_value: Optional[Cache] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor]:\n-        self_attention_outputs = self.attention(\n+        self_attention_output, _ = self.attention(\n             hidden_states,\n-            attention_mask=attention_mask,\n-            head_mask=head_mask,\n-            output_attentions=output_attentions,\n-            past_key_values=past_key_values,\n+            attention_mask,\n+            head_mask,\n+            past_key_value=past_key_value,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+        attention_output = self_attention_output\n \n         if self.is_decoder and encoder_hidden_states is not None:\n             if not hasattr(self, \"crossattention\"):\n@@ -414,24 +553,21 @@ def forward(\n                     \" by setting `config.add_cross_attention=True`\"\n                 )\n \n-            cross_attention_outputs = self.crossattention(\n-                attention_output,\n-                attention_mask=encoder_attention_mask,\n-                head_mask=head_mask,\n-                encoder_hidden_states=encoder_hidden_states,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                cache_position=cache_position,\n+            cross_attention_output, _ = self.crossattention(\n+                self_attention_output,\n+                None,  # attention_mask\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                **kwargs,\n             )\n-            attention_output = cross_attention_outputs[0]\n-            outputs = outputs + cross_attention_outputs[1:]  # add cross attentions if we output attention weights\n+            attention_output = cross_attention_output\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -441,13 +577,12 @@ def feed_forward_chunk(self, attention_output):\n \n # Copied from transformers.models.bert.modeling_bert.BertEncoder with Bert->RobertaPreLayerNorm\n class RobertaPreLayerNormEncoder(nn.Module):\n-    def __init__(self, config, layer_idx=None):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.layer = nn.ModuleList(\n             [RobertaPreLayerNormLayer(config, layer_idx=i) for i in range(config.num_hidden_layers)]\n         )\n-        self.gradient_checkpointing = False\n \n     def forward(\n         self,\n@@ -458,77 +593,26 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = False,\n-        output_hidden_states: Optional[bool] = False,\n-        return_dict: Optional[bool] = True,\n         cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n-\n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        if use_cache and self.config.is_decoder and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n-\n-        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n-            logger.warning_once(\n-                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-            )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-\n         for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n+                past_key_value=past_key_values,\n                 cache_position=cache_position,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    past_key_values,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n+            past_key_values=past_key_values if use_cache else None,\n         )\n \n \n@@ -550,10 +634,23 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n @auto_docstring\n class RobertaPreLayerNormPreTrainedModel(PreTrainedModel):\n-    config: RobertaPreLayerNormConfig\n+    config_class = RobertaPreLayerNormConfig\n     base_model_prefix = \"roberta_prelayernorm\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"RobertaPreLayerNormEmbeddings\", \"RobertaPreLayerNormSelfAttention\"]\n+    _no_split_modules = [\n+        \"RobertaPreLayerNormEmbeddings\",\n+        \"RobertaPreLayerNormSelfAttention\",\n+        \"RobertaPreLayerNormCrossAttention\",\n+    ]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": RobertaPreLayerNormLayer,\n+        \"attentions\": RobertaPreLayerNormSelfAttention,\n+        \"cross_attentions\": RobertaPreLayerNormCrossAttention,\n+    }\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->RobertaPreLayerNormLMHead\n     def _init_weights(self, module):\n@@ -596,6 +693,7 @@ def __init__(self, config, add_pooling_layer=True):\n         \"\"\"\n         super().__init__(config)\n         self.config = config\n+        self.gradient_checkpointing = False\n \n         self.embeddings = RobertaPreLayerNormEmbeddings(config)\n         self.encoder = RobertaPreLayerNormEncoder(config)\n@@ -620,6 +718,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -631,11 +730,10 @@ def forward(\n         inputs_embeds: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -648,70 +746,35 @@ def forward(\n \n             [What are token type IDs?](../glossary#token-type-ids)\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if self.config.is_decoder:\n             use_cache = use_cache if use_cache is not None else self.config.use_cache\n         else:\n             use_cache = False\n \n-        if input_ids is not None and inputs_embeds is not None:\n-            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        batch_size, seq_length = input_shape\n-        device = input_ids.device if input_ids is not None else inputs_embeds.device\n-\n-        past_key_values_length = 0\n-        if past_key_values is not None:\n-            past_key_values_length = (\n-                past_key_values[0][0].shape[-2]\n-                if not isinstance(past_key_values, Cache)\n-                else past_key_values.get_seq_length()\n+        return_legacy_cache = False\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            logger.warning_once(\n+                \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n+                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n+                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n+            return_legacy_cache = True\n+            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n-        if attention_mask is None:\n-            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if token_type_ids is None:\n-            if hasattr(self.embeddings, \"token_type_ids\"):\n-                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n-                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n-                token_type_ids = buffered_token_type_ids_expanded\n-            else:\n-                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n-\n-        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n-        # ourselves in which case we just need to make it broadcastable to all heads.\n-        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape)\n-\n-        # If a 2D or 3D attention mask is provided for the cross-attention\n-        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n-        if self.config.is_decoder and encoder_hidden_states is not None:\n-            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n-            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n-            if encoder_attention_mask is None:\n-                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n-            encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        if input_ids is not None:\n+            device = input_ids.device\n+            input_shape = input_ids.shape\n         else:\n-            encoder_extended_attention_mask = None\n+            device = inputs_embeds.device\n+            input_shape = inputs_embeds.shape[:-1]\n \n-        # Prepare head mask if needed\n-        # 1.0 in head_mask indicate we keep the head\n-        # attention_probs has shape bsz x n_heads x N x N\n-        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n-        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n-        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+        seq_length = input_shape[1]\n+        past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        if cache_position is None:\n+            cache_position = torch.arange(past_key_values_length, past_key_values_length + seq_length, device=device)\n \n         embedding_output = self.embeddings(\n             input_ids=input_ids,\n@@ -720,34 +783,159 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             past_key_values_length=past_key_values_length,\n         )\n+\n+        attention_mask, encoder_attention_mask = self._create_attention_masks(\n+            input_shape=input_shape,\n+            attention_mask=attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n+            embedding_output=embedding_output,\n+            encoder_hidden_states=encoder_hidden_states,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n         encoder_outputs = self.encoder(\n             embedding_output,\n-            attention_mask=extended_attention_mask,\n+            attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_extended_attention_mask,\n+            encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            **kwargs,\n         )\n         sequence_output = encoder_outputs[0]\n         sequence_output = self.LayerNorm(sequence_output)\n         pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+        if return_legacy_cache:\n+            encoder_outputs.past_key_values = encoder_outputs.past_key_values.to_legacy_cache()\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n             past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n+    # Copied from transformers.models.bert.modeling_bert.BertModel._create_attention_masks\n+    def _create_attention_masks(\n+        self,\n+        input_shape,\n+        attention_mask,\n+        encoder_attention_mask,\n+        embedding_output,\n+        encoder_hidden_states,\n+        cache_position,\n+        past_key_values,\n+    ):\n+        if attention_mask is not None and attention_mask.dim() == 2:\n+            if self.config.is_decoder:\n+                attention_mask = create_causal_mask(\n+                    config=self.config,\n+                    input_embeds=embedding_output,\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                )\n+            else:\n+                attention_mask = self._update_full_mask(\n+                    attention_mask,\n+                    embedding_output,\n+                )\n+        elif attention_mask is not None and attention_mask.dim() == 3:\n+            if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                raise ValueError(\n+                    \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                    f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                )\n+            attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        if encoder_attention_mask is not None:\n+            if encoder_attention_mask.dim() == 2:\n+                encoder_attention_mask = self._update_cross_attn_mask(\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    embedding_output.shape[:2],\n+                    embedding_output,\n+                )\n+            else:\n+                if \"flash\" in self.config._attn_implementation or self.config._attn_implementation == \"flex_attention\":\n+                    raise ValueError(\n+                        \"Passing attention mask with a 3D/4D shape does not work with type \"\n+                        f\"{self.config._attn_implementation} - please use either `sdpa` or `eager` instead.\"\n+                    )\n+                encoder_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+\n+        return attention_mask, encoder_attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_full_mask\n+    def _update_full_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, None],\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        if attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(attention_mask, torch.Tensor):\n+                    attention_mask = make_flex_block_causal_mask(attention_mask, is_causal=False)\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n+\n+        return attention_mask\n+\n+    # Copied from transformers.models.bart.modeling_bart.BartPreTrainedModel._update_cross_attn_mask\n+    def _update_cross_attn_mask(\n+        self,\n+        encoder_hidden_states: Union[torch.Tensor, None],\n+        encoder_attention_mask: Union[torch.Tensor, None],\n+        input_shape: torch.Size,\n+        inputs_embeds: torch.Tensor,\n+    ):\n+        # expand encoder attention mask\n+        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n+            if \"flash\" in self.config._attn_implementation:\n+                encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self.config._attn_implementation == \"sdpa\":\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n+            elif self.config._attn_implementation == \"flex_attention\":\n+                if isinstance(encoder_attention_mask, torch.Tensor):\n+                    encoder_attention_mask = make_flex_block_causal_mask(\n+                        encoder_attention_mask,\n+                        query_length=input_shape[-1],\n+                        is_causal=False,\n+                    )\n+            else:\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask(\n+                    encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1]\n+                )\n+\n+        return encoder_attention_mask\n+\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -778,6 +966,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -790,12 +979,10 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n+        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        **kwargs,\n+        cache_position: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -828,7 +1015,6 @@ def forward(\n \n         >>> prediction_logits = outputs.logits\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         if labels is not None:\n             use_cache = False\n \n@@ -843,9 +1029,9 @@ def forward(\n             encoder_attention_mask=encoder_attention_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            cache_position=cache_position,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -862,10 +1048,6 @@ def forward(\n                 **kwargs,\n             )\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((lm_loss,) + output) if lm_loss is not None else output\n-\n         return CausalLMOutputWithCrossAttentions(\n             loss=lm_loss,\n             logits=prediction_scores,\n@@ -906,6 +1088,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head.decoder = new_embeddings\n \n+    @can_return_tuple\n     @auto_docstring\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaForMaskedLM.forward with ROBERTA->ROBERTA_PRELAYERNORM,Roberta->RobertaPreLayerNorm,roberta->roberta_prelayernorm\n     def forward(\n@@ -919,9 +1102,7 @@ def forward(\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MaskedLMOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -938,8 +1119,6 @@ def forward(\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked), the\n             loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta_prelayernorm(\n             input_ids,\n             attention_mask=attention_mask,\n@@ -949,9 +1128,8 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         prediction_scores = self.lm_head(sequence_output)\n@@ -963,10 +1141,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             masked_lm_loss = loss_fct(prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n \n-        if not return_dict:\n-            output = (prediction_scores,) + outputs[2:]\n-            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n-\n         return MaskedLMOutput(\n             loss=masked_lm_loss,\n             logits=prediction_scores,\n@@ -1025,6 +1199,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaForSequenceClassification.forward with roberta->roberta_prelayernorm\n     def forward(\n@@ -1036,9 +1211,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1055,18 +1228,15 @@ def forward(\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta_prelayernorm(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         sequence_output = outputs[0]\n         logits = self.classifier(sequence_output)\n@@ -1096,10 +1266,6 @@ def forward(\n                 loss_fct = BCEWithLogitsLoss()\n                 loss = loss_fct(logits, labels)\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return SequenceClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1121,6 +1287,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1131,9 +1298,7 @@ def forward(\n         position_ids: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, num_choices, sequence_length)`):\n@@ -1166,7 +1331,6 @@ def forward(\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n \n         flat_input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n@@ -1186,9 +1350,8 @@ def forward(\n             attention_mask=flat_attention_mask,\n             head_mask=head_mask,\n             inputs_embeds=flat_inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n         pooled_output = outputs[1]\n \n@@ -1203,10 +1366,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(reshaped_logits, labels)\n \n-        if not return_dict:\n-            output = (reshaped_logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return MultipleChoiceModelOutput(\n             loss=loss,\n             logits=reshaped_logits,\n@@ -1231,6 +1390,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaForTokenClassification.forward with roberta->roberta_prelayernorm\n     def forward(\n@@ -1242,9 +1402,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], TokenClassifierOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1259,18 +1417,15 @@ def forward(\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta_prelayernorm(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1285,10 +1440,6 @@ def forward(\n             loss_fct = CrossEntropyLoss()\n             loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n \n-        if not return_dict:\n-            output = (logits,) + outputs[2:]\n-            return ((loss,) + output) if loss is not None else output\n-\n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n@@ -1332,6 +1483,7 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @can_return_tuple\n     @auto_docstring\n     # Copied from transformers.models.roberta.modeling_roberta.RobertaForQuestionAnswering.forward with roberta->roberta_prelayernorm\n     def forward(\n@@ -1344,9 +1496,7 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], QuestionAnsweringModelOutput]:\n         r\"\"\"\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1359,18 +1509,15 @@ def forward(\n \n             [What are token type IDs?](../glossary#token-type-ids)\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         outputs = self.roberta_prelayernorm(\n             input_ids,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n             inputs_embeds=inputs_embeds,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n+            **kwargs,\n         )\n \n         sequence_output = outputs[0]\n@@ -1397,10 +1544,6 @@ def forward(\n             end_loss = loss_fct(end_logits, end_positions)\n             total_loss = (start_loss + end_loss) / 2\n \n-        if not return_dict:\n-            output = (start_logits, end_logits) + outputs[2:]\n-            return ((total_loss,) + output) if total_loss is not None else output\n-\n         return QuestionAnsweringModelOutput(\n             loss=total_loss,\n             start_logits=start_logits,\n@@ -1410,22 +1553,6 @@ def forward(\n         )\n \n \n-def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_length=0):\n-    \"\"\"\n-    Replace non-padding symbols with their position numbers. Position numbers begin at padding_idx+1. Padding symbols\n-    are ignored. This is modified from fairseq's `utils.make_positions`.\n-\n-    Args:\n-        x: torch.Tensor x:\n-\n-    Returns: torch.Tensor\n-    \"\"\"\n-    # The series of casts and type-conversions here are carefully balanced to both work with ONNX export and XLA.\n-    mask = input_ids.ne(padding_idx).int()\n-    incremental_indices = (torch.cumsum(mask, dim=1).type_as(mask) + past_key_values_length) * mask\n-    return incremental_indices.long() + padding_idx\n-\n-\n __all__ = [\n     \"RobertaPreLayerNormForCausalLM\",\n     \"RobertaPreLayerNormForMaskedLM\","
        },
        {
            "sha": "d7614e59c2d68e4ce2aa709184f29227d826f972",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "f9206cae9ae818cacc7c6dbc3ecd69ab8f525bce",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 26,
            "deletions": 23,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "9d9cc95059a0dba874214d796815d94b54d4725f",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 24,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "bb2a8649ce9bfc0b7c54fb1ac73bd1666491c6f0",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "cedd1fabbb3fa5fba27048706115687e4664622c",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "46265698671140851e2f8b1ef00cef60a2cc044c",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "ab0d77b5623e5475c77568ceb3600724a2f58785",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "23fcd7c3227ee9edd70cbc83fdfd77e84a8e2a09",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "00f31596e688ab12bbfa6fe40510a6b9eeaf3bcf",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "60b14e35f781d25186553e3167a410d6699e2d45",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 592,
            "deletions": 641,
            "changes": 1233,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "510f522c93b2326a72e469f8b22495d86df24028",
            "filename": "src/transformers/models/xlm_roberta/modular_xlm_roberta.py",
            "status": "added",
            "additions": 559,
            "deletions": 0,
            "changes": 559,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodular_xlm_roberta.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "522d63aad884e09d7b6918bd82d3b79cc0917776",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 587,
            "deletions": 646,
            "changes": 1233,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "d4937d424d3173a377a1223ed43aab20f431fe09",
            "filename": "src/transformers/models/xlm_roberta_xl/modular_xlm_roberta_xl.py",
            "status": "added",
            "additions": 777,
            "deletions": 0,
            "changes": 777,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodular_xlm_roberta_xl.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "eaf1362d3664001352721cfa33779e23a41d11a9",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 487,
            "deletions": 356,
            "changes": 843,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "193143d7b46ad215ac72636b32b373511ad5feee",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "19094754f8bbfa8ad585dd83b2d66dd6b4643a55",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 132,
            "deletions": 67,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "eecb9205df3ef129e6ce2d0218fea2bb342db38a",
            "filename": "tests/models/bert_generation/test_modeling_bert_generation.py",
            "status": "modified",
            "additions": 129,
            "deletions": 3,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "59f86c88cd6c744d66008a891d83ee95595959f9",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 130,
            "deletions": 8,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "3a1823cc8c01834c09950e56f631d450f9e7f7f2",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 127,
            "deletions": 4,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "a6b12a9f65ae5340fd8c73270df5f78920a8f7a7",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "a500a32e3236dc56215273941d82eb028828c1a4",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "db2215696fd7a142a8bcf761eb6dae4ca749bc5b",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "009e9dfc22c1169cdbf1c0985b669f27c55616c5",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 129,
            "deletions": 8,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "7605be9e2c841a068046131db22909342e939675",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 127,
            "deletions": 6,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "23a6017168a335ac8f0b5b58d7746e2b6e7217f4",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 136,
            "deletions": 0,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "a19c6a13d2200b9b16f3442145534c689ee0cec8",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "e6d0a66c961beefe91ca7e697f6606c298259b8e",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "6ab20ba5feb071a4a4c87972f34198557f9d9e1b",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 244,
            "deletions": 6,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "298c7ad3a27ba9d70202650679a6b656e00a9790",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 130,
            "deletions": 5,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        },
        {
            "sha": "8d6325bfe0a06e7b5bff74f23e746f1ef9312b78",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 21,
            "deletions": 5,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/155f7e2e6237bc4f623aecf9bfd97442d5723963/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=155f7e2e6237bc4f623aecf9bfd97442d5723963"
        }
    ],
    "stats": {
        "total": 20016,
        "additions": 13160,
        "deletions": 6856
    }
}