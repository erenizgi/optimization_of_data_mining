{
    "author": "co63oc",
    "message": "Fix typos in strings and comments (#37799)",
    "sha": "d5fa7d2d199b41711a2e16aec84ee54613633898",
    "files": [
        {
            "sha": "fced0e5758fda0e421daa5028077730fa31e67b9",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1146,9 +1146,9 @@ def stft(frames: np.array, windowing_function: np.array, fft_window_size: Option\n             tutorial]https://download.ni.com/evaluation/pxi/Understanding%20FFTs%20and%20Windowing.pdf\n         fft_window_size (`int`, *optional*):\n             Size of the window om which the Fourier transform is applied. This controls the frequency resolution of the\n-            spectrogram. 400 means that the fourrier transform is computed on windows of 400 samples. The number of\n+            spectrogram. 400 means that the fourier transform is computed on windows of 400 samples. The number of\n             frequency bins (`nb_frequency_bins`) used to divide the window into equal strips is equal to\n-            `(1+fft_window_size)//2`. An increase of the fft_window_size slows the calculus time proportionnally.\n+            `(1+fft_window_size)//2`. An increase of the fft_window_size slows the calculus time proportionally.\n \n     Example:\n "
        },
        {
            "sha": "f5850a864c00002bf9e3e8f10d109c412ad16710",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -850,7 +850,7 @@ def finalize(\n                     beam_hyp.add(final_tokens, final_score, beam_indices=beam_index, generated_len=generated_len)\n                     ids_collect.append(beam_id)\n \n-            # due to overly complex constraints or other factors, sometimes we can't gaurantee a successful\n+            # due to overly complex constraints or other factors, sometimes we can't guarantee a successful\n             # generation. In these cases we simply return the highest scoring outputs.\n             if len(ids_collect) < self.num_beam_hyps_to_keep:\n                 for beam_id in range(self.num_beams):"
        },
        {
            "sha": "12f612bd26c4dbe1ca8206d19235c469e3002ddf",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -192,7 +192,7 @@ class GenerationConfig(PushToHubMixin):\n             our [cache documentation](https://huggingface.co/docs/transformers/en/kv_cache) for further information.\n         cache_config (`CacheConfig` or `dict`, *optional*, default to `None`):\n             Arguments used in the key-value cache class can be passed in `cache_config`. Can be passed as a `Dict` and\n-            it will be converted to its repsective `CacheConfig` internally.\n+            it will be converted to its respective `CacheConfig` internally.\n             Otherwise can be passed as a `CacheConfig` class matching the indicated `cache_implementation`.\n         return_legacy_cache (`bool`, *optional*, default to `True`):\n             Whether to return the legacy or new format of the cache when `DynamicCache` is used by default.\n@@ -235,7 +235,7 @@ class GenerationConfig(PushToHubMixin):\n             The parameter for repetition penalty. 1.0 means no penalty. See [this\n             paper](https://arxiv.org/pdf/1909.05858.pdf) for more details.\n         encoder_repetition_penalty (`float`, *optional*, defaults to 1.0):\n-            The paramater for encoder_repetition_penalty. An exponential penalty on sequences that are not in the\n+            The parameter for encoder_repetition_penalty. An exponential penalty on sequences that are not in the\n             original input. 1.0 means no penalty.\n         length_penalty (`float`, *optional*, defaults to 1.0):\n             Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n@@ -385,7 +385,7 @@ class GenerationConfig(PushToHubMixin):\n             inference.\n         disable_compile (`bool`, *optional*):\n             Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when\n-            specific criteria are met, including using a compileable cache. Please open an issue if you find the\n+            specific criteria are met, including using a compilable cache. Please open an issue if you find the\n             need to use this flag.\n \n         > Wild card\n@@ -710,7 +710,7 @@ def validate(self, is_init=False):\n                     UserWarning,\n                 )\n \n-        # 3. detect incorrect paramaterization specific to advanced beam modes\n+        # 3. detect incorrect parameterization specific to advanced beam modes\n         else:\n             # constrained beam search\n             if self.constraints is not None or self.force_words_ids is not None:"
        },
        {
            "sha": "fdb0e7a26f6f62c046e7a13990d5e1081365c146",
            "filename": "src/transformers/generation/flax_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_logits_process.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -271,7 +271,7 @@ def __call__(self, input_ids: jnp.ndarray, scores: jnp.ndarray, cur_len: int) ->\n \n class FlaxSuppressTokensAtBeginLogitsProcessor(FlaxLogitsProcessor):\n     r\"\"\"\n-    [`FlaxLogitsProcessor`] supressing a list of tokens as soon as the `generate` function starts generating using\n+    [`FlaxLogitsProcessor`] suppressing a list of tokens as soon as the `generate` function starts generating using\n     `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are not sampled at the\n     beginning of the generation.\n "
        },
        {
            "sha": "34c7ea532cc9d636ffd3aa5131f5f80a65fd75e9",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -543,7 +543,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n class MinPLogitsWarper(LogitsProcessor):\n     \"\"\"\n     [`LogitsProcessor`] that performs min-p, i.e. keeps all tokens that are above a minimum probability, scaled by the\n-    probability of the most likely token. As a result, the filter becomes more agressive in the presence of\n+    probability of the most likely token. As a result, the filter becomes more aggressive in the presence of\n     high-probability tokens, which is a sign of a confident output that we shouldn't deviate from.\n \n     Often used together with [`TemperatureLogitsWarper`]. Used as an alternative to [`TopPLogitsWarper`] and\n@@ -738,7 +738,7 @@ class EpsilonLogitsWarper(LogitsProcessor):\n \n     >>> # With epsilon sampling, the output gets restricted to high-probability tokens. Note that this is similar to\n     >>> # Top P sampling, which restricts tokens based on their cumulative probability.\n-    >>> # Pro tip: The paper recomends using `epsilon_cutoff` values between 3e-4 and 9e-4\n+    >>> # Pro tip: The paper recommends using `epsilon_cutoff` values between 3e-4 and 9e-4\n     >>> outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)\n     >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n     A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n@@ -819,7 +819,7 @@ class EtaLogitsWarper(LogitsProcessor):\n \n     >>> # With eta sampling, the output gets restricted to high-probability tokens. You can see it as a dynamic form of\n     >>> # epsilon sampling that adapts its cutoff probability based on the entropy (high entropy = lower cutoff).\n-    >>> # Pro tip: The paper recomends using `eta_cutoff` values between 3e-4 to 4e-3\n+    >>> # Pro tip: The paper recommends using `eta_cutoff` values between 3e-4 to 4e-3\n     >>> outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)\n     >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n     A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n@@ -1348,7 +1348,7 @@ class PrefixConstrainedLogitsProcessor(LogitsProcessor):\n     >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n     Alice and Bob are friends\n \n-    >>> # We can contrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.\n+    >>> # We can constrain it with `prefix_allowed_tokens_fn` to force a certain behavior based on a prefix.\n     >>> # For instance, we can force an entire entity to be generated when its beginning is detected.\n     >>> entity = tokenizer(\" Bob Marley\", return_tensors=\"pt\").input_ids[0]  # 3 tokens\n     >>> def prefix_allowed_tokens_fn(batch_id, input_ids):\n@@ -1791,7 +1791,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n \n class SuppressTokensAtBeginLogitsProcessor(LogitsProcessor):\n     r\"\"\"\n-    [`SuppressTokensAtBeginLogitsProcessor`] supresses a list of tokens as soon as the `generate` function starts\n+    [`SuppressTokensAtBeginLogitsProcessor`] suppresses a list of tokens as soon as the `generate` function starts\n     generating using `begin_index` tokens. This should ensure that the tokens defined by `begin_suppress_tokens` are\n     not generated at the beginning. Originally created for\n     [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper).\n@@ -2642,7 +2642,7 @@ def update_scores(self, scores: torch.FloatTensor, g_values: torch.FloatTensor)\n         We assume that the scores are in the log space.\n         Args:\n             scores (`torch.FloatTensor`): Scores (batch_size, vocab_size).\n-            g_values (`torch.FloatTensor`): G valus (batch_size, vocab_size, depth).\n+            g_values (`torch.FloatTensor`): G values (batch_size, vocab_size, depth).\n \n         Returns:\n             Updated scores (batch_size, vocab_size).\n@@ -2668,7 +2668,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         if self.debug_mode:\n             scores = torch.ones_like(scores)\n \n-        # Currently indices is just a arange to compute watermarking on the desnse logits.\n+        # Currently indices is just a arange to compute watermarking on the dense logits.\n         all_indices = torch.stack([torch.arange(vocab_size, device=self.device) for _ in range(batch_size)])\n \n         if self.state is None:"
        },
        {
            "sha": "8302319d981b101215ddc50497b6eb5f52216bfb",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -343,7 +343,7 @@ def _len_greater_than_cur_len():\n                 )\n \n             def _match_found():\n-                # Finaly, runs the actual comparison. Can only be called if the previous comparisons do not yield\n+                # Finally, runs the actual comparison. Can only be called if the previous comparisons do not yield\n                 # an answer (otherwise we get indexing exceptions)\n                 compare_len = self.bad_word_seqs_len[bad_word_seq_number] - 1\n                 return tf.cond("
        },
        {
            "sha": "510186cafc048b139282d2d9cf088781a398a604",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -962,7 +962,7 @@ def generate(\n                 raise ValueError(\n                     \"Beam search decoding cannot return more sequences than it has beams. Please set num_beams >=\"\n                     f\" num_return_sequences, got {generation_config.num_beams} and\"\n-                    f\" {generation_config.num_return_sequences} (respectivelly)\"\n+                    f\" {generation_config.num_return_sequences} (respectively)\"\n                 )\n \n             # 11. broadcast inputs to the desired number of beams\n@@ -994,7 +994,7 @@ def generate(\n                 raise ValueError(\n                     \"Beam search decoding cannot return more sequences than it has beams. Please set num_beams >=\"\n                     f\" num_return_sequences, got {generation_config.num_beams} and\"\n-                    f\" {generation_config.num_return_sequences} (respectivelly)\"\n+                    f\" {generation_config.num_return_sequences} (respectively)\"\n                 )\n \n             # 11. prepare logits warper\n@@ -1626,7 +1626,7 @@ def greedy_search(\n         )\n         use_cache = model_kwargs.pop(\"use_cache\", self.generation_config.use_cache)\n         use_xla = not tf.executing_eagerly()\n-        # TODO (Joao): fix cache format or find programatic way to detect cache index\n+        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n         # GPT2 and other models has a slightly different cache structure, with a different batch axis\n         model_name = str(self.decoder) if \"EncoderDecoder\" in str(self) else str(self)\n         cache_batch_axis = 1 if any(model_prefix in model_name for model_prefix in (\"TFGPT2\", \"TFCTRL\")) else 0\n@@ -1910,7 +1910,7 @@ def sample(\n         )\n         use_cache = model_kwargs.pop(\"use_cache\", self.generation_config.use_cache)\n         use_xla = not tf.executing_eagerly()\n-        # TODO (Joao): fix cache format or find programatic way to detect cache index\n+        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n         # GPT2 and other models has a slightly different cache structure, with a different batch axis\n         model_name = str(self.decoder) if \"EncoderDecoder\" in str(self) else str(self)\n         cache_batch_axis = 1 if any(model_prefix in model_name for model_prefix in (\"TFGPT2\", \"TFCTRL\")) else 0\n@@ -2253,7 +2253,7 @@ def unflatten_beam_dim(tensor, num_beams, batch_axis=0):\n \n         use_cache = model_kwargs.pop(\"use_cache\", self.generation_config.use_cache)\n         use_xla = not tf.executing_eagerly()\n-        # TODO (Joao): fix cache format or find programatic way to detect cache index\n+        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n         # GPT2 and other models has a slightly different cache structure, with a different batch axis\n         model_name = str(self.decoder) if \"EncoderDecoder\" in str(self) else str(self)\n         cache_batch_axis = 1 if any(model_prefix in model_name for model_prefix in (\"TFGPT2\", \"TFCTRL\")) else 0\n@@ -2788,7 +2788,7 @@ def gather_fn(tensor):\n         model_kwargs.pop(\"use_cache\", None)\n \n         use_xla = not tf.executing_eagerly()\n-        # TODO (Joao): fix cache format or find programatic way to detect cache index\n+        # TODO (Joao): fix cache format or find programmatic way to detect cache index\n         # GPT2 and other models has a slightly different cache structure, with a different batch axis\n         model_name = str(self.decoder) if \"EncoderDecoder\" in str(self) else str(self)\n         cache_batch_axis = 1 if any(model_prefix in model_name for model_prefix in (\"TFGPT2\", \"TFCTRL\")) else 0"
        },
        {
            "sha": "f2733ce9cb153d086d82841827b3f2eec9ae20da",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -362,7 +362,7 @@ class GenerationMixin:\n            inherit from `GenerationMixin` to benefit from all generation-related automation in our codebase;\n         - `BarkModel` has a custom `generate` method and one of its inner models calls `GenerationMixin.generate`.\n             However, its `generate` does not share the same interface as `GenerationMixin.generate`. In this case,\n-            `BarkModel` shoud NOT inherit from `GenerationMixin`, as it breaks the `generate` interface.\n+            `BarkModel` should NOT inherit from `GenerationMixin`, as it breaks the `generate` interface.\n \n     The class exposes [`~generation.GenerationMixin.generate`], which can be used for:\n         - *greedy decoding* if `num_beams=1` and `do_sample=False`\n@@ -392,7 +392,7 @@ def _cache_dependant_input_preparation(\n         - Exception 1: when passing input_embeds, input_ids may be missing entries\n         - Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n         - Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        - Excpetion 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n+        - Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n           generate the first token for each sequence. Later use the generated Input ids for continuation.\n \n         The current implementation does not rely on ``self`` and could be\n@@ -967,7 +967,7 @@ def _get_candidate_generator(\n                     assistant_model=assistant_model,\n                     assistant_prune_lm_head=True,  # prune LM head of assistant model\n                 )\n-                # Since we prune the LM head, we cannot use the repetition penalty on the assistant model due to mismaches between token ids and logits index\n+                # Since we prune the LM head, we cannot use the repetition penalty on the assistant model due to mismatches between token ids and logits index\n                 assistant_model.generation_config.repetition_penalty = None\n                 candidate_generator = UniversalSpeculativeDecodingGenerator(\n                     input_ids=input_ids,"
        },
        {
            "sha": "5dbff5e9b0491defdbe2e71d98033f917479835d",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -171,7 +171,7 @@ def find_tied_parameters(model: \"nn.Module\", **kwargs):\n     ```\n     \"\"\"\n \n-    # get ALL model parameters and thier names\n+    # get ALL model parameters and their names\n     all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n \n     # get ONLY unique named parameters,\n@@ -187,7 +187,7 @@ def find_tied_parameters(model: \"nn.Module\", **kwargs):\n     for tied_param_name in tied_param_names:\n         tied_param = all_named_parameters[tied_param_name]\n         for param_name, param in no_duplicate_named_parameters.items():\n-            # compare if parameters are the same, if so, group thier names together\n+            # compare if parameters are the same, if so, group their names together\n             if param is tied_param:\n                 if param_name not in tied_param_groups:\n                     tied_param_groups[param_name] = []"
        },
        {
            "sha": "a5bae1fe9b7e24e8b40261c2813a5d3ca2295af0",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -329,7 +329,7 @@ def generate(\n         This util function is designed to test exported models by simulating the generation process.\n         It processes the input prompt tokens sequentially (no parallel prefill).\n         This generate function is not intended to replace the original `generate` method, and the support\n-        for leveraging the original `generate` is potentially planed!\n+        for leveraging the original `generate` is potentially planned!\n \n         Args:\n             exported_program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`."
        },
        {
            "sha": "fb0f604bfefce179f931ebb01c96a790a01a4336",
            "filename": "src/transformers/integrations/hqq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhqq.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -28,7 +28,7 @@ def autoname_modules(model):\n         module.name = name\n \n \n-# Get the linear_tag from a modul name. For example: model.layers.31.self_attn.k_proj -> self_attn.k_proj\n+# Get the linear_tag from a module name. For example: model.layers.31.self_attn.k_proj -> self_attn.k_proj\n def name_to_linear_tag(name):\n     return \".\".join([n for n in name.split(\".\") if ((n not in [\"model\", \"layers\"]) and (not n.isnumeric()))])\n \n@@ -86,9 +86,9 @@ def prepare_for_hqq_linear(model, quantization_config=None, modules_to_not_conve\n     \"\"\"\n     Prepares nn.Linear layers for HQQ quantization.\n     Since each layer type can have separate quantization parameters, we need to do the following:\n-    1- tag each module with its neme via autoname_modules()\n+    1- tag each module with its name via autoname_modules()\n     2- Extract linear_tags (e.g. ['self_attn.q_proj', ...])\n-    3- Map quantization parameters as a dictionary linear_tag -> quant_params as HQQLinear exepects it, this is referred to as patch_params\n+    3- Map quantization parameters as a dictionary linear_tag -> quant_params as HQQLinear expects it, this is referred to as patch_params\n     \"\"\"\n \n     modules_to_not_convert = [] if modules_to_not_convert is None else modules_to_not_convert"
        },
        {
            "sha": "00414afadb9a1f4bc1654110826d40ec1c954320",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -160,7 +160,7 @@ def distribute_module(\n     output_fn=None,\n ) -> nn.Module:\n     \"\"\"\n-    Copy pasted from torch's function but we remove the communications (partitionning)\n+    Copy pasted from torch's function but we remove the communications (partitioning)\n     as well as buffer registering that is similarly not efficient.\n     \"\"\"\n     if len(module._forward_pre_hooks) == 0:\n@@ -225,7 +225,7 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-        # this op cannot be asynch, otherwise it completely breaks the outputs of models\n+        # this op cannot be async, otherwise it completely breaks the outputs of models\n         torch.distributed.all_reduce(outputs[0], op=torch.distributed.ReduceOp.SUM, async_op=False)\n         return outputs\n "
        },
        {
            "sha": "0481312af8a8a3f82488434d9a1ded2ee4ec1233",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -343,7 +343,7 @@ def forward(self, outputs, targets):\n \n         # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n         # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+        # The 1 is a constant that doesn't change the matching, it can be omitted.\n         class_cost = -out_prob[:, target_ids]\n \n         # Compute the L1 cost between boxes"
        },
        {
            "sha": "228a2aa3c76dee81d6478a3585ef275e674c3bb6",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -99,7 +99,7 @@ def forward(self, outputs, targets):\n         target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n         # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n         # but approximate it in 1 - proba[target class].\n-        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+        # The 1 is a constant that doesn't change the matching, it can be omitted.\n         if self.use_focal_loss:\n             out_prob = F.sigmoid(outputs[\"logits\"].flatten(0, 1))\n             out_prob = out_prob[:, target_ids]"
        },
        {
            "sha": "b761615ac2eaf93dc79d0430474f8ccf50e89fab",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -593,7 +593,7 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.Tensor:\n \n class AlignVisionEncoder(nn.Module):\n     r\"\"\"\n-    Forward propogates the embeddings through each vision encoder (EfficientNet) block.\n+    Forward propagates the embeddings through each vision encoder (EfficientNet) block.\n \n     Args:\n         config ([`AlignVisionConfig`]):"
        },
        {
            "sha": "ff0db00d7667673f27fdf04fca2af9b326f86673",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -36,7 +36,7 @@ class AlignProcessorKwargs(ProcessingKwargs, total=False):\n class AlignProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an ALIGN processor which wraps [`EfficientNetImageProcessor`] and\n-    [`BertTokenizer`]/[`BertTokenizerFast`] into a single processor that interits both the image processor and\n+    [`BertTokenizer`]/[`BertTokenizerFast`] into a single processor that inherits both the image processor and\n     tokenizer functionalities. See the [`~AlignProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n     information.\n     The preferred way of passing kwargs is as a dictionary per modality, see usage example below."
        },
        {
            "sha": "8a4e9cd8e4d6e786979481ec96a19783788de69a",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1936,7 +1936,7 @@ def forward(\n         params = None\n         if future_values is not None:\n             # outputs.last_hidden_state and trend\n-            # loc is 4rd last and scale is 3rd last output\n+            # loc is 4th last and scale is 3rd last output\n             params = self.output_params(outputs[0] + outputs[1])\n             distribution = self.output_distribution(params, loc=outputs[-3], scale=outputs[-2])\n "
        },
        {
            "sha": "a7832a53d55d452ed3aa0eec570a05c09a8e98e5",
            "filename": "src/transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -164,7 +164,7 @@ def get_encoder_attention_layer_array(layer_index: int, name: str, original_shap\n     new_model = BertForMaskedLM.from_pretrained(pytorch_dump_path)\n     print(new_model.eval())\n \n-    print(\"Model conversion was done sucessfully!\")\n+    print(\"Model conversion was done successfully!\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "c4ce602548da5b8946eeacc9d0dfdb818e37203a",
            "filename": "src/transformers/models/blip_2/configuration_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fconfiguration_blip_2.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -235,7 +235,7 @@ class Blip2Config(PretrainedConfig):\n         num_query_tokens (`int`, *optional*, defaults to 32):\n             The number of query tokens passed through the Transformer.\n         image_text_hidden_size (`int`, *optional*, defaults to 256):\n-            Dimentionality of the hidden state of the image-text fusion layer.\n+            Dimensionality of the hidden state of the image-text fusion layer.\n \n         image_token_index (`int`, *optional*):\n             Token index of special image token."
        },
        {
            "sha": "650ef15fb72abbeabba05339d5f7963ed42998a3",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -899,7 +899,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwriten because of the fixed-shape attention mask creation\n+        # Overwritten because of the fixed-shape attention mask creation\n \n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries"
        },
        {
            "sha": "7f9c3f600158e95ff5dc8c5a2cef4c6985e8c246",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -49,17 +49,17 @@ class ClapFeatureExtractor(SequenceFeatureExtractor):\n             The sampling rate at which the audio files should be digitalized expressed in hertz (Hz). This only serves\n             to warn users if the audio fed to the feature extractor does not have the same sampling rate.\n         hop_length (`int`,*optional*, defaults to 480):\n-            Length of the overlaping windows for the STFT used to obtain the Mel Spectrogram. The audio will be split\n+            Length of the overlapping windows for the STFT used to obtain the Mel Spectrogram. The audio will be split\n             in smaller `frames` with a step of `hop_length` between each frame.\n         max_length_s (`int`, *optional*, defaults to 10):\n             The maximum input length of the model in seconds. This is used to pad the audio.\n         fft_window_size (`int`, *optional*, defaults to 1024):\n             Size of the window (in samples) on which the Fourier transform is applied. This controls the frequency\n-            resolution of the spectrogram. 400 means that the fourrier transform is computed on windows of 400 samples.\n+            resolution of the spectrogram. 400 means that the fourier transform is computed on windows of 400 samples.\n         padding_value (`float`, *optional*, defaults to 0.0):\n             Padding value used to pad the audio. Should correspond to silences.\n         return_attention_mask (`bool`, *optional*, defaults to `False`):\n-            Whether or not the model should return the attention masks coresponding to the input.\n+            Whether or not the model should return the attention masks corresponding to the input.\n         frequency_min (`float`, *optional*, defaults to 0):\n             The lowest frequency of interest. The STFT will not be computed for values below this.\n         frequency_max (`float`, *optional*, defaults to 14000):\n@@ -141,7 +141,7 @@ def to_dict(self) -> Dict[str, Any]:\n         Serializes this instance to a Python dictionary.\n \n         Returns:\n-            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance, excpet for the\n+            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance, except for the\n             mel filter banks, which do not need to be saved or printed as they are too long.\n         \"\"\"\n         output = copy.deepcopy(self.__dict__)"
        },
        {
            "sha": "1988bb95c6deacadc00d6f011ae0255986520d58",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1067,7 +1067,7 @@ def forward(\n CLAP_AUDIO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returnes by the [`ClapFeatureExtractor`] class that you can also\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n             retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n@@ -1105,7 +1105,7 @@ def forward(\n \n             [What are position IDs?](../glossary#position-ids)\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returnes by the [`ClapFeatureExtractor`] class that you can also\n+            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n             retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         return_loss (`bool`, *optional*):\n             Whether or not to return the contrastive loss."
        },
        {
            "sha": "1c6226c458b59927127e70c166e9f1bce02f9175",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -127,7 +127,7 @@ def __init__(\n         if kwargs.pop(\"add_bos_token\", False):\n             model_id = kwargs.pop(\"name_or_path\", \"\")\n             raise ValueError(\n-                \"Currenty GPT2's fast tokenizer does NOT support adding a BOS token. \"\n+                \"Currently GPT2's fast tokenizer does NOT support adding a BOS token. \"\n                 \"Instead you should use GPT2's slow tokenizer class `CodeGenTokenizer` as follows: \\n\"\n                 f\"`CodeGenTokenizer.from_pretrained('{model_id}')`\\nor\\n\"\n                 f\"`AutoTokenizer.from_pretrained('{model_id}', use_fast=False)`\\n\""
        },
        {
            "sha": "85a1d4b64ac008162fc048c681b0805756ac299d",
            "filename": "src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -277,7 +277,7 @@ def final():\n \n def convert_cvt_checkpoint(cvt_model, image_size, cvt_file_name, pytorch_dump_folder):\n     \"\"\"\n-    Fucntion to convert the microsoft cvt checkpoint to huggingface checkpoint\n+    Function to convert the microsoft cvt checkpoint to huggingface checkpoint\n     \"\"\"\n     img_labels_file = \"imagenet-1k-id2label.json\"\n     num_labels = 1000"
        },
        {
            "sha": "32746a38dd0617c67ee8e2d028b8a82faf3b9512",
            "filename": "src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -58,7 +58,7 @@\n     # activation function weight\n     r\"transformer\\.encoder\\.layers\\.(\\d+)\\.activation\\.weight\": r\"encoder.layers.\\1.activation_fn.weight\",\n     #########################################################################################################################################\n-    # decoder layers: 2 times output projection, 2 feedforward neural networks and 3 layernorms + activiation function weight\n+    # decoder layers: 2 times output projection, 2 feedforward neural networks and 3 layernorms + activation function weight\n     r\"transformer\\.decoder\\.layers\\.(\\d+)\\.self_attn\\.out_proj\\.(bias|weight)\": r\"decoder.layers.\\1.self_attn.self_attn.output_proj.\\2\",\n     r\"transformer\\.decoder\\.layers\\.(\\d+)\\.cross_attn\\.out_proj\\.(bias|weight)\": r\"decoder.layers.\\1.cross_attn.cross_attn.output_proj.\\2\",\n     # FFNs\n@@ -144,7 +144,7 @@ def write_model(model_name, pretrained_model_weights_path, pytorch_dump_folder_p\n     config.label2id = {v: k for k, v in id2label.items()}\n     # load original model from local path\n     loaded = torch.load(pretrained_model_weights_path, map_location=torch.device(\"cpu\"), weights_only=True)[\"model\"]\n-    # Renaming the original model state dictionary to HF compatibile\n+    # Renaming the original model state dictionary to HF compatible\n     all_keys = list(loaded.keys())\n     new_keys = convert_old_keys_to_new_keys(all_keys)\n     state_dict = {}"
        },
        {
            "sha": "f1a6ef7093b732cb46662fa679f70ee8790f527d",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1297,7 +1297,7 @@ def __init__(\n     ):\n         \"\"\"\n         Autoregressive model on either lyric tokens or music tokens, or both. The attention pattern should be properly\n-        set fro each configuration.\n+        set for each configuration.\n \n         Args:\n             config (`JukeboxPriorConfig`):"
        },
        {
            "sha": "dbbc43502dd2b5ea97099febaa5e446982e2fa55",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -142,7 +142,7 @@ def merge_patches(patches: torch.Tensor, batch_size: int, padding: int) -> torch\n         return patches\n \n     if n_patches_per_batch < 4:\n-        # for each batch, atleast 4 small patches are required to\n+        # for each batch, at least 4 small patches are required to\n         # recreate a large square patch from merging them and later padding is applied\n         # 3 x (8x8) patches becomes 1 x ( 8x8 ) patch (extra patch ignored, no padding)\n         # 4 x (8x8) patches becomes 1 x (16x16) patch (padding later)"
        },
        {
            "sha": "380543c092a3cbab06dbddf5f16f55c3fff07be5",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -118,7 +118,7 @@ def forward(self, input_ids: torch.Tensor, input_embeds: Optional[torch.Tensor]\n \n         # Setting the position-ids to the registered buffer in constructor, it helps\n         # when tracing the model without passing position-ids, solves\n-        # isues similar to issue #5664\n+        # issues similar to issue #5664\n         if hasattr(self, \"position_ids\"):\n             position_ids = self.position_ids[:, :seq_length]\n         else:"
        },
        {
            "sha": "667c7ab3f6c000880a09f447cee99e4b700d0d05",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -72,7 +72,7 @@ class DonutImageProcessor(BaseImageProcessor):\n             Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n         do_pad (`bool`, *optional*, defaults to `True`):\n             Whether to pad the image. If `random_padding` is set to `True` in `preprocess`, each image is padded with a\n-            random amont of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n+            random amount of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n             padded to the largest image size in the batch.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n@@ -349,7 +349,7 @@ def preprocess(\n                 Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n             do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n                 Whether to pad the image. If `random_padding` is set to `True`, each image is padded with a random\n-                amont of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n+                amount of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n                 padded to the largest image size in the batch.\n             random_padding (`bool`, *optional*, defaults to `self.random_padding`):\n                 Whether to use random padding when padding the image. If `True`, each image in the batch with be padded"
        },
        {
            "sha": "2c89feea43ee30ae8f6e0194a62937c8928c4e62",
            "filename": "src/transformers/models/fastspeech2_conformer/configuration_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fconfiguration_fastspeech2_conformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -142,7 +142,7 @@ class FastSpeech2ConformerConfig(PretrainedConfig):\n             speaker id embedding layer.\n         num_languages (`int`, *optional*):\n             Number of languages. If set to > 1, assume that the language ids will be provided as the input and use the\n-            languge id embedding layer.\n+            language id embedding layer.\n         speaker_embed_dim (`int`, *optional*):\n             Speaker embedding dimension. If set to > 0, assume that speaker_embedding will be provided as the input.\n         is_encoder_decoder (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "a9faf1d03e75db8c554b9322aef46766a4093dc0",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -391,7 +391,7 @@ def __init__(\n         dropout_rate=0.5,\n     ):\n         \"\"\"\n-        Initilize variance predictor module.\n+        Initialize variance predictor module.\n \n         Args:\n             input_dim (`int`): Input dimension."
        },
        {
            "sha": "33a312bdee09bf503f213312bf38177c6ac8cf9c",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -948,7 +948,7 @@ def forward(\n \n         # Setting the position-ids to the registered buffer in constructor, it helps\n         # when tracing the model without passing position-ids, solves\n-        # isues similar to issue #5664\n+        # issues similar to issue #5664\n         if position_ids is None:\n             if hasattr(self, \"position_ids\"):\n                 position_ids = self.position_ids[:, :slen]"
        },
        {
            "sha": "3b9ec55dc86eddde5c60ef3b820e39bc0eb688a3",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -360,7 +360,7 @@ def forward(self, hidden_state):\n         x = self.projection_in(hidden_state).permute(0, 3, 1, 2).contiguous()\n         q, ctx, gates = torch.split(x, (num_channels, num_channels, self.focal_level + 1), 1)\n \n-        # context aggreation\n+        # context aggregation\n         ctx_all = 0\n         for level in range(self.focal_level):\n             ctx = self.focal_layers[level](ctx)\n@@ -379,7 +379,7 @@ def forward(self, hidden_state):\n         if self.use_post_layernorm_in_modulation:\n             x_out = self.layernorm(x_out)\n \n-        # post linear porjection\n+        # post linear projection\n         x_out = self.projection_out(x_out)\n         x_out = self.projection_dropout(x_out)\n         return x_out\n@@ -415,7 +415,7 @@ class FocalNetLayer(nn.Module):\n         dim (`int`):\n             Number of input channels.\n         input_resolution (`Tuple[int]`):\n-            Input resulotion.\n+            Input resolution.\n         drop_path (`float`, *optional*, defaults to 0.0):\n             Stochastic depth rate.\n     \"\"\""
        },
        {
            "sha": "960b69ed31bb4cc84712486b8cf7df6c4b88eb79",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -244,7 +244,7 @@ def _tokenize_prompts_with_image_and_batch(\n     - pad all the sequences to this length so we can convert them into a 3D tensor.\n     \"\"\"\n \n-    # If not tool use, tranform the coordinates while tokenizing\n+    # If not tool use, transform the coordinates while tokenizing\n     if scale_factors is not None:\n         transformed_prompt_tokens = []\n         for prompt_seq, scale_factor_seq in zip(prompts, scale_factors):"
        },
        {
            "sha": "a1680b7f5aa9eb13336ecdc9792f474c0bc389af",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -96,7 +96,7 @@ class Gemma3TextConfig(PretrainedConfig):\n             Scaling factor when applying tanh softcapping on the attention scores.\n         cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention. NOTE: if you apply new rope type\n+            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n             accordingly.\n             Expected contents:"
        },
        {
            "sha": "a138acef3823a1b28f033f0c0c745a4376a0318b",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -140,7 +140,7 @@ def pan_and_scan(\n     ):\n         \"\"\"\n         Pan and Scan and image, by cropping into smaller images when the aspect ratio exceeds\n-        minumum allowed ratio.\n+        minimum allowed ratio.\n \n         Args:\n             image (`np.ndarray`):"
        },
        {
            "sha": "7a18d456f352a273328243f41f91101c1b4666a1",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -108,7 +108,7 @@ def pan_and_scan_batched(\n     ):\n         \"\"\"\n         Pan and Scan an image, by cropping into smaller images when the aspect ratio exceeds\n-        minumum allowed ratio.\n+        minimum allowed ratio.\n \n         Args:\n             image (`torch.Tensor`):"
        },
        {
            "sha": "892b8898b6288eb8505c624da26f140d71f109da",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1270,7 +1270,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()"
        },
        {
            "sha": "ecac4921d2e910ae431c7e86c07e694775f790be",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -128,7 +128,7 @@ class Gemma3TextConfig(Gemma2Config):\n             Scaling factor when applying tanh softcapping on the attention scores.\n         cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n         rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings used in gloabl attention. NOTE: if you apply new rope type\n+            Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n             accordingly.\n             Expected contents:\n@@ -926,7 +926,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()"
        },
        {
            "sha": "bf5934c644a41fc4e5deaaa14def1069cda3a279",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1495,7 +1495,7 @@ def forward(\n         >>> processor = AutoProcessor.from_pretrained(\"microsoft/git-base-vatex\")\n         >>> model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-vatex\")\n \n-        >>> # set seed for reproducability\n+        >>> # set seed for reproducibility\n         >>> np.random.seed(45)\n \n "
        },
        {
            "sha": "c7855c07ea39a4b33ceeb2b30ec50ec388caf72a",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -199,7 +199,7 @@ def __init__(\n \n         if self.hidden_size % self.num_attention_heads != 0:\n             raise ValueError(\n-                \"The hidden size is not divisble by the number of attention heads! Make sure to update them!\"\n+                \"The hidden size is not divisible by the number of attention heads! Make sure to update them!\"\n             )\n \n "
        },
        {
            "sha": "84998cfdefa817dee5ed87deb19424eed1661d79",
            "filename": "src/transformers/models/grounding_dino/convert_grounding_dino_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -402,7 +402,7 @@ def convert_grounding_dino_checkpoint(args):\n         \"grounding-dino-tiny\": \"https://huggingface.co/ShilongLiu/GroundingDino/resolve/main/groundingdino_swint_ogc.pth\",\n         \"grounding-dino-base\": \"https://huggingface.co/ShilongLiu/GroundingDino/resolve/main/groundingdino_swinb_cogcoor.pth\",\n     }\n-    # Define default GroundingDino configuation\n+    # Define default GroundingDino configuration\n     config = get_grounding_dino_config(model_name)\n \n     # Load original checkpoint"
        },
        {
            "sha": "755cfaf5d9930921b968100d07136e0c50969154",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1850,7 +1850,7 @@ def forward(\n \n             # In original implementation they apply layer norm before outputting intermediate hidden states\n             # Though that's not through between layers so the layers use as input the output of the previous layer\n-            # withtout layer norm\n+            # without layer norm\n             if output_hidden_states:\n                 all_hidden_states += (self.layer_norm(hidden_states),)\n "
        },
        {
            "sha": "8664db8a42f36075606177dba0996ae30510a0f1",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1425,7 +1425,7 @@ def __init__(self, config, *inputs, **kwargs):\n \n \n @add_start_docstrings(\n-    \"The bare TFHubert Model transformer outputing raw hidden-states without any specific head on top.\",\n+    \"The bare TFHubert Model transformer outputting raw hidden-states without any specific head on top.\",\n     HUBERT_START_DOCSTRING,\n )\n class TFHubertModel(TFHubertPreTrainedModel):"
        },
        {
            "sha": "f56d13efc91ddb1b1485cb8b87675b75eb652e15",
            "filename": "src/transformers/models/ibert/configuration_ibert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fconfiguration_ibert.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -74,8 +74,8 @@ class IBertConfig(PretrainedConfig):\n         quant_mode (`bool`, *optional*, defaults to `False`):\n             Whether to quantize the model or not.\n         force_dequant (`str`, *optional*, defaults to `\"none\"`):\n-            Force dequantize specific nonlinear layer. Dequatized layers are then executed with full precision.\n-            `\"none\"`, `\"gelu\"`, `\"softmax\"`, `\"layernorm\"` and `\"nonlinear\"` are supported. As deafult, it is set as\n+            Force dequantize specific nonlinear layer. Dequantized layers are then executed with full precision.\n+            `\"none\"`, `\"gelu\"`, `\"softmax\"`, `\"layernorm\"` and `\"nonlinear\"` are supported. As default, it is set as\n             `\"none\"`, which does not dequantize any layers. Please specify `\"gelu\"`, `\"softmax\"`, or `\"layernorm\"` to\n             dequantize GELU, Softmax, or LayerNorm, respectively. `\"nonlinear\"` will dequantize all nonlinear layers,\n             i.e., GELU, Softmax, and LayerNorm."
        },
        {
            "sha": "ca8b304be6326ade13844886cb9b4ef0e08daad7",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -276,7 +276,7 @@ def sample_indices_fn(\n \n         Args:\n             metadata (`VideoMetadata`):\n-                `VideoMetadata` object containing metadat about the video, such as \"total_num_frames\" or \"fps\".\n+                `VideoMetadata` object containing metadata about the video, such as \"total_num_frames\" or \"fps\".\n             num_frames (`int`, *optional*):\n                 Number of frames to sample uniformly. If None, all frames are sampled.\n             initial_shift (`bool`, `float` or `int`, defaults to `0`):"
        },
        {
            "sha": "ff09660d6269a880f34beeebe9c6125b804f213f",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -246,7 +246,7 @@ def __init__(\n         self.out_dim_keys_values = attention_ratio * key_dim * num_attention_heads + key_dim * num_attention_heads\n         self.out_dim_projection = attention_ratio * key_dim * num_attention_heads\n         self.resolution_out = resolution_out\n-        # resolution_in is the intial resolution, resoloution_out is final resolution after downsampling\n+        # resolution_in is the initial resolution, resolution_out is final resolution after downsampling\n         self.keys_values = MLPLayerWithBN(input_dim, self.out_dim_keys_values)\n         self.queries_subsample = LevitSubsample(stride, resolution_in)\n         self.queries = MLPLayerWithBN(input_dim, key_dim * num_attention_heads)\n@@ -370,7 +370,7 @@ def __init__(\n         self.layers = []\n         self.config = config\n         self.resolution_in = resolution_in\n-        # resolution_in is the intial resolution, resolution_out is final resolution after downsampling\n+        # resolution_in is the initial resolution, resolution_out is final resolution after downsampling\n         for _ in range(depths):\n             self.layers.append(\n                 LevitResidualLayer("
        },
        {
            "sha": "fd9d68fb7b53197f65f3bfa7609920b777a4370b",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -55,7 +55,7 @@\n \n def get_factors(dividend: int) -> Set[int]:\n     \"\"\"\n-    Calculate all factors of a given number, i.e. a dividor that leaves\n+    Calculate all factors of a given number, i.e. a divisor that leaves\n     no remainder. For example, if dividend=12, it will return {1, 2, 3, 4, 6, 12}.\n \n     Args:"
        },
        {
            "sha": "bedba000c7f6d2431d7bb1dc558894f6209b321d",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -60,7 +60,7 @@ class LlavaNextVideoImageProcessor(BaseImageProcessor):\n         image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n             A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n             based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processinf videos.\n+            method. Not used for processing videos.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_center_crop (`bool`, *optional*, defaults to `True`):"
        },
        {
            "sha": "107267bb51636cab984fb13d338569189f9e387e",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -405,7 +405,7 @@ def __init__(\n         \"\"\"\n         super().__init__()\n         if cost_class == 0 and cost_mask == 0 and cost_dice == 0:\n-            raise ValueError(\"All costs cant be 0\")\n+            raise ValueError(\"All costs can't be 0\")\n \n         self.num_points = num_points\n         self.cost_class = cost_class"
        },
        {
            "sha": "60016172d259aa14843c0cba361e1735599ec01b",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -829,7 +829,7 @@ def __init__(self, cost_class: float = 1.0, cost_mask: float = 1.0, cost_dice: f\n         \"\"\"\n         super().__init__()\n         if cost_class == 0 and cost_mask == 0 and cost_dice == 0:\n-            raise ValueError(\"All costs cant be 0\")\n+            raise ValueError(\"All costs can't be 0\")\n         self.cost_class = cost_class\n         self.cost_mask = cost_mask\n         self.cost_dice = cost_dice"
        },
        {
            "sha": "2e50a3b6b64aff07ff009d74d0d5d2efbdff1bff",
            "filename": "src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -98,7 +98,7 @@ def add_megatron_checkpoint_args(parser):\n         default=128,\n         help=(\n             \"Pad the vocab size to be divisible by this value. \"\n-            \"This is added for computational efficieny reasons. \"\n+            \"This is added for computational efficiency reasons. \"\n             \"Only used when converting a Transformers checkpoint to a Megatron checkpoint.\"\n         ),\n     )\n@@ -235,7 +235,7 @@ def transformers_to_megatron_fix_query_key_value_ordering(\n     param, checkpoint_version, num_splits, num_heads, hidden_size\n ):\n     \"\"\"\n-    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM chekpoint versions. Input\n+    Permutes layout of param tensor to the one compatible with respective NVIDIA Megatron-LM checkpoint versions. Input\n     is [num_splits * num_heads * hidden_size, :] and output is [num_heads * hidden_size * num_splits, :] for version\n     1.0 and [num_heads * num_splits * hidden_size, :] for version 2.0 and later. If param is the weight tensor of the\n     self-attention block, the param needs to be already transposed before calling this function.\n@@ -348,7 +348,7 @@ def convert_checkpoint_from_megatron_to_transformers(args):\n         raise ValueError(\n             \"Megatron-LM checkpoint does not contain arguments. This utility only supports Megatron-LM checkpoints\"\n             \" containing all the megatron arguments. This is because it loads all config related to model\"\n-            \" architecture, the tensor and pipeline model parallel size from the checkpoint insead of user having to\"\n+            \" architecture, the tensor and pipeline model parallel size from the checkpoint instead of user having to\"\n             \" manually specify all the details. Please save Megatron-LM checkpoint along with all the megatron\"\n             \" arguments to use this utility.\"\n         )"
        },
        {
            "sha": "bea8d9c637b0de932287da2b2b05fc4986ee0414",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1601,7 +1601,7 @@ def generate(\n         # 7. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistance with the unbatched CFG)\n+        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None\n@@ -2617,7 +2617,7 @@ def generate(\n         # 7. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistance with the unbatched CFG)\n+        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None"
        },
        {
            "sha": "f0a7e4dee03875788c12f6ec471afbfd295688c4",
            "filename": "src/transformers/models/musicgen_melody/feature_extraction_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Ffeature_extraction_musicgen_melody.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -54,7 +54,7 @@ class MusicgenMelodyFeatureExtractor(SequenceFeatureExtractor):\n         sampling_rate (`int`, *optional*, defaults to 32000):\n             The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n         hop_length (`int`, *optional*, defaults to 4096):\n-            Length of the overlaping windows for the STFT used to obtain the Mel Frequency coefficients.\n+            Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n         chunk_length (`int`, *optional*, defaults to 30):\n             The maximum number of chunks of `sampling_rate` samples used to trim and pad longer or shorter audio\n             sequences."
        },
        {
            "sha": "a3cc95690d1d464bbc82602ee24f836f67b31d57",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -92,7 +92,7 @@ class MusicgenMelodyOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-            Sequence of conditional hidden-states representing the concatenation of the projeted text encoder output and the projeted audio encoder output.\n+            Sequence of conditional hidden-states representing the concatenation of the projected text encoder output and the projected audio encoder output.\n             Used as a conditional signal.\n     \"\"\"\n \n@@ -757,8 +757,8 @@ def _init_weights(self, module):\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n             `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n         encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-            Sequence of conditional hidden-states representing the concatenation of the projeted text encoder output and the projeted audio encoder output.\n-            Used as a conditional signal and will thus be concatenated to the projeted `decoder_input_ids`.\n+            Sequence of conditional hidden-states representing the concatenation of the projected text encoder output and the projected audio encoder output.\n+            Used as a conditional signal and will thus be concatenated to the projected `decoder_input_ids`.\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n             This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n@@ -818,7 +818,7 @@ def _init_weights(self, module):\n             [What are attention masks?](../glossary#attention-mask)\n         encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states representing the concatenation of the text encoder output and the processed audio encoder output.\n-            Used as a conditional signal and will thus be concatenated to the projeted `decoder_input_ids`.\n+            Used as a conditional signal and will thus be concatenated to the projected `decoder_input_ids`.\n         encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, encoder_sequence_length)`, *optional*):\n             Mask to avoid performing attention on conditional hidden states. Mask values\n             selected in `[0, 1]`:\n@@ -1522,7 +1522,7 @@ def generate(\n         # 7. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistance with the unbatched CFG)\n+        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None\n@@ -2478,7 +2478,7 @@ def generate(\n         # 7. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistance with the unbatched CFG)\n+        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None"
        },
        {
            "sha": "e8b80221c4460358db024653dda14f7cc0c0aa90",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -425,7 +425,7 @@ def forward(self, hidden_states: torch.Tensor, padding_mask: Optional[torch.Tens\n         r\"\"\"\n         The goal of this forward pass is to have the same number of operation as the equivalent `NllbMoeDenseActDense`\n         (mlp) layer. This means that all of the hidden states should be processed at most twice ( since we are using a\n-        top_2 gating mecanism). This means that we keep the complexity to O(batch_size x sequence_length x hidden_dim)\n+        top_2 gating mechanism). This means that we keep the complexity to O(batch_size x sequence_length x hidden_dim)\n         instead of O(num_experts x batch_size x sequence_length x hidden_dim).\n \n         1- Get the `router_probs` from the `router`. The shape of the `router_mask` is `(batch_size X sequence_length,"
        },
        {
            "sha": "a4e94ab3f0248f53a1bd186ee0ae23fa361d5a3b",
            "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -376,7 +376,7 @@ class NougatTokenizerFast(PreTrainedTokenizerFast):\n             contains everything needed to load the tokenizer.\n \n         clean_up_tokenization_spaces (`str`, *optional*, defaults to `False`):\n-            Wether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n+            Whether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n             spaces.\n \n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):"
        },
        {
            "sha": "6ee52db488e893e7f5f4b9295f2cf2776072e24d",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -268,7 +268,7 @@ def convert_omdet_turbo_checkpoint(args):\n             \"https://huggingface.co/omlab/OmDet-Turbo_tiny_SWIN_T/resolve/main/ViT-B-16.pt\",\n         ],\n     }\n-    # Define default OmDetTurbo configuation\n+    # Define default OmDetTurbo configuration\n     config = get_omdet_turbo_config(model_name, use_timm_backbone)\n \n     # Load original checkpoint"
        },
        {
            "sha": "924e3b1dcad76b0566e408aa371cd3eeb5b308b0",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -471,7 +471,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()"
        },
        {
            "sha": "2238426cd0844fdf0572bb8dd424962d6128e277",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1807,7 +1807,7 @@ class PatchTSMixerForTimeSeriesClassificationOutput(ModelOutput):\n \n     Args:\n         prediction_outputs (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n-            Prediction output from the classfication head.\n+            Prediction output from the classification head.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n             Backbone embeddings before passing through the head.\n         hidden_states (`tuple(torch.FloatTensor)`, *optional*):"
        },
        {
            "sha": "b7c7a8fd5522492f31690d7915094f5fe96162d8",
            "filename": "src/transformers/models/pegasus/modeling_flax_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1487,7 +1487,7 @@ def update_inputs_for_generation(self, model_outputs, model_kwargs):\n \n     Summarization example:\n \n-    ```pyton\n+    ```python\n     >>> from transformers import AutoTokenizer, FlaxPegasusForConditionalGeneration\n \n     >>> model = FlaxPegasusForConditionalGeneration.from_pretrained('google/pegasus-large')"
        },
        {
            "sha": "8579cf08afa231b5aef5b308d0a0712493a94e18",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -127,7 +127,7 @@ def get_resize_output_image_size(\n     ratio = max(height / max_height, width / max_width)\n \n     if ratio > 1:\n-        # Orgiginal implementation uses `round` which utilises bankers rounding, which can lead to surprising results\n+        # Original implementation uses `round` which utilises bankers rounding, which can lead to surprising results\n         # Here we use floor to ensure the image is always smaller than the given \"longest_edge\"\n         height = int(math.floor(height / ratio))\n         width = int(math.floor(width / ratio))"
        },
        {
            "sha": "184a7c2a37a0046027f0b46d17513b9b5a4e9d4e",
            "filename": "src/transformers/models/pvt/convert_pvt_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fconvert_pvt_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -35,7 +35,7 @@\n def create_rename_keys(config):\n     rename_keys = []\n     for i in range(config.num_encoder_blocks):\n-        # Remane embedings' paramters\n+        # Rename embeddings' parameters\n         rename_keys.append((f\"pos_embed{i + 1}\", f\"pvt.encoder.patch_embeddings.{i}.position_embeddings\"))\n \n         rename_keys.append((f\"patch_embed{i + 1}.proj.weight\", f\"pvt.encoder.patch_embeddings.{i}.projection.weight\"))"
        },
        {
            "sha": "4ec2368ab2ad8364832c6899c694b7002ac49804",
            "filename": "src/transformers/models/rembert/modeling_tf_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1037,7 +1037,7 @@ class TFRemBertPreTrainedModel(TFPreTrainedModel):\n \n \n @add_start_docstrings(\n-    \"The bare RemBERT Model transformer outputing raw hidden-states without any specific head on top.\",\n+    \"The bare RemBERT Model transformer outputting raw hidden-states without any specific head on top.\",\n     REMBERT_START_DOCSTRING,\n )\n class TFRemBertModel(TFRemBertPreTrainedModel):"
        },
        {
            "sha": "6f697ad14e05b6164936a66242775ba29a1b0286",
            "filename": "src/transformers/models/roformer/modeling_tf_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -911,7 +911,7 @@ class TFRoFormerPreTrainedModel(TFPreTrainedModel):\n \n \n @add_start_docstrings(\n-    \"The bare RoFormer Model transformer outputing raw hidden-states without any specific head on top.\",\n+    \"The bare RoFormer Model transformer outputting raw hidden-states without any specific head on top.\",\n     ROFORMER_START_DOCSTRING,\n )\n class TFRoFormerModel(TFRoFormerPreTrainedModel):"
        },
        {
            "sha": "d1eda956e0702d4ee27781098b289abc44e98827",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -2171,7 +2171,7 @@ def __init__(\n         config: SeamlessM4TConfig,\n         embed_tokens_decoder: Optional[nn.Embedding] = None,\n     ):\n-        # update config - used principaly for bos_token_id etc.\n+        # update config - used principality for bos_token_id etc.\n         config = copy.deepcopy(config)\n         for param, val in config.to_dict().items():\n             if param.startswith(\"t2u_\"):"
        },
        {
            "sha": "5895a85ace5afad9df3a8b8dc5788982bce02d2e",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -184,7 +184,7 @@ class SeamlessM4Tv2TextToUnitOutput(ModelOutput):\n \n             [What are input IDs?](../glossary#input-ids)\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+            Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n             [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n     \"\"\"\n \n@@ -202,7 +202,7 @@ class SeamlessM4Tv2TextToUnitOutput(ModelOutput):\n M4T_SPEECH_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+            Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n             [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n         \"\"\"\n \n@@ -2461,7 +2461,7 @@ def __init__(\n         config: SeamlessM4Tv2Config,\n         embed_tokens_decoder: Optional[nn.Embedding] = None,\n     ):\n-        # update config - used principaly for bos_token_id etc.\n+        # update config - used principality for bos_token_id etc.\n         config = copy.deepcopy(config)\n         for param, val in config.to_dict().items():\n             if param.startswith(\"t2u_\"):\n@@ -4035,7 +4035,7 @@ def generate(\n \n         Args:\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n             return_intermediate_token_ids (`bool`, *optional*):\n                 If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want\n@@ -4485,7 +4485,7 @@ def generate(\n \n                 [What are input IDs?](../glossary#input-ids)\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n             return_intermediate_token_ids (`bool`, *optional*):\n                 If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want"
        },
        {
            "sha": "79aa6f59832c0cfabd3fb7b31eac05839a2769ca",
            "filename": "src/transformers/models/seggpt/convert_seggpt_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fconvert_seggpt_to_hf.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -114,7 +114,7 @@ def convert_seggpt_checkpoint(args):\n     verify_logits = args.verify_logits\n     push_to_hub = args.push_to_hub\n \n-    # Define default GroundingDINO configuation\n+    # Define default GroundingDINO configuration\n     config = SegGptConfig()\n \n     # Load original checkpoint"
        },
        {
            "sha": "0150f2d7a934195f305e07d23faf69542088e06c",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -62,7 +62,7 @@ class SegGptEncoderOutput(ModelOutput):\n         intermediate_hidden_states (`Tuple[torch.FloatTensor]`, *optional*, returned when `config.intermediate_hidden_state_indices` is set):\n             Tuple of `torch.FloatTensor` of shape `(batch_size, patch_height, patch_width, hidden_size)`.\n             Each element in the Tuple corresponds to the output of the layer specified in `config.intermediate_hidden_state_indices`.\n-            Additionaly, each feature passes through a LayerNorm.\n+            Additionally, each feature passes through a LayerNorm.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor"
        },
        {
            "sha": "62e136b357524d44a7137b25c482259422e31b95",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1979,10 +1979,10 @@ def forward(\n             load the weights associated with the model, only the configuration. Check out the\n             [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n         encoder ([`SpeechT5EncoderWithSpeechPrenet`] or [`SpeechT5EncoderWithTextPrenet`] or `None`):\n-            The Transformer encoder module that applies the appropiate speech or text encoder prenet. If `None`,\n+            The Transformer encoder module that applies the appropriate speech or text encoder prenet. If `None`,\n             [`SpeechT5EncoderWithoutPrenet`] will be used and the `input_values` are assumed to be hidden states.\n         decoder ([`SpeechT5DecoderWithSpeechPrenet`] or [`SpeechT5DecoderWithTextPrenet`] or `None`):\n-            The Transformer decoder module that applies the appropiate speech or text decoder prenet. If `None`,\n+            The Transformer decoder module that applies the appropriate speech or text decoder prenet. If `None`,\n             [`SpeechT5DecoderWithoutPrenet`] will be used and the `decoder_input_values` are assumed to be hidden\n             states.\n \"\"\""
        },
        {
            "sha": "df70b4576d746e32c1c0b0cefd7325454e2c11f7",
            "filename": "src/transformers/models/t5/convert_t5x_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fconvert_t5x_checkpoint_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -175,7 +175,7 @@ def make_state_dict(converted_params, is_encoder_only: bool):\n \n \n def load_t5x_weights_in_t5(model, config, t5x_checkpoint_path, is_encoder_only):\n-    \"\"\"Replaces the params in model witht the T5X converted params.\"\"\"\n+    \"\"\"Replaces the params in model with the T5X converted params.\"\"\"\n     variables = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n     converted = convert_t5x_to_pytorch(\n         variables,"
        },
        {
            "sha": "4672260c169b4ef82ea43842c89698872698e9b9",
            "filename": "src/transformers/models/tapas/modeling_tf_tapas.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tf_tapas.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -2344,11 +2344,11 @@ def _calculate_expected_result(\n     if avg_approximation == AverageApproximationFunction.RATIO:\n         average_result = sum_result / (count_result + EPSILON_ZERO_DIVISION)\n     elif avg_approximation == AverageApproximationFunction.FIRST_ORDER:\n-        # The sum of all probabilities exept that correspond to other cells\n+        # The sum of all probabilities except that correspond to other cells\n         ex = tf.reduce_sum(scaled_probability_per_cell, axis=1, keepdims=True) - scaled_probability_per_cell + 1\n         average_result = tf.reduce_sum(numeric_values_masked * scaled_probability_per_cell / ex, axis=1)\n     elif avg_approximation == AverageApproximationFunction.SECOND_ORDER:\n-        # The sum of all probabilities exept that correspond to other cells\n+        # The sum of all probabilities except that correspond to other cells\n         ex = tf.reduce_sum(scaled_probability_per_cell, axis=1, keepdims=True) - scaled_probability_per_cell + 1\n         pointwise_var = scaled_probability_per_cell * (1 - scaled_probability_per_cell)\n         var = tf.reduce_sum(pointwise_var, axis=1, keepdims=True) - pointwise_var"
        },
        {
            "sha": "a23868c6dde9d1d67158e290e845001fe281681c",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -2359,7 +2359,7 @@ def _process_date_patterns():\n     \"second\",\n     \"third\",\n     \"fourth\",\n-    \"fith\",\n+    \"fifth\",\n     \"sixth\",\n     \"seventh\",\n     \"eighth\","
        },
        {
            "sha": "b2c43815f59b558ec066700da86347ccd263de85",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1364,7 +1364,7 @@ def forward(\n \n         if inputs_embeds is None:\n             if self.embed_tokens is None:\n-                raise ValueError(\"You have to intialize the model with valid token embeddings\")\n+                raise ValueError(\"You have to initialize the model with valid token embeddings\")\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if pixel_values is not None:"
        },
        {
            "sha": "aaa8c94e2fdee670ec7e6d121dd7dc380c198cbd",
            "filename": "src/transformers/models/umt5/convert_umt5_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -200,7 +200,7 @@ def make_state_dict(converted_params, is_encoder_only: bool):\n \n \n def load_t5x_weights_in_t5(model, config, t5x_checkpoint_path, is_encoder_only, scalable_attention):\n-    \"\"\"Replaces the params in model witht the T5X converted params.\"\"\"\n+    \"\"\"Replaces the params in model with the T5X converted params.\"\"\"\n     variables = checkpoints.load_t5x_checkpoint(t5x_checkpoint_path)\n     converted = convert_t5x_to_pytorch(\n         variables, num_layers=config.num_layers, is_encoder_only=is_encoder_only, scalable_attention=scalable_attention"
        },
        {
            "sha": "8ddf93edbe094e806816ea1f8a58bb1a27aba5c7",
            "filename": "src/transformers/models/unispeech/configuration_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -164,7 +164,7 @@ class UniSpeechConfig(PretrainedConfig):\n         eos_token_id (`int`, *optional*, defaults to 2):\n             The id of the \"end-of-sequence\" token.\n         replace_prob (`float`, *optional*, defaults to 0.5):\n-            Propability that transformer feature is replaced by quantized feature for pretraining.\n+            Probability that transformer feature is replaced by quantized feature for pretraining.\n \n     Example:\n "
        },
        {
            "sha": "d58ff6bd6502beb31862d901eb29d336882ade5a",
            "filename": "src/transformers/models/univnet/convert_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -56,7 +56,7 @@ def get_kernel_predictor_key_mapping(config: UnivNetConfig, old_prefix: str = \"\"\n def get_key_mapping(config: UnivNetConfig):\n     mapping = {}\n \n-    # NOTE: inital conv layer keys are the same\n+    # NOTE: initial conv layer keys are the same\n \n     # LVC Residual blocks\n     for i in range(len(config.resblock_stride_sizes)):"
        },
        {
            "sha": "5c6576362399e7123585a98a376eb0993b304b30",
            "filename": "src/transformers/models/univnet/feature_extraction_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Ffeature_extraction_univnet.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -64,7 +64,7 @@ class UnivNetFeatureExtractor(SequenceFeatureExtractor):\n             The number of FFT components to use. If `None`, this is determined using\n             `transformers.audio_utils.optimal_fft_length`.\n         max_length_s (`int`, *optional*, defaults to 10):\n-            The maximum input lenght of the model in seconds. This is used to pad the audio.\n+            The maximum input length of the model in seconds. This is used to pad the audio.\n         fmin (`float`, *optional*, defaults to 0.0):\n             Minimum mel frequency in Hz.\n         fmax (`float`, *optional*):"
        },
        {
            "sha": "baa30704f781864509a137a4a8424c68496b1730",
            "filename": "src/transformers/models/vilt/configuration_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fconfiguration_vilt.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -39,7 +39,7 @@ class ViltConfig(PretrainedConfig):\n             The vocabulary size of the `token_type_ids` passed when calling [`ViltModel`]. This is used when encoding\n             text.\n         modality_type_vocab_size (`int`, *optional*, defaults to 2):\n-            The vocabulary size of the modalities passed when calling [`ViltModel`]. This is used after concatening the\n+            The vocabulary size of the modalities passed when calling [`ViltModel`]. This is used after concatenating the\n             embeddings of the text and image modalities.\n         max_position_embeddings (`int`, *optional*, defaults to 40):\n             The maximum sequence length that this model might ever be used with."
        },
        {
            "sha": "5aa9e1e1cf1a9c4af246f72d660159f622115ca3",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -139,7 +139,7 @@ def visual_embed(self, pixel_values, pixel_mask, max_image_length=200):\n         x_mask = x_mask.flatten(1)\n \n         if max_image_length < 0 or max_image_length is None or not isinstance(max_image_length, int):\n-            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrinked)\n+            # suppose aug is 800 x 1333, then, maximum effective res is 800 x 1333 (if one side gets bigger, the other will be constrained and be shrunk)\n             # (800 // self.patch_size) * (1333 // self.patch_size) is the maximum number of patches that single image can get.\n             # if self.patch_size = 32, 25 * 41 = 1025\n             # if res is 384 x 640, 12 * 20 = 240"
        },
        {
            "sha": "941dfee11da934d4b9ff20a5b4c1dbb985c1ef0b",
            "filename": "src/transformers/models/vision_encoder_decoder/configuration_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fconfiguration_vision_encoder_decoder.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -85,7 +85,7 @@ def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         if \"encoder\" not in kwargs or \"decoder\" not in kwargs:\n             raise ValueError(\n-                f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n+                f\"A configuration of type {self.model_type} cannot be instantiated because \"\n                 f\"not both `encoder` and `decoder` sub-configurations are passed, but only {kwargs}\"\n             )\n "
        },
        {
            "sha": "1e71728de5b815ca5a4013fa53b117d802983baa",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1496,7 +1496,7 @@ def _get_feature_vector_attention_mask(\n \n \n @add_start_docstrings(\n-    \"The bare TFWav2Vec2 Model transformer outputing raw hidden-states without any specific head on top.\",\n+    \"The bare TFWav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\",\n     WAV2VEC2_START_DOCSTRING,\n )\n class TFWav2Vec2Model(TFWav2Vec2PreTrainedModel):"
        },
        {
            "sha": "11eb1f081afe21e15e124e7d97ed49d6cdf91907",
            "filename": "src/transformers/models/wavlm/configuration_wavlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fwavlm%2Fconfiguration_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fwavlm%2Fconfiguration_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fconfiguration_wavlm.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -101,7 +101,7 @@ class WavLMConfig(PretrainedConfig):\n             [SpecAugment: A Simple Data Augmentation Method for Automatic Speech\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n-            Propability of each feature vector along the time axis to be chosen as the start of the vector span to be\n+            Probability of each feature vector along the time axis to be chosen as the start of the vector span to be\n             masked. Approximately `mask_time_prob * sequence_length // mask_time_length` feature vectors will be masked\n             along the time axis. This is only relevant if `apply_spec_augment is True`.\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -111,7 +111,7 @@ class WavLMConfig(PretrainedConfig):\n             irrespectively of `mask_feature_prob`. Only relevant if ''mask_time_prob*len(time_axis)/mask_time_length <\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n-            Propability of each feature vector along the feature axis to be chosen as the start of the vector span to\n+            Probability of each feature vector along the feature axis to be chosen as the start of the vector span to\n             be masked. Approximately `mask_time_prob * hidden_size // mask_time_length` feature vectors will be masked\n             along the time axis. This is only relevant if `apply_spec_augment is True`.\n         mask_feature_length (`int`, *optional*, defaults to 10):"
        },
        {
            "sha": "f839548320fb44218c73fb70397c279d4349530d",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -474,7 +474,7 @@ def post_process_depth_estimation(\n             outputs_flipped ([`ZoeDepthDepthEstimatorOutput`], *optional*):\n                 Raw outputs of the model from flipped input (averaged out in the end).\n             do_remove_padding (`bool`, *optional*):\n-                By default ZoeDepth addes padding equal to `int((height / 2) * 3)` (and similarly for width) to fix the\n+                By default ZoeDepth adds padding equal to `int((height / 2) * 3)` (and similarly for width) to fix the\n                 boundary artifacts in the output depth map, so we need remove this padding during post_processing. The\n                 parameter exists here in case the user changed the image preprocessing to not include padding.\n "
        },
        {
            "sha": "49837d531283fc2ca8311c4ac7880817fa25f937",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -37,7 +37,7 @@ class AwqQuantizer(HfQuantizer):\n     4-bit quantization for Activation-aware Weight Quantization(AWQ) (https://arxiv.org/abs/2306.00978)\n     \"\"\"\n \n-    # AWQ requires data callibration - we support only inference\n+    # AWQ requires data calibration - we support only inference\n     requires_calibration = True\n \n     required_packages = [\"awq\", \"accelerate\"]"
        },
        {
            "sha": "7aa120f00c620f212dd5d6e8de23989617c8faef",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -69,7 +69,7 @@ def update_missing_keys_after_loading(self, model, missing_keys: List[str], pref\n             return missing_keys\n \n         # We expect some keys to be missing for\n-        # compresed models\n+        # compressed models\n         # This is fine as the weights are reconstructed by ModelCompressor\n         # in _process_model_after_weight_loading\n "
        },
        {
            "sha": "f420a3e12d732915eda740bbc2652ba1a704b6fe",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1728,14 +1728,14 @@ def check_torch_load_is_safe():\n \n # docstyle-ignore\n LIBROSA_IMPORT_ERROR = \"\"\"\n-{0} requires thes librosa library. But that was not found in your environment. You can install them with pip:\n+{0} requires the librosa library. But that was not found in your environment. You can install them with pip:\n `pip install librosa`\n Please note that you may need to restart your runtime after installation.\n \"\"\"\n \n # docstyle-ignore\n PRETTY_MIDI_IMPORT_ERROR = \"\"\"\n-{0} requires thes pretty_midi library. But that was not found in your environment. You can install them with pip:\n+{0} requires the pretty_midi library. But that was not found in your environment. You can install them with pip:\n `pip install pretty_midi`\n Please note that you may need to restart your runtime after installation.\n \"\"\""
        },
        {
            "sha": "bc560f68171d0df6dfc2d1a65f6fd1b4068eafd3",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -1120,7 +1120,7 @@ class VptqLayerConfig(QuantizationConfigMixin):\n         group_size (`int`, *optional*, defaults to `-1`): depends on out-features\n         indices_as_float (`bool`, *optional*, defaults to `False`): for Finetuning\n         is_indice_packed (`bool`, *optional*, defaults to `True`): should always be True\n-        num_centroids (`list`, *optional*, defaults to `[-1, -1]`): centriod numbers of clusters\n+        num_centroids (`list`, *optional*, defaults to `[-1, -1]`): centroid numbers of clusters\n         num_res_centroids (`list`, *optional*, defaults to `[-1, -1]`): ditto for residual\n         outlier_size (`int`, *optional*, defaults to `1`): outliers\n         vector_lens (`list`, *optional*, defaults to `[-1, -1]`): centroid vector length in quantization"
        },
        {
            "sha": "948ca66e3f80d4a6b3c6b423fc5451c3d52aa81d",
            "filename": "tests/models/bert_generation/test_tokenization_bert_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_tokenization_bert_generation.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -146,7 +146,7 @@ def test_tokenization_base_easy_symbols(self):\n     def test_tokenization_base_hard_symbols(self):\n         symbols = (\n             'This is a very long text with a lot of weird characters, such as: . , ~ ? ( ) \" [ ] ! : - . Also we will'\n-            \" add words that should not exsist and be tokenized to <unk>, such as saoneuhaoesuth\"\n+            \" add words that should not exist and be tokenized to <unk>, such as saoneuhaoesuth\"\n         )\n         original_tokenizer_encodings = [\n             871,"
        },
        {
            "sha": "ef53889fa1dc2789da823370d427f4377cda56ba",
            "filename": "tests/models/big_bird/test_tokenization_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_tokenization_big_bird.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -170,7 +170,7 @@ def test_tokenization_base_easy_symbols(self):\n     def test_tokenization_base_hard_symbols(self):\n         symbols = (\n             'This is a very long text with a lot of weird characters, such as: . , ~ ? ( ) \" [ ] ! : - . Also we will'\n-            \" add words that should not exsist and be tokenized to <unk>, such as saoneuhaoesuth\"\n+            \" add words that should not exist and be tokenized to <unk>, such as saoneuhaoesuth\"\n         )\n         original_tokenizer_encodings = [65, 871, 419, 358, 946, 991, 2521, 452, 358, 1357, 387, 7751, 3536, 112, 985, 456, 126, 865, 938, 5400, 5734, 458, 1368, 467, 786, 2462, 5246, 1159, 633, 865, 4519, 457, 582, 852, 2557, 427, 916, 508, 405, 34324, 497, 391, 408, 11342, 1244, 385, 100, 938, 985, 456, 574, 362, 12597, 3200, 3129, 1172, 66]  # fmt: skip\n         self.assertListEqual(original_tokenizer_encodings, self.big_tokenizer.encode(symbols))"
        },
        {
            "sha": "c59e7f4e148ce03b440b8b0c585355fc3a260a6e",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -438,7 +438,7 @@ def test_retain_grad_hidden_states_attentions(self):\n         if self.has_attentions:\n             self.assertIsNotNone(attentions.grad)\n \n-    # override as the `logit_scale` parameter initilization is different for BRIDGE TOWER\n+    # override as the `logit_scale` parameter initialization is different for BRIDGE TOWER\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "60989eb91d9a826753eeef9f495d758c0a18b135",
            "filename": "tests/models/bridgetower/test_processor_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_processor_bridgetower.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -55,7 +55,7 @@ def get_image_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # Some kwargs tests are overriden from common tests to handle shortest_edge\n+    # Some kwargs tests are overridden from common tests to handle shortest_edge\n     # and size_divisor behaviour\n \n     @require_torch"
        },
        {
            "sha": "322fa9d68f4eb78274be034f8427ecd4932a750a",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -924,7 +924,7 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # override as the `logit_scale` parameter initilization is different for FLAVA\n+    # override as the `logit_scale` parameter initialization is different for FLAVA\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -933,7 +933,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\" or name == \"flava.logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "9638ff4a87fa756722bafa241261a19a574ac41f",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -137,7 +137,7 @@ def test_eager_matches_fa2_generate(self):\n \n     @unittest.skip(\n         reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n+        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n     )\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n@@ -275,7 +275,7 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n \n     @unittest.skip(\n         reason=\"HybridCache can't be gathered because it is not iterable. Adding a simple iter and dumping `distributed_iterator`\"\n-        \" as in Dynamic Cache doesnt work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n+        \" as in Dynamic Cache doesn't work. NOTE: @gante all cache objects would need better compatibility with multi gpu setting\"\n     )\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "30587a8f55ea5be8d6ad6c6c0ceffea1bbf189ea",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -88,7 +88,7 @@ def test_text_with_image_tokens(self):\n \n         image = self.prepare_image_inputs()\n \n-        # If text has no image tokens, iamge should be `None`\n+        # If text has no image tokens, image should be `None`\n         with self.assertRaises(ValueError):\n             _ = processor(text=text_no_image, images=image, return_tensors=\"np\")\n "
        },
        {
            "sha": "fc552ab4afeaab3d85516ed321c94ff1282b0df2",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -478,8 +478,8 @@ def test_local_attn_probs(self):\n         # the last 2 tokens are masked, and should have 0 attn_probs\n         self.assertTrue(torch.all(attn_probs[:, :, -mask_tokens:, -mask_tokens:] == 0))\n \n-        # in loacal attention each token can only attend to the previous window_size tokens (including itself)\n-        # here window_size is 4, so a token at index 5 can only attend to indcies [2, 3, 4, 5]\n+        # in local attention each token can only attend to the previous window_size tokens (including itself)\n+        # here window_size is 4, so a token at index 5 can only attend to indices [2, 3, 4, 5]\n         # and the attn_probs should be 0 for token [0, 1]\n         self.assertTrue(torch.all(attn_probs[:, :, 5, 2:6] != 0))\n         self.assertTrue(torch.all(attn_probs[:, :, 5, :2] == 0))"
        },
        {
            "sha": "b55c590efb69c7ae8c8a38e508dc68e32477cb89",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -769,7 +769,7 @@ def test_cross_attention_mask(self):\n         encoding1 = processor(images=image, text=text1, return_tensors=\"pt\").to(torch_device)\n         encoding2 = processor(images=image, text=text2, return_tensors=\"pt\").to(torch_device)\n         # If we batch the text and cross attention masking is working the batched result should be equal to\n-        # The singe text result\n+        # The single text result\n         encoding_batched = processor(\n             images=[image] * len(text_batched), text=text_batched, padding=\"longest\", return_tensors=\"pt\"\n         ).to(torch_device)"
        },
        {
            "sha": "f47330ec673ac42b2cda6b5f3ebb490c61a9da13",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -658,7 +658,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info"
        },
        {
            "sha": "9680b6e487d2bd3eff0e9554d9047ba866b216a1",
            "filename": "tests/models/llava/test_configuration_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -56,7 +56,7 @@ def test_pixtral_reload(self):\n \n     def test_arbitrary_reload(self):\n         \"\"\"\n-        Simple test for reloading arbirarily composed subconfigs\n+        Simple test for reloading arbitrarily composed subconfigs\n         \"\"\"\n         default_values = LlavaConfig().to_diff_dict()\n         default_values[\"vision_config\"][\"model_type\"] = \"pixtral\""
        },
        {
            "sha": "a692566340ca6b525e992724a128550a6effb518",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -553,8 +553,8 @@ def test_pixtral(self):\n         # image = Image.open(requests.get(url, stream=True).raw)\n         inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(model.device)\n         generate_ids = model.generate(**inputs, max_new_tokens=500)\n-        ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        print(ouptut)\n+        output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        print(output)\n \n         # fmt: off\n         EXPECTED_GENERATION = \"\"\"\n@@ -573,7 +573,7 @@ def test_pixtral(self):\n \"\"\"\n         # fmt: on\n         # check that both inputs are handled correctly and generate the same output\n-        self.assertEqual(ouptut, EXPECTED_GENERATION)\n+        self.assertEqual(output, EXPECTED_GENERATION)\n \n     @slow\n     @require_bitsandbytes"
        },
        {
            "sha": "14a558794240a9676c6356357b87d40fc0dbdab0",
            "filename": "tests/models/longformer/test_tokenization_longformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_tokenization_longformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -200,7 +200,7 @@ def test_embeded_special_tokens(self):\n                 tokens_r_str = tokenizer_r.convert_ids_to_tokens(tokens_r[\"input_ids\"])\n                 tokens_p_str = tokenizer_p.convert_ids_to_tokens(tokens_p[\"input_ids\"])\n \n-                # Rust correctly handles the space before the mask while python doesnt\n+                # Rust correctly handles the space before the mask while python doesn't\n                 self.assertSequenceEqual(tokens_p[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n                 self.assertSequenceEqual(tokens_r[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n "
        },
        {
            "sha": "0d98e7919e7bb8dd5e79eb32fdd1797bc11bec36",
            "filename": "tests/models/luke/test_tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_tokenization_luke.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -157,7 +157,7 @@ def test_embeded_special_tokens(self):\n \n                 tokens_p_str = tokenizer_p.convert_ids_to_tokens(tokens_p[\"input_ids\"])\n \n-                # Rust correctly handles the space before the mask while python doesnt\n+                # Rust correctly handles the space before the mask while python doesn't\n                 self.assertSequenceEqual(tokens_p[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n \n                 self.assertSequenceEqual("
        },
        {
            "sha": "faf074902dcb12f877ee9e7201c2b1f3bcc3d03a",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -142,7 +142,7 @@ def create_and_check_maskformer_model(self, config, pixel_values, pixel_mask, ou\n \n             output = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n             output = model(pixel_values, output_hidden_states=True)\n-        # the correct shape of output.transformer_decoder_hidden_states ensure the correcteness of the\n+        # the correct shape of output.transformer_decoder_hidden_states ensure the correctness of the\n         # encoder and pixel decoder\n         self.parent.assertEqual(\n             output.transformer_decoder_last_hidden_state.shape,"
        },
        {
            "sha": "e87a7ae75185828be2ffcf7663c6e64cecae432a",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -333,7 +333,7 @@ def test_process_interleaved_images_prompts_image_error(self):\n             processor(text=text, images=None, padding=True)\n \n     def test_unstructured_kwargs_batched(self):\n-        # Overriden because Mllama expects images in nested format. For 2 images it can't infer\n+        # Overridden because Mllama expects images in nested format. For 2 images it can't infer\n         # the correct nesting, so we better throw an error\n         if \"image_processor\" not in self.processor_class.attributes:\n             self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")"
        },
        {
            "sha": "9aa3ae19d008ae479b36bba2dfd17b76e4536d7d",
            "filename": "tests/models/mluke/test_tokenization_mluke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmluke%2Ftest_tokenization_mluke.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -120,7 +120,7 @@ def test_embeded_special_tokens(self):\n \n                 tokens_p_str = tokenizer_p.convert_ids_to_tokens(tokens_p[\"input_ids\"])\n \n-                # Rust correctly handles the space before the mask while python doesnt\n+                # Rust correctly handles the space before the mask while python doesn't\n                 self.assertSequenceEqual(tokens_p[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n \n                 self.assertSequenceEqual("
        },
        {
            "sha": "7b98d740b31249dce2b42f0a4e95403e50b3f133",
            "filename": "tests/models/mt5/test_modeling_flax_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -34,7 +34,7 @@ class MT5IntegrationTest(unittest.TestCase):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n "
        },
        {
            "sha": "09c78b047ab96113d381aa8e0d5e2f2e0853e167",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -853,7 +853,7 @@ def test_encoder_decoder_shared_weights(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -1063,7 +1063,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -1100,7 +1100,7 @@ class MT5IntegrationTest(unittest.TestCase):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n "
        },
        {
            "sha": "f7b77014d409285c1d8cd81da8313086e3cf6bac",
            "filename": "tests/models/mt5/test_modeling_tf_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -33,7 +33,7 @@ class TFMT5ModelIntegrationTest(unittest.TestCase):\n     @slow\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparision run:\n+        For comparison run:\n         >>> import t5  # pip install t5==0.7.1\n         >>> from t5.data.sentencepiece_vocabulary import SentencePieceVocabulary\n "
        },
        {
            "sha": "e725cffb53fdd03957066072304cace06156ec83",
            "filename": "tests/models/mvp/test_tokenization_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_tokenization_mvp.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -182,7 +182,7 @@ def test_embeded_special_tokens(self):\n                 tokens_r_str = tokenizer_r.convert_ids_to_tokens(tokens_r[\"input_ids\"])\n                 tokens_p_str = tokenizer_p.convert_ids_to_tokens(tokens_p[\"input_ids\"])\n \n-                # Rust correctly handles the space before the mask while python doesnt\n+                # Rust correctly handles the space before the mask while python doesn't\n                 self.assertSequenceEqual(tokens_p[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n                 self.assertSequenceEqual(tokens_r[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n "
        },
        {
            "sha": "2eb3f2fbfc70fcec581f8294c896c8132318efa9",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -491,11 +491,11 @@ def test_top_2_routing(self):\n         mask = mask.reshape(-1)\n         set_seed(0)\n         hidden_states = torch.rand((self.batch_size, self.sequence_length, self.config.hidden_size))\n-        classfier = torch.nn.Linear(self.config.hidden_size, self.config.num_experts)\n+        classifier = torch.nn.Linear(self.config.hidden_size, self.config.num_experts)\n         hf_router = NllbMoeTop2Router(self.config)\n \n         _, _, hidden_dim = hidden_states.shape\n-        logits = classfier(hidden_states.reshape((self.batch_size * self.sequence_length), hidden_dim))\n+        logits = classifier(hidden_states.reshape((self.batch_size * self.sequence_length), hidden_dim))\n         top_1_mask, router_probs = hf_router.route_tokens(logits, padding_mask=mask)\n         torch.argmax(top_1_mask, dim=-1)\n         router_mask = router_probs.bool()"
        },
        {
            "sha": "ea00be138db873535a6ad315e13f4560d661680f",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -329,7 +329,7 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"VLMs doen't accept inputs embeds and pixel values at the same time. So if the test passed for bacbone LM, it passes for VLM also\"\n+        reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n     )\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass"
        },
        {
            "sha": "4dad8b75fbbebd8b18f076874047d695a33ebaed",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -326,7 +326,7 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"VLMs doen't accept inputs embeds and pixel values at the same time. So if the test passed for bacbone LM, it passes for VLM also\"\n+        reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n     )\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass"
        },
        {
            "sha": "8d18a163e8a8ed518e19b860eddc8dd75b233ea0",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -526,7 +526,7 @@ def test_training_gradient_checkpointing(self):\n             loss = model(**inputs).loss\n             loss.backward()\n \n-    # override as the `logit_scale` parameter initilization is different for Pix2Struct\n+    # override as the `logit_scale` parameter initialization is different for Pix2Struct\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -535,7 +535,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n-                    # check if `logit_scale` is initilized as per the original implementation\n+                    # check if `logit_scale` is initialized as per the original implementation\n                     if name == \"logit_scale\":\n                         self.assertAlmostEqual(\n                             param.data.item(),"
        },
        {
            "sha": "caaeb78a2e9449d3b6ee9315dedfc2cc515f6d00",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -160,7 +160,7 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"image_std\"))\n             self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n-    # The following tests are overriden as PixtralImageProcessor can return images of different sizes\n+    # The following tests are overridden as PixtralImageProcessor can return images of different sizes\n     # and thus doesn't support returning batched tensors\n \n     def test_call_pil(self):"
        },
        {
            "sha": "0a4a773faac21807f25724020348ffad179c6e51",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -575,7 +575,7 @@ def test_encoder_decoder_shared_weights(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)"
        },
        {
            "sha": "b9632b21bbe51ebc2afc782464a08631ec8688fd",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -978,7 +978,7 @@ def test_causal_lm_from_pretrained(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.check_causal_lm_from_pretrained(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)"
        },
        {
            "sha": "5a1e7615fff8cadcdd7ed9ee5be616aafdec9ea2",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -368,7 +368,7 @@ def test_load_balancing_loss(self):\n         padded_result = model(padded_input_ids, attention_mask=padded_attention_mask)\n         torch.testing.assert_close(result.aux_loss.cpu(), padded_result.aux_loss.cpu(), rtol=1e-4, atol=1e-4)\n \n-        # We make sure that the loss of includding padding tokens != the loss without padding tokens\n+        # We make sure that the loss of including padding tokens != the loss without padding tokens\n         # if attention_mask=None --> we don't exclude padding tokens\n         include_padding_result = model(padded_input_ids, attention_mask=None)\n "
        },
        {
            "sha": "e213ccd819bd19e63437b2a77ef8b1314107b928",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -226,7 +226,7 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successfull forward with no modifications\n+            _ = model(**input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n             patch_size = config.vision_config.patch_size"
        },
        {
            "sha": "af3cf160322a246edc990dd1f2e0b40fbed70062",
            "filename": "tests/models/qwen3_moe/test_modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_moe%2Ftest_modeling_qwen3_moe.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -362,7 +362,7 @@ def test_load_balancing_loss(self):\n         padded_result = model(padded_input_ids, attention_mask=padded_attention_mask)\n         torch.testing.assert_close(result.aux_loss.cpu(), padded_result.aux_loss.cpu(), rtol=1e-4, atol=1e-4)\n \n-        # We make sure that the loss of includding padding tokens != the loss without padding tokens\n+        # We make sure that the loss of including padding tokens != the loss without padding tokens\n         # if attention_mask=None --> we don't exclude padding tokens\n         include_padding_result = model(padded_input_ids, attention_mask=None)\n "
        },
        {
            "sha": "0189129e8c377f8d50eb6187e47365f3aff71fd2",
            "filename": "tests/models/rag/test_modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_rag.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -67,7 +67,7 @@\n \n \n def _assert_tensors_equal(a, b, atol=1e-12, prefix=\"\"):\n-    \"\"\"If tensors not close, or a and b arent both tensors, raise a nice Assertion error.\"\"\"\n+    \"\"\"If tensors not close, or a and b aren't both tensors, raise a nice Assertion error.\"\"\"\n     if a is None and b is None:\n         return True\n     try:"
        },
        {
            "sha": "0fe0290a38455c9fd34a9445585cad2393acc83e",
            "filename": "tests/models/roberta/test_tokenization_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_tokenization_roberta.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -198,7 +198,7 @@ def test_embeded_special_tokens(self):\n                 tokens_r_str = tokenizer_r.convert_ids_to_tokens(tokens_r[\"input_ids\"])\n                 tokens_p_str = tokenizer_p.convert_ids_to_tokens(tokens_p[\"input_ids\"])\n \n-                # Rust correctly handles the space before the mask while python doesnt\n+                # Rust correctly handles the space before the mask while python doesn't\n                 self.assertSequenceEqual(tokens_p[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n                 self.assertSequenceEqual(tokens_r[\"input_ids\"], [0, 250, 6, 50264, 3823, 487, 21992, 3645, 4, 2])\n "
        },
        {
            "sha": "cce221a2192aa894ce11130f86c6147552c7d21a",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -236,13 +236,13 @@ def test_training_gradient_checkpointing(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n@@ -668,7 +668,7 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info"
        },
        {
            "sha": "a62ef3b743b3489bd5b5b10a65980502af41d080",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -375,7 +375,7 @@ def test_apply_chat_template_video_special_processing(self):\n \n     @require_av\n     def test_apply_chat_template_video_frame_sampling(self):\n-        # overriden because SmolVLM has special preprocessing for videos\n+        # overridden because SmolVLM has special preprocessing for videos\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")"
        },
        {
            "sha": "7858cd4b1b16c298023a98c78e4a12a2179b7ea5",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -858,7 +858,7 @@ def test_encoder_decoder_shared_weights(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_encoder_decoder_shared_weights(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -1066,7 +1066,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)"
        },
        {
            "sha": "63dd76eef3e37aa07d16ea5ed509fe93ad19ac09",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -319,7 +319,7 @@ def test_generate_with_past_key_values(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_generate_with_past_key_values(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)"
        },
        {
            "sha": "c5ba7484aa67a9177b962b09b9a973e1f465fa72",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -484,7 +484,7 @@ def test_with_sequence_classification_head(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_with_sequence_classification_head(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -701,7 +701,7 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n-    @unittest.skipIf(torch_device == \"cpu\", \"Cant do half precision\")\n+    @unittest.skipIf(torch_device == \"cpu\", \"Can't do half precision\")\n     def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n@@ -741,7 +741,7 @@ class Umt5IntegrationTest(unittest.TestCase):\n     )\n     def test_small_integration_test(self):\n         \"\"\"\n-        For comparison run the kaggle notbook available here : https://www.kaggle.com/arthurzucker/umt5-inference\n+        For comparison run the kaggle notebook available here : https://www.kaggle.com/arthurzucker/umt5-inference\n         \"\"\"\n \n         model = UMT5ForConditionalGeneration.from_pretrained(\"google/umt5-small\", return_dict=True).to(torch_device)"
        },
        {
            "sha": "91efea72f5fb88a74b2f5f6201ac0a7e6ba4b8b9",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -200,7 +200,7 @@ def create_and_check_model_forward(self, config, inputs_dict):\n \n     def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n         model = TFWhisperModel(config=config).get_decoder()\n-        # take a slice so we're shorter than the seqeuence length and can append later\n+        # take a slice so we're shorter than the sequence length and can append later\n         input_ids = inputs_dict[\"decoder_input_ids\"][:, :-10]\n         attention_mask = inputs_dict[\"decoder_attention_mask\"][:, :-10]\n "
        },
        {
            "sha": "9ec71d635eaee48ac5f250a92739502534ac6fde",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -104,7 +104,7 @@ def set_begin_index(self, begin_index: int):\n             self.begin_index = begin_index\n \n         def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n-            # we don't want to randomely sample timestamp tokens\n+            # we don't want to randomly sample timestamp tokens\n             if input_ids.shape[-1] != self.begin_index:\n                 scores[:, self.timestamp_begin :] = -float(\"inf\")\n \n@@ -562,7 +562,7 @@ def test_generate_with_head_masking(self):\n \n     @parameterized.expand([(\"offloaded\",)])\n     @pytest.mark.generate\n-    @unittest.skip(reason=\"Whisper doesnt work with offloaded cache implementation yet\")\n+    @unittest.skip(reason=\"Whisper doesn't work with offloaded cache implementation yet\")\n     def test_offloaded_cache_implementation(self, cache_implementation):\n         pass\n "
        },
        {
            "sha": "3444a58b32fe0d81bb385e6fec0e042e3ebf9d7a",
            "filename": "tests/models/xglm/test_tokenization_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_tokenization_xglm.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -181,7 +181,7 @@ def test_tokenization_base_easy_symbols(self):\n     def test_tokenization_base_hard_symbols(self):\n         symbols = (\n             'This is a very long text with a lot of weird characters, such as: . , ~ ? ( ) \" [ ] ! : - . Also we will'\n-            \" add words that should not exsist and be tokenized to unk, such as saoneuhaoesuth\"\n+            \" add words that should not exist and be tokenized to unk, such as saoneuhaoesuth\"\n         )\n         original_tokenizer_encodings = [2, 1018, 67, 11, 1988, 2617, 5631, 278, 11, 3407, 48, 71630, 28085, 4, 3234, 157, 13, 6, 5, 6, 4, 3526, 768, 15, 659, 57, 298, 3983, 864, 129, 21, 6, 5, 13675, 377, 652, 7580, 10341, 155, 2817, 422, 1666, 7, 1674, 53, 113, 202277, 17892, 33, 60, 87, 4, 3234, 157, 61, 2667, 52376, 19, 88, 23, 735]  # fmt: skip\n "
        },
        {
            "sha": "41f37f846863a199e80be44d2d07d7fff96acf45",
            "filename": "tests/models/xlm_roberta/test_tokenization_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_tokenization_xlm_roberta.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -258,7 +258,7 @@ def test_tokenization_base_easy_symbols(self):\n     def test_tokenization_base_hard_symbols(self):\n         symbols = (\n             'This is a very long text with a lot of weird characters, such as: . , ~ ? ( ) \" [ ] ! : - . Also we will'\n-            \" add words that should not exsist and be tokenized to <unk>, such as saoneuhaoesuth\"\n+            \" add words that should not exist and be tokenized to <unk>, such as saoneuhaoesuth\"\n         )\n         original_tokenizer_encodings = [\n             0,"
        },
        {
            "sha": "a11ac6106cd55cee9039da8328099e64a3efc57f",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -324,7 +324,7 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n \n     def test_past_key_values_format(self):\n         \"\"\"\n-        Overwritting to pass the expected cache shapes (Zamba2 has cache shape = [batch_size, 0] for mamba layers)\n+        Overwriting to pass the expected cache shapes (Zamba2 has cache shape = [batch_size, 0] for mamba layers)\n         \"\"\"\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         batch_size, seq_length = inputs[\"input_ids\"].shape"
        },
        {
            "sha": "692c0df43b2092066d61331614ae94077193a95d",
            "filename": "tests/quantization/autoround/test_auto_round.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -152,7 +152,7 @@ def test_quantized_model_multi_gpu(self):\n \n     def test_convert_from_gptq(self):\n         \"\"\"\n-        Simple test that checks if auto-round work properly wth gptq format\n+        Simple test that checks if auto-round work properly with gptq format\n         \"\"\"\n         model_name = \"ybelkada/opt-125m-gptq-4bit\"\n \n@@ -170,7 +170,7 @@ def test_convert_from_gptq(self):\n     @require_intel_extension_for_pytorch\n     def test_convert_from_awq_cpu(self):\n         \"\"\"\n-        Simple test that checks if auto-round work properly wth awq format\n+        Simple test that checks if auto-round work properly with awq format\n         \"\"\"\n         model_name = \"casperhansen/opt-125m-awq\"\n \n@@ -187,7 +187,7 @@ def test_convert_from_awq_cpu(self):\n \n     def test_mixed_bits(self):\n         \"\"\"\n-        Simple test that checks if auto-round work properly wth mixed bits\n+        Simple test that checks if auto-round work properly with mixed bits\n         \"\"\"\n         model_name = \"facebook/opt-125m\"\n         model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\")"
        },
        {
            "sha": "de6e008316337d2f3eddf9346fc5b8502ae1c732",
            "filename": "tests/quantization/bnb/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fbnb%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fbnb%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2FREADME.md?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -67,7 +67,7 @@ Same solution as above.\n \n ### `RuntimeError: CUDA error: an illegal memory access was encountered ... consider passing CUDA_LAUNCH_BLOCKING=1`\n \n-Run your script by pre-pending `CUDA_LAUNCH_BLOCKING=1` and you should observe an error as described in the next section.\n+Run your script by prepending `CUDA_LAUNCH_BLOCKING=1` and you should observe an error as described in the next section.\n \n ### `CUDA illegal memory error: an illegal memory access at line...`:\n "
        },
        {
            "sha": "cfd489b2bed383b458cf17e9d49bb81c6d41d570",
            "filename": "tests/quantization/quark_integration/test_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquark_integration%2Ftest_quark.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -97,7 +97,7 @@ def test_device_and_dtype_assignment(self):\n \n     def test_original_dtype(self):\n         r\"\"\"\n-        A simple test to check if the model succesfully stores the original dtype\n+        A simple test to check if the model successfully stores the original dtype\n         \"\"\"\n         self.assertTrue(hasattr(self.quantized_model.config, \"_pre_quantization_dtype\"))\n         self.assertFalse(hasattr(self.model_fp16.config, \"_pre_quantization_dtype\"))"
        },
        {
            "sha": "f82dea2f4f8be9118ca96d47c32eace85f33bc54",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -3429,7 +3429,7 @@ def test_attn_implementation_composite_models(self):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         \"\"\"\n         Tests if non-composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         \"\"\"\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n@@ -3467,7 +3467,7 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n         Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n-        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        This tests only by looking at layer names, as usually SDPA layers are called \"SDPAAttention\".\n         In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n         is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n         See https://github.com/huggingface/transformers/pull/32238 for more info"
        },
        {
            "sha": "f4ffc3cb11760ca973abab99d1a5228767f9cde3",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5fa7d2d199b41711a2e16aec84ee54613633898/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=d5fa7d2d199b41711a2e16aec84ee54613633898",
            "patch": "@@ -185,7 +185,7 @@ def test_config_from_string(self):\n     def test_config_common_kwargs_is_complete(self):\n         base_config = PretrainedConfig()\n         missing_keys = [key for key in base_config.__dict__ if key not in config_common_kwargs]\n-        # If this part of the test fails, you have arguments to addin config_common_kwargs above.\n+        # If this part of the test fails, you have arguments to add in config_common_kwargs above.\n         self.assertListEqual(\n             missing_keys,\n             ["
        }
    ],
    "stats": {
        "total": 404,
        "additions": 202,
        "deletions": 202
    }
}