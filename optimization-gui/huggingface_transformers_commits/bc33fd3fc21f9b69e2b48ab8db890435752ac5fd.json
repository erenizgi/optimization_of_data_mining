{
    "author": "JJJYmmm",
    "message": "Add processor and intergration test for qwen3vl (#41277)\n\n* support aux loss in qwen3vlmoe\n\n* update qwen3vl processor test!\n\n* add integration tests for qwen3vl-30a3\n\n* remove duplicated decorator\n\n* code clean\n\n* fix consistency\n\n* do not inherit from nn.Linear for better quantization\n\n* pass check",
    "sha": "bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
    "files": [
        {
            "sha": "a6eec74f8009ff1767f61f72f99f16e082a05958",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig\n@@ -1104,7 +1104,7 @@ def get_placeholder_mask(\n         return special_image_mask, special_video_mask\n \n     @auto_docstring\n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1235,8 +1235,6 @@ def forward(\n         return Qwen3VLModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=self.rope_deltas,\n         )\n \n@@ -1313,8 +1311,7 @@ def language_model(self):\n     def visual(self):\n         return self.model.visual\n \n-    @can_return_tuple\n-    @auto_docstring\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1372,8 +1369,6 @@ def forward(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=outputs.rope_deltas,\n         )\n "
        },
        {
            "sha": "9c479c6bdd2328f0a7d8d37612a6c82c5374312a",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -33,7 +33,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n from ...utils.generic import check_model_inputs\n from ...video_utils import VideoInput\n from ..qwen2_5_vl.modeling_qwen2_5_vl import (\n@@ -1006,7 +1006,7 @@ def get_video_features(\n         return self.get_image_features(pixel_values_videos, video_grid_thw)\n \n     @auto_docstring\n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1137,8 +1137,6 @@ def forward(\n         return Qwen3VLModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=self.rope_deltas,\n         )\n \n@@ -1151,6 +1149,7 @@ class Qwen3VLForConditionalGeneration(Qwen2_5_VLForConditionalGeneration):\n     config: Qwen3VLConfig\n     _checkpoint_conversion_mapping = {}\n \n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1208,8 +1207,6 @@ def forward(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=outputs.rope_deltas,\n         )\n "
        },
        {
            "sha": "25358aa79bff482437632829f6319effad36f138",
            "filename": "src/transformers/models/qwen3_vl_moe/configuration_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fconfiguration_qwen3_vl_moe.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -80,6 +80,8 @@ class Qwen3VLMoeTextConfig(PretrainedConfig):\n             Number of routed experts.\n         norm_topk_prob (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the topk probabilities.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n         mlp_only_layers (`List[int]`, *optional*, defaults to `[]`):\n             Indicate which layers use Qwen3VLMoeMLP rather than Qwen3VLMoeSparseMoeBlock\n             The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n@@ -178,6 +180,7 @@ def __init__(\n         num_experts_per_tok=4,\n         num_experts=60,\n         norm_topk_prob=True,\n+        router_aux_loss_coef=0.001,\n         mlp_only_layers=None,\n         rope_scaling=None,\n         head_dim=None,\n@@ -213,6 +216,7 @@ def __init__(\n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_experts = num_experts\n         self.norm_topk_prob = norm_topk_prob\n+        self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)"
        },
        {
            "sha": "7a9952786bfbd6a8c7d41e76ab8aa0f29542bf0e",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 177,
            "deletions": 54,
            "changes": 231,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -37,7 +37,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n@@ -64,25 +64,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class Qwen3VLMoeTextRouter(nn.Linear):\n-    def __init__(self, config):\n-        super().__init__(config.hidden_size, config.num_experts, bias=False)\n-        self.hidden_size = config.hidden_size\n-        self.top_k = config.num_experts_per_tok\n-        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n-        # self.norm_topk_prob = config.norm_topk_prob\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n-        router_logits = super().forward(hidden_states)\n-        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n-        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n-        return router_weights, router_logits, router_indices\n-\n-\n class Qwen3VLMoeTextExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -150,11 +131,23 @@ def __init__(self, config):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.num_experts = config.num_experts\n-        self.gate = Qwen3VLMoeTextRouter(config)\n+        self.top_k = config.num_experts_per_tok\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n         self.experts = Qwen3VLMoeTextExperts(config)\n \n+        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n+        # self.norm_topk_prob = config.norm_topk_prob\n+\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        router_weights, router_logits, router_indices = self.gate(hidden_states)\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n+        router_logits = self.gate(hidden_states)\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n+        hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n         routed_out = self.experts(hidden_states, router_weights, router_indices)\n         return routed_out\n \n@@ -971,6 +964,36 @@ def _deepstack_process(\n         return hidden_states\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Qwen3VLMoe causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class Qwen3VLMoeCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+    aux_loss: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1216,7 +1239,7 @@ def get_placeholder_mask(\n         return special_image_mask, special_video_mask\n \n     @auto_docstring\n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1347,39 +1370,90 @@ def forward(\n         return Qwen3VLMoeModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=self.rope_deltas,\n         )\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for Qwen3VLMoe causal language model (or autoregressive) outputs.\n-    \"\"\"\n-)\n-class Qwen3VLMoeCausalLMOutputWithPast(ModelOutput):\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n     r\"\"\"\n-    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-        Language modeling loss (for next-token prediction).\n-    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n \n-        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-        `past_key_values` input) to speed up sequential decoding.\n-    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-        The rope index difference between sequence length and multimodal rope.\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n     \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Cache] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    rope_deltas: Optional[torch.LongTensor] = None\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n \n \n class Qwen3VLMoeForConditionalGeneration(Qwen3VLMoePreTrainedModel, GenerationMixin):\n@@ -1425,8 +1499,7 @@ def language_model(self):\n     def visual(self):\n         return self.model.visual\n \n-    @can_return_tuple\n-    @auto_docstring\n+    @check_model_inputs\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -1454,8 +1527,46 @@ def forward(\n             The temporal, height and width of feature shape of each video in LLM.\n \n         Example:\n-            TODO: Add example\n-        \"\"\"\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration\n+\n+        >>> model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image in short.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> # Preparation for inference\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+        >>> inputs = inputs.to(model.device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=128)\n+        >>> generated_ids_trimmed = [\n+            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+        ]\n+        >>> processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"A woman in a plaid shirt sits on a sandy beach at sunset, smiling as she gives a high-five to a yellow Labrador Retriever wearing a harness. The ocean waves roll in the background.\"\n+        ```\"\"\"\n+\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,\n@@ -1480,12 +1591,24 @@ def forward(\n         if labels is not None:\n             loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n+        aux_loss = None\n+        if kwargs.get(\"output_router_logits\", False):\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.config.text_config.num_experts,\n+                self.config.text_config.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.config.text_config.router_aux_loss_coef * aux_loss.to(\n+                    loss.device\n+                )  # make sure to reside in the same device\n+\n         return Qwen3VLMoeCausalLMOutputWithPast(\n             loss=loss,\n+            aux_loss=aux_loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n             rope_deltas=outputs.rope_deltas,\n         )\n "
        },
        {
            "sha": "30dda5f99497ceea6310c0b90e22a7e531c774e6",
            "filename": "src/transformers/models/qwen3_vl_moe/modular_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 140,
            "deletions": 23,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodular_qwen3_vl_moe.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -14,21 +14,27 @@\n # limitations under the License.\n \"\"\"PyTorch Qwen3-VL-MOE model.\"\"\"\n \n+from typing import Optional, Union\n+\n import torch\n import torch.nn as nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import PreTrainedModel\n-from ...utils import logging\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, logging\n from ..qwen3_moe.modeling_qwen3_moe import (\n     Qwen3MoeDecoderLayer,\n     Qwen3MoePreTrainedModel,\n     Qwen3MoeRMSNorm,\n+    load_balancing_loss_func,\n )\n from ..qwen3_vl.configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLVisionConfig\n from ..qwen3_vl.modeling_qwen3_vl import (\n+    Qwen3VLCausalLMOutputWithPast,\n     Qwen3VLForConditionalGeneration,\n     Qwen3VLModel,\n     Qwen3VLTextAttention,\n@@ -98,6 +104,8 @@ class Qwen3VLMoeTextConfig(PretrainedConfig):\n             Number of routed experts.\n         norm_topk_prob (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the topk probabilities.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n         mlp_only_layers (`List[int]`, *optional*, defaults to `[]`):\n             Indicate which layers use Qwen3VLMoeMLP rather than Qwen3VLMoeSparseMoeBlock\n             The list contains layer index, from 0 to num_layers-1 if we have num_layers layers\n@@ -196,6 +204,7 @@ def __init__(\n         num_experts_per_tok=4,\n         num_experts=60,\n         norm_topk_prob=True,\n+        router_aux_loss_coef=0.001,\n         mlp_only_layers=None,\n         rope_scaling=None,\n         head_dim=None,\n@@ -231,6 +240,7 @@ def __init__(\n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_experts = num_experts\n         self.norm_topk_prob = norm_topk_prob\n+        self.router_aux_loss_coef = router_aux_loss_coef\n         self.mlp_only_layers = [] if mlp_only_layers is None else mlp_only_layers\n \n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n@@ -288,25 +298,6 @@ class Qwen3VLMoeTextRMSNorm(Qwen3MoeRMSNorm):\n     pass\n \n \n-class Qwen3VLMoeTextRouter(nn.Linear):\n-    def __init__(self, config):\n-        super().__init__(config.hidden_size, config.num_experts, bias=False)\n-        self.hidden_size = config.hidden_size\n-        self.top_k = config.num_experts_per_tok\n-        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n-        # self.norm_topk_prob = config.norm_topk_prob\n-\n-    def forward(self, hidden_states):\n-        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n-        router_logits = super().forward(hidden_states)\n-        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n-        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n-        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n-        routing_weights = routing_weights.to(hidden_states.dtype)\n-        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n-        return router_weights, router_logits, router_indices\n-\n-\n class Qwen3VLMoeTextExperts(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -374,11 +365,23 @@ def __init__(self, config):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.num_experts = config.num_experts\n-        self.gate = Qwen3VLMoeTextRouter(config)\n+        self.top_k = config.num_experts_per_tok\n+        self.gate = nn.Linear(config.hidden_size, config.num_experts, bias=False)\n         self.experts = Qwen3VLMoeTextExperts(config)\n \n+        # since all the models use norm_topk_prob, we don't need to have a extra check for it\n+        # self.norm_topk_prob = config.norm_topk_prob\n+\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n-        router_weights, router_logits, router_indices = self.gate(hidden_states)\n+        batch_size = hidden_states.shape[0]\n+        hidden_states = hidden_states.reshape(-1, self.hidden_size)\n+        router_logits = self.gate(hidden_states)\n+        routing_weights = torch.nn.functional.softmax(router_logits, dim=-1, dtype=torch.float)\n+        routing_weights, router_indices = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+        router_weights = torch.zeros_like(router_logits).scatter_(1, router_indices, routing_weights)\n+        hidden_states = hidden_states.reshape(batch_size, -1, self.hidden_size)\n         routed_out = self.experts(hidden_states, router_weights, router_indices)\n         return routed_out\n \n@@ -415,12 +418,126 @@ class Qwen3VLMoeTextModel(Qwen3VLTextModel):\n     pass\n \n \n+class Qwen3VLMoeCausalLMOutputWithPast(Qwen3VLCausalLMOutputWithPast):\n+    aux_loss: Optional[torch.FloatTensor] = None\n+\n+\n class Qwen3VLMoeModel(Qwen3VLModel):\n     pass\n \n \n class Qwen3VLMoeForConditionalGeneration(Qwen3VLForConditionalGeneration):\n-    pass\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each image in LLM.\n+        video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n+            The temporal, height and width of feature shape of each video in LLM.\n+\n+        Example:\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration\n+\n+        >>> model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this image in short.\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> # Preparation for inference\n+        >>> inputs = processor.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\"\n+        )\n+        >>> inputs = inputs.to(model.device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=128)\n+        >>> generated_ids_trimmed = [\n+            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n+        ]\n+        >>> processor.batch_decode(generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"A woman in a plaid shirt sits on a sandy beach at sunset, smiling as she gives a high-five to a yellow Labrador Retriever wearing a harness. The ocean waves roll in the background.\"\n+        ```\"\"\"\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        aux_loss = None\n+        if kwargs.get(\"output_router_logits\", False):\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.config.text_config.num_experts,\n+                self.config.text_config.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.config.text_config.router_aux_loss_coef * aux_loss.to(\n+                    loss.device\n+                )  # make sure to reside in the same device\n+\n+        return Qwen3VLMoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            rope_deltas=outputs.rope_deltas,\n+        )\n \n \n __all__ = ["
        },
        {
            "sha": "9ce056a207ac6dc08c1186951f3c514242ba58bb",
            "filename": "tests/models/qwen3_vl/test_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 14,
            "deletions": 17,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -37,15 +37,14 @@\n @require_vision\n @require_torch\n @require_torchvision\n-@unittest.skip(\"The checkpoint is not yet released\")\n class Qwen3VLProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = Qwen3VLProcessor\n \n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         processor = Qwen3VLProcessor.from_pretrained(\n-            \"Qwen/Qwen3-VL-4B-Instruct\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n+            \"Qwen/Qwen3-VL-235B-A22B-Instruct\", patch_size=4, max_pixels=56 * 56, min_pixels=28 * 28\n         )\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image_token = processor.image_token\n@@ -139,21 +138,15 @@ def test_processor(self):\n             processor(images=image_input)\n \n     def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        video_processor = self.get_video_processor()\n-\n-        processor = Qwen3VLProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n+        processor = self.get_processor()\n \n-        input_str = \"lower newer\"\n+        text = self.prepare_text_inputs(modalities=[\"image\", \"video\"])\n         image_input = self.prepare_image_inputs()\n         video_inputs = self.prepare_video_inputs()\n+        inputs_dict = {\"text\": text, \"images\": image_input, \"videos\": video_inputs}\n+        inputs = processor(**inputs_dict, return_tensors=\"pt\", do_sample_frames=False)\n \n-        inputs = processor(text=input_str, images=image_input, videos=video_inputs, do_sample_frames=False)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n \n     @require_torch\n     @require_av\n@@ -299,6 +292,9 @@ def test_apply_chat_template_video_frame_sampling(self):\n         out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n         self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n \n+        # for fast test, set the longest edge to 8192\n+        processor.video_processor.size[\"longest_edge\"] = 8192\n+\n         # Add video URL for return dict and load with `num_frames` arg\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n@@ -311,9 +307,10 @@ def test_apply_chat_template_video_frame_sampling(self):\n             tokenize=True,\n             return_dict=True,\n             num_frames=num_frames,\n+            fps=None,  # if pass num_frames, fps should be None\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 360)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 256)\n \n         # Load with `fps` arg\n         fps = 1\n@@ -325,7 +322,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             fps=fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 900)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 224)\n \n         # Load with `fps` and `num_frames` args, should raise an error\n         with self.assertRaises(ValueError):\n@@ -346,7 +343,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 27000)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 224)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video\n@@ -365,7 +362,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             do_sample_frames=False,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 160)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 216)\n \n     def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         processor = self.get_processor()"
        },
        {
            "sha": "63ea91e9d0279b686622be1fef3a04cc8beefc5a",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 282,
            "deletions": 0,
            "changes": 282,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc33fd3fc21f9b69e2b48ab8db890435752ac5fd/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=bc33fd3fc21f9b69e2b48ab8db890435752ac5fd",
            "patch": "@@ -17,13 +17,18 @@\n import unittest\n \n from transformers import (\n+    AutoProcessor,\n     Qwen3VLMoeConfig,\n     Qwen3VLMoeForConditionalGeneration,\n     Qwen3VLMoeModel,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n+    cleanup,\n+    require_flash_attn,\n     require_torch,\n+    require_torch_gpu,\n+    slow,\n     torch_device,\n )\n \n@@ -296,3 +301,280 @@ def test_video_forward(self):\n                 video_grid_thw=video_grid_thw,\n             )\n             self.assertIsNotNone(outputs)\n+\n+\n+@require_torch\n+@unittest.skip(\"The checkpoint is not yet released\")\n+class Qwen3VLMoeIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+        self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen3-VL-30B-A3B-Instruct\")\n+        self.processor.tokenizer.padding_side = \"left\"\n+        self.message = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What kind of dog is this?\"},\n+                ],\n+            }\n+        ]\n+        self.message2 = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What kind of dog is this?\"},\n+                ],\n+            }\n+        ]\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_small_model_integration_test(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+\n+        inputs = self.processor.apply_chat_template(\n+            self.message, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        )\n+        expected_input_ids = [151644, 872, 198, 151652, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655, 151655]  # fmt: skip\n+        self.assertListEqual(expected_input_ids, inputs.input_ids[0].tolist()[:17])\n+\n+        expected_pixel_slice = torch.tensor(\n+            [\n+                [-0.0902, -0.0824, -0.0824],\n+                [-0.2627, -0.2627, -0.2627],\n+                [-0.0824, -0.0902, -0.0902],\n+                [-0.0118, -0.0510, -0.1137],\n+                [-0.5137, -0.5529, -0.6078],\n+                [-0.6941, -0.6314, -0.5765],\n+            ],\n+            dtype=torch.float32,\n+            device=\"cpu\",\n+        )\n+        self.assertTrue(torch.allclose(expected_pixel_slice, inputs.pixel_values[:6, :3], atol=3e-3))\n+\n+        # verify generation\n+        inputs = inputs.to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        EXPECTED_DECODED_TEXT = \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a small wild cat native to the grasslands and steppes\"\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_batch(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+        batch_messages = [self.message] * 2\n+        inputs = self.processor.apply_chat_template(\n+            batch_messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a small wild cat native to the grasslands and montane regions\",\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a small wild cat native to the grasslands and montane regions\"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_with_video(self):\n+        processor = AutoProcessor.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", max_image_size={\"longest_edge\": 50176}\n+        )\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=torch.float16, device_map=\"auto\"\n+        )\n+        questions = [\"How long is the video? Describe the it in short.\"]\n+        video_urls = [\"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"]\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"video\": video_url,\n+                        },\n+                        {\"type\": \"text\", \"text\": question},\n+                    ],\n+                }\n+            ]\n+            for question, video_url in zip(questions, video_urls)\n+        ]\n+        inputs = processor.apply_chat_template(\n+            messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\", padding=True\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+        EXPECTED_DECODED_TEXT = [\"user\\n<0.3 seconds><1.4 seconds><2.5 seconds><3.6 seconds><4.7 seconds><5.8 seconds>How long is the video? Describe the it in short.\\nassistant\\nThe video is 6 seconds long. It shows a man playing tennis on an indoor court. He is wearing a white shirt and black shorts. He\"]  # fmt: skip\n+\n+        self.assertEqual(\n+            processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_expand(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+        inputs = self.processor.apply_chat_template(\n+            self.message, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, num_beams=2, num_return_sequences=2)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThe animal in the image is not a dog. It is a **Pallas's cat** (*Otocolobus manul*), also known\",\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThe animal in the image is not a dog. It is a **Pallas's cat** (also known as the manul), a wild f\"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_batch_wo_image(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+        message_wo_image = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who are you?\"}]},\n+        ]\n+        batched_messages = [self.message, message_wo_image]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and steppes\",\n+            \"user\\nWho are you?\\nassistant\\nI am Qwen, a large-scale language model developed by Alibaba Cloud's Tongyi Lab. I can assist you with answering questions, creating text such\"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    def test_small_model_integration_test_batch_different_resolutions(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\", dtype=\"auto\", device_map=\"auto\"\n+        )\n+        batched_messages = [self.message, self.message2]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and steppes\",\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nBased on the image provided, the animals are not dogs. They are two cats.\\n\\nHere is a description of the animals in the image:\\n\\n-  \"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_flashatt2(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n+            dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=\"auto\",\n+        )\n+        batched_messages = [self.message, self.message2]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and montane regions\",\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nBased on the image provided, there is no dog present. The animals in the picture are two cats.\\n\\nHere are some observations about the cats in the\"\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @slow\n+    @require_flash_attn\n+    @require_torch_gpu\n+    def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n+        model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n+            dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+            device_map=\"auto\",\n+        )\n+        message_wo_image = [\n+            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Who are you?\"}]},\n+        ]\n+        batched_messages = [self.message, message_wo_image]\n+        inputs = self.processor.apply_chat_template(\n+            batched_messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            padding=True,\n+        ).to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\nWhat kind of dog is this?\\nassistant\\nThis is a Pallas's cat, also known as the manul. It's a wild cat species native to the grasslands and montane regions\",\n+            \"user\\nWho are you?\\nassistant\\nI am Qwen, a large-scale language model developed by Alibaba Cloud's Tongyi Lab. I can assist you with answering questions, creating text such\"\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        }
    ],
    "stats": {
        "total": 731,
        "additions": 623,
        "deletions": 108
    }
}