{
    "author": "yonigozlan",
    "message": "Add Idefics2/3 and SmolVLM Fast image processors + improvements for fast image processors (#38157)\n\n* add working idefics2 fast and improvements for fast nested images processing\n\n* add fast image processors idefics 3 and smolvlm\n\n* cleanup tests\n\n* fic doc idefics2\n\n* PR review and fix issues after merge\n\n* Force providing disable_grouping to group_images_by_shape\n\n* simplify group_images_by_shape\n\n* fix modular\n\n* Fix nits after review",
    "sha": "d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
    "files": [
        {
            "sha": "1bdb7a0b1668111b723198136ca8a346d9b537f4",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -162,7 +162,7 @@ To load and run a model using Flash Attention-2, simply change the code snippet\n ```diff\n model = Idefics2ForConditionalGeneration.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n-+    torch_dtype=torch.float16,    \n++    torch_dtype=torch.float16,\n +    attn_implementation=\"flash_attention_2\",\n ).to(device)\n ```\n@@ -184,7 +184,7 @@ Quantizing a model is as simple as passing a `quantization_config` to the model.\n + )\n model = Idefics2ForConditionalGeneration.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n-+    torch_dtype=torch.float16,    \n++    torch_dtype=torch.float16,\n +    quantization_config=quantization_config,\n ).to(device)\n ```\n@@ -218,7 +218,10 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] Idefics2ImageProcessor\n     - preprocess\n \n+## Idefics2ImageProcessorFast\n+[[autodoc]] Idefics2ImageProcessorFast\n+    - preprocess\n \n ## Idefics2Processor\n [[autodoc]] Idefics2Processor\n-    - __call__\n\\ No newline at end of file\n+    - __call__"
        },
        {
            "sha": "5a5b45bd39575a5f991cbb805c5758b74aa63f5f",
            "filename": "docs/source/en/model_doc/idefics3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics3.md?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -80,6 +80,9 @@ This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts)\n [[autodoc]] Idefics3ImageProcessor\n     - preprocess\n \n+## Idefics3ImageProcessorFast\n+[[autodoc]] Idefics3ImageProcessorFast\n+    - preprocess\n \n ## Idefics3Processor\n [[autodoc]] Idefics3Processor"
        },
        {
            "sha": "f63ff7c40a9ce4b6ad38f8994c25c776b22e3bec",
            "filename": "docs/source/en/model_doc/smolvlm.md",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmolvlm.md?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -32,7 +32,7 @@ SmolVLM2 is an adaptation of the Idefics3 model with two main differences:\n \n Input images are processed either by upsampling (if resizing is enabled) or at their original resolution. The resizing behavior depends on two parameters: do_resize and size.\n \n-Videos should not be upsampled. \n+Videos should not be upsampled.\n \n If `do_resize` is set to `True`, the model resizes images so that the longest edge is 4*512 pixels by default.\n The default resizing behavior can be customized by passing a dictionary to the `size` parameter. For example, `{\"longest_edge\": 4 * 512}` is the default, but you can change it to a different value if needed.\n@@ -192,11 +192,14 @@ print(generated_texts[0])\n [[autodoc]] SmolVLMForConditionalGeneration\n     - forward\n \n-\n ## SmolVLMImageProcessor\n [[autodoc]] SmolVLMImageProcessor\n     - preprocess\n \n+## SmolVLMImageProcessorFast\n+[[autodoc]] SmolVLMImageProcessorFast\n+    - preprocess\n+\n ## SmolVLMVideoProcessor\n [[autodoc]] SmolVLMVideoProcessor\n     - preprocess"
        },
        {
            "sha": "90c911525f7773a34ca9f97f180d5b127649f3c2",
            "filename": "src/transformers/commands/add_fast_image_processor.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_fast_image_processor.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -396,7 +396,7 @@ def add_fast_image_processor_file(\n \n     content_header = get_fast_image_processing_content_header(content_base_file)\n     content_base_file = (\n-        f\"@auto_docstring(\\n\"\n+        f\"@auto_docstring\\n\"\n         f\"class {fast_image_processor_name}(BaseImageProcessorFast):\\n\"\n         \"    # This generated class can be used as a starting point for the fast image processor.\\n\"\n         \"    # if the image processor is only used for simple augmentations, such as resizing, center cropping, rescaling, or normalizing,\\n\""
        },
        {
            "sha": "b493215ac7bc4a93aefded8fdd5fdc6e8230ae12",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 30,
            "deletions": 55,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -184,6 +184,7 @@ class DefaultFastImageProcessorKwargs(TypedDict, total=False):\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n     device: Optional[\"torch.device\"]\n+    disable_grouping: Optional[bool]\n \n \n @auto_docstring\n@@ -480,18 +481,35 @@ def _prepare_input_images(\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n         Prepare the input images for processing.\n+\n+        Args:\n+            images (`ImageInput`):\n+                The input images to process.\n+            do_convert_rgb (`bool`, *optional*):\n+                Whether to convert the images to RGB.\n+            input_data_format (`str` or `ChannelDimension`, *optional*):\n+                The input data format of the images.\n+            device (`torch.device`, *optional*):\n+                The device to put the processed images on.\n+\n+        Returns:\n+            List[`torch.Tensor`]: The processed images.\n         \"\"\"\n+\n+        # Get structured images (potentially nested)\n         images = self._prepare_images_structure(images)\n-        process_image_fn = partial(\n-            self._process_image,\n-            do_convert_rgb=do_convert_rgb,\n-            input_data_format=input_data_format,\n-            device=device,\n+\n+        process_image_partial = partial(\n+            self._process_image, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n         )\n-        # todo: yoni - check if we can parallelize this efficiently\n-        processed_images = []\n-        for image in images:\n-            processed_images.append(process_image_fn(image))\n+\n+        # Check if we have nested structure, assuming the nesting is consistent\n+        has_nested_structure = len(images) > 0 and isinstance(images[0], (list, tuple))\n+\n+        if has_nested_structure:\n+            processed_images = [[process_image_partial(img) for img in nested_list] for nested_list in images]\n+        else:\n+            processed_images = [process_image_partial(img) for img in images]\n \n         return processed_images\n \n@@ -621,11 +639,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -635,7 +654,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:\n@@ -656,47 +675,3 @@ def to_dict(self):\n         encoder_dict.pop(\"_valid_processor_keys\", None)\n         encoder_dict.pop(\"_valid_kwargs_names\", None)\n         return encoder_dict\n-\n-\n-class SemanticSegmentationMixin:\n-    def post_process_semantic_segmentation(self, outputs, target_sizes: Optional[list[tuple]] = None):\n-        \"\"\"\n-        Converts the output of [`MobileNetV2ForSemanticSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n-\n-        Args:\n-            outputs ([`MobileNetV2ForSemanticSegmentation`]):\n-                Raw outputs of the model.\n-            target_sizes (`list[Tuple]` of length `batch_size`, *optional*):\n-                List of tuples corresponding to the requested final size (height, width) of each prediction. If unset,\n-                predictions will not be resized.\n-\n-        Returns:\n-            semantic_segmentation: `list[torch.Tensor]` of length `batch_size`, where each item is a semantic\n-            segmentation map of shape (height, width) corresponding to the target_sizes entry (if `target_sizes` is\n-            specified). Each entry of each `torch.Tensor` correspond to a semantic class id.\n-        \"\"\"\n-        logits = outputs.logits\n-\n-        # Resize logits and compute semantic segmentation maps\n-        if target_sizes is not None:\n-            if len(logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n-\n-            # if is_torch_tensor(target_sizes):\n-            #     target_sizes = target_sizes.numpy()\n-\n-            semantic_segmentation = []\n-\n-            for idx in range(len(logits)):\n-                resized_logits = torch.nn.functional.interpolate(\n-                    logits[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n-                )\n-                semantic_map = resized_logits[0].argmax(dim=0)\n-                semantic_segmentation.append(semantic_map)\n-        else:\n-            semantic_segmentation = logits.argmax(dim=1)\n-            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n-\n-        return semantic_segmentation"
        },
        {
            "sha": "38acfc250a7ab9975b7c5c83f71df43d16f21d90",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 112,
            "deletions": 20,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -12,6 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from collections import defaultdict\n from collections.abc import Collection, Iterable\n from math import ceil\n from typing import Optional, Union\n@@ -841,37 +842,128 @@ def _cast_tensor_to_float(x):\n     return x.float()\n \n \n+def _group_images_by_shape(nested_images, is_nested: bool = False):\n+    \"\"\"Helper function to flatten a single level of nested image structures and group by shape.\"\"\"\n+    grouped_images = defaultdict(list)\n+    grouped_images_index = {}\n+    nested_images = [nested_images] if not is_nested else nested_images\n+    for i, sublist in enumerate(nested_images):\n+        for j, image in enumerate(sublist):\n+            key = (i, j) if is_nested else j\n+            shape = image.shape[1:]\n+            grouped_images[shape].append(image)\n+            grouped_images_index[key] = (shape, len(grouped_images[shape]) - 1)\n+\n+    return grouped_images, grouped_images_index\n+\n+\n+def _reconstruct_nested_structure(indices, processed_images):\n+    \"\"\"Helper function to reconstruct a single level nested structure.\"\"\"\n+    # Find the maximum outer index\n+    max_outer_idx = max(idx[0] for idx in indices.keys())\n+\n+    # Create the outer list\n+    result = [None] * (max_outer_idx + 1)\n+\n+    # Group indices by outer index\n+    nested_indices = defaultdict(list)\n+    for i, j in indices.keys():\n+        nested_indices[i].append(j)\n+\n+    for i in range(max_outer_idx + 1):\n+        if i in nested_indices:\n+            inner_max_idx = max(nested_indices[i])\n+            inner_list = [None] * (inner_max_idx + 1)\n+            for j in range(inner_max_idx + 1):\n+                if (i, j) in indices:\n+                    shape, idx = indices[(i, j)]\n+                    inner_list[j] = processed_images[shape][idx]\n+            result[i] = inner_list\n+\n+    return result\n+\n+\n def group_images_by_shape(\n-    images: list[\"torch.Tensor\"],\n-) -> tuple[dict[tuple[int, int], list[\"torch.Tensor\"]], dict[int, tuple[tuple[int, int], int]]]:\n+    images: Union[list[\"torch.Tensor\"], \"torch.Tensor\"],\n+    disable_grouping: bool,\n+    is_nested: bool = False,\n+) -> tuple[\n+    dict[tuple[int, int], list[\"torch.Tensor\"]], dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]\n+]:\n     \"\"\"\n     Groups images by shape.\n     Returns a dictionary with the shape as key and a list of images with that shape as value,\n     and a dictionary with the index of the image in the original list as key and the shape and index in the grouped list as value.\n-    \"\"\"\n-    grouped_images = {}\n-    grouped_images_index = {}\n-    for i, image in enumerate(images):\n-        shape = image.shape[1:]\n-        if shape not in grouped_images:\n-            grouped_images[shape] = []\n-        grouped_images[shape].append(image)\n-        grouped_images_index[i] = (shape, len(grouped_images[shape]) - 1)\n-    # stack images with the same shape\n-    grouped_images = {shape: torch.stack(images, dim=0) for shape, images in grouped_images.items()}\n+\n+    The function supports both flat lists of tensors and nested structures.\n+    The input must be either all flat or all nested, not a mix of both.\n+\n+    Args:\n+        images (Union[list[\"torch.Tensor\"], \"torch.Tensor\"]):\n+            A list of images or a single tensor\n+        disable_grouping (bool):\n+            Whether to disable grouping. If None, will be set to True if the images are on CPU, and False otherwise.\n+            This choice is based on empirical observations, as detailed here: https://github.com/huggingface/transformers/pull/38157\n+        is_nested (bool, *optional*, defaults to False):\n+            Whether the images are nested.\n+\n+    Returns:\n+        tuple[dict[tuple[int, int], list[\"torch.Tensor\"]], dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]]:\n+            - A dictionary with shape as key and list of images with that shape as value\n+            - A dictionary mapping original indices to (shape, index) tuples\n+    \"\"\"\n+    # If disable grouping is not explicitely provided, we favor disabling it if the images are on CPU, and enabling it otherwise.\n+    if disable_grouping is None:\n+        device = images[0][0].device if is_nested else images[0].device\n+        disable_grouping = device == \"cpu\"\n+\n+    if disable_grouping:\n+        if is_nested:\n+            return {(i, j): images[i][j].unsqueeze(0) for i in range(len(images)) for j in range(len(images[i]))}, {\n+                (i, j): ((i, j), 0) for i in range(len(images)) for j in range(len(images[i]))\n+            }\n+        else:\n+            return {i: images[i].unsqueeze(0) for i in range(len(images))}, {i: (i, 0) for i in range(len(images))}\n+\n+    # Handle single level nested structure\n+    grouped_images, grouped_images_index = _group_images_by_shape(images, is_nested)\n+\n+    # Stack images with the same shape\n+    grouped_images = {shape: torch.stack(images_list, dim=0) for shape, images_list in grouped_images.items()}\n+\n     return grouped_images, grouped_images_index\n \n \n def reorder_images(\n-    processed_images: dict[tuple[int, int], \"torch.Tensor\"], grouped_images_index: dict[int, tuple[int, int]]\n-) -> list[\"torch.Tensor\"]:\n+    processed_images: dict[tuple[int, int], \"torch.Tensor\"],\n+    grouped_images_index: dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]],\n+    is_nested: bool = False,\n+) -> Union[list[\"torch.Tensor\"], \"torch.Tensor\"]:\n     \"\"\"\n-    Reconstructs a list of images in the original order.\n+    Reconstructs images in the original order, preserving the original structure (nested or not).\n+    The input structure is either all flat or all nested.\n+\n+    Args:\n+        processed_images (dict[tuple[int, int], \"torch.Tensor\"]):\n+            Dictionary mapping shapes to batched processed images.\n+        grouped_images_index (dict[Union[int, tuple[int, int]], tuple[tuple[int, int], int]]):\n+            Dictionary mapping original indices to (shape, index) tuples.\n+        is_nested (bool, *optional*, defaults to False):\n+            Whether the images are nested. Cannot be infered from the input, as some processing functions outputs nested images.\n+            even with non nested images,e.g functions splitting images into patches. We thus can't deduce is_nested from the input.\n+\n+\n+    Returns:\n+        Union[list[\"torch.Tensor\"], \"torch.Tensor\"]:\n+            Images in the original structure.\n     \"\"\"\n-    return [\n-        processed_images[grouped_images_index[i][0]][grouped_images_index[i][1]]\n-        for i in range(len(grouped_images_index))\n-    ]\n+    if not is_nested:\n+        return [\n+            processed_images[grouped_images_index[i][0]][grouped_images_index[i][1]]\n+            for i in range(len(grouped_images_index))\n+        ]\n+\n+    return _reconstruct_nested_structure(grouped_images_index, processed_images)\n \n \n class NumpyToTensor:"
        },
        {
            "sha": "5087531a5335d2775ae37810b70189baf8fc6c0d",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -95,8 +95,8 @@\n             (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"hiera\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"idefics\", (\"IdeficsImageProcessor\",)),\n-            (\"idefics2\", (\"Idefics2ImageProcessor\",)),\n-            (\"idefics3\", (\"Idefics3ImageProcessor\",)),\n+            (\"idefics2\", (\"Idefics2ImageProcessor\", \"Idefics2ImageProcessorFast\")),\n+            (\"idefics3\", (\"Idefics3ImageProcessor\", \"Idefics3ImageProcessorFast\")),\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n@@ -148,6 +148,7 @@\n             (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n+            (\"smolvlm\", (\"SmolVLMImageProcessor\", \"SmolVLMImageProcessorFast\")),\n             (\"superglue\", (\"SuperGlueImageProcessor\",)),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "b2e36bc4bc6d4e7ce4482e788e4d3d4235116a41",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -27,13 +27,7 @@\n from ...image_processing_utils import ImageProcessingMixin\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils import TOKENIZER_CONFIG_FILE\n-from ...utils import (\n-    FEATURE_EXTRACTOR_NAME,\n-    PROCESSOR_NAME,\n-    VIDEO_PROCESSOR_NAME,\n-    cached_file,\n-    logging,\n-)\n+from ...utils import FEATURE_EXTRACTOR_NAME, PROCESSOR_NAME, VIDEO_PROCESSOR_NAME, cached_file, logging\n from ...video_processing_utils import BaseVideoProcessor\n from .auto_factory import _LazyAutoMapping\n from .configuration_auto import (\n@@ -118,6 +112,7 @@\n         (\"shieldgemma2\", \"ShieldGemma2Processor\"),\n         (\"siglip\", \"SiglipProcessor\"),\n         (\"siglip2\", \"Siglip2Processor\"),\n+        (\"smolvlm\", \"SmolVLMProcessor\"),\n         (\"speech_to_text\", \"Speech2TextProcessor\"),\n         (\"speech_to_text_2\", \"Speech2Text2Processor\"),\n         (\"speecht5\", \"SpeechT5Processor\"),"
        },
        {
            "sha": "35ef30a16653c8e17d3f22c2843569ad58bc6e2a",
            "filename": "src/transformers/models/beit/image_processing_beit_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -105,14 +105,15 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         if do_reduce_labels:\n             images = self.reduce_label(images)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -122,7 +123,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "2eac0fe337b0a56145a62050f55ce7f50dc0bba4",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -223,6 +223,7 @@ def pad(\n         images: list[\"torch.Tensor\"],\n         constant_values: Union[float, Iterable[float]] = 0,\n         return_pixel_mask: bool = True,\n+        disable_grouping: Optional[bool] = False,\n     ) -> tuple:\n         \"\"\"\n         Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n@@ -235,6 +236,8 @@ def pad(\n                 The value to use for the padding if `mode` is `\"constant\"`.\n             return_pixel_mask (`bool`, *optional*, defaults to `True`):\n                 Whether to return a pixel mask.\n+            disable_grouping (`bool`, *optional*, defaults to `False`):\n+                Whether to disable grouping of images by size.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -245,7 +248,7 @@ def pad(\n         \"\"\"\n         pad_size = get_max_height_width(images)\n \n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         processed_masks_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -283,11 +286,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -299,7 +303,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:\n@@ -314,7 +318,9 @@ def _preprocess(\n \n         data = {}\n         if do_pad:\n-            processed_images, processed_masks = self.pad(processed_images, return_pixel_mask=True)\n+            processed_images, processed_masks = self.pad(\n+                processed_images, return_pixel_mask=True, disable_grouping=disable_grouping\n+            )\n             processed_masks = torch.stack(processed_masks, dim=0) if return_tensors else processed_masks\n             data[\"pixel_mask\"] = processed_masks\n "
        },
        {
            "sha": "96df32630f3c893797cf3e2fa3a47e7c6dd0e096",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -19,11 +19,7 @@\n import numpy as np\n \n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n+from ...image_transforms import get_resize_output_image_size, resize, to_channel_dimension_format\n from ...image_utils import (\n     ChannelDimension,\n     ImageInput,"
        },
        {
            "sha": "130fcc19639ebf306066191d41b93d92e6569ec4",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -153,10 +153,11 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -168,7 +169,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "cc6030ef4d05b4c1aecfe12982c27ee222d18870",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -17,17 +17,8 @@\n from typing import TYPE_CHECKING, Optional, Union\n \n from ...image_processing_base import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, PILImageResampling, SizeDict\n from ...utils import (\n     TensorType,\n     auto_docstring,\n@@ -85,10 +76,11 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         # Group images by size for batched scaling\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize"
        },
        {
            "sha": "8ec0235544170b941a63930816370c1f7934cf48",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -16,19 +16,9 @@\n \n from typing import Optional, Union\n \n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import group_images_by_shape, reorder_images\n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -230,11 +220,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_align_long_axis:\n@@ -254,7 +245,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "58b1038006029e4c95f69c67cb1d5e572b8c7445",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -176,18 +176,19 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]],\n         keep_aspect_ratio: bool,\n         ensure_multiple_of: Optional[int],\n         do_pad: bool,\n         size_divisor: Optional[int],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         if do_reduce_labels:\n             images = self.reduce_label(images)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -203,7 +204,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "12d214969d3cb98165d6de975317c1d4e0fa8037",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -224,18 +224,19 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        return_tensors: Optional[Union[str, TensorType]],\n         keep_aspect_ratio: bool,\n         ensure_multiple_of: Optional[int],\n         do_pad: bool,\n         size_divisor: Optional[int],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         if do_reduce_labels:\n             images = self.reduce_label(images)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -251,7 +252,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "41689e3dc080e7795fc967bc63671aae704e07d7",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -17,19 +17,9 @@\n from functools import lru_cache\n from typing import Optional, Union\n \n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import group_images_by_shape, reorder_images\n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -181,11 +171,12 @@ def _preprocess(\n         include_top: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -195,7 +186,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "854a8780e28f2edb43c34e8e2195d5fd75de4fa7",
            "filename": "src/transformers/models/flava/image_processing_flava_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -367,10 +367,11 @@ def _preprocess_image(\n         do_map_pixels: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> \"torch.Tensor\":\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -380,7 +381,7 @@ def _preprocess_image(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:\n@@ -432,6 +433,7 @@ def _preprocess(\n         codebook_do_normalize: Optional[bool],\n         codebook_image_mean: Optional[Union[float, list[float]]],\n         codebook_image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n@@ -448,6 +450,7 @@ def _preprocess(\n             do_map_pixels=False,\n             image_mean=image_mean,\n             image_std=image_std,\n+            disable_grouping=disable_grouping,\n             return_tensors=return_tensors,\n         )\n         data = {\n@@ -468,6 +471,7 @@ def _preprocess(\n                 do_map_pixels=codebook_do_map_pixels,\n                 image_mean=codebook_image_mean,\n                 image_std=codebook_image_std,\n+                disable_grouping=disable_grouping,\n                 return_tensors=return_tensors,\n             )\n             data[\"codebook_pixel_values\"] = codebook_processed_images"
        },
        {
            "sha": "89cb3df19637ba8f7b255816b77a6a557481d419",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -25,12 +25,7 @@\n     group_images_by_shape,\n     reorder_images,\n )\n-from ...image_utils import (\n-    IMAGENET_STANDARD_MEAN,\n-    IMAGENET_STANDARD_STD,\n-    ImageInput,\n-    SizeDict,\n-)\n+from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -205,12 +200,13 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         # Group images by size for batched processing\n         processed_images_grouped = {}\n         num_crops_grouped = {}\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         for shape_images, stacked_images in grouped_images.items():\n             if do_pan_and_scan:\n                 pas_images, num_crops = self._process_images_for_pan_and_scan(\n@@ -224,7 +220,9 @@ def _preprocess(\n                 stacked_images = [stacked_images] + pas_images\n                 # Group images by size for batched resizing (this will typically group thumbnails together and cropped patches together)\n                 processed_image_patches_grouped = {}\n-                grouped_image_patches, grouped_image_patches_index = group_images_by_shape(stacked_images)\n+                grouped_image_patches, grouped_image_patches_index = group_images_by_shape(\n+                    stacked_images, disable_grouping=disable_grouping\n+                )\n                 for shape, stacked_image_patches in grouped_image_patches.items():\n                     stacked_image_patches = self.resize(\n                         image=stacked_image_patches,\n@@ -254,7 +252,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize"
        },
        {
            "sha": "452b4c3f58b7fb1a8619e051d62ba671c5ff692c",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -23,13 +23,7 @@\n     group_images_by_shape,\n     reorder_images,\n )\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -177,10 +171,11 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         if crop_to_patches:\n-            grouped_images, grouped_images_index = group_images_by_shape(images)\n+            grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n             processed_images_grouped = {}\n             num_patches = {}\n             for shape, stacked_images in grouped_images.items():\n@@ -200,7 +195,7 @@ def _preprocess(\n             num_patches = [1] * len(images)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -210,7 +205,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "c6715adc9ab8614bbb463b6b700fcdc2ca671d22",
            "filename": "src/transformers/models/idefics2/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2F__init__.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_idefics2 import *\n     from .image_processing_idefics2 import *\n+    from .image_processing_idefics2_fast import *\n     from .modeling_idefics2 import *\n     from .processing_idefics2 import *\n else:"
        },
        {
            "sha": "a213ef958362af285cc43804d73aea9cd36cf8e1",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2_fast.py",
            "status": "added",
            "additions": 318,
            "deletions": 0,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -0,0 +1,318 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    make_nested_list_of_images,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n+from .image_processing_idefics2 import convert_to_rgb\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_resize_output_image_size(image: \"torch.Tensor\", size: SizeDict) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to resize.\n+        size (`SizeDict`):\n+            Size of the output image containing the keys \"shortest_edge\" and \"longest_edge\".\n+\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    height, width = image.size()[-2:]\n+\n+    min_len = size.shortest_edge\n+    max_len = size.longest_edge\n+    aspect_ratio = width / height\n+\n+    if width >= height and width > max_len:\n+        width = max_len\n+        height = int(width / aspect_ratio)\n+    elif height > width and height > max_len:\n+        height = max_len\n+        width = int(height * aspect_ratio)\n+    height = max(height, min_len)\n+    width = max(width, min_len)\n+    return height, width\n+\n+\n+def get_max_height_width(images_list: list[list[\"torch.Tensor\"]]) -> tuple[int, int]:\n+    \"\"\"\n+    Get the maximum height and width across all images in a batch.\n+    \"\"\"\n+    image_sizes = []\n+    for images in images_list:\n+        for image in images:\n+            image_sizes.append(image.size()[-2:])\n+\n+    max_height = max(size[0] for size in image_sizes)\n+    max_width = max(size[1] for size in image_sizes)\n+    return (max_height, max_width)\n+\n+\n+def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"torch.Tensor\":\n+    \"\"\"\n+    Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to make the pixel mask for.\n+        output_size (`Tuple[int, int]`):\n+            Output size of the mask.\n+    \"\"\"\n+    input_height, input_width = image.size()[-2:]\n+    mask = torch.zeros(output_size, dtype=torch.int64, device=image.device)\n+    mask[:input_height, :input_width] = 1\n+    return mask\n+\n+\n+class Idefics2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_image_splitting (`bool`, *optional*, defaults to `False`):\n+        Whether to split the image into a sequence 4 equal sub-images concatenated with the original image.\n+    do_pad (`bool`, *optional*, defaults to `True`):\n+        Whether to pad images to the largest height and width in the batch.\n+    \"\"\"\n+\n+    do_image_splitting: Optional[bool]\n+    do_pad: Optional[bool]\n+\n+\n+@auto_docstring\n+class Idefics2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = True\n+    do_convert_rgb = True\n+    do_image_splitting = False\n+    size = {\"shortest_edge\": 378, \"longest_edge\": 980}\n+    model_input_names = [\"pixel_values\", \"pixel_attention_mask\"]\n+    valid_kwargs = Idefics2FastImageProcessorKwargs\n+\n+    def convert_to_rgb(self, image: ImageInput) -> ImageInput:\n+        \"\"\"\n+        Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image\n+        as is.\n+        \"\"\"\n+        return convert_to_rgb(image)\n+\n+    def resize(\n+        self, image: torch.Tensor, size: SizeDict, interpolation: Optional[\"F.InterpolationMode\"] = None, **kwargs\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize an image using torchvision's functional resize.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+\n+        if size.shortest_edge and size.longest_edge:\n+            new_size = get_resize_output_image_size(image, size)\n+        elif size.height and size.width:\n+            new_size = (size.height, size.width)\n+        else:\n+            raise ValueError(\"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys.\")\n+\n+        image = F.resize(image, size=new_size, interpolation=interpolation, **kwargs)\n+        return image\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+    ) -> ImageInput:\n+        \"\"\"\n+        Prepare a nested images structure for processing.\n+        \"\"\"\n+        return make_nested_list_of_images(images)\n+\n+    def split_images(\n+        self,\n+        images: \"torch.Tensor\",\n+    ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Split a batch of images into 4 equal sub-images, and concatenate that sequence with the original image.\n+        \"\"\"\n+        height, width = images.size()[-2:]\n+\n+        mid_width = width // 2\n+        mid_height = height // 2\n+\n+        batch_split_images = [\n+            images[..., :mid_height, :mid_width],\n+            images[..., :mid_height, mid_width:],\n+            images[..., mid_height:, :mid_width],\n+            images[..., mid_height:, mid_width:],\n+            images,\n+        ]\n+\n+        # transpose the batch dimension to the first dimension\n+        batch_split_images = [[image[i] for image in batch_split_images] for i in range(len(batch_split_images[0]))]\n+        return batch_split_images\n+\n+    def pad(\n+        self, image: \"torch.Tensor\", padded_size: tuple[int, int], fill: int = 0\n+    ) -> tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n+        \"\"\"\n+        Pad an image to the specified size and create the corresponding pixel mask.\n+        \"\"\"\n+        original_size = image.shape[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+\n+        # Only pad if necessary\n+        if original_size != padded_size:\n+            # torchvision's pad takes a 4-element tuple for 2D padding: (left, top, right, bottom)\n+            padding = (0, 0, padding_right, padding_bottom)\n+            # Use constant padding to match slow implementation\n+            image = F.pad(image, padding, fill=fill, padding_mode=\"constant\")\n+\n+        # Create pixel mask to match the slow implementation\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics2FastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[list[\"torch.Tensor\"]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        do_image_splitting: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Process a batch of images for the model.\n+        \"\"\"\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        split_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_image_splitting:\n+                stacked_images = self.split_images(stacked_images)\n+            split_images_grouped[shape] = stacked_images\n+        split_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+        if do_image_splitting:\n+            # flattenened the doubly nested list to a nested list\n+            for i, group_images in enumerate(split_images):\n+                split_images[i] = [image for sublist in group_images for image in sublist]\n+\n+        # Group images by size for further processing\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            split_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+\n+        if do_pad:\n+            # Get max images per batch\n+            max_num_images = max(len(images_) for images_ in processed_images)\n+            max_height, max_width = get_max_height_width(processed_images)\n+\n+            processed_images_padded = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(processed_images[0][0].shape[0], max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            pixel_attention_masks = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            for i, images in enumerate(processed_images):\n+                for j, image in enumerate(images):\n+                    processed_images_padded[i, j], pixel_attention_masks[i, j] = self.pad(\n+                        image, (max_height, max_width)\n+                    )\n+            processed_images = processed_images_padded\n+        if do_pad:\n+            data = {\"pixel_values\": processed_images, \"pixel_attention_mask\": pixel_attention_masks}\n+        elif return_tensors == \"pt\":\n+            data = {\"pixel_values\": torch.stack([torch.stack(images) for images in processed_images])}\n+        else:\n+            data = {\"pixel_values\": processed_images}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Idefics2ImageProcessorFast\"]"
        },
        {
            "sha": "e1dd3bfda7fbce5af140b438ec00f3ff51718ed5",
            "filename": "src/transformers/models/idefics3/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2F__init__.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_idefics3 import *\n     from .image_processing_idefics3 import *\n+    from .image_processing_idefics3_fast import *\n     from .modeling_idefics3 import *\n     from .processing_idefics3 import *\n else:"
        },
        {
            "sha": "9a28515cc066a93143159bd6c29f134ba8477fed",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -770,6 +770,7 @@ def preprocess(\n                     split_image_array, rows, cols = self.split_image(\n                         image,\n                         max_image_size=max_image_size,\n+                        resample=resample,\n                         input_data_format=input_data_format,\n                     )\n                     split_image_arrays.extend(split_image_array)"
        },
        {
            "sha": "ec379d45a8004128c0f588e35db90695b5acbb66",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "added",
            "additions": 507,
            "deletions": 0,
            "changes": 507,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -0,0 +1,507 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    make_nested_list_of_images,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum\n+\n+\n+def _resize_output_size_rescale_to_max_len(\n+    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        height (`int`):\n+            Height of the input image.\n+        width (`int`):\n+            Width of the input image.\n+        min_len (`int`, *optional*, defaults to 1):\n+            Minimum size of the output image.\n+        max_len (`int`, *optional*, defaults to the maximum size of the image):\n+            Maximum size of the output image.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    max_len = max(height, width) if max_len is None else max_len\n+    aspect_ratio = width / height\n+\n+    if width >= height:\n+        width = max_len\n+        height = int(width / aspect_ratio)\n+        if height % 2 != 0:\n+            height += 1\n+    elif height > width:\n+        height = max_len\n+        width = int(height * aspect_ratio)\n+        if width % 2 != 0:\n+            width += 1\n+\n+    # Avoid resizing to a size smaller than min_len\n+    height = max(height, min_len)\n+    width = max(width, min_len)\n+    return height, width\n+\n+\n+def _resize_output_size_scale_below_upper_bound(\n+    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        height (`int`):\n+            Height of the input image.\n+        width (`int`):\n+            Width of the input image.\n+        max_len (`Dict[str, int]`, *optional*, defaults to the maximum size of the image):\n+            Defines the maximum dimensions of the image.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    max_len = max(height, width) if max_len is None else max_len\n+\n+    aspect_ratio = width / height\n+    if width >= height and width > max_len:\n+        width = max_len\n+        height = int(width / aspect_ratio)\n+    elif height > width and height > max_len:\n+        height = max_len\n+        width = int(height * aspect_ratio)\n+\n+    # Avoid resizing to a size smaller than 1\n+    height = max(height, 1)\n+    width = max(width, 1)\n+    return height, width\n+\n+\n+def get_resize_output_image_size(\n+    image,\n+    resolution_max_side: int,\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to resize.\n+        resolution_max_side (`int`):\n+            The longest edge of the image will be resized to this value. The shortest edge will be resized to keep the\n+            input aspect ratio.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    height, width = image.size()[-2:]\n+\n+    # Find the output size, when rescaling the longest edge to max_len and preserving the aspect ratio\n+    height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=resolution_max_side)\n+    # Find the output size when scaling the image to be below the MAX_IMAGE_SIZE\n+    height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=MAX_IMAGE_SIZE)\n+    return height, width\n+\n+\n+def get_max_height_width(images_list: list[list[\"torch.Tensor\"]]) -> tuple[int, int]:\n+    \"\"\"\n+    Get the maximum height and width across all images in a batch.\n+    \"\"\"\n+    image_sizes = []\n+    for images in images_list:\n+        for image in images:\n+            image_sizes.append(image.size()[-2:])\n+\n+    max_height = max(size[0] for size in image_sizes)\n+    max_width = max(size[1] for size in image_sizes)\n+    return (max_height, max_width)\n+\n+\n+def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"torch.Tensor\":\n+    \"\"\"\n+    Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to make the pixel mask for.\n+        output_size (`Tuple[int, int]`):\n+            Output size of the mask.\n+    \"\"\"\n+    input_height, input_width = image.size()[-2:]\n+    mask = torch.zeros(output_size, dtype=torch.int64, device=image.device)\n+    mask[:input_height, :input_width] = 1\n+    return mask\n+\n+\n+class Idefics3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_pad (`bool`, *optional*):\n+        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+    do_image_splitting (`bool`, *optional*, defaults to `True`):\n+        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n+        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n+    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n+        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n+    return_row_col_info (`bool`, *optional*, defaults to `False`):\n+        Whether to return the row and column information of the images.\n+    \"\"\"\n+\n+    do_pad: Optional[bool]\n+    do_image_splitting: Optional[bool]\n+    max_image_size: Optional[dict[str, int]]\n+    return_row_col_info: Optional[bool]\n+\n+\n+@auto_docstring\n+class Idefics3ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.LANCZOS\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"longest_edge\": 4 * 364}\n+    max_image_size = {\"longest_edge\": 364}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_image_splitting = True\n+    do_pad = True\n+    return_row_col_info = False\n+    valid_kwargs = Idefics3FastImageProcessorKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+    ) -> ImageInput:\n+        \"\"\"\n+        Prepare a nested images structure for processing.\n+        \"\"\"\n+        return make_nested_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image. The longest edge of the image is resized to size.longest_edge, with the shortest edge\n+        resized to keep the input aspect ratio. Can also be used with size.height and size.width.\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing when resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if interpolation == F.InterpolationMode.LANCZOS:\n+            logger.warning_once(\n+                \"You have used fast image processor with LANCZOS resample which not yet supported for torch.Tensor. \"\n+                \"BICUBIC resample will be used as an alternative. Please fall back to slow image processor if you \"\n+                \"want full consistency with the original model.\"\n+            )\n+            interpolation = F.InterpolationMode.BICUBIC\n+\n+        if size.longest_edge:\n+            size = get_resize_output_image_size(image, resolution_max_side=size.longest_edge)\n+        elif size.height and size.width:\n+            size = (size.height, size.width)\n+        else:\n+            raise ValueError(\"size must be a dictionary with key 'longest_edge' or 'height' and 'width'.\")\n+\n+        return F.resize(image, size, interpolation=interpolation, antialias=antialias)\n+\n+    def split_images(\n+        self,\n+        images: torch.Tensor,\n+        max_image_size: dict[str, int],\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Split an image into squares of side max_image_size and the original image resized to max_image_size.\n+        That means that a single image becomes a sequence of images.\n+        This is a \"trick\" to spend more compute on each image with no changes in the vision encoder.\n+        1) If one side of the original image is larger than `max_image_size`, resize it to `max_image_size` while preserving the aspect ratio.\n+        2) Divide the resulting image into `ceil(height / max_image_size)` x `ceil(width / max_image_size)`\n+        sub-images of the same size each (image_size, image_size). Typically, 364x364.\n+        3) Returns the list of the crops and the original image, in addition to the number of splits for the height and the width.\n+        Args:\n+            images (`torch.Tensor`):\n+                Images to split.\n+            max_image_size (`Dict[str, int]`):\n+                Maximum size of the output image. If the image is larger than this size, it will be split into\n+                patches of this size, and the original image will be concatenated with the patches, resized to max_size.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+        \"\"\"\n+        batch_size, num_channels, height, width = images.size()\n+        height_dim, width_dim = 2, 3\n+\n+        max_height = max_width = max_image_size[\"longest_edge\"]\n+\n+        frames = []\n+        if height > max_height or width > max_width:\n+            # Calculate the number of splits\n+            num_splits_h = math.ceil(height / max_height)\n+            num_splits_w = math.ceil(width / max_width)\n+\n+            # Split the images by height, then by width\n+            frames = (\n+                images.unfold(height_dim, size=max_height, step=max_height)\n+                .unfold(width_dim, size=max_width, step=max_width)\n+                .contiguous()\n+                .view(batch_size, num_channels, -1, max_height, max_width)\n+                .permute(0, 2, 1, 3, 4)\n+            )  # batch_size x n_frames x num_channels x height x width\n+\n+            # For the global image at the end, we resize it to match the max_image_size, for cpu memory efficiency\n+            global_image_height, global_image_width = max_height, max_width\n+            images = self.resize(\n+                images, SizeDict(height=global_image_height, width=global_image_width), interpolation=interpolation\n+            )\n+\n+            frames = torch.cat((frames, images.unsqueeze(1)), dim=1)\n+        else:\n+            num_splits_h, num_splits_w = 0, 0\n+            frames = images.unsqueeze(1)\n+\n+        num_splits_h = [num_splits_h] * batch_size\n+        num_splits_w = [num_splits_w] * batch_size\n+\n+        return frames, num_splits_h, num_splits_w\n+\n+    def resize_for_vision_encoder(\n+        self,\n+        image: torch.Tensor,\n+        vision_encoder_max_size: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.\n+        Args:\n+            image (`torch.Tensor`):\n+                Images to resize.\n+            vision_encoder_max_size (`int`):\n+                Maximum size of the output image. If the image is larger than this size, it will be split into\n+                patches of this size, and the original image will be concatenated with the patches, resized to max_size.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+        \"\"\"\n+        height, width = image.size()[-2:]\n+\n+        aspect_ratio = width / height\n+        if width >= height:\n+            width = math.ceil(width / vision_encoder_max_size) * vision_encoder_max_size\n+            height = int(width / aspect_ratio)\n+            height = math.ceil(height / vision_encoder_max_size) * vision_encoder_max_size\n+        elif height > width:\n+            height = math.ceil(height / vision_encoder_max_size) * vision_encoder_max_size\n+            width = int(height * aspect_ratio)\n+            width = math.ceil(width / vision_encoder_max_size) * vision_encoder_max_size\n+        new_size = SizeDict(height=height, width=width)\n+        return self.resize(image, size=new_size, interpolation=interpolation)\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: tuple[int, int],\n+        fill: int = 0,\n+        return_pixel_mask: bool = True,\n+    ):\n+        original_size = image.shape[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+\n+        # Only pad if necessary\n+        if original_size != padded_size:\n+            padding = (0, 0, padding_right, padding_bottom)\n+            image = F.pad(image, padding, fill=fill, padding_mode=\"constant\")\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = None\n+        if return_pixel_mask:\n+            pixel_mask = torch.zeros_like(image[..., 0, :, :], dtype=torch.int64)\n+            pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Idefics3FastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[list[\"torch.Tensor\"]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        do_image_splitting: Optional[bool],\n+        max_image_size: Optional[dict[str, int]],\n+        return_row_col_info: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Process a batch of images for the model.\n+        \"\"\"\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        split_images_grouped = {}\n+        if do_image_splitting:\n+            rows_grouped = {}\n+            cols_grouped = {}\n+            for shape, stacked_images in grouped_images.items():\n+                stacked_images = self.resize_for_vision_encoder(\n+                    stacked_images, max_image_size[\"longest_edge\"], interpolation=interpolation\n+                )\n+                stacked_images, rows, cols = self.split_images(\n+                    stacked_images, max_image_size=max_image_size, interpolation=interpolation\n+                )\n+                split_images_grouped[shape] = stacked_images\n+                rows_grouped[shape] = rows\n+                cols_grouped[shape] = cols\n+            processed_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+            rows = reorder_images(rows_grouped, grouped_images_index, is_nested=True)\n+            cols = reorder_images(cols_grouped, grouped_images_index, is_nested=True)\n+            # flattenened the doubly nested list to a nested list\n+            for i, group_images in enumerate(processed_images):\n+                processed_images[i] = [image for sublist in group_images for image in sublist]\n+        else:\n+            for shape, stacked_images in grouped_images.items():\n+                # We square the images to max_image_size\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=SizeDict(height=max_image_size[\"longest_edge\"], width=max_image_size[\"longest_edge\"]),\n+                    interpolation=interpolation,\n+                )\n+            split_images_grouped[shape] = stacked_images\n+            processed_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+            rows = [[0] * len(images) for images in processed_images]\n+            cols = [[0] * len(images) for images in processed_images]\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            processed_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+        if do_pad:\n+            # Get max images per batch\n+            max_num_images = max(len(images_) for images_ in processed_images)\n+            max_height, max_width = get_max_height_width(processed_images)\n+\n+            processed_images_padded = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(processed_images[0][0].shape[0], max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            pixel_attention_masks = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            for i, images in enumerate(processed_images):\n+                for j, image in enumerate(images):\n+                    processed_images_padded[i, j], pixel_attention_masks[i, j] = self.pad(\n+                        image, (max_height, max_width)\n+                    )\n+            processed_images = processed_images_padded\n+\n+        if do_pad:\n+            data = {\"pixel_values\": processed_images, \"pixel_attention_mask\": pixel_attention_masks}\n+        elif return_tensors == \"pt\":\n+            data = {\"pixel_values\": torch.stack([torch.stack(images) for images in processed_images])}\n+        else:\n+            data = {\"pixel_values\": processed_images}\n+        # This is needed for generating correct text inputs in the processor - we don't pad to the max number of images\n+        encoding = BatchFeature(data=data, tensor_type=return_tensors)\n+\n+        if return_row_col_info:\n+            encoding[\"rows\"] = rows\n+            encoding[\"cols\"] = cols\n+\n+        return encoding\n+\n+    def to_dict(self):\n+        encoder_dict = super().to_dict()\n+        encoder_dict.pop(\"_valid_processor_keys\", None)\n+        encoder_dict.pop(\"return_row_col_info\", None)\n+        return encoder_dict\n+\n+\n+__all__ = [\"Idefics3ImageProcessorFast\"]"
        },
        {
            "sha": "c22612da5858bffc76e8d0bc2ea5b4aa3dd245fc",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -91,6 +91,7 @@ def _preprocess(\n         apply_ocr: bool,\n         ocr_lang: Optional[str],\n         tesseract_config: Optional[str],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n@@ -111,7 +112,7 @@ def _preprocess(\n                 boxes_batch.append(boxes)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -121,7 +122,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # flip color channels from RGB to BGR (as Detectron2 requires this)"
        },
        {
            "sha": "c7580bb528da3ac8b0558990643753f0862be605",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -16,11 +16,7 @@\n \n from typing import Optional, Union\n \n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n from ...image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD, ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n@@ -106,6 +102,7 @@ def _preprocess(\n         ocr_lang: Optional[str],\n         tesseract_config: Optional[str],\n         return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: Optional[bool],\n         **kwargs,\n     ) -> BatchFeature:\n         # Tesseract OCR to get words + normalized bounding boxes\n@@ -125,7 +122,7 @@ def _preprocess(\n                 boxes_batch.append(boxes)\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -135,7 +132,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "fc76a013c90ddffe8171dc945be5dfb8e3114dde",
            "filename": "src/transformers/models/llama4/image_processing_llama4_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fimage_processing_llama4_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -393,13 +393,14 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         possible_resolutions = find_supported_resolutions(max_num_chunks=max_patches, patch_size=size)\n         possible_resolutions = torch.tensor(possible_resolutions, device=images[0].device)\n         # process images by batch, grouped by shape\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         grouped_processed_images = {}\n         grouped_aspect_ratios = {}\n         for shape, stacked_images in grouped_images.items():"
        },
        {
            "sha": "5a1502f78d4434dd3ea7ead4ec518bf373f3d239",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -16,9 +16,7 @@\n \n from typing import Optional, Union\n \n-from ...image_processing_utils import (\n-    BatchFeature,\n-)\n+from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n@@ -147,10 +145,11 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_pad:\n@@ -162,7 +161,7 @@ def _preprocess(\n \n         # Group images by size for batched resizing\n         # Needed in case do_pad is False, or padding returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(padded_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(padded_images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -172,7 +171,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "3356f514ed1d95b19a0660f992fb3d8b15f9bc57",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -242,7 +242,9 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         processed_images = []\n         image_sizes = []\n@@ -271,7 +273,9 @@ def _preprocess(\n \n             # Group images by size for batched processing\n             processed_image_patches_grouped = {}\n-            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(image_patches)\n+            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(\n+                image_patches, disable_grouping=disable_grouping\n+            )\n             for shape, stacked_image_patches in grouped_image_patches.items():\n                 if do_resize:\n                     stacked_image_patches = self.resize("
        },
        {
            "sha": "6eba44938c58b344ec4eef918019762977909ab9",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -248,7 +248,9 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n         batch_num_images: list[int],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         processed_images = []\n         image_sizes = []\n@@ -287,7 +289,9 @@ def _preprocess(\n \n             # Group images by size for batched processing\n             processed_image_patches_grouped = {}\n-            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(image_patches)\n+            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(\n+                image_patches, disable_grouping=disable_grouping\n+            )\n             for shape, stacked_image_patches in grouped_image_patches.items():\n                 if do_resize:\n                     stacked_image_patches = self.resize("
        },
        {
            "sha": "4920124522b87815251a2dce8aae1f88269242af",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -169,7 +169,9 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n         batch_num_images: list[int],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         processed_images = []\n         image_sizes = []\n@@ -208,7 +210,9 @@ def _preprocess(\n \n             # Group images by size for batched processing\n             processed_image_patches_grouped = {}\n-            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(image_patches)\n+            grouped_image_patches, grouped_image_patches_index = group_images_by_shape(\n+                image_patches, disable_grouping=disable_grouping\n+            )\n             for shape, stacked_image_patches in grouped_image_patches.items():\n                 if do_resize:\n                     stacked_image_patches = self.resize("
        },
        {
            "sha": "640083ba82dd6e85853bdca8cb5c7a080e84751f",
            "filename": "src/transformers/models/perceiver/image_processing_perceiver_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fimage_processing_perceiver_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -96,11 +96,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:\n@@ -112,7 +113,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize"
        },
        {
            "sha": "d1cbf474ec3bc04407dbc76304fd22abf27eff72",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -23,11 +23,7 @@\n     group_images_by_shape,\n     reorder_images,\n )\n-from ...image_utils import (\n-    ImageInput,\n-    PILImageResampling,\n-    SizeDict,\n-)\n+from ...image_utils import ImageInput, PILImageResampling, SizeDict\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n@@ -38,9 +34,7 @@\n     is_vision_available,\n     logging,\n )\n-from .image_processing_pixtral import (\n-    get_resize_output_image_size,\n-)\n+from .image_processing_pixtral import get_resize_output_image_size\n \n \n logger = logging.get_logger(__name__)\n@@ -164,12 +158,13 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n     ) -> BatchFeature:\n         patch_size = get_size_dict(patch_size, default_to_square=True)\n         patch_size = SizeDict(**patch_size)\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -181,7 +176,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         batch_image_sizes = [grouped_images_index[i][0] for i in range(len(grouped_images_index))]\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():"
        },
        {
            "sha": "21895c99403582052c8f77a3e654249252f083c2",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -16,11 +16,7 @@\n \n from typing import Optional, Union\n \n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    BatchFeature,\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import (\n     ChannelDimension,\n     get_resize_output_image_size,\n@@ -225,11 +221,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -241,7 +238,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_center_crop:"
        },
        {
            "sha": "762ed117dfe528bd5f5c58c690e368b22324ee75",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -139,6 +139,7 @@ def _preprocess(\n         do_convert_rgb: bool,\n         input_data_format: Optional[Union[str, ChannelDimension]],\n         device: Optional[Union[str, torch.device]],\n+        disable_grouping: Optional[bool],\n     ):\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n@@ -191,7 +192,7 @@ def _preprocess(\n         resized_height, resized_width = height, width\n \n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_resize:\n@@ -210,7 +211,7 @@ def _preprocess(\n \n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize\n@@ -270,6 +271,7 @@ def preprocess(\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         device: Optional[\"torch.device\"] = None,\n+        disable_grouping: Optional[bool] = False,\n         **kwargs,\n     ):\n         r\"\"\"\n@@ -360,6 +362,7 @@ def preprocess(\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,\n                     device=device,\n+                    disable_grouping=disable_grouping,\n                 )\n                 pixel_values.extend(patches)\n                 vision_grid_thws.append(image_grid_thw)\n@@ -393,6 +396,7 @@ def preprocess(\n                     do_convert_rgb=do_convert_rgb,\n                     input_data_format=input_data_format,\n                     device=device,\n+                    disable_grouping=disable_grouping,\n                 )\n                 pixel_values_videos.extend(patches)\n                 vision_grid_thws_videos.append(video_grid_thw)"
        },
        {
            "sha": "5205a84b25c45f7f378d22808f1a307afb58c909",
            "filename": "src/transformers/models/smolvlm/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2F__init__.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_smolvlm import *\n     from .image_processing_smolvlm import *\n+    from .image_processing_smolvlm_fast import *\n     from .modeling_smolvlm import *\n     from .processing_smolvlm import *\n else:"
        },
        {
            "sha": "d382218aa1a14a450a0875d250f8dc094bac13d9",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -767,6 +767,7 @@ def preprocess(\n                     split_image_array, rows, cols = self.split_image(\n                         image,\n                         max_image_size=max_image_size,\n+                        resample=resample,\n                         input_data_format=input_data_format,\n                     )\n                     split_image_arrays.extend(split_image_array)"
        },
        {
            "sha": "5ee34ae39d55cc22a02f4e9d85b99f5bb1850f80",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "added",
            "additions": 497,
            "deletions": 0,
            "changes": 497,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -0,0 +1,497 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/smolvlm/modular_smolvlm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_smolvlm.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+# Written by Orr Zohar\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ImageInput,\n+    PILImageResampling,\n+    make_nested_list_of_images,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring, is_torchvision_available, logging\n+\n+\n+if is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SmolVLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_pad (`bool`, *optional*):\n+        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+    do_image_splitting (`bool`, *optional*, defaults to `True`):\n+        Whether to split the image into sub-images concatenated with the original image. They are split into patches\n+        such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n+    max_image_size (`Dict`, *optional*, defaults to `{\"longest_edge\": 364}`):\n+        Maximum resolution of the patches of images accepted by the model. This is a dictionary containing the key \"longest_edge\".\n+    return_row_col_info (`bool`, *optional*, defaults to `False`):\n+        Whether to return the row and column information of the images.\n+    \"\"\"\n+\n+    do_pad: Optional[bool]\n+    do_image_splitting: Optional[bool]\n+    max_image_size: Optional[dict[str, int]]\n+    return_row_col_info: Optional[bool]\n+\n+\n+MAX_IMAGE_SIZE = 4096  # 4k resolution as absolute maximum\n+\n+\n+def _resize_output_size_rescale_to_max_len(\n+    height: int, width: int, min_len: Optional[int] = 1, max_len: Optional[int] = None\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        height (`int`):\n+            Height of the input image.\n+        width (`int`):\n+            Width of the input image.\n+        min_len (`int`, *optional*, defaults to 1):\n+            Minimum size of the output image.\n+        max_len (`int`, *optional*, defaults to the maximum size of the image):\n+            Maximum size of the output image.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    max_len = max(height, width) if max_len is None else max_len\n+    aspect_ratio = width / height\n+\n+    if width >= height:\n+        width = max_len\n+        height = int(width / aspect_ratio)\n+        if height % 2 != 0:\n+            height += 1\n+    elif height > width:\n+        height = max_len\n+        width = int(height * aspect_ratio)\n+        if width % 2 != 0:\n+            width += 1\n+\n+    # Avoid resizing to a size smaller than min_len\n+    height = max(height, min_len)\n+    width = max(width, min_len)\n+    return height, width\n+\n+\n+def _resize_output_size_scale_below_upper_bound(\n+    height: int, width: int, max_len: Optional[dict[str, int]] = None\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        height (`int`):\n+            Height of the input image.\n+        width (`int`):\n+            Width of the input image.\n+        max_len (`Dict[str, int]`, *optional*, defaults to the maximum size of the image):\n+            Defines the maximum dimensions of the image.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    max_len = max(height, width) if max_len is None else max_len\n+\n+    aspect_ratio = width / height\n+    if width >= height and width > max_len:\n+        width = max_len\n+        height = int(width / aspect_ratio)\n+    elif height > width and height > max_len:\n+        height = max_len\n+        width = int(height * aspect_ratio)\n+\n+    # Avoid resizing to a size smaller than 1\n+    height = max(height, 1)\n+    width = max(width, 1)\n+    return height, width\n+\n+\n+def get_resize_output_image_size(\n+    image,\n+    resolution_max_side: int,\n+) -> tuple[int, int]:\n+    \"\"\"\n+    Get the output size of the image after resizing given a dictionary specifying the max and min sizes.\n+    Args:\n+        image (`torch.Tensor`):\n+            Image to resize.\n+        resolution_max_side (`int`):\n+            The longest edge of the image will be resized to this value. The shortest edge will be resized to keep the\n+            input aspect ratio.\n+    Returns:\n+        The output size of the image after resizing.\n+    \"\"\"\n+    height, width = image.size()[-2:]\n+\n+    # Find the output size, when rescaling the longest edge to max_len and preserving the aspect ratio\n+    height, width = _resize_output_size_rescale_to_max_len(height, width, max_len=resolution_max_side)\n+    # Find the output size when scaling the image to be below the MAX_IMAGE_SIZE\n+    height, width = _resize_output_size_scale_below_upper_bound(height, width, max_len=MAX_IMAGE_SIZE)\n+    return height, width\n+\n+\n+def get_max_height_width(images_list: list[list[\"torch.Tensor\"]]) -> tuple[int, int]:\n+    \"\"\"\n+    Get the maximum height and width across all images in a batch.\n+    \"\"\"\n+    image_sizes = []\n+    for images in images_list:\n+        for image in images:\n+            image_sizes.append(image.size()[-2:])\n+\n+    max_height = max(size[0] for size in image_sizes)\n+    max_width = max(size[1] for size in image_sizes)\n+    return (max_height, max_width)\n+\n+\n+@auto_docstring\n+class SmolVLMImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.LANCZOS\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"longest_edge\": 4 * 364}\n+    max_image_size = {\"longest_edge\": 364}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    do_image_splitting = True\n+    do_pad = True\n+    return_row_col_info = False\n+    valid_kwargs = SmolVLMFastImageProcessorKwargs\n+\n+    def _prepare_images_structure(\n+        self,\n+        images: ImageInput,\n+    ) -> ImageInput:\n+        \"\"\"\n+        Prepare a nested images structure for processing.\n+        \"\"\"\n+        return make_nested_list_of_images(images)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Resize an image. The longest edge of the image is resized to size.longest_edge, with the shortest edge\n+        resized to keep the input aspect ratio. Can also be used with size.height and size.width.\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+            antialias (`bool`, *optional*, defaults to `True`):\n+                Whether to use antialiasing when resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if interpolation == F.InterpolationMode.LANCZOS:\n+            logger.warning_once(\n+                \"You have used fast image processor with LANCZOS resample which not yet supported for torch.Tensor. \"\n+                \"BICUBIC resample will be used as an alternative. Please fall back to slow image processor if you \"\n+                \"want full consistency with the original model.\"\n+            )\n+            interpolation = F.InterpolationMode.BICUBIC\n+\n+        if size.longest_edge:\n+            size = get_resize_output_image_size(image, resolution_max_side=size.longest_edge)\n+        elif size.height and size.width:\n+            size = (size.height, size.width)\n+        else:\n+            raise ValueError(\"size must be a dictionary with key 'longest_edge' or 'height' and 'width'.\")\n+\n+        return F.resize(image, size, interpolation=interpolation, antialias=antialias)\n+\n+    def split_images(\n+        self,\n+        images: torch.Tensor,\n+        max_image_size: dict[str, int],\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Split an image into squares of side max_image_size and the original image resized to max_image_size.\n+        That means that a single image becomes a sequence of images.\n+        This is a \"trick\" to spend more compute on each image with no changes in the vision encoder.\n+        1) If one side of the original image is larger than `max_image_size`, resize it to `max_image_size` while preserving the aspect ratio.\n+        2) Divide the resulting image into `ceil(height / max_image_size)` x `ceil(width / max_image_size)`\n+        sub-images of the same size each (image_size, image_size). Typically, 364x364.\n+        3) Returns the list of the crops and the original image, in addition to the number of splits for the height and the width.\n+        Args:\n+            images (`torch.Tensor`):\n+                Images to split.\n+            max_image_size (`Dict[str, int]`):\n+                Maximum size of the output image. If the image is larger than this size, it will be split into\n+                patches of this size, and the original image will be concatenated with the patches, resized to max_size.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+        \"\"\"\n+        batch_size, num_channels, height, width = images.size()\n+        height_dim, width_dim = 2, 3\n+\n+        max_height = max_width = max_image_size[\"longest_edge\"]\n+\n+        frames = []\n+        if height > max_height or width > max_width:\n+            # Calculate the number of splits\n+            num_splits_h = math.ceil(height / max_height)\n+            num_splits_w = math.ceil(width / max_width)\n+\n+            # Split the images by height, then by width\n+            frames = (\n+                images.unfold(height_dim, size=max_height, step=max_height)\n+                .unfold(width_dim, size=max_width, step=max_width)\n+                .contiguous()\n+                .view(batch_size, num_channels, -1, max_height, max_width)\n+                .permute(0, 2, 1, 3, 4)\n+            )  # batch_size x n_frames x num_channels x height x width\n+\n+            # For the global image at the end, we resize it to match the max_image_size, for cpu memory efficiency\n+            global_image_height, global_image_width = max_height, max_width\n+            images = self.resize(\n+                images, SizeDict(height=global_image_height, width=global_image_width), interpolation=interpolation\n+            )\n+\n+            frames = torch.cat((frames, images.unsqueeze(1)), dim=1)\n+        else:\n+            num_splits_h, num_splits_w = 0, 0\n+            frames = images.unsqueeze(1)\n+\n+        num_splits_h = [num_splits_h] * batch_size\n+        num_splits_w = [num_splits_w] * batch_size\n+\n+        return frames, num_splits_h, num_splits_w\n+\n+    def resize_for_vision_encoder(\n+        self,\n+        image: torch.Tensor,\n+        vision_encoder_max_size: int,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resize images to be multiples of `vision_encoder_max_size` while preserving the aspect ratio.\n+        Args:\n+            image (`torch.Tensor`):\n+                Images to resize.\n+            vision_encoder_max_size (`int`):\n+                Maximum size of the output image. If the image is larger than this size, it will be split into\n+                patches of this size, and the original image will be concatenated with the patches, resized to max_size.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                `InterpolationMode` filter to use when resizing the image e.g. `InterpolationMode.BICUBIC`.\n+        \"\"\"\n+        height, width = image.size()[-2:]\n+\n+        aspect_ratio = width / height\n+        if width >= height:\n+            width = math.ceil(width / vision_encoder_max_size) * vision_encoder_max_size\n+            height = int(width / aspect_ratio)\n+            height = math.ceil(height / vision_encoder_max_size) * vision_encoder_max_size\n+        elif height > width:\n+            height = math.ceil(height / vision_encoder_max_size) * vision_encoder_max_size\n+            width = int(height * aspect_ratio)\n+            width = math.ceil(width / vision_encoder_max_size) * vision_encoder_max_size\n+        new_size = SizeDict(height=height, width=width)\n+        return self.resize(image, size=new_size, interpolation=interpolation)\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: tuple[int, int],\n+        fill: int = 0,\n+        return_pixel_mask: bool = True,\n+    ):\n+        original_size = image.shape[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+\n+        # Only pad if necessary\n+        if original_size != padded_size:\n+            padding = (0, 0, padding_right, padding_bottom)\n+            image = F.pad(image, padding, fill=fill, padding_mode=\"constant\")\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = None\n+        if return_pixel_mask:\n+            pixel_mask = torch.zeros_like(image[..., 0, :, :], dtype=torch.int64)\n+            pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[SmolVLMFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[list[\"torch.Tensor\"]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        do_image_splitting: Optional[bool],\n+        max_image_size: Optional[dict[str, int]],\n+        return_row_col_info: Optional[bool],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Process a batch of images for the model.\n+        \"\"\"\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(stacked_images, size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index, is_nested=True)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            resized_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        split_images_grouped = {}\n+        if do_image_splitting:\n+            rows_grouped = {}\n+            cols_grouped = {}\n+            for shape, stacked_images in grouped_images.items():\n+                stacked_images = self.resize_for_vision_encoder(\n+                    stacked_images, max_image_size[\"longest_edge\"], interpolation=interpolation\n+                )\n+                stacked_images, rows, cols = self.split_images(\n+                    stacked_images, max_image_size=max_image_size, interpolation=interpolation\n+                )\n+                split_images_grouped[shape] = stacked_images\n+                rows_grouped[shape] = rows\n+                cols_grouped[shape] = cols\n+            processed_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+            rows = reorder_images(rows_grouped, grouped_images_index, is_nested=True)\n+            cols = reorder_images(cols_grouped, grouped_images_index, is_nested=True)\n+            # flattenened the doubly nested list to a nested list\n+            for i, group_images in enumerate(processed_images):\n+                processed_images[i] = [image for sublist in group_images for image in sublist]\n+        else:\n+            for shape, stacked_images in grouped_images.items():\n+                # We square the images to max_image_size\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=SizeDict(height=max_image_size[\"longest_edge\"], width=max_image_size[\"longest_edge\"]),\n+                    interpolation=interpolation,\n+                )\n+            split_images_grouped[shape] = stacked_images\n+            processed_images = reorder_images(split_images_grouped, grouped_images_index, is_nested=True)\n+            rows = [[0] * len(images) for images in processed_images]\n+            cols = [[0] * len(images) for images in processed_images]\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(\n+            processed_images, is_nested=True, disable_grouping=disable_grouping\n+        )\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index, is_nested=True)\n+        if do_pad:\n+            # Get max images per batch\n+            max_num_images = max(len(images_) for images_ in processed_images)\n+            max_height, max_width = get_max_height_width(processed_images)\n+\n+            processed_images_padded = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(processed_images[0][0].shape[0], max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            pixel_attention_masks = torch.zeros(\n+                len(processed_images),\n+                max_num_images,\n+                *(max_height, max_width),\n+                device=processed_images[0][0].device,\n+            )\n+            for i, images in enumerate(processed_images):\n+                for j, image in enumerate(images):\n+                    processed_images_padded[i, j], pixel_attention_masks[i, j] = self.pad(\n+                        image, (max_height, max_width)\n+                    )\n+            processed_images = processed_images_padded\n+\n+        if do_pad:\n+            data = {\"pixel_values\": processed_images, \"pixel_attention_mask\": pixel_attention_masks}\n+        elif return_tensors == \"pt\":\n+            data = {\"pixel_values\": torch.stack([torch.stack(images) for images in processed_images])}\n+        else:\n+            data = {\"pixel_values\": processed_images}\n+        # This is needed for generating correct text inputs in the processor - we don't pad to the max number of images\n+        encoding = BatchFeature(data=data, tensor_type=return_tensors)\n+\n+        if return_row_col_info:\n+            encoding[\"rows\"] = rows\n+            encoding[\"cols\"] = cols\n+\n+        return encoding\n+\n+    def to_dict(self):\n+        encoder_dict = super().to_dict()\n+        encoder_dict.pop(\"_valid_processor_keys\", None)\n+        encoder_dict.pop(\"return_row_col_info\", None)\n+        return encoder_dict\n+\n+\n+__all__ = [\"SmolVLMImageProcessorFast\"]"
        },
        {
            "sha": "d1b1861e6f9ffc9d3c30761c1bc3397a7eaccd32",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -34,7 +34,12 @@\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import (\n+    LossKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n from ..auto import AutoModel\n from .configuration_smolvlm import SmolVLMConfig, SmolVLMVisionConfig\n "
        },
        {
            "sha": "d4fffa0c40fd3b7eecb098d2d17c10455e64bb20",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -25,6 +25,7 @@\n from ...utils import auto_docstring, can_return_tuple, logging\n from ..idefics3.configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n from ..idefics3.image_processing_idefics3 import Idefics3ImageProcessor\n+from ..idefics3.image_processing_idefics3_fast import Idefics3ImageProcessorFast\n from ..idefics3.modeling_idefics3 import (\n     Idefics3BaseModelOutputWithPast,\n     Idefics3ForConditionalGeneration,\n@@ -160,6 +161,10 @@ class SmolVLMImageProcessor(Idefics3ImageProcessor):\n     pass\n \n \n+class SmolVLMImageProcessorFast(Idefics3ImageProcessorFast):\n+    pass\n+\n+\n class SmolVLMBaseModelOutputWithPast(Idefics3BaseModelOutputWithPast):\n     pass\n \n@@ -396,6 +401,7 @@ def forward(self, **super_kwargs):\n     \"SmolVLMVisionConfig\",\n     \"SmolVLMConfig\",\n     \"SmolVLMImageProcessor\",\n+    \"SmolVLMImageProcessorFast\",\n     \"SmolVLMForConditionalGeneration\",\n     \"SmolVLMPreTrainedModel\",\n     \"SmolVLMModel\","
        },
        {
            "sha": "cc8235f1141ebc281372029224086d0a9899c1dc",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -100,11 +100,11 @@ def _preprocess(\n         rescale_factor: float,\n         do_pad: bool,\n         pad_size: int,\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n-        interpolation: Optional[\"F.InterpolationMode\"],\n         **kwargs,\n     ) -> BatchFeature:\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_image_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_rescale:"
        },
        {
            "sha": "3e6571f159e14742fbdc2117942798d860114ee8",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -93,6 +93,7 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> BatchFeature:\n@@ -102,7 +103,7 @@ def _preprocess(\n         This method overrides the base class method to include padding and pixel mask generation.\n         \"\"\"\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n \n         for shape, stacked_images in grouped_images.items():\n@@ -112,7 +113,7 @@ def _preprocess(\n         resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n \n         # Group images by size for further processing\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n \n         for shape, stacked_images in grouped_images.items():\n@@ -127,7 +128,9 @@ def _preprocess(\n         # Handle padding if required\n         data = {}\n         if do_pad:\n-            pixel_values, pixel_mask = self._pad_batch(processed_images, return_tensors)\n+            pixel_values, pixel_mask = self._pad_batch(\n+                processed_images, return_tensors, disable_grouping=disable_grouping\n+            )\n             data = {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask}\n         else:\n             # If no padding, just return the processed images\n@@ -195,6 +198,7 @@ def _pad_batch(\n         self,\n         images: list[\"torch.Tensor\"],\n         return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: Optional[bool],\n     ) -> tuple:\n         \"\"\"\n         Pad a batch of images to the same size based on the maximum dimensions.\n@@ -210,7 +214,7 @@ def _pad_batch(\n         max_size = get_max_height_width(images)\n \n         # Group images by shape before padding\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_images = {}\n         processed_masks = {}\n "
        },
        {
            "sha": "91af75500b11d69862f6aad334e704d0dd330b74",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -202,10 +202,12 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: Optional[bool] = None,\n         size_divisibility: Optional[int] = None,\n+        disable_grouping: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n+        **kwargs,\n     ) -> BatchFeature:\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n-        grouped_trimaps, grouped_trimaps_index = group_images_by_shape(trimaps)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        grouped_trimaps, grouped_trimaps_index = group_images_by_shape(trimaps, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape in grouped_images:\n             stacked_images = grouped_images[shape]"
        },
        {
            "sha": "793c386fdc75daa8cad9a54baa68428f08bf91d2",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -188,11 +188,12 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n             if do_rescale:"
        },
        {
            "sha": "77344e55b69fe4c712f6ca9847894a09fc2e7d81",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -204,6 +204,15 @@ class ImageProcessorArgs:\n         \"shape\": None,\n     }\n \n+    disable_grouping = {\n+        \"description\": \"\"\"\n+    Whether to disable grouping of images by size to process them individually and not in batches.\n+    If None, will be set to True if the images are on CPU, and False otherwise. This choice is based on\n+    empirical observations, as detailed here: https://github.com/huggingface/transformers/pull/38157\n+    \"\"\",\n+        \"shape\": None,\n+    }\n+\n \n class ModelArgs:\n     labels = {"
        },
        {
            "sha": "d9ba788b1f441e12603f103b0951e929d4ee93d7",
            "filename": "tests/models/beit/test_image_processing_beit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_image_processing_beit.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -298,11 +298,10 @@ def test_slow_fast_equivalence(self):\n         image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n         image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(image_encoding_slow.pixel_values - image_encoding_fast.pixel_values)).item(), 1e-3\n+        self._assert_slow_fast_tensors_equivalence(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            image_encoding_slow.labels.float(), image_encoding_fast.labels.float()\n         )\n-        self.assertTrue(torch.allclose(image_encoding_slow.labels, image_encoding_fast.labels, atol=1e-1))\n \n     def test_slow_fast_equivalence_batched(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n@@ -324,7 +323,5 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.labels.float(), encoding_fast.labels.float())"
        },
        {
            "sha": "8fbb6fcca7b1fdced02af283e5bf974983166d53",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 12,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -19,14 +19,11 @@\n import requests\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n-if is_torch_available():\n-    import torch\n-\n if is_vision_available():\n     from PIL import Image\n \n@@ -124,10 +121,6 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"size\"))\n             self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n \n-    def _assertEquivalence(self, a, b):\n-        self.assertTrue(torch.allclose(a, b, atol=1e-1))\n-        self.assertLessEqual(torch.mean(torch.abs(a - b)).item(), 1e-3)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):\n@@ -146,8 +139,8 @@ def test_slow_fast_equivalence(self):\n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n-        self._assertEquivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-        self._assertEquivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())\n \n     @require_vision\n     @require_torch\n@@ -170,5 +163,5 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n-        self._assertEquivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n-        self._assertEquivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_mask.float(), encoding_fast.pixel_mask.float())"
        },
        {
            "sha": "adbfc80072e546e8d17ddceb28aad4f716993901",
            "filename": "tests/models/flava/test_image_processing_flava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -418,15 +418,8 @@ def test_slow_fast_equivalence(self):\n         encoding_fast = image_processor_fast(\n             dummy_image, return_tensors=\"pt\", return_codebook_pixels=True, return_image_mask=True\n         )\n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n-        self.assertTrue(\n-            torch.allclose(encoding_slow.codebook_pixel_values, encoding_fast.codebook_pixel_values, atol=1e-1)\n-        )\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.codebook_pixel_values - encoding_fast.codebook_pixel_values)).item(),\n-            1e-3,\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.codebook_pixel_values, encoding_fast.codebook_pixel_values\n         )"
        },
        {
            "sha": "49da68e942be0e8d1ad30f1b792ad2092ff5d119",
            "filename": "tests/models/gemma3/test_image_processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_image_processing_gemma3.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -286,7 +286,4 @@ def test_slow_fast_equivalence_batched_pas(self):\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n         torch.testing.assert_close(encoding_slow.num_crops, encoding_fast.num_crops)\n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)"
        },
        {
            "sha": "53b44eba61519964c66ab0f2b15c4d02532f4080",
            "filename": "tests/models/got_ocr2/test_image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -125,10 +125,7 @@ def test_slow_fast_equivalence_crop_to_patches(self):\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n         torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     def test_slow_fast_equivalence_batched_crop_to_patches(self):\n         # Prepare image inputs so that we have two groups of images with equal resolution with a group of images with\n@@ -144,10 +141,7 @@ def test_slow_fast_equivalence_batched_crop_to_patches(self):\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n         torch.testing.assert_close(encoding_slow.num_patches, encoding_fast.num_patches)\n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     def test_crop_to_patches(self):\n         # test slow image processor"
        },
        {
            "sha": "f0f246a3847b1b39b258045909d966f6554c0259",
            "filename": "tests/models/idefics2/test_image_processing_idefics2.py",
            "status": "modified",
            "additions": 123,
            "deletions": 31,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fidefics2%2Ftest_image_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fidefics2%2Ftest_image_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_image_processing_idefics2.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -1,4 +1,5 @@\n-# Copyright 2024 HuggingFace Inc.\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -12,13 +13,12 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import unittest\n \n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n \n@@ -28,6 +28,8 @@\n \n     from transformers import Idefics2ImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import Idefics2ImageProcessorFast\n \n if is_torch_available():\n     import torch\n@@ -88,10 +90,6 @@ def prepare_image_processor_dict(self):\n         }\n \n     def get_expected_values(self, image_inputs, batched=False):\n-        \"\"\"\n-        This function computes the expected height and width when providing images to BridgeTowerImageProcessor,\n-        assuming do_resize is set to True with a scalar size and size_divisor.\n-        \"\"\"\n         if not batched:\n             shortest_edge = self.size[\"shortest_edge\"]\n             longest_edge = self.size[\"longest_edge\"]\n@@ -142,11 +140,6 @@ def prepare_image_inputs(\n         numpify=False,\n         torchify=False,\n     ):\n-        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n-        or a list of PyTorch tensors if one specifies torchify=True.\n-\n-        One can specify whether the images are of the same resolution or not.\n-        \"\"\"\n         assert not (numpify and torchify), \"You cannot specify both numpy and PyTorch tensors at the same time\"\n \n         batch_size = batch_size if batch_size is not None else self.batch_size\n@@ -162,23 +155,19 @@ def prepare_image_inputs(\n                 if equal_resolution:\n                     width = height = max_resolution\n                 else:\n-                    # To avoid getting image width/height 0\n                     if size_divisor is not None:\n-                        # If `size_divisor` is defined, the image needs to have width/size >= `size_divisor`\n                         min_resolution = max(size_divisor, min_resolution)\n                     width, height = np.random.choice(np.arange(min_resolution, max_resolution), 2)\n                 images.append(np.random.randint(255, size=(num_channels, width, height), dtype=np.uint8))\n             images_list.append(images)\n \n         if not numpify and not torchify:\n-            # PIL expects the channel dimension as last dimension\n             images_list = [[Image.fromarray(np.moveaxis(image, 0, -1)) for image in images] for images in images_list]\n \n         if torchify:\n             images_list = [[torch.from_numpy(image) for image in images] for images in images_list]\n \n         if numpify:\n-            # Numpy images are typically in channels last format\n             images_list = [[image.transpose(1, 2, 0) for image in images] for images in images_list]\n \n         return images_list\n@@ -188,6 +177,7 @@ def prepare_image_inputs(\n @require_vision\n class Idefics2ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Idefics2ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Idefics2ImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -198,22 +188,23 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n \n     def test_call_numpy(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random numpy tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n             for sample_images in image_inputs:\n@@ -238,7 +229,7 @@ def test_call_numpy_4_channels(self):\n             image_processor_dict = self.image_processor_dict\n             image_processor_dict[\"image_mean\"] = [0.5, 0.5, 0.5, 0.5]\n             image_processor_dict[\"image_std\"] = [0.5, 0.5, 0.5, 0.5]\n-            image_processing = self.image_processing_class(**image_processor_dict)\n+            image_processing = image_processing_class(**image_processor_dict)\n             # create random numpy tensors\n             self.image_processor_tester.num_channels = 4\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n@@ -266,7 +257,7 @@ def test_call_numpy_4_channels(self):\n     def test_call_pil(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PIL images\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n             for images in image_inputs:\n@@ -288,7 +279,7 @@ def test_call_pil(self):\n     def test_call_pytorch(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PyTorch tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n@@ -308,3 +299,104 @@ def test_call_pytorch(self):\n                 tuple(encoded_images.shape),\n                 (self.image_processor_tester.batch_size, *expected_output_image_shape),\n             )\n+\n+    def test_image_splitting(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_dict = self.image_processor_dict.copy()\n+            image_processor_dict[\"do_image_splitting\"] = True\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=True, torchify=True, num_images=1\n+            )\n+\n+            result = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            self.assertEqual(result.pixel_values.shape[1], 5)\n+\n+            image_processor_dict[\"do_image_splitting\"] = False\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            result = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            if len(result.pixel_values.shape) == 5:\n+                self.assertEqual(result.pixel_values.shape[1], 1)\n+            else:\n+                self.assertEqual(result.pixel_values.shape[1], self.image_processor_tester.num_channels)\n+\n+    def test_pixel_attention_mask(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor_dict = self.image_processor_dict.copy()\n+            image_processor_dict[\"do_pad\"] = True\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            result = image_processing(image_inputs, return_tensors=\"pt\")\n+            self.assertIn(\"pixel_attention_mask\", result)\n+\n+            self.assertEqual(result.pixel_attention_mask.shape[-2:], result.pixel_values.shape[-2:])\n+\n+            image_processor_dict[\"do_pad\"] = False\n+            image_processor_dict[\"do_image_splitting\"] = False\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            equal_size_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+            result = image_processing(equal_size_inputs, return_tensors=\"pt\")\n+            self.assertNotIn(\"pixel_attention_mask\", result)\n+\n+    def test_convert_rgb(self):\n+        for image_processing_class in self.image_processor_list:\n+            rgba_image = Image.new(\"RGBA\", (100, 100), (255, 0, 0, 128))\n+\n+            # Test with do_convert_rgb=True - this should work for all processors\n+            image_processor_dict = self.image_processor_dict.copy()\n+            image_processor_dict[\"do_convert_rgb\"] = True\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            result = image_processing([rgba_image], return_tensors=\"pt\")\n+            self.assertIsNotNone(result.pixel_values)\n+            rgb_image = rgba_image.convert(\"RGB\")\n+\n+            image_processor_dict[\"do_convert_rgb\"] = False\n+            image_processing = image_processing_class(**image_processor_dict)\n+\n+            # Use the RGB image instead of RGBA when do_convert_rgb=False\n+            result = image_processing([rgb_image], return_tensors=\"pt\")\n+            self.assertIsNotNone(result.pixel_values)\n+\n+            # Additional test: verifying proper handling of regular RGB images\n+            rgb_image = Image.new(\"RGB\", (100, 100), (255, 0, 0))\n+            result = image_processing([rgb_image], return_tensors=\"pt\")\n+            self.assertIsNotNone(result.pixel_values)\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, num_images=5, torchify=True\n+        )\n+        # pop some images to have non homogenous batches:\n+        indices_to_pop = [i if np.random.random() < 0.5 else None for i in range(len(dummy_images))]\n+        for i in indices_to_pop:\n+            if i is not None:\n+                dummy_images[i].pop()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_attention_mask.float(), encoding_fast.pixel_attention_mask.float()\n+        )"
        },
        {
            "sha": "7a1eb4f44fc1cefea0afb5a0b70e65b0472b2a17",
            "filename": "tests/models/idefics3/test_image_processing_idefics3.py",
            "status": "modified",
            "additions": 96,
            "deletions": 19,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -1,3 +1,4 @@\n+# coding=utf-8\n # Copyright 2024 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,10 +17,11 @@\n import unittest\n \n import numpy as np\n+import requests\n \n from transformers.image_utils import PILImageResampling\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n \n@@ -29,6 +31,9 @@\n \n     from transformers import Idefics3ImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import Idefics3ImageProcessorFast\n+\n \n if is_torch_available():\n     import torch\n@@ -164,6 +169,7 @@ def prepare_image_inputs(\n @require_vision\n class Idefics3ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Idefics3ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Idefics3ImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -174,25 +180,26 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"resample\"))\n-        self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n-        self.assertTrue(hasattr(image_processing, \"max_image_size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n+            self.assertTrue(hasattr(image_processing, \"max_image_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n \n     def test_call_numpy(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random numpy tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n             for sample_images in image_inputs:\n@@ -216,7 +223,7 @@ def test_call_numpy_4_channels(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n             image_processor_dict = self.image_processor_dict\n-            image_processing = self.image_processing_class(**image_processor_dict)\n+            image_processing = image_processing_class(**image_processor_dict)\n             # create random numpy tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n \n@@ -239,7 +246,7 @@ def test_call_numpy_4_channels(self):\n     def test_call_pil(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PIL images\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n             for images in image_inputs:\n@@ -261,7 +268,7 @@ def test_call_pil(self):\n     def test_call_pytorch(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PyTorch tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n@@ -281,3 +288,73 @@ def test_call_pytorch(self):\n                 tuple(encoded_images.shape),\n                 (self.image_processor_tester.batch_size, *expected_output_image_shape),\n             )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        dummy_image = dummy_image.resize((100, 150))\n+        image_processor_slow = self.image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+        image_processor_fast = self.fast_image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\", return_row_col_info=True)\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\", return_row_col_info=True)\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_attention_mask.float(), encoding_fast.pixel_attention_mask.float()\n+        )\n+        self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n+        self.assertEqual(encoding_slow.cols, encoding_fast.cols)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, num_images=5, torchify=True\n+        )\n+        # pop some images to have non homogenous batches:\n+        indices_to_pop = [i if np.random.random() < 0.5 else None for i in range(len(dummy_images))]\n+        for i in indices_to_pop:\n+            if i is not None:\n+                dummy_images[i].pop()\n+\n+        image_processor_slow = self.image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+        image_processor_fast = self.fast_image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\", return_row_col_info=True)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\", return_row_col_info=True)\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=3e-1)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_attention_mask.float(), encoding_fast.pixel_attention_mask.float()\n+        )\n+        self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n+        self.assertEqual(encoding_slow.cols, encoding_fast.cols)"
        },
        {
            "sha": "4b8d50489e8bf565ea2bfc182f34f949bb1e3035",
            "filename": "tests/models/layoutlmv2/test_image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 40,
            "deletions": 22,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -15,9 +15,21 @@\n import unittest\n \n import requests\n-\n-from transformers.testing_utils import require_pytesseract, require_torch, require_vision\n-from transformers.utils import is_pytesseract_available, is_torch_available, is_torchvision_available\n+from packaging import version\n+\n+from transformers.testing_utils import (\n+    require_pytesseract,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_pytesseract_available,\n+    is_torch_available,\n+    is_torchvision_available,\n+)\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -157,16 +169,8 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n-        self.assertTrue(\n-            torch.allclose(\n-                encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255, atol=1e-1\n-            )\n-        )\n-        self.assertLessEqual(\n-            torch.mean(\n-                torch.abs(encoding_slow.pixel_values.float() - encoding_fast.pixel_values.float()) / 255\n-            ).item(),\n-            1e-3,\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255\n         )\n \n     @require_vision\n@@ -190,14 +194,28 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n-        self.assertTrue(\n-            torch.allclose(\n-                encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255, atol=1e-1\n-            )\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255\n         )\n-        self.assertLessEqual(\n-            torch.mean(\n-                torch.abs(encoding_slow.pixel_values.float() - encoding_fast.pixel_values.float()) / 255\n-            ).item(),\n-            1e-3,\n+\n+    # Overriding as we can't use torch.testing.assert_close on int8 tensors\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    def test_can_compile_fast_image_processor(self):\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values.float() / 255, output_compiled.pixel_values.float() / 255\n         )"
        },
        {
            "sha": "43ee7cfc273a40ce101584b65d861c5b992ee754",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 37,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import time\n import unittest\n \n import numpy as np\n@@ -214,29 +213,6 @@ def test_call_pytorch(self):\n             expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list)\n             self.assertEqual(tuple(batch_encoded_images.shape), expected_output_image_shape)\n \n-    @require_vision\n-    @require_torch\n-    def test_fast_is_faster_than_slow(self):\n-        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n-            self.skipTest(reason=\"Skipping speed test\")\n-\n-        if self.image_processing_class is None or self.fast_image_processing_class is None:\n-            self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\n-\n-        def measure_time(image_processor, image):\n-            start = time.time()\n-            _ = image_processor(image, return_tensors=\"pt\")\n-            return time.time() - start\n-\n-        image_inputs_list = self.image_processor_tester.prepare_image_inputs(torchify=True)\n-        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n-        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n-\n-        fast_time = measure_time(image_processor_fast, image_inputs_list)\n-        slow_time = measure_time(image_processor_slow, image_inputs_list)\n-\n-        self.assertLessEqual(fast_time, slow_time)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):\n@@ -255,9 +231,7 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n-        torch.testing.assert_close(\n-            encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0], rtol=100, atol=1e-1\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0])\n \n     @require_vision\n     @require_torch\n@@ -282,14 +256,8 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n         for i in range(len(encoding_slow.pixel_values)):\n-            self.assertTrue(\n-                torch.allclose(encoding_slow.pixel_values[i][0], encoding_fast.pixel_values[i][0], atol=1e-1)\n-            )\n-            self.assertLessEqual(\n-                torch.mean(torch.abs(encoding_slow.pixel_values[i][0] - encoding_fast.pixel_values[i][0])).item(), 1e-3\n-            )\n-            torch.testing.assert_close(\n-                encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0], rtol=100, atol=1e-1\n+            self._assert_slow_fast_tensors_equivalence(\n+                encoding_slow.pixel_values[i][0], encoding_fast.pixel_values[i][0]\n             )\n \n     @slow\n@@ -309,8 +277,8 @@ def test_can_compile_fast_image_processor(self):\n         image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n         output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n \n-        torch.testing.assert_close(\n-            output_eager.pixel_values[0][0], output_compiled.pixel_values[0][0], rtol=1e-4, atol=1e-4\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values[0][0], output_compiled.pixel_values[0][0], atol=1e-4, rtol=1e-4, mean_atol=1e-5\n         )\n \n     @unittest.skip(reason=\"PixtralImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")  # FIXME Amy"
        },
        {
            "sha": "f671f8f1301ab1051b1b53b0cfb8d2decc707e3a",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -362,6 +362,4 @@ def test_slow_fast_equivalence(self):\n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n-        torch.testing.assert_close(\n-            encoding_slow.pixel_values, encoding_fast.pixel_values, rtol=100, atol=1e-2\n-        )  # @yoni bit weird that we have such diffs\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)"
        },
        {
            "sha": "118fcc6b819363ecbd3ed43d0ffc01656612977a",
            "filename": "tests/models/siglip2/test_image_processing_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -18,14 +18,11 @@\n import requests\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n-if is_torch_available():\n-    import torch\n-\n if is_vision_available():\n     from PIL import Image\n \n@@ -150,7 +147,6 @@ def test_image_processor_from_dict_with_kwargs(self):\n     def test_call_numpy_4_channels(self):\n         pass\n \n-    # increase mean tolerance to 1e-3 -> 2e-3\n     # Ignore copy\n     def test_slow_fast_equivalence(self):\n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n@@ -167,10 +163,7 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n-        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-1)\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     # increase mean tolerance to 1e-3 -> 2e-3\n     # Ignore copy\n@@ -193,7 +186,4 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n-        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-1)\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)"
        },
        {
            "sha": "34b893f8175197095e3798c9515629dad01788cb",
            "filename": "tests/models/smolvlm/test_image_processing_smolvlm.py",
            "status": "modified",
            "additions": 96,
            "deletions": 19,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -1,3 +1,4 @@\n+# coding=utf-8\n # Copyright 2024 HuggingFace Inc.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n@@ -16,10 +17,11 @@\n import unittest\n \n import numpy as np\n+import requests\n \n from transformers.image_utils import PILImageResampling\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n \n@@ -29,6 +31,9 @@\n \n     from transformers import SmolVLMImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import SmolVLMImageProcessorFast\n+\n \n if is_torch_available():\n     import torch\n@@ -164,6 +169,7 @@ def prepare_image_inputs(\n @require_vision\n class SmolVLMImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = SmolVLMImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = SmolVLMImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -174,25 +180,26 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"resample\"))\n-        self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n-        self.assertTrue(hasattr(image_processing, \"max_image_size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n+            self.assertTrue(hasattr(image_processing, \"max_image_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"do_image_splitting\"))\n \n     def test_call_numpy(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random numpy tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n             for sample_images in image_inputs:\n@@ -216,7 +223,7 @@ def test_call_numpy_4_channels(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n             image_processor_dict = self.image_processor_dict\n-            image_processing = self.image_processing_class(**image_processor_dict)\n+            image_processing = image_processing_class(**image_processor_dict)\n             # create random numpy tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n \n@@ -239,7 +246,7 @@ def test_call_numpy_4_channels(self):\n     def test_call_pil(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PIL images\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n             for images in image_inputs:\n@@ -261,7 +268,7 @@ def test_call_pil(self):\n     def test_call_pytorch(self):\n         for image_processing_class in self.image_processor_list:\n             # Initialize image_processing\n-            image_processing = self.image_processing_class(**self.image_processor_dict)\n+            image_processing = image_processing_class(**self.image_processor_dict)\n             # create random PyTorch tensors\n             image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n \n@@ -281,3 +288,73 @@ def test_call_pytorch(self):\n                 tuple(encoded_images.shape),\n                 (self.image_processor_tester.batch_size, *expected_output_image_shape),\n             )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        dummy_image = dummy_image.resize((100, 150))\n+        image_processor_slow = self.image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+        image_processor_fast = self.fast_image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\", return_row_col_info=True)\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\", return_row_col_info=True)\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_attention_mask.float(), encoding_fast.pixel_attention_mask.float()\n+        )\n+        self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n+        self.assertEqual(encoding_slow.cols, encoding_fast.cols)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, num_images=5, torchify=True\n+        )\n+        # pop some images to have non homogenous batches:\n+        indices_to_pop = [i if np.random.random() < 0.5 else None for i in range(len(dummy_images))]\n+        for i in indices_to_pop:\n+            if i is not None:\n+                dummy_images[i].pop()\n+\n+        image_processor_slow = self.image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+        image_processor_fast = self.fast_image_processing_class(\n+            **self.image_processor_dict, resample=PILImageResampling.BICUBIC\n+        )\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\", return_row_col_info=True)\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\", return_row_col_info=True)\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=3e-1)\n+        self._assert_slow_fast_tensors_equivalence(\n+            encoding_slow.pixel_attention_mask.float(), encoding_fast.pixel_attention_mask.float()\n+        )\n+        self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n+        self.assertEqual(encoding_slow.cols, encoding_fast.cols)"
        },
        {
            "sha": "f8b0f545dd8224cab3b4bc7d138697e2f4f32cbf",
            "filename": "tests/models/swin2sr/test_image_processing_swin2sr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -197,7 +197,7 @@ def test_slow_fast_equivalence_batched(self):\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n \n-        encoded_slow = image_processor_slow(image_inputs, return_tensors=\"pt\").pixel_values\n-        encoded_fast = image_processor_fast(image_inputs, return_tensors=\"pt\").pixel_values\n+        encoded_slow = image_processor_slow(image_inputs, return_tensors=\"pt\")\n+        encoded_fast = image_processor_fast(image_inputs, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoded_slow, encoded_fast, atol=1e-1))\n+        self._assert_slow_fast_tensors_equivalence(encoded_slow.pixel_values, encoded_fast.pixel_values)"
        },
        {
            "sha": "a22ab5ee0cf578b895618302cc9f07e6681d16b6",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -312,10 +312,7 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, trimaps=dummy_trimap, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, trimaps=dummy_trimap, return_tensors=\"pt\")\n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     def test_slow_fast_equivalence_batched(self):\n         # this only checks on equal resolution, since the slow processor doesn't work otherwise\n@@ -338,10 +335,7 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, trimaps=dummy_trimaps, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, trimaps=dummy_trimaps, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     @slow\n     @require_torch_accelerator"
        },
        {
            "sha": "623414974fbc71120625932df74966168312aab8",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d29482cc9194afbe59f3203a9f5df89ab48a2ec9/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=d29482cc9194afbe59f3203a9f5df89ab48a2ec9",
            "patch": "@@ -162,6 +162,10 @@ def setUp(self):\n \n         self.image_processor_list = image_processor_list\n \n+    def _assert_slow_fast_tensors_equivalence(self, slow_tensor, fast_tensor, atol=1e-1, rtol=1e-3, mean_atol=5e-3):\n+        torch.testing.assert_close(slow_tensor, fast_tensor, atol=atol, rtol=rtol)\n+        self.assertLessEqual(torch.mean(torch.abs(slow_tensor - fast_tensor)).item(), mean_atol)\n+\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):\n@@ -179,10 +183,7 @@ def test_slow_fast_equivalence(self):\n \n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n-        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-3)\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 5e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     @require_vision\n     @require_torch\n@@ -205,10 +206,7 @@ def test_slow_fast_equivalence_batched(self):\n         encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n \n-        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-3)\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 5e-3\n-        )\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n \n     @require_vision\n     @require_torch\n@@ -577,8 +575,10 @@ def test_can_compile_fast_image_processor(self):\n \n         image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n         output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n-\n-        torch.testing.assert_close(output_eager.pixel_values, output_compiled.pixel_values, rtol=1e-4, atol=1e-4)\n+        print(output_eager.pixel_values.dtype, output_compiled.pixel_values.dtype)\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )\n \n \n class AnnotationFormatTestMixin:"
        }
    ],
    "stats": {
        "total": 2452,
        "additions": 2025,
        "deletions": 427
    }
}