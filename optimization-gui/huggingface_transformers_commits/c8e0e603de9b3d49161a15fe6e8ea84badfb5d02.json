{
    "author": "Cyrilvallez",
    "message": "Detect and use device context manager or global device in `from_pretrained` (#37216)\n\n* Update modeling_utils.py\n\n* improve\n\n* Update modeling_utils.py\n\n* Update test_modeling_common.py\n\n* Update test_modeling_timm_backbone.py\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* Update test_modeling_common.py\n\n* CIs",
    "sha": "c8e0e603de9b3d49161a15fe6e8ea84badfb5d02",
    "files": [
        {
            "sha": "7c7209db519f63108326c4a586bc472d11d8bf2b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 32,
            "deletions": 1,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c8e0e603de9b3d49161a15fe6e8ea84badfb5d02",
            "patch": "@@ -287,6 +287,21 @@ def _wrapper(*args, **kwargs):\n     return _wrapper\n \n \n+def get_torch_context_manager_or_global_device():\n+    \"\"\"\n+    Test if a device context manager is currently in use, or if it is not the case, check if the default device\n+    is not \"cpu\". This is used to infer the correct device to load the model on, in case `device_map` is not provided.\n+    \"\"\"\n+    device_in_context = torch.tensor([]).device\n+    default_device = torch.get_default_device()\n+    # This case means no context manager was used -> we still check if the default that was potentially set is not cpu\n+    if device_in_context == default_device:\n+        if default_device != torch.device(\"cpu\"):\n+            return default_device\n+        return None\n+    return device_in_context\n+\n+\n def get_parameter_device(parameter: Union[nn.Module, \"ModuleUtilsMixin\"]):\n     try:\n         return next(parameter.parameters()).device\n@@ -4153,6 +4168,19 @@ def from_pretrained(\n         else:\n             _adapter_model_path = None\n \n+        # Potentially detect context manager or global device, and use it (only if no device_map was provided)\n+        if device_map is None:\n+            device_in_context = get_torch_context_manager_or_global_device()\n+            if device_in_context == torch.device(\"meta\"):\n+                raise ValueError(\n+                    (\n+                        \"`from_pretrained` is not compatible with a meta device context manager or `torch.set_default_device('meta')` \"\n+                        \"as its purpose is to load weights. If you want to initialize a model on the meta device, use the context manager \"\n+                        \"or global device with `from_config`, or `ModelClass(config)`\"\n+                    )\n+                )\n+            device_map = device_in_context\n+\n         # change device_map into a map if we passed an int, a str or a torch.device\n         if isinstance(device_map, torch.device):\n             device_map = {\"\": device_map}\n@@ -4177,7 +4205,10 @@ def from_pretrained(\n                 raise ValueError(\"DeepSpeed Zero-3 is not compatible with passing a `device_map`.\")\n             if not is_accelerate_available():\n                 raise ValueError(\n-                    \"Using a `device_map` or `tp_plan` requires `accelerate`. You can install it with `pip install accelerate`\"\n+                    (\n+                        \"Using a `device_map`, `tp_plan`, `torch.device` context manager or setting `torch.set_default_device(device)` \"\n+                        \"requires `accelerate`. You can install it with `pip install accelerate`\"\n+                    )\n                 )\n \n         # handling bnb config from kwargs, remove after `load_in_{4/8}bit` deprecation."
        },
        {
            "sha": "582bdab0b583597d1b89b998fbcf84cc91690f7e",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=c8e0e603de9b3d49161a15fe6e8ea84badfb5d02",
            "patch": "@@ -168,6 +168,18 @@ def test_save_load_low_cpu_mem_usage_checkpoints(self):\n     def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n         pass\n \n+    @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n+    def test_can_load_with_device_context_manager(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n+    def test_can_load_with_global_device_set(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n+    def test_cannot_load_with_meta_device_context_manager(self):\n+        pass\n+\n     @unittest.skip(reason=\"model weights aren't tied in TimmBackbone.\")\n     def test_tie_model_weights(self):\n         pass"
        },
        {
            "sha": "5323760fe0aba78552d6002f1db085dc1a2ccfc3",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8e0e603de9b3d49161a15fe6e8ea84badfb5d02/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=c8e0e603de9b3d49161a15fe6e8ea84badfb5d02",
            "patch": "@@ -4454,6 +4454,73 @@ def test_generation_tester_mixin_inheritance(self):\n                 ),\n             )\n \n+    @require_torch_accelerator\n+    def test_can_load_with_device_context_manager(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        # Need to specify index 0 here, as `torch_device` is simply the str of the type, e.g. \"cuda\"\n+        device = torch.device(torch_device, index=0)\n+        for model_class in self.all_model_classes:\n+            # Need to deepcopy here as it is modified in-place in save_pretrained (it sets sdpa for default attn, which\n+            # is not supported for e.g. dpt_hybrid)\n+            model = model_class(copy.deepcopy(config))\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                with device:\n+                    new_model = model_class.from_pretrained(tmpdirname)\n+                unique_devices = {param.device for param in new_model.parameters()} | {\n+                    buffer.device for buffer in new_model.buffers()\n+                }\n+\n+            self.assertEqual(\n+                unique_devices, {device}, f\"All parameters should be on {device}, but found {unique_devices}.\"\n+            )\n+\n+    @require_torch_accelerator\n+    def test_can_load_with_global_device_set(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        # Need to specify index 0 here, as `torch_device` is simply the str of the type, e.g. \"cuda\"\n+        device = torch.device(torch_device, index=0)\n+        default_device = torch.get_default_device()\n+        for model_class in self.all_model_classes:\n+            # Need to deepcopy here as it is modified in-place in save_pretrained (it sets sdpa for default attn, which\n+            # is not supported for e.g. dpt_hybrid)\n+            model = model_class(copy.deepcopy(config))\n+\n+            # set a global gpu device\n+            torch.set_default_device(device)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                new_model = model_class.from_pretrained(tmpdirname)\n+                unique_devices = {param.device for param in new_model.parameters()} | {\n+                    buffer.device for buffer in new_model.buffers()\n+                }\n+\n+            # set back the correct device\n+            torch.set_default_device(default_device)\n+\n+            self.assertEqual(\n+                unique_devices, {device}, f\"All parameters should be on {device}, but found {unique_devices}.\"\n+            )\n+\n+    def test_cannot_load_with_meta_device_context_manager(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            # Need to deepcopy here as it is modified in-place in save_pretrained (it sets sdpa for default attn, which\n+            # is not supported for e.g. dpt_hybrid)\n+            model = model_class(copy.deepcopy(config))\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # This should raise an error with meta device\n+                with self.assertRaises(ValueError, msg=\"`from_pretrained` is not compatible with a meta device\"):\n+                    with torch.device(\"meta\"):\n+                        _ = model_class.from_pretrained(tmpdirname)\n+\n \n global_rng = random.Random()\n "
        }
    ],
    "stats": {
        "total": 112,
        "additions": 111,
        "deletions": 1
    }
}