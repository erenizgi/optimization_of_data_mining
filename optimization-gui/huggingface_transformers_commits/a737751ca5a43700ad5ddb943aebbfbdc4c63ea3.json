{
    "author": "LysandreJik",
    "message": "Imports",
    "sha": "a737751ca5a43700ad5ddb943aebbfbdc4c63ea3",
    "files": [
        {
            "sha": "dd5731c523ab1b4ce24bbac3f9504b4b987466c6",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a737751ca5a43700ad5ddb943aebbfbdc4c63ea3/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a737751ca5a43700ad5ddb943aebbfbdc4c63ea3/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=a737751ca5a43700ad5ddb943aebbfbdc4c63ea3",
            "patch": "@@ -253,10 +253,18 @@ Example:\n transformers chat https://router.huggingface.co/v1 HuggingFaceTB/SmolLM3-3B\n ```\n \n+\n Linked PRs: \n - https://github.com/huggingface/transformers/pull/40997\n - https://github.com/huggingface/transformers/pull/41487\n \n+\n+### Removal of the `run` method\n+\n+The `transformers run` (previously `transformers-cli run`) is an artefact of the past, was not documented nor tested,\n+and isn't part of any public documentation. We're removing it for now and ask you to please let us know in case\n+this is a method you are using; in which case we should bring it back with better support.\n+\n ## Environment variables\n \n - Legacy environment variables like `TRANSFORMERS_CACHE`, `PYTORCH_TRANSFORMERS_CACHE`, and `PYTORCH_PRETRAINED_BERT_CACHE` have been removed. Please use `HF_HOME` instead."
        },
        {
            "sha": "7e9927b72dcafebccc066178b630eca588a0e8f0",
            "filename": "src/transformers/cli/run.py",
            "status": "removed",
            "additions": 0,
            "deletions": 101,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Fcli%2Frun.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/afc0c04d802e988d09001a25dc5d956128fe0166/src%2Ftransformers%2Fcli%2Frun.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Frun.py?ref=afc0c04d802e988d09001a25dc5d956128fe0166",
            "patch": "@@ -1,101 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from enum import Enum\n-from typing import Annotated\n-\n-import typer\n-\n-from ..pipelines import PipelineDataFormat, get_supported_tasks, pipeline\n-from ..utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-TaskEnum = Enum(\"TaskEnum\", {task.upper(): task for task in get_supported_tasks()}, type=str)\n-FormatEnum = Enum(\"FormatEnum\", {fmt.upper(): fmt for fmt in PipelineDataFormat.SUPPORTED_FORMATS}, type=str)\n-\n-\n-def run(\n-    task: Annotated[TaskEnum, typer.Argument(help=\"Task to run\", case_sensitive=False)],  # type: ignore\n-    input: Annotated[str | None, typer.Option(help=\"Path to the file to use for inference\")] = None,\n-    output: Annotated[\n-        str | None, typer.Option(help=\"Path to the file that will be used post to write results.\")\n-    ] = None,\n-    model: Annotated[\n-        str | None,\n-        typer.Option(\n-            help=\"Name or path to the model to instantiate. If not provided, will use the default model for that task.\"\n-        ),\n-    ] = None,\n-    config: Annotated[\n-        str | None,\n-        typer.Option(\n-            help=\"Name or path to the model's config to instantiate. If not provided, will use the model's one.\"\n-        ),\n-    ] = None,\n-    tokenizer: Annotated[\n-        str | None, typer.Option(help=\"Name of the tokenizer to use. If not provided, will use the model's one.\")\n-    ] = None,\n-    column: Annotated[\n-        str | None,\n-        typer.Option(help=\"Name of the column to use as input. For multi columns input use 'column1,columns2'\"),\n-    ] = None,\n-    format: Annotated[FormatEnum, typer.Option(help=\"Input format to read from\", case_sensitive=False)] = \"pipe\",  # type: ignore\n-    device: Annotated[\n-        int, typer.Option(help=\"Indicate the device to run onto, -1 indicates CPU, >= 0 indicates GPU.\")\n-    ] = -1,\n-    overwrite: Annotated[bool, typer.Option(help=\"Allow overwriting the output file.\")] = False,\n-):\n-    \"\"\"Run a pipeline on a given input file.\"\"\"\n-    # Initialize pipeline\n-    pipe = pipeline(task=task, model=model, config=config, tokenizer=tokenizer, device=device)\n-\n-    # Initialize reader\n-    reader = PipelineDataFormat.from_str(\n-        format=_try_infer_format_from_ext(input) if format == \"infer\" else format,\n-        output_path=output,\n-        input_path=input,\n-        column=column if column else pipe.default_input_names,\n-        overwrite=overwrite,\n-    )\n-\n-    # Run\n-    outputs = []\n-    for entry in reader:\n-        output = pipe(**entry) if reader.is_multi_columns else pipe(entry)\n-        if isinstance(output, dict):\n-            outputs.append(output)\n-        else:\n-            outputs += output\n-\n-    # Saving data\n-    if pipe.binary_output:\n-        binary_path = reader.save_binary(outputs)\n-        logger.warning(f\"Current pipeline requires output to be in binary format, saving at {binary_path}\")\n-    else:\n-        reader.save(outputs)\n-\n-\n-def _try_infer_format_from_ext(path: str) -> str:\n-    if not path:\n-        return \"pipe\"\n-\n-    for ext in PipelineDataFormat.SUPPORTED_FORMATS:\n-        if path.endswith(ext):\n-            return ext\n-\n-    raise Exception(\n-        f\"Unable to determine file format from file extension {path}. \"\n-        f\"Please provide the format through --format {PipelineDataFormat.SUPPORTED_FORMATS}\"\n-    )"
        }
    ],
    "stats": {
        "total": 109,
        "additions": 8,
        "deletions": 101
    }
}