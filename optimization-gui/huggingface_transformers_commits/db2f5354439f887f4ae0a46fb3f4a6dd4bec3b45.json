{
    "author": "jiqing-feng",
    "message": "update bnb ground truth (#39117)\n\n* update bnb resulte\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* set seed to avoid sampling different results\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix int8 tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix typo\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* add comments\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45",
    "files": [
        {
            "sha": "fd72d13505c313230e7738c69bc808523dd249f2",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45",
            "patch": "@@ -27,6 +27,7 @@\n     AutoTokenizer,\n     BitsAndBytesConfig,\n     pipeline,\n+    set_seed,\n )\n from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n@@ -111,6 +112,8 @@ class Base4bitTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Hello my name is John Doe, I am a student at the University\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John and I am 25 years old.\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a student at the University of\")\n+    # Expected values on Intel XPU and NV A100\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Alina. I have been working as a professional\")\n     MAX_NEW_TOKENS = 10\n \n     def setUp(self):\n@@ -513,6 +516,8 @@ def test_pipeline(self):\n             max_new_tokens=self.MAX_NEW_TOKENS,\n         )\n \n+        # Avoid sampling different outputs\n+        set_seed(42)\n         # Real second forward pass\n         pipeline_output = self.pipe(self.input_text)\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)"
        },
        {
            "sha": "304d97879f293a58ba93e71975737bc0a34ae1fc",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=db2f5354439f887f4ae0a46fb3f4a6dd4bec3b45",
            "patch": "@@ -27,6 +27,7 @@\n     AutoTokenizer,\n     BitsAndBytesConfig,\n     pipeline,\n+    set_seed,\n )\n from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n@@ -113,6 +114,8 @@ class BaseMixedInt8Test(unittest.TestCase):\n     MAX_NEW_TOKENS = 10\n     # Expected values with offload\n     EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a professional photographer based in\")\n+    # Expected values on Intel XPU and NV A100\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Alina. I have been working as a professional\")\n \n     def setUp(self):\n         # Models and tokenizer\n@@ -649,6 +652,8 @@ def test_pipeline(self):\n             max_new_tokens=self.MAX_NEW_TOKENS,\n         )\n \n+        # Avoid sampling different outputs\n+        set_seed(42)\n         # Real second forward pass\n         pipeline_output = self.pipe(self.input_text)\n         self.assertIn(pipeline_output[0][\"generated_text\"], self.EXPECTED_OUTPUTS)"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}