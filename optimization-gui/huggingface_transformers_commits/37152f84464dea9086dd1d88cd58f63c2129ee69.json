{
    "author": "cyyever",
    "message": "Fix typos in English/Chinese documentation (#41031)\n\n* Fix typos and formatting in English docs\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typos and formatting in Chinese docs\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "37152f84464dea9086dd1d88cd58f63c2129ee69",
    "files": [
        {
            "sha": "ef32bf26ee0295994af0d14b20e64ba08df6721e",
            "filename": "docs/source/en/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fen%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fen%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Frun_scripts.md?ref=37152f84464dea9086dd1d88cd58f63c2129ee69",
            "patch": "@@ -104,7 +104,7 @@ torchrun \\\n     ...\n ```\n \n-PyTorch supports TPUs, hardware designed to accelerate performance, through the [PyTorch/XLA](https://github.com/pytorch/xla/blob/master/README.md) package. Launch the `xla_spawn.py` script and use `num _cores` to set the number of TPU cores to train with.\n+PyTorch supports TPUs, hardware designed to accelerate performance, through the [PyTorch/XLA](https://github.com/pytorch/xla/blob/master/README.md) package. Launch the `xla_spawn.py` script and use `num_cores` to set the number of TPU cores to train with.\n \n ```bash\n python xla_spawn.py --num_cores 8 pytorch/summarization/run_summarization.py \\"
        },
        {
            "sha": "2b26d9f9fc7f1603adad6dd41728898835bea7b9",
            "filename": "docs/source/en/video_processors.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fen%2Fvideo_processors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fen%2Fvideo_processors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fvideo_processors.md?ref=37152f84464dea9086dd1d88cd58f63c2129ee69",
            "patch": "@@ -14,17 +14,16 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-\n # Video Processor\n \n-A **Video Processor** is a utility responsible for preparing input features for video models, as well as handling the post-processing of their outputs. It provides transformations such as resizing, normalization, and conversion into PyTorch. \n+A **Video Processor** is a utility responsible for preparing input features for video models, as well as handling the post-processing of their outputs. It provides transformations such as resizing, normalization, and conversion into PyTorch.\n \n The video processor extends the functionality of image processors by allowing the models to handle videos with a distinct set of arguments compared to images. It serves as the bridge between raw video data and the model, ensuring that input features are optimized for the VLM.\n \n Use [`~BaseVideoProcessor.from_pretrained`] to load a video processors configuration (image size, whether to normalize and rescale, etc.) from a video model on the Hugging Face [Hub](https://hf.co) or local directory. The configuration for each pretrained model should be saved in a [video_preprocessor_config.json] file but older models might have the config saved in [preprocessor_config.json](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf/blob/main/preprocessor_config.json) file. Note that the latter is less preferred and will be removed in the future.\n \n+## Usage Example\n \n-### Usage Example\n Here's an example of how to load a video processor with [`llava-hf/llava-onevision-qwen2-0.5b-ov-hf`](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf) model:\n \n ```python"
        },
        {
            "sha": "a8863896235f0d02b352aa9d9bb41f83181df7ac",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=37152f84464dea9086dd1d88cd58f63c2129ee69",
            "patch": "@@ -236,7 +236,7 @@ deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n }\n ```\n \n-这会启用`optimizer offload `和一些其他重要功能。您可以尝试不同的buffer大小，有关详细信息，请参见下面的讨论。\n+这会启用`optimizer offload`和一些其他重要功能。您可以尝试不同的buffer大小，有关详细信息，请参见下面的讨论。\n \n 关于这种启用类型的实际使用示例，请参阅 [此帖](https://github.com/huggingface/transformers/issues/8771#issuecomment-759176685)。\n "
        },
        {
            "sha": "7c497c6f1c652445083bc5db8fa8a683c69b3f6b",
            "filename": "docs/source/zh/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fpipeline_tutorial.md?ref=37152f84464dea9086dd1d88cd58f63c2129ee69",
            "patch": "@@ -306,5 +306,5 @@ pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"loa\n output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n ```\n \n-请注意，您可以将`checkpoint `替换为任何支持大模型加载的Hugging Face模型，比如BLOOM！\n+请注意，您可以将`checkpoint`替换为任何支持大模型加载的Hugging Face模型，比如BLOOM！\n "
        },
        {
            "sha": "228ba55c0d0e9057ffd31ede222c580b2ede4b26",
            "filename": "docs/source/zh/tasks/asr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37152f84464dea9086dd1d88cd58f63c2129ee69/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftasks%2Fasr.md?ref=37152f84464dea9086dd1d88cd58f63c2129ee69",
            "patch": "@@ -83,7 +83,7 @@ DatasetDict({\n })\n ```\n \n-虽然数据集包含 `lang_id `和 `english_transcription` 等许多有用的信息，但在本指南中，\n+虽然数据集包含 `lang_id` 和 `english_transcription` 等许多有用的信息，但在本指南中，\n 您将专注于 `audio` 和 `transcription`。使用 [`~datasets.Dataset.remove_columns`] 方法删除其他列：\n \n ```py\n@@ -167,7 +167,7 @@ Wav2Vec2 分词器仅训练了大写字符，因此您需要确保文本与分\n 它还会动态地将您的文本和标签填充到其批次中最长元素的长度（而不是整个数据集），以使它们具有统一的长度。\n 虽然可以通过在 `tokenizer` 函数中设置 `padding=True` 来填充文本，但动态填充更有效。\n \n-与其他数据整理器不同，这个特定的数据整理器需要对 `input_values` 和 `labels `应用不同的填充方法：\n+与其他数据整理器不同，这个特定的数据整理器需要对 `input_values` 和 `labels` 应用不同的填充方法：\n \n ```py\n >>> import torch"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 7,
        "deletions": 8
    }
}