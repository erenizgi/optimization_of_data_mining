{
    "author": "SunMarc",
    "message": "transformers serve quantization docs + some api fixes for bitsandbytes (#41253)\n\n* doc\n\n* fix api\n\n* fix\n\n* fix\n\n* fix\n\n* fix args\n\n* minor doc fix\n\n* fix\n\n* style\n\n* rm check for now\n\n* fix\n\n* style\n\n* Update docs/source/en/serving.md\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* add log and update value\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "81b4f9882c8a46c8274084503d297874bb372260",
    "files": [
        {
            "sha": "7ae109ecac836eb712de23d3a1b67fe70b0504eb",
            "filename": "docs/source/en/serving.md",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/81b4f9882c8a46c8274084503d297874bb372260/docs%2Fsource%2Fen%2Fserving.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/81b4f9882c8a46c8274084503d297874bb372260/docs%2Fsource%2Fen%2Fserving.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserving.md?ref=81b4f9882c8a46c8274084503d297874bb372260",
            "patch": "@@ -383,6 +383,30 @@ transformers serve \\\n   --attn_implementation \"sdpa\"\n ```\n \n+### Quantization\n+\n+transformers serve is compatible with all [quantization methods](https://huggingface.co/docs/transformers/main/quantization/overview) supported in transformers. Quantization can significantly reduce memory usage and improve inference speed, with two main workflows: pre-quantized models and on-the-fly quantization.\n+\n+#### Pre-quantized Models\n+\n+For models that are already quantized (e.g., GPTQ, AWQ, bitsandbytes), simply choose a quantized model name for serving.\n+Make sure to install the required libraries listed in the quantization documentation.\n+\n+> [!TIP]\n+> Pre-quantized models generally provide the best balance of performance and accuracy.\n+\n+#### On the fly quantization\n+\n+If you want to quantize a model at runtime, you can specify the --quantization flag in the CLI. Note that not all quantization methods support on-the-fly conversion. The full list of supported methods is available in the quantization [overview](https://huggingface.co/docs/transformers/main/quantization/overview). \n+\n+Currently, with transformers serve, we only supports some methods: [\"bnb-4bit\", \"bnb-8bit\"]\n+\n+For example, to enable 4-bit quantization with bitsandbytes, you need to pass add `--quantization bnb-4bit`: \n+\n+```sh\n+transformers serve --quantization bnb-4bit\n+```\n+\n ### Performance tips\n \n - Use an efficient attention backend when available:\n@@ -397,6 +421,4 @@ transformers serve \\\n \n - `--dtype {bfloat16|float16}` typically improve throughput and memory use vs. `float32`\n \n-- `--load_in_4bit`/`--load_in_8bit` can reduce memory footprint for LoRA setups\n-\n - `--force-model <repo_id>` avoids per-request model hints and helps produce stable, repeatable runs"
        },
        {
            "sha": "9f3ba83656b61c6e2fff2a20561c10a6928870c4",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 15,
            "deletions": 29,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/81b4f9882c8a46c8274084503d297874bb372260/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/81b4f9882c8a46c8274084503d297874bb372260/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=81b4f9882c8a46c8274084503d297874bb372260",
            "patch": "@@ -377,14 +377,10 @@ def __init__(\n                 help=\"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n             ),\n         ] = None,\n-        load_in_8bit: Annotated[\n-            bool, typer.Option(help=\"Whether to use 8 bit precision for the base model - works only with LoRA.\")\n-        ] = False,\n-        load_in_4bit: Annotated[\n-            bool, typer.Option(help=\"Whether to use 4 bit precision for the base model - works only with LoRA.\")\n-        ] = False,\n-        bnb_4bit_quant_type: Annotated[str, typer.Option(help=\"Quantization type.\")] = \"nf4\",\n-        use_bnb_nested_quant: Annotated[bool, typer.Option(help=\"Whether to use nested quantization.\")] = False,\n+        quantization: Annotated[\n+            Optional[str],\n+            typer.Option(help=\"Which quantization method to use. choices: 'bnb-4bit', 'bnb-8bit'\"),\n+        ] = None,\n         host: Annotated[str, typer.Option(help=\"Interface the server will listen to.\")] = \"localhost\",\n         port: Annotated[int, typer.Option(help=\"Port the server will listen to.\")] = 8000,\n         model_timeout: Annotated[\n@@ -424,10 +420,7 @@ def __init__(\n         self.dtype = dtype\n         self.trust_remote_code = trust_remote_code\n         self.attn_implementation = attn_implementation\n-        self.load_in_8bit = load_in_8bit\n-        self.load_in_4bit = load_in_4bit\n-        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n-        self.use_bnb_nested_quant = use_bnb_nested_quant\n+        self.quantization = quantization\n         self.host = host\n         self.port = port\n         self.model_timeout = model_timeout\n@@ -1688,22 +1681,20 @@ def get_quantization_config(self) -> Optional[\"BitsAndBytesConfig\"]:\n         Returns:\n             `Optional[BitsAndBytesConfig]`: The quantization config.\n         \"\"\"\n-        if self.load_in_4bit:\n+        if self.quantization == \"bnb-4bit\":\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,\n-                # For consistency with model weights, we use the same value as `dtype`\n-                bnb_4bit_compute_dtype=self.dtype,\n-                bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n-                bnb_4bit_use_double_quant=self.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=self.dtype,\n-            )\n-        elif self.load_in_8bit:\n-            quantization_config = BitsAndBytesConfig(\n-                load_in_8bit=True,\n+                bnb_4bit_quant_type=\"nf4\",\n+                bnb_4bit_use_double_quant=True,\n             )\n+        elif self.quantization == \"bnb-8bit\":\n+            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n         else:\n             quantization_config = None\n \n+        if quantization_config is not None:\n+            logger.info(f\"Quantization applied with the following config: {quantization_config}\")\n+\n         return quantization_config\n \n     def process_model_name(self, model_id: str) -> str:\n@@ -1750,27 +1741,22 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n             revision=revision,\n             trust_remote_code=self.trust_remote_code,\n         )\n-\n         dtype = self.dtype if self.dtype in [\"auto\", None] else getattr(torch, self.dtype)\n         quantization_config = self.get_quantization_config()\n \n         model_kwargs = {\n             \"revision\": revision,\n             \"attn_implementation\": self.attn_implementation,\n             \"dtype\": dtype,\n-            \"device_map\": \"auto\",\n+            \"device_map\": self.device,\n             \"trust_remote_code\": self.trust_remote_code,\n+            \"quantization_config\": quantization_config,\n         }\n-        if quantization_config is not None:\n-            model_kwargs[\"quantization_config\"] = quantization_config\n \n         config = AutoConfig.from_pretrained(model_id, **model_kwargs)\n         architecture = getattr(transformers, config.architectures[0])\n         model = architecture.from_pretrained(model_id, **model_kwargs)\n \n-        if getattr(model, \"hf_device_map\", None) is None:\n-            model = model.to(self.device)\n-\n         has_default_max_length = (\n             model.generation_config.max_new_tokens is None and model.generation_config.max_length == 20\n         )"
        }
    ],
    "stats": {
        "total": 70,
        "additions": 39,
        "deletions": 31
    }
}