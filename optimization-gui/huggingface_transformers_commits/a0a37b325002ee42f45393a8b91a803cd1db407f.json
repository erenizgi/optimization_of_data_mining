{
    "author": "jiqing-feng",
    "message": "Gpt oss optim (#40304)\n\n* enable fast index selecting\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update model\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix gpt-oss tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix check tensor\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "a0a37b325002ee42f45393a8b91a803cd1db407f",
    "files": [
        {
            "sha": "32362fc2af828b813b38e337253d074a8d0fd8ce",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0a37b325002ee42f45393a8b91a803cd1db407f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0a37b325002ee42f45393a8b91a803cd1db407f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=a0a37b325002ee42f45393a8b91a803cd1db407f",
            "patch": "@@ -95,7 +95,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n         batch_size = hidden_states.shape[0]\n         hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n         num_experts = routing_weights.shape[1]\n-        if self.training:\n+        if hidden_states.device.type == \"cpu\" or self.training:\n             next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts)\n@@ -104,8 +104,10 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 # are hit this time around\n                 expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n             for expert_idx in expert_hit[:]:\n+                # expert_idx only have 1 element, so we can use scale for fast indexing\n+                expert_idx = expert_idx[0]\n                 with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n+                    _, token_idx = torch.where(expert_mask[expert_idx])\n                 current_state = hidden_states[token_idx]\n                 gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n                 gate, up = gate_up[..., ::2], gate_up[..., 1::2]"
        },
        {
            "sha": "4e0264678a3dfb8df96f233ef9d763f6d8caf66f",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0a37b325002ee42f45393a8b91a803cd1db407f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0a37b325002ee42f45393a8b91a803cd1db407f/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=a0a37b325002ee42f45393a8b91a803cd1db407f",
            "patch": "@@ -94,7 +94,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n         batch_size = hidden_states.shape[0]\n         hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n         num_experts = routing_weights.shape[1]\n-        if self.training:\n+        if hidden_states.device.type == \"cpu\" or self.training:\n             next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(router_indices, num_classes=num_experts)\n@@ -103,8 +103,10 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 # are hit this time around\n                 expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n             for expert_idx in expert_hit[:]:\n+                # expert_idx only have 1 element, so we can use scale for fast indexing\n+                expert_idx = expert_idx[0]\n                 with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx[0]])\n+                    _, token_idx = torch.where(expert_mask[expert_idx])\n                 current_state = hidden_states[token_idx]\n                 gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n                 gate, up = gate_up[..., ::2], gate_up[..., 1::2]"
        },
        {
            "sha": "82c694360a05b421154906bac0a593a709df162f",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a0a37b325002ee42f45393a8b91a803cd1db407f/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a0a37b325002ee42f45393a8b91a803cd1db407f/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=a0a37b325002ee42f45393a8b91a803cd1db407f",
            "patch": "@@ -124,6 +124,13 @@ def test_eager_padding_matches_padding_free_with_position_ids(self):\n     def test_flex_attention_with_grads(self):\n         pass\n \n+    @unittest.skipIf(torch_device == \"cpu\", \"GptOss does not support flex officially\")\n+    def test_generate_compile_model_forward_fullgraph(self):\n+        return super().test_generate_compile_model_forward_fullgraph()\n+\n+    def test_batching_equivalence(self, **kwargs):\n+        return super().test_batching_equivalence(atol=5e-4, rtol=1e-3)\n+\n \n RESULTS_PATH = Path(__file__).parent.parent.parent / \"fixtures/gpt_oss/integration_tests.json\"\n "
        }
    ],
    "stats": {
        "total": 19,
        "additions": 15,
        "deletions": 4
    }
}