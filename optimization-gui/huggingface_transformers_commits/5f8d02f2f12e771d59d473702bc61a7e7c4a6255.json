{
    "author": "Rocketknight1",
    "message": "[v5] Return a BatchEncoding dict from apply_chat_template by default (#41626)\n\n* Flip the default return type for `apply_chat_template` to match the underlying tokenizer\n\n* Remove test_tokenization_for_chat tests, which no longer do anything useful\n\n* Remove test_tokenization_for_chat tests, which no longer do anything useful\n\n* Fix test_encode_message tests\n\n* Fix test_encode_message tests\n\n* Return dicts for Processor too\n\n* Fix mistral-common tests\n\n* Catch one of the processors too\n\n* revert test bug!\n\n* nit fix\n\n* nit fix",
    "sha": "5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
    "files": [
        {
            "sha": "15a086703cf7fab69fc79fe307d2afe2001effb3",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -206,7 +206,7 @@ def apply_chat_template(\n         tokenizer_kwargs = {**processed_kwargs[\"template_kwargs\"], **text_kwargs}\n         tokenizer_kwargs[\"return_tensors\"] = None  # let's not return tensors here\n         tokenize = tokenizer_kwargs.pop(\"tokenize\", False)\n-        return_dict = tokenizer_kwargs.pop(\"return_dict\", False)\n+        return_dict = tokenizer_kwargs.pop(\"return_dict\", True)\n \n         encoded_instruct_inputs = self.tokenizer.apply_chat_template(\n             conversations,"
        },
        {
            "sha": "c3487aca431b4066f81f03ad6f0f00ec2e5ff158",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -1603,7 +1603,7 @@ def apply_chat_template(\n             conversations = [conversation]\n \n         tokenize = processed_kwargs[\"template_kwargs\"].pop(\"tokenize\", False)\n-        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", False)\n+        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", True)\n         mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n \n         if tokenize:"
        },
        {
            "sha": "713d0cbc6bc12775f413c4f99762876effbcd57d",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -1378,7 +1378,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         **kwargs,\n     ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n         \"\"\""
        },
        {
            "sha": "24228738fcde74d4bb7d704dd40e1ff3abff53ce",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -1588,7 +1588,7 @@ def apply_chat_template(\n         truncation: bool = False,\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_dict: bool = False,\n+        return_dict: bool = True,\n         return_assistant_tokens_mask: bool = False,\n         tokenizer_kwargs: Optional[dict[str, Any]] = None,\n         **kwargs,\n@@ -1661,14 +1661,11 @@ def apply_chat_template(\n             set, will return a dict of tokenizer outputs instead.\n         \"\"\"\n \n-        if return_dict and not tokenize:\n-            raise ValueError(\n-                \"`return_dict=True` is incompatible with `tokenize=False`, because there is no dict \"\n-                \"of tokenizer outputs to return.\"\n-            )\n+        if not tokenize:\n+            return_dict = False  # dicts are only returned by the tokenizer anyway\n \n-        if return_assistant_tokens_mask and not return_dict:\n-            raise ValueError(\"`return_assistant_tokens_mask=True` is incompatible with `return_dict=False`\")\n+        if return_assistant_tokens_mask and not (return_dict and tokenize):\n+            raise ValueError(\"`return_assistant_tokens_mask=True` requires `return_dict=True` and `tokenize=True`\")\n \n         if tokenizer_kwargs is None:\n             tokenizer_kwargs = {}\n@@ -1783,13 +1780,17 @@ def encode_message_with_chat_template(\n             )\n \n         if conversation_history is None or len(conversation_history) == 0:\n-            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+            return self.apply_chat_template(\n+                [message], add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+            )\n \n         conversation = conversation_history + [message]\n-        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+        tokens = self.apply_chat_template(\n+            conversation, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n+        )\n \n         prefix_tokens = self.apply_chat_template(\n-            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+            conversation_history, add_generation_prompt=False, tokenize=True, return_dict=False, **kwargs\n         )\n         # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n         # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`."
        },
        {
            "sha": "da6741940c904f26a1d4d93937cc3c0561ca87dd",
            "filename": "tests/models/blenderbot/test_tokenization_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_tokenization_blenderbot.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -18,7 +18,6 @@\n from functools import cached_property\n \n from transformers import BlenderbotTokenizer, BlenderbotTokenizerFast\n-from transformers.testing_utils import require_jinja\n \n \n class Blenderbot3BTokenizerTests(unittest.TestCase):\n@@ -51,24 +50,3 @@ def test_3B_tokenization_same_as_parlai(self):\n     def test_3B_tokenization_same_as_parlai_rust_tokenizer(self):\n         assert self.rust_tokenizer_3b.add_prefix_space\n         assert self.rust_tokenizer_3b([\" Sam\", \"Sam\"]).input_ids == [[5502, 2], [5502, 2]]\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tok = self.tokenizer_3b\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tok.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 2],\n-            [553, 366, 265, 4792, 3879, 73, 311, 21, 228, 228, 6950, 8, 228, 3490, 287, 2273, 304, 21, 2],\n-            [3490, 287, 2273, 304, 21, 228, 228, 6950, 8, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
        },
        {
            "sha": "4a4840dfd9f30db628be1c73e94c05263b8dc794",
            "filename": "tests/models/bloom/test_tokenization_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_tokenization_bloom.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -18,7 +18,7 @@\n from datasets import load_dataset\n \n from transformers import BloomTokenizerFast\n-from transformers.testing_utils import require_jinja, require_tokenizers\n+from transformers.testing_utils import require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -137,28 +137,6 @@ def test_encodings_from_xnli_dataset(self):\n         predicted_text = [tokenizer.decode(x, clean_up_tokenization_spaces=False) for x in output_tokens]\n         self.assertListEqual(predicted_text, input_text)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2],\n-            [5448, 1306, 267, 66799, 44799, 37143, 17, 2, 59414, 4, 2, 229126, 427, 11890, 1152, 17, 2],\n-            [229126, 427, 11890, 1152, 17, 2, 59414, 4, 2],\n-        ]\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_add_prefix_space_fast(self):\n         tokenizer_w_prefix = self.get_rust_tokenizer(add_prefix_space=True)\n         tokenizer_wo_prefix = self.get_rust_tokenizer(add_prefix_space=False)"
        },
        {
            "sha": "ce56bbeb6a8454284ccde010fdb2b585bc3d4dc0",
            "filename": "tests/models/cohere/test_tokenization_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_tokenization_cohere.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -146,32 +146,6 @@ def test_pretrained_model_lists(self):\n         self.assertGreaterEqual(len(self.tokenizer_class.pretrained_vocab_files_map), 1)\n         self.assertGreaterEqual(len(list(self.tokenizer_class.pretrained_vocab_files_map.values())[0]), 1)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = self.get_rust_tokenizer()\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65, 59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59, 45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8],\n-            [5, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 59, 65,\n-            59, 60, 45, 53, 71, 60, 55, 51, 45, 54, 99, 38, 65, 243, 394, 204, 336, 84, 88, 887, 374, 216, 74, 286, 22, 8,\n-            36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61, 58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 61, 59,\n-            45, 58, 71, 60, 55, 51, 45, 54, 99, 38, 48, 420, 87, 9, 8, 36, 99, 59, 60, 41, 58, 60, 71, 55, 46, 71, 60, 61,\n-            58, 54, 71, 60, 55, 51, 45, 54, 99, 38, 36, 99, 43, 48, 41, 60, 42, 55, 60, 71, 60, 55, 51, 45, 54, 99, 38,\n-            54, 567, 235, 693, 276, 411, 243, 22, 8]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_jinja\n     def test_tokenization_for_tool_use(self):\n         tokenizer = self.get_rust_tokenizer()"
        },
        {
            "sha": "0bae68e4b0e3674e166abeb72fc975f5ac458474",
            "filename": "tests/models/gemma/test_tokenization_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_tokenization_gemma.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -27,7 +27,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tokenizers,\n@@ -428,25 +427,6 @@ def test_some_edge_cases(self):\n         # a dummy prefix space is not added by the sp_model as it was de-activated\n         self.assertEqual(tokens, tokenizer.sp_model.encode(\"▁▁\", out_type=str))\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GemmaTokenizer.from_pretrained(\"hf-internal-testing/dummy-gemma\")\n-\n-        test_chats = [\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        expected_tokens = [[235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108, 235322, 235371, 571, 235298, 2997, 73786, 105776, 108, 7731, 577, 4664, 692, 35606, 235371, 571, 235298, 615, 73786, 108], [235322, 235371, 571, 235298, 2997, 73786, 1645, 108, 4521, 149907, 235371, 571, 235298, 615, 73786, 108]]  # fmt: skip\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     def test_save_fast_load_slow(self):\n         # Ensure that we can save a fast tokenizer and load it as a slow tokenizer\n         slow_tokenizer = self.tokenizer"
        },
        {
            "sha": "be6b90bc463780f9131ae59da38b30877e5f6211",
            "filename": "tests/models/gpt2/test_tokenization_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_tokenization_gpt2.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -19,7 +19,7 @@\n \n from transformers import AutoTokenizer, GPT2Tokenizer, GPT2TokenizerFast\n from transformers.models.gpt2.tokenization_gpt2 import VOCAB_FILES_NAMES\n-from transformers.testing_utils import require_jinja, require_tiktoken, require_tokenizers\n+from transformers.testing_utils import require_tiktoken, require_tokenizers\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -281,28 +281,6 @@ def test_special_tokens_mask_input_pairs_and_bos_token(self):\n                 filtered_sequence = [x for x in filtered_sequence if x is not None]\n                 self.assertEqual(encoded_sequence, filtered_sequence)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(self.tmpdirname)\n-        tokenizer.chat_template = \"{% for message in messages %}{{ message.content }}{{ eos_token }}{% endfor %}\"\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [[20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20],\n-                          [20, 1, 20, 10, 20, 4, 3, 10, 20, 10, 20, 3, 0, 20, 20, 20, 0, 10, 20, 20, 20, 6, 20, 1, 6, 20, 20, 20, 3, 0, 0, 1, 20, 20, 20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20],\n-                          [20, 7, 20, 3, 10, 6, 1, 10, 20, 3, 3, 6, 10, 20, 1, 20, 20, 20, 20, 3, 0, 0, 1, 20, 20]]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n     @require_tiktoken\n     def test_tokenization_tiktoken(self):\n         from tiktoken import encoding_name_for_model"
        },
        {
            "sha": "c77eaecede2abf33f154defced5412497c80ae36",
            "filename": "tests/models/gpt_sw3/test_tokenization_gpt_sw3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_sw3%2Ftest_tokenization_gpt_sw3.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -15,7 +15,7 @@\n import unittest\n \n from transformers import GPTSw3Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_jinja, require_sentencepiece, require_tokenizers, slow\n+from transformers.testing_utils import get_tests_dir, require_sentencepiece, require_tokenizers, slow\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -127,36 +127,3 @@ def test_tokenizer_integration(self):\n             model_name=\"AI-Sweden-Models/gpt-sw3-126m\",\n             sequences=sequences,\n         )\n-\n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = GPTSw3Tokenizer(SAMPLE_VOCAB)\n-        tokenizer.chat_template = (\n-            \"{{ eos_token }}{{ bos_token }}\"\n-            \"{% for message in messages %}\"\n-            \"{% if message['role'] == 'user' %}{{ 'User: ' + message['content']}}\"\n-            \"{% else %}{{ 'Bot: ' + message['content']}}{% endif %}\"\n-            \"{{ message['text'] }}{{ bos_token }}\"\n-            \"{% endfor %}\"\n-            \"Bot:\"\n-        )\n-        # This is in English, but it's just here to make sure the chat control tokens are being added properly\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"assistant\", \"content\": \"Nice to meet you.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 530, 339, 265, 878, 708, 727, 275, 347, 541, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 575, 541, 419],\n-            [2000, 1, 575, 541, 419, 984, 429, 281, 264, 1261, 291, 260, 1, 968, 263, 314, 419, 366, 354, 294, 360, 1, 575, 541, 419]\n-            ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)"
        },
        {
            "sha": "d69965b1b268b8b9f83b94f968b5e5f93358add2",
            "filename": "tests/models/llama/test_tokenization_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 27,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_tokenization_llama.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -32,7 +32,6 @@\n from transformers.testing_utils import (\n     get_tests_dir,\n     nested_simplify,\n-    require_jinja,\n     require_read_token,\n     require_sentencepiece,\n     require_tiktoken,\n@@ -702,32 +701,6 @@ def test_fast_post_processor(self):\n         with self.assertRaises(ValueError):\n             tokenizer = LlamaTokenizerFast(SAMPLE_VOCAB, eos_token=None, add_bos_token=True, add_eos_token=True)\n \n-    @require_jinja\n-    def test_tokenization_for_chat(self):\n-        tokenizer = LlamaTokenizer.from_pretrained(\"huggyllama/llama-7b\", legacy=False)\n-\n-        test_chats = [\n-            [{\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}],\n-            [\n-                {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n-                {\"role\": \"user\", \"content\": \"Hello!\"},\n-                {\"role\": \"assistant\", \"content\": \"Nice to meet you.\"},\n-            ],\n-            [{\"role\": \"user\", \"content\": \"Hello!\"}],\n-        ]\n-        # Matt: The third test case tests the default system message, but if this is ever changed in the\n-        #       class/repo code then that test will fail, and the case will need to be updated.\n-        tokenized_chats = [tokenizer.apply_chat_template(test_chat) for test_chat in test_chats]\n-        # fmt: off\n-        expected_tokens = [\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962],\n-            [1, 29961, 25580, 29962, 3532, 14816, 29903, 6778, 13, 3492, 526, 263, 8444, 13563, 7451, 29889, 13, 29966, 829, 14816, 29903, 6778, 13, 13, 10994, 29991, 518, 29914, 25580, 29962, 20103, 304, 5870, 366, 29889, 29871, 2],\n-            [1, 29961, 25580, 29962, 15043, 29991, 518, 29914, 25580, 29962]\n-        ]\n-        # fmt: on\n-        for tokenized_chat, expected_tokens in zip(tokenized_chats, expected_tokens):\n-            self.assertListEqual(tokenized_chat, expected_tokens)\n-\n \n @require_sentencepiece\n @require_tokenizers"
        },
        {
            "sha": "f33501cdc432b1f2f9d93862527c75bb7647785b",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 38,
            "deletions": 24,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -799,7 +799,9 @@ def test_apply_chat_template_basic(self):\n \n         # Test 2:\n         # without tokenize\n-        self.assertEqual(self.tokenizer.apply_chat_template(conversation, tokenize=True), expected_tokenized.tokens)\n+        self.assertEqual(\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True).input_ids, expected_tokenized.tokens\n+        )\n \n         with self.assertRaises(\n             ValueError, msg=\"Kwargs [unk_args] are not supported by `MistralCommonTokenizer.apply_chat_template`.\"\n@@ -824,7 +826,7 @@ def test_apply_chat_template_continue_final_message(self):\n             expected_tokenized.text,\n         )\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, continue_final_message=True).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -846,7 +848,7 @@ def test_apply_chat_template_with_add_generation_prompt(self):\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversation, tokenize=True, add_generation_prompt=add_generation_prompt\n             )\n-            self.assertEqual(token_outputs, expected_tokenized.tokens)\n+            self.assertEqual(token_outputs.input_ids, expected_tokenized.tokens)\n \n         # Test 2:\n         # with continue_final_message\n@@ -958,18 +960,16 @@ def test_apply_chat_template_with_image(self):\n                 },\n             ]\n \n-            output = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer.apply_chat_template(conversation).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n-        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=True)\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True)\n         self.assertEqual(output_dict[\"input_ids\"], expected_tokenized.tokens)\n         self.assertEqual(len(output_dict[\"pixel_values\"]), len(expected_tokenized.images))\n         for o, e in zip(output_dict[\"pixel_values\"], expected_tokenized.images):\n             self.assertTrue(np.allclose(o, e))\n \n-        output_dict = self.tokenizer.apply_chat_template(\n-            conversation, tokenize=True, return_dict=True, return_tensors=\"pt\"\n-        )\n+        output_dict = self.tokenizer.apply_chat_template(conversation, tokenize=True, return_tensors=\"pt\")\n         self.assertEqual(output_dict[\"input_ids\"].tolist()[0], expected_tokenized.tokens)\n         expected_images_pt_tensor = torch.from_numpy(np.stack(expected_tokenized.images))\n         self.assertTrue(torch.allclose(output_dict[\"pixel_values\"], expected_images_pt_tensor))\n@@ -1013,7 +1013,7 @@ def test_apply_chat_template_with_audio(self):\n                 },\n             ]\n \n-            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True)\n+            output = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True).input_ids\n             self.assertEqual(output, expected_tokenized.tokens)\n \n         output_dict = self.tokenizer_audio.apply_chat_template(conversation, tokenize=True, return_dict=True)\n@@ -1041,14 +1041,14 @@ def test_apply_chat_template_with_truncation(self):\n         # Test 1:\n         # with truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=True, max_length=20).input_ids,\n             expected_tokenized.tokens[:20],\n         )\n \n         # Test 2:\n         # without truncation\n         self.assertEqual(\n-            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20),\n+            self.tokenizer.apply_chat_template(conversation, tokenize=True, truncation=False, max_length=20).input_ids,\n             expected_tokenized.tokens,\n         )\n \n@@ -1130,7 +1130,7 @@ def test_batch_apply_chat_template(self):\n         ]\n \n         text_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=False)\n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True)\n+        token_outputs = self.tokenizer.apply_chat_template(conversations, tools=tools, tokenize=True).input_ids\n \n         self.assertEqual(len(text_outputs), len(token_outputs))\n         self.assertEqual(len(text_outputs), len(expected_tokenized))\n@@ -1202,7 +1202,7 @@ def test_batch_apply_chat_template_images(self):\n             ChatCompletionRequest.from_openai(ref_conversation)\n         )\n \n-        output = self.tokenizer.apply_chat_template(conversations, tokenize=True)\n+        output = self.tokenizer.apply_chat_template(conversations, tokenize=True).input_ids\n         self.assertEqual(output, [expected_tokenized.tokens] * 3)\n \n         output = self.tokenizer.apply_chat_template(conversations, tokenize=True, return_dict=True)\n@@ -1248,7 +1248,9 @@ def test_batch_apply_chat_template_with_continue_final_message(self):\n             for conversation in conversations\n         ]\n \n-        token_outputs = self.tokenizer.apply_chat_template(conversations, tokenize=True, continue_final_message=True)\n+        token_outputs = self.tokenizer.apply_chat_template(\n+            conversations, tokenize=True, continue_final_message=True\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, expected_tokenized):\n             self.assertEqual(output, expected.tokens)\n@@ -1297,7 +1299,7 @@ def test_batch_apply_chat_template_with_add_generation_prompt(self):\n             ]\n             token_outputs = self.tokenizer.apply_chat_template(\n                 conversations, tokenize=True, add_generation_prompt=add_generation_prompt\n-            )\n+            ).input_ids\n             for output, expected in zip(token_outputs, expected_tokenized):\n                 self.assertEqual(output, expected.tokens)\n \n@@ -1331,7 +1333,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # with truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=True, max_length=20\n-        )\n+        ).input_ids\n \n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens[:20])\n@@ -1340,7 +1342,7 @@ def test_batch_apply_chat_template_with_truncation(\n         # without truncation\n         token_outputs = self.tokenizer.apply_chat_template(\n             self.fixture_conversations, tokenize=True, truncation=False, max_length=20\n-        )\n+        ).input_ids\n         self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n         for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n             self.assertEqual(output, expected.tokens)\n@@ -1358,15 +1360,17 @@ def test_batch_apply_chat_template_with_padding(\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             if padding == PaddingStrategy.MAX_LENGTH:\n                 # No padding if no max length is provided\n-                token_outputs = self.tokenizer.apply_chat_template(self.fixture_conversations, padding=padding)\n+                token_outputs = self.tokenizer.apply_chat_template(\n+                    self.fixture_conversations, padding=padding, return_dict=False\n+                )\n                 self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n                 for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n                     self.assertEqual(output, expected.tokens)\n \n             max_length = 20 if padding == PaddingStrategy.MAX_LENGTH else None\n \n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length\n+                self.fixture_conversations, tokenize=True, padding=padding, max_length=max_length, return_dict=False\n             )\n \n             if padding != PaddingStrategy.MAX_LENGTH:\n@@ -1390,7 +1394,7 @@ def test_batch_apply_chat_template_with_padding(\n \n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, padding=padding\n+                self.fixture_conversations, tokenize=True, padding=padding, return_dict=False\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1402,7 +1406,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n         max_length = 20\n         for padding in [True, \"max_length\", PaddingStrategy.LONGEST, PaddingStrategy.MAX_LENGTH]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1411,7 +1420,12 @@ def test_batch_apply_chat_template_with_padding_and_truncation(\n                 )\n         for padding in [False, \"do_not_pad\", PaddingStrategy.DO_NOT_PAD]:\n             token_outputs = self.tokenizer.apply_chat_template(\n-                self.fixture_conversations, tokenize=True, truncation=True, padding=padding, max_length=max_length\n+                self.fixture_conversations,\n+                tokenize=True,\n+                truncation=True,\n+                padding=padding,\n+                max_length=max_length,\n+                return_dict=False,\n             )\n             self.assertEqual(len(token_outputs), len(self.tokenized_fixture_conversations))\n             for output, expected in zip(token_outputs, self.tokenized_fixture_conversations):\n@@ -1421,7 +1435,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 1:\n         # with tokenize\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=True, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertIsInstance(token_outputs, torch.Tensor)\n         self.assertEqual(\n@@ -1432,7 +1446,7 @@ def test_batch_apply_chat_template_return_tensors(self):\n         # Test 2:\n         # without tokenize, should ignore return_tensors\n         token_outputs = self.tokenizer.apply_chat_template(\n-            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True\n+            self.fixture_conversations, tokenize=False, return_tensors=\"pt\", padding=True, return_dict=False\n         )\n         self.assertEqual(token_outputs, [t.text for t in self.tokenized_fixture_conversations])\n "
        },
        {
            "sha": "6fd20a2cf4739535123315c07a109de0c0e5fd06",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8d02f2f12e771d59d473702bc61a7e7c4a6255/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=5f8d02f2f12e771d59d473702bc61a7e7c4a6255",
            "patch": "@@ -323,7 +323,7 @@ def test_encode_message(self):\n         ]\n \n         # First, test the default case, where we encode the whole conversation at once\n-        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True)\n+        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True, return_dict=False)\n \n         # Now, test the message-by-message encoding\n         tokens = []"
        }
    ],
    "stats": {
        "total": 271,
        "additions": 57,
        "deletions": 214
    }
}