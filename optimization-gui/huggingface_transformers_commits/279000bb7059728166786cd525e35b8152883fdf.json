{
    "author": "drisspg",
    "message": "Name change AOPermod -> ModuleFqn (#38456)\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "279000bb7059728166786cd525e35b8152883fdf",
    "files": [
        {
            "sha": "ac9fbc7ca726c58cbb70305708f576075a0e42a5",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/279000bb7059728166786cd525e35b8152883fdf/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/279000bb7059728166786cd525e35b8152883fdf/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=279000bb7059728166786cd525e35b8152883fdf",
            "patch": "@@ -62,7 +62,7 @@ Install torchao from PyPi or the PyTorch index with the following commands.\n # Stable release from Pypi which will default to CUDA 12.6\n pip install --upgrade torchao transformers\n ```\n-</hfoption> \n+</hfoption>\n <hfoption id=\"PyTorch Index\">\n Stable Release from the PyTorch index\n ```bash\n@@ -276,18 +276,18 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ### Per Module Quantization\n #### 1. Skip quantization for certain layers\n-With `AOPerModuleConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n+With `ModuleFqnToConfig` we can specify a default configuration for all layers while skipping quantization for certain layers.\n ```py\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n \n model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n \n-from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig\n+from torchao.quantization import Int4WeightOnlyConfig, ModuleFqnToConfig\n config = Int4WeightOnlyConfig(group_size=128)\n \n # set default to int4 (for linears), and skip quantizing `model.layers.0.self_attn.q_proj`\n-quant_config = AOPerModuleConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n+quant_config = ModuleFqnToConfig({\"_default\": config, \"model.layers.0.self_attn.q_proj\": None})\n quantization_config = TorchAoConfig(quant_type=quant_config)\n quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", torch_dtype=torch.bfloat16, quantization_config=quantization_config)\n # lm_head is not quantized and model.layers.0.self_attn.q_proj is not quantized\n@@ -311,7 +311,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n \n model_id = \"facebook/opt-125m\"\n \n-from torchao.quantization import Int4WeightOnlyConfig, AOPerModuleConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType\n+from torchao.quantization import Int4WeightOnlyConfig, ModuleFqnToConfig, Int8DynamicActivationInt4WeightConfig, IntxWeightOnlyConfig, PerAxis, MappingType\n \n weight_dtype = torch.int8\n granularity = PerAxis(0)\n@@ -322,7 +322,7 @@ embedding_config = IntxWeightOnlyConfig(\n     mapping_type=mapping_type,\n )\n linear_config = Int8DynamicActivationInt4WeightConfig(group_size=128)\n-quant_config = AOPerModuleConfig({\"_default\": linear_config, \"model.decoder.embed_tokens\": embedding_config, \"model.decoder.embed_positions\": None})\n+quant_config = ModuleFqnToConfig({\"_default\": linear_config, \"model.decoder.embed_tokens\": embedding_config, \"model.decoder.embed_positions\": None})\n # set `include_embedding` to True in order to include embedding in quantization\n # when `include_embedding` is True, we'll remove input embedding from `modules_not_to_convert` as well\n quantization_config = TorchAoConfig(quant_type=quant_config, include_embedding=True)\n@@ -427,8 +427,8 @@ quantized_model.save_pretrained(output_dir, safe_serialization=False)\n \n # reload the quantized model\n reloaded_model = AutoModelForCausalLM.from_pretrained(\n-    output_dir, \n-    device_map=\"auto\", \n+    output_dir,\n+    device_map=\"auto\",\n     torch_dtype=torch.bfloat16\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n@@ -463,8 +463,8 @@ quantized_model.save_pretrained(output_dir, safe_serialization=False)\n \n # reload the quantized model\n reloaded_model = AutoModelForCausalLM.from_pretrained(\n-    output_dir, \n-    device_map=\"cpu\", \n+    output_dir,\n+    device_map=\"cpu\",\n     torch_dtype=torch.bfloat16\n )\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")"
        },
        {
            "sha": "22b2a88ee4d25cce963cd60fea7bc42abcf61493",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/279000bb7059728166786cd525e35b8152883fdf/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/279000bb7059728166786cd525e35b8152883fdf/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=279000bb7059728166786cd525e35b8152883fdf",
            "patch": "@@ -261,12 +261,12 @@ def create_quantized_param(\n                 model.tie_weights()\n                 setattr(model.config.get_text_config(decoder=True), \"tie_word_embeddings\", False)\n \n-            # handle AOPerModuleConfig, introduced in torchao 0.11.0+\n-            if self.quantization_config._get_ao_version() > version.Version(\"0.10.0\"):\n-                from torchao.quantization import AOPerModuleConfig\n+            # handle ModuleFqnToConfig, introduced in torchao 0.12.0+\n+            if self.quantization_config._get_ao_version() >= version.Version(\"0.12.0\"):\n+                from torchao.quantization import ModuleFqnToConfig\n \n                 config = self.quantization_config.get_apply_tensor_subclass()\n-                if isinstance(config, AOPerModuleConfig):\n+                if isinstance(config, ModuleFqnToConfig):\n                     module_fqn, _ = param_name.rsplit(\".\", 1)\n                     c = None\n                     if module_fqn in config.module_fqn_to_config:"
        },
        {
            "sha": "c756606a95ee97ac90512e349679a6a2a3cb81f4",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/279000bb7059728166786cd525e35b8152883fdf/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/279000bb7059728166786cd525e35b8152883fdf/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=279000bb7059728166786cd525e35b8152883fdf",
            "patch": "@@ -43,10 +43,10 @@\n         TensorCoreTiledLayout,\n     )\n     from torchao.quantization import (\n-        AOPerModuleConfig,\n         Int8WeightOnlyConfig,\n         IntxWeightOnlyConfig,\n         MappingType,\n+        ModuleFqnToConfig,\n         PerAxis,\n     )\n     from torchao.quantization.autoquant import AQMixin\n@@ -226,7 +226,7 @@ def test_include_input_output_embeddings(self):\n             granularity=granularity,\n             mapping_type=mapping_type,\n         )\n-        config = AOPerModuleConfig(\n+        config = ModuleFqnToConfig(\n             {\"_default\": None, \"model.embed_tokens\": embedding_config, \"lm_head\": embedding_config}\n         )\n         # need set `include_input_output_embeddings` to True\n@@ -253,7 +253,7 @@ def test_include_input_output_embeddings(self):\n     @require_torchao_version_greater_or_equal(\"0.11.0\")\n     def test_per_module_config_skip(self):\n         linear_config = Int8WeightOnlyConfig()\n-        config = AOPerModuleConfig({\"_default\": linear_config, \"model.layers.0.self_attn.q_proj\": None})\n+        config = ModuleFqnToConfig({\"_default\": linear_config, \"model.layers.0.self_attn.q_proj\": None})\n         quant_config = TorchAoConfig(quant_type=config)\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 17,
        "deletions": 17
    }
}