{
    "author": "faaany",
    "message": "[docs] no hard-coding cuda (#36043)\n\nmake device-agnostic",
    "sha": "531d1511f5a550d3bbe9af1ea7e0a6b935d48a2f",
    "files": [
        {
            "sha": "d02c007b115f7223a23485661f70663c1fe971ad",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/531d1511f5a550d3bbe9af1ea7e0a6b935d48a2f/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/531d1511f5a550d3bbe9af1ea7e0a6b935d48a2f/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=531d1511f5a550d3bbe9af1ea7e0a6b935d48a2f",
            "patch": "@@ -57,15 +57,15 @@ More concretely, key-value cache acts as a memory bank for these generative mode\n   >>> from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n \n   >>> model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n-  >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+  >>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n   >>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n   >>> past_key_values = DynamicCache()\n   >>> messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n-  >>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+  >>> inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n \n   >>> generated_ids = inputs.input_ids\n-  >>> cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n+  >>> cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)\n   >>> max_new_tokens = 10\n \n   >>> for _ in range(max_new_tokens):\n@@ -139,7 +139,7 @@ Cache quantization can be detrimental in terms of latency if the context length\n >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16).to(\"cuda:0\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n >>> inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n \n >>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n@@ -168,7 +168,7 @@ Use `cache_implementation=\"offloaded_static\"` for an offloaded static cache (see\n >>> ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n \n >>> tokenizer = AutoTokenizer.from_pretrained(ckpt)\n->>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n+>>> model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n >>> inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n \n >>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n@@ -278,7 +278,7 @@ Note that you can use this cache only for models that support sliding window, e.\n >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\", torch_dtype=torch.float16).to(\"cuda:0\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"teknium/OpenHermes-2.5-Mistral-7B\", torch_dtype=torch.float16, device_map=\"auto\")\n >>> inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n \n >>> # can be used by passing in cache implementation\n@@ -298,7 +298,7 @@ Unlike other cache classes, this one can't be used directly by indicating a `cac\n >>> from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n->>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16).to(\"cuda:0\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", torch_dtype=torch.float16, device_map=\"auto\")\n >>> inputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n \n >>> # get our cache, specify number of sink tokens and window size\n@@ -377,25 +377,27 @@ Sometimes you would want to first fill-in cache object with key/values for certa\n >>> import copy\n >>> import torch\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n+>>> from accelerate.test_utils.testing import get_backend\n \n+>>> DEVICE, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n >>> model_id = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n->>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+>>> model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=DEVICE)\n >>> tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n >>> # Init StaticCache with big enough max-length (1024 tokens for the below example)\n >>> # You can also init a DynamicCache, if that suits you better\n->>> prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n+>>> prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=DEVICE, dtype=torch.bfloat16)\n \n >>> INITIAL_PROMPT = \"You are a helpful assistant. \"\n->>> inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(DEVICE)\n >>> # This is the common prompt cached, we need to run forward without grad to be abel to copy\n >>> with torch.no_grad():\n ...      prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n \n >>> prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n >>> responses = []\n >>> for prompt in prompts:\n-...     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n+...     new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(DEVICE)\n ...     past_key_values = copy.deepcopy(prompt_cache)\n ...     outputs = model.generate(**new_inputs, past_key_values=past_key_values,max_new_tokens=20)\n ...     response = tokenizer.batch_decode(outputs)[0]"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 13,
        "deletions": 11
    }
}