{
    "author": "alex-jw-brooks",
    "message": "Use Gradient Checkpointing Layer in Jamba & Blip Related Models (#38310)\n\n* Use gradient checkpointing class in blip classes\n\n* Use gradient checkpointing class in jamba/bamba",
    "sha": "e64ed0304c53798af2a3c4c5882473a0b3d28e37",
    "files": [
        {
            "sha": "1b8e12d1c3b2d37b4c1bd07227882a23fa36bfb8",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -24,7 +24,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from functools import partial\n from typing import Callable, Optional, Tuple, TypedDict, Union\n \n import torch\n@@ -38,6 +37,7 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -938,7 +938,7 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-class BambaDecoderLayer(nn.Module):\n+class BambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\"):\n         super().__init__()\n \n@@ -1154,30 +1154,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **kwargs),\n-                    hidden_states,\n-                    layer_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=layer_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "9db52ebfbc5d26f086ace00755cd19f4fae50bc4",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 11,
            "deletions": 25,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -19,7 +19,6 @@\n # limitations under the License.\n \"\"\"PyTorch Bamba model.\"\"\"\n \n-from functools import partial\n from typing import Optional, Tuple, TypedDict, Union\n \n import torch\n@@ -928,30 +927,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    partial(decoder_layer.__call__, **kwargs),\n-                    hidden_states,\n-                    layer_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                    position_embeddings,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=layer_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                    position_embeddings=position_embeddings,\n-                    **kwargs,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "356f48eaf94f11067e83034e5f441ba7d8ce3b66",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -25,6 +25,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging, torch_int\n@@ -405,7 +406,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class BlipEncoderLayer(nn.Module):\n+class BlipEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: BlipConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -548,19 +549,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        },
        {
            "sha": "ffbca32eb9d8fb22523fa21898b76e3487b7cbf3",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 11,
            "deletions": 22,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     BaseModelOutputWithPoolingAndCrossAttentions,\n@@ -317,7 +318,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class BlipTextLayer(nn.Module):\n+class BlipTextLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_num):\n         super().__init__()\n         self.config = config\n@@ -421,27 +422,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "ea591bf730de6e7a4d0a5bca0c0383945d8c57a0",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 39,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -26,6 +26,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -373,7 +374,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->Blip2\n-class Blip2EncoderLayer(nn.Module):\n+class Blip2EncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Blip2Config):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -527,19 +528,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -847,7 +841,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class Blip2QFormerLayer(nn.Module):\n+class Blip2QFormerLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -988,31 +982,22 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                    query_length,\n+            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n+                logger.warning(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n+                use_cache = False\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+                query_length,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "8018dbe76a9552c205ac21d9f15028788497c076",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 24,
            "deletions": 39,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -277,7 +278,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipEncoderLayer with Blip->InstructBlip\n-class InstructBlipEncoderLayer(nn.Module):\n+class InstructBlipEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: InstructBlipConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -423,19 +424,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -744,7 +738,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class InstructBlipQFormerLayer(nn.Module):\n+class InstructBlipQFormerLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -885,31 +879,22 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                    query_length,\n+            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n+                logger.warning(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n+                use_cache = False\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+                query_length,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "cc18bbf90b63f0bae83c1f63690f3b5dd813a6a6",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 24,
            "deletions": 39,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -29,6 +29,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -247,7 +248,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class InstructBlipVideoEncoderLayer(nn.Module):\n+class InstructBlipVideoEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: InstructBlipVideoConfig):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -352,19 +353,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -606,7 +600,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class InstructBlipVideoQFormerLayer(nn.Module):\n+class InstructBlipVideoQFormerLayer(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -746,31 +740,22 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n-                if use_cache:\n-                    logger.warning(\n-                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                    )\n-                    use_cache = False\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                    query_length,\n+            if getattr(self.config, \"gradient_checkpointing\", False) and self.training and use_cache:\n+                logger.warning(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n+                use_cache = False\n+\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+                query_length,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:"
        },
        {
            "sha": "d60190161ef6e9ba4588c3a03a8e4730c17cdea0",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 25,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e64ed0304c53798af2a3c4c5882473a0b3d28e37/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=e64ed0304c53798af2a3c4c5882473a0b3d28e37",
            "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -894,7 +895,7 @@ def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tens\n         return final_hidden_states, router_logits\n \n \n-class JambaAttentionDecoderLayer(nn.Module):\n+class JambaAttentionDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JambaConfig, layer_idx: int):\n         super().__init__()\n         num_experts = config.layers_num_experts[layer_idx]\n@@ -976,7 +977,7 @@ def forward(\n         return outputs\n \n \n-class JambaMambaDecoderLayer(nn.Module):\n+class JambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: JambaConfig, layer_idx: int):\n         super().__init__()\n         num_experts = config.layers_num_experts[layer_idx]\n@@ -1186,29 +1187,16 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    layer_mask,\n-                    position_ids,\n-                    past_key_values,\n-                    output_attentions,\n-                    output_router_logits,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    attention_mask=layer_mask,\n-                    position_ids=position_ids,\n-                    past_key_value=past_key_values,\n-                    output_attentions=output_attentions,\n-                    output_router_logits=output_router_logits,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=layer_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                output_router_logits=output_router_logits,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n \n             hidden_states = layer_outputs[0]\n "
        }
    ],
    "stats": {
        "total": 357,
        "additions": 128,
        "deletions": 229
    }
}