{
    "author": "cyyever",
    "message": "Fix typing for None valued variables (#37004)\n\nFix typing for None-able variables",
    "sha": "de77f5b1ec37ea214cda08a1ca33a841ce097d75",
    "files": [
        {
            "sha": "2589856a199aca41a8a70320b7a33576cd938e3a",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,6 +19,7 @@\n import os\n import re\n from contextlib import contextmanager\n+from typing import Optional\n \n from transformers.utils.import_utils import export\n \n@@ -284,7 +285,7 @@ def wrapped_init(self, *args, **kwargs):\n \n @export(backends=(\"torch\",))\n @contextmanager\n-def model_addition_debugger_context(model, debug_path: str = None):\n+def model_addition_debugger_context(model, debug_path: Optional[str] = None):\n     \"\"\"\n     # Model addition debugger - context manager for model adders\n     This context manager is a power user tool intended for model adders."
        },
        {
            "sha": "60a3642f87c6b64771584b361ff6aef3ac06cb87",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 56,
            "deletions": 56,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -42,7 +42,7 @@ class BaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -62,7 +62,7 @@ class BaseModelOutputWithNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -92,8 +92,8 @@ class BaseModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -115,8 +115,8 @@ class BaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -153,7 +153,7 @@ class BaseModelOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -186,7 +186,7 @@ class BaseModelOutputWithCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -233,8 +233,8 @@ class BaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n             input) to speed up sequential decoding.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -280,7 +280,7 @@ class BaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -327,12 +327,12 @@ class MoECausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n-    z_loss: torch.FloatTensor = None\n-    aux_loss: torch.FloatTensor = None\n+    z_loss: Optional[torch.FloatTensor] = None\n+    aux_loss: Optional[torch.FloatTensor] = None\n     router_logits: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -362,7 +362,7 @@ class MoEModelOutput(ModelOutput):\n             loss and the z_loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     router_probs: Optional[Tuple[torch.FloatTensor]] = None\n@@ -403,7 +403,7 @@ class MoeModelOutputWithPast(ModelOutput):\n             loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -452,7 +452,7 @@ class MoeCausalLMOutputWithPast(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     aux_loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -504,7 +504,7 @@ class MoEModelOutputWithPastAndCrossAttentions(ModelOutput):\n             loss and the z_loss for Mixture of Experts models.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -563,7 +563,7 @@ class Seq2SeqModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -633,7 +633,7 @@ class Seq2SeqMoEModelOutput(ModelOutput):\n             modules.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -669,7 +669,7 @@ class CausalLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -704,7 +704,7 @@ class CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -747,7 +747,7 @@ class CausalLMOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -784,7 +784,7 @@ class SequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -814,7 +814,7 @@ class MaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -869,7 +869,7 @@ class Seq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -938,11 +938,11 @@ class Seq2SeqMoEOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    encoder_z_loss: torch.FloatTensor = None\n-    decoder_z_loss: torch.FloatTensor = None\n-    encoder_aux_loss: torch.FloatTensor = None\n-    decoder_aux_loss: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    encoder_z_loss: Optional[torch.FloatTensor] = None\n+    decoder_z_loss: Optional[torch.FloatTensor] = None\n+    encoder_aux_loss: Optional[torch.FloatTensor] = None\n+    decoder_aux_loss: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -979,7 +979,7 @@ class NextSentencePredictorOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1008,7 +1008,7 @@ class SequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1063,7 +1063,7 @@ class Seq2SeqSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1099,7 +1099,7 @@ class MultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1128,7 +1128,7 @@ class TokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1159,8 +1159,8 @@ class QuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1217,8 +1217,8 @@ class Seq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1261,7 +1261,7 @@ class SemanticSegmenterOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1289,7 +1289,7 @@ class ImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1311,7 +1311,7 @@ class ImageClassifierOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -1340,7 +1340,7 @@ class DepthEstimatorOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    predicted_depth: torch.FloatTensor = None\n+    predicted_depth: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1368,7 +1368,7 @@ class ImageSuperResolutionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    reconstruction: torch.FloatTensor = None\n+    reconstruction: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1396,8 +1396,8 @@ class Wav2Vec2BaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    extract_features: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    extract_features: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1428,8 +1428,8 @@ class XVectorOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    embeddings: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    embeddings: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1456,7 +1456,7 @@ class BackboneOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    feature_maps: Tuple[torch.FloatTensor] = None\n+    feature_maps: Optional[Tuple[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -1491,8 +1491,8 @@ class BaseModelOutputWithPoolingAndProjection(ModelOutput):\n             Text embeddings before the projection layer, used to mimic the last hidden state of the teacher encoder.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     projection_state: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1548,7 +1548,7 @@ class Seq2SeqSpectrogramOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    spectrogram: torch.FloatTensor = None\n+    spectrogram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1617,7 +1617,7 @@ class Seq2SeqTSModelOutput(ModelOutput):\n             Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1713,7 +1713,7 @@ class SampleTSPredictionOutput(ModelOutput):\n             Sampled values from the chosen distribution.\n     \"\"\"\n \n-    sequences: torch.FloatTensor = None\n+    sequences: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -1739,7 +1739,7 @@ class MaskedImageModelingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    reconstruction: torch.FloatTensor = None\n+    reconstruction: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "92ddcc88d4c214e3be0dcec5ef7ece963df6ed37",
            "filename": "src/transformers/models/bamba/convert_mamba_ssm_checkpoint.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fconvert_mamba_ssm_checkpoint.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import re\n from os import path\n-from typing import Dict, Union\n+from typing import Dict, Optional, Union\n \n import torch\n from huggingface_hub import split_torch_state_dict_into_shards\n@@ -172,7 +172,7 @@ def convert_mamba_ssm_checkpoint_file_to_huggingface_model_file(\n     mamba_ssm_checkpoint_path: str,\n     precision: str,\n     output_dir: str,\n-    tokenizer_path: str = None,\n+    tokenizer_path: Optional[str] = None,\n     save_model: Union[bool, str] = True,\n ) -> None:\n     # load tokenizer if provided, this will be used to set the"
        },
        {
            "sha": "e50b25b18f4be01a9d815339cf5a7d3d566a6247",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -175,7 +175,7 @@ def save_pretrained(\n \n         super().save_pretrained(save_directory, push_to_hub, **kwargs)\n \n-    def _load_voice_preset(self, voice_preset: str = None, **kwargs):\n+    def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n         voice_preset_paths = self.speaker_embeddings[voice_preset]\n \n         voice_preset_dict = {}"
        },
        {
            "sha": "6cb31b2eae5937b9bfa871a90b323d56dc7a2e15",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -412,7 +412,7 @@ def __call__(\n \n class FlaxBigBirdBlockSparseAttention(nn.Module):\n     config: BigBirdConfig\n-    block_sparse_seed: int = None\n+    block_sparse_seed: Optional[int] = None\n     dtype: jnp.dtype = jnp.float32\n \n     def setup(self):\n@@ -1262,7 +1262,7 @@ def __call__(self, hidden_states, input_tensor, deterministic: bool = True):\n \n class FlaxBigBirdAttention(nn.Module):\n     config: BigBirdConfig\n-    layer_id: int = None\n+    layer_id: Optional[int] = None\n     causal: bool = False\n     dtype: jnp.dtype = jnp.float32\n \n@@ -1362,7 +1362,7 @@ def __call__(self, hidden_states, attention_output, deterministic: bool = True):\n \n class FlaxBigBirdLayer(nn.Module):\n     config: BigBirdConfig\n-    layer_id: int = None\n+    layer_id: Optional[int] = None\n     dtype: jnp.dtype = jnp.float32  # the dtype of the computation\n \n     def setup(self):"
        },
        {
            "sha": "abc0954c6aac57d53bd73c41255c52ba71d426dc",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -180,7 +180,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "e4b8f87b550ab04a958951f68520640400015801",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -176,7 +176,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "476feaef0dc68118e4498ffed06def2cd616d72d",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -169,7 +169,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "c4a4428f7bc60f63373ecda1c4301fbc76c0cba2",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -92,7 +92,7 @@ def __init__(\n         return_attention_mask=False,  # pad inputs to max length with silence token (zero) and no attention mask\n         frequency_min: float = 0,\n         frequency_max: float = 14_000,\n-        top_db: int = None,\n+        top_db: Optional[int] = None,\n         truncation: str = \"fusion\",\n         padding: str = \"repeatpad\",\n         **kwargs,\n@@ -258,7 +258,7 @@ def _get_input_mel(self, waveform: np.array, max_length, truncation, padding) ->\n     def __call__(\n         self,\n         raw_speech: Union[np.ndarray, List[float], List[np.ndarray], List[List[float]]],\n-        truncation: str = None,\n+        truncation: Optional[str] = None,\n         padding: Optional[str] = None,\n         max_length: Optional[int] = None,\n         sampling_rate: Optional[int] = None,"
        },
        {
            "sha": "4a42c8f9acf9db7f1755746591d43afdeaa4ed25",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -204,7 +204,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "6aebd11dbe9e9028f50d916f6394b5a93e7e5dfa",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -1360,7 +1360,7 @@ def __init__(self, config: CLIPSegConfig):\n \n     def get_conditional_embeddings(\n         self,\n-        batch_size: int = None,\n+        batch_size: Optional[int] = None,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "48e213693ba273aee75a7eeebc080e11236887a2",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -287,7 +287,7 @@ def __init__(self, config: DacConfig):\n         self.quantizers = nn.ModuleList([DacVectorQuantize(config) for i in range(config.n_codebooks)])\n         self.quantizer_dropout = quantizer_dropout\n \n-    def forward(self, hidden_state, n_quantizers: int = None):\n+    def forward(self, hidden_state, n_quantizers: Optional[int] = None):\n         \"\"\"\n         Quantizes the input tensor using a fixed set of codebooks and returns corresponding codebook vectors.\n         Args:\n@@ -608,7 +608,7 @@ def __init__(self, config: DacConfig):\n     def encode(\n         self,\n         input_values: torch.Tensor,\n-        n_quantizers: int = None,\n+        n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n     ):\n         \"\"\"\n@@ -681,7 +681,7 @@ def decode(\n     def forward(\n         self,\n         input_values: torch.Tensor,\n-        n_quantizers: int = None,\n+        n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n     ):\n         \"\"\""
        },
        {
            "sha": "e87c855be59e2ed936ac2a8ae223baf4b99e7d38",
            "filename": "src/transformers/models/deberta_v2/tokenization_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Ftokenization_deberta_v2.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -462,7 +462,7 @@ def _run_split_on_punc(self, text):\n \n         return [\"\".join(x) for x in output]\n \n-    def save_pretrained(self, path: str, filename_prefix: str = None):\n+    def save_pretrained(self, path: str, filename_prefix: Optional[str] = None):\n         filename = VOCAB_FILES_NAMES[list(VOCAB_FILES_NAMES.keys())[0]]\n         if filename_prefix is not None:\n             filename = filename_prefix + \"-\" + filename"
        },
        {
            "sha": "f99ac6c324ba55400e1343907ec7dacec4cb91f9",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -182,7 +182,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,"
        },
        {
            "sha": "e82eaa75b98ab1b6b1f90a64a33119e8fc390879",
            "filename": "src/transformers/models/deprecated/graphormer/configuration_graphormer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"Graphormer model configuration\"\"\"\n \n+from typing import Optional\n+\n from ....configuration_utils import PretrainedConfig\n from ....utils import logging\n \n@@ -159,8 +161,8 @@ def __init__(\n         traceable: bool = False,\n         q_noise: float = 0.0,\n         qn_block_size: int = 8,\n-        kdim: int = None,\n-        vdim: int = None,\n+        kdim: Optional[int] = None,\n+        vdim: Optional[int] = None,\n         bias: bool = True,\n         self_attention: bool = True,\n         pad_token_id=0,"
        },
        {
            "sha": "ac4b6d7a13cc2b14320c918bf8d3b4cedcba8b90",
            "filename": "src/transformers/models/deprecated/transfo_xl/tokenization_transfo_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Ftokenization_transfo_xl.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -162,7 +162,7 @@ def __init__(\n         lower_case=False,\n         delimiter=None,\n         vocab_file=None,\n-        pretrained_vocab_file: str = None,\n+        pretrained_vocab_file: Optional[str] = None,\n         never_split=None,\n         unk_token=\"<unk>\",\n         eos_token=\"<eos>\","
        },
        {
            "sha": "bde5830a517f57e2b2269c2afd4b82b317cda158",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -280,7 +280,7 @@ def preprocess(\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n         patch_size: List[int] = None,\n-        num_frames: int = None,\n+        num_frames: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n         crop_size: Dict[str, int] = None,"
        },
        {
            "sha": "466b14f6badf9ce235e2abb61d5214f3ab5bcc2e",
            "filename": "src/transformers/models/deprecated/van/convert_van_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -22,7 +22,7 @@\n from dataclasses import dataclass, field\n from functools import partial\n from pathlib import Path\n-from typing import List\n+from typing import List, Optional\n \n import torch\n import torch.nn as nn\n@@ -163,7 +163,7 @@ def convert_weight_and_push(\n         print(f\"Pushed {checkpoint_name}\")\n \n \n-def convert_weights_and_push(save_directory: Path, model_name: str = None, push_to_hub: bool = True):\n+def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = None, push_to_hub: bool = True):\n     filename = \"imagenet-1k-id2label.json\"\n     num_labels = 1000\n "
        },
        {
            "sha": "2dbb1d0202f0da1f4394e33ce2de8fa082c92e3c",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -196,7 +196,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "72d77edf9a3ada1a44f2f21504168072e57adb7d",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -161,7 +161,7 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = False,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n@@ -299,14 +299,14 @@ def _preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: bool = None,\n-        ensure_multiple_of: int = None,\n+        ensure_multiple_of: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n         if do_reduce_labels:\n@@ -340,14 +340,14 @@ def _preprocess_image(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: bool = None,\n-        ensure_multiple_of: int = None,\n+        ensure_multiple_of: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -391,7 +391,7 @@ def _preprocess_segmentation_map(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         keep_aspect_ratio: bool = None,\n-        ensure_multiple_of: int = None,\n+        ensure_multiple_of: Optional[int] = None,\n         do_reduce_labels: bool = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -437,17 +437,17 @@ def preprocess(\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n         do_resize: bool = None,\n-        size: int = None,\n+        size: Optional[int] = None,\n         keep_aspect_ratio: bool = None,\n-        ensure_multiple_of: int = None,\n+        ensure_multiple_of: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,"
        },
        {
            "sha": "2cba3bd8a23750313edd68c1b4823d805c78fde6",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -398,8 +398,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:"
        },
        {
            "sha": "a5abafc361b6b4e71e365d7b7f3caa3368f82d92",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -311,8 +311,8 @@ def tf_to_pt_weight_rename(self, tf_weight):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:"
        },
        {
            "sha": "ac56bc8d783a5895496520fc62c7bada7fa5f61a",
            "filename": "src/transformers/models/esm/configuration_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -172,7 +172,7 @@ def to_dict(self):\n \n @dataclass\n class EsmFoldConfig:\n-    esm_type: str = None\n+    esm_type: Optional[str] = None\n     fp16_esm: bool = True\n     use_esm_attn_map: bool = False\n     esm_ablate_pairwise: bool = False"
        },
        {
            "sha": "960c8189aec03ee506e390b835362cb5a8e98429",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -249,7 +249,7 @@ def __init__(\n         codebook_size: bool = None,\n         codebook_resample: int = PILImageResampling.LANCZOS,\n         codebook_do_center_crop: bool = True,\n-        codebook_crop_size: int = None,\n+        codebook_crop_size: Optional[int] = None,\n         codebook_do_rescale: bool = True,\n         codebook_rescale_factor: Union[int, float] = 1 / 255,\n         codebook_do_map_pixels: bool = True,"
        },
        {
            "sha": "e8d6e872432fd23bc13e2228f203f4e35e873b78",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -104,8 +104,8 @@ def __init__(\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_rgb: bool = None,\n         do_pan_and_scan: bool = None,\n-        pan_and_scan_min_crop_size: int = None,\n-        pan_and_scan_max_num_crops: int = None,\n+        pan_and_scan_min_crop_size: Optional[int] = None,\n+        pan_and_scan_max_num_crops: Optional[int] = None,\n         pan_and_scan_min_ratio_to_activate: float = None,\n         **kwargs,\n     ) -> None:\n@@ -253,8 +253,8 @@ def preprocess(\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         do_convert_rgb: bool = None,\n         do_pan_and_scan: bool = None,\n-        pan_and_scan_min_crop_size: int = None,\n-        pan_and_scan_max_num_crops: int = None,\n+        pan_and_scan_min_crop_size: Optional[int] = None,\n+        pan_and_scan_max_num_crops: Optional[int] = None,\n         pan_and_scan_min_ratio_to_activate: float = None,\n     ) -> PIL.Image.Image:\n         \"\"\""
        },
        {
            "sha": "4255a1ebb3dd24f85d0d1d290ea075995c6e2d5f",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -509,7 +509,7 @@ def __init__(\n         is_cross_attention: bool = False,\n         config: PretrainedConfig = None,\n         qk_layer_norms: bool = False,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.hidden_size = hidden_size\n@@ -675,7 +675,7 @@ def forward(\n \n # this was adapted from LlamaDecoderLayer\n class IdeficsDecoderLayer(nn.Module):\n-    def __init__(self, config: IdeficsConfig, layer_idx: int = None):\n+    def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = IdeficsAttention(\n@@ -754,7 +754,7 @@ def forward(\n \n \n class IdeficsGatedCrossAttentionLayer(nn.Module):\n-    def __init__(self, config: IdeficsConfig, layer_idx: int = None):\n+    def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.cross_attn = IdeficsAttention("
        },
        {
            "sha": "c69945e2825569f7865772921b3296ba2ac8b75b",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -89,7 +89,9 @@ class Idefics2Processor(ProcessorMixin):\n     image_processor_class = \"Idefics2ImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: str = None, **kwargs):\n+    def __init__(\n+        self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n+    ):\n         if image_processor is None:\n             raise ValueError(\"You need to specify an `image_processor`.\")\n         if tokenizer is None:"
        },
        {
            "sha": "0f1cf7b248622843804bd92f9e2bbf208640cf9d",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -133,7 +133,9 @@ class Idefics3Processor(ProcessorMixin):\n     image_processor_class = \"Idefics3ImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: str = None, **kwargs):\n+    def __init__(\n+        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n+    ):\n         if image_processor is None:\n             raise ValueError(\"You need to specify an `image_processor`.\")\n         if tokenizer is None:"
        },
        {
            "sha": "0d5731bf7befec4f1e984b0f538f55552992ba5b",
            "filename": "src/transformers/models/levit/convert_levit_timm_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fconvert_levit_timm_to_pytorch.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,6 +19,7 @@\n from collections import OrderedDict\n from functools import partial\n from pathlib import Path\n+from typing import Optional\n \n import timm\n import torch\n@@ -79,7 +80,7 @@ def convert_weight_and_push(\n         print(f\"Pushed {checkpoint_name}\")\n \n \n-def convert_weights_and_push(save_directory: Path, model_name: str = None, push_to_hub: bool = True):\n+def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = None, push_to_hub: bool = True):\n     filename = \"imagenet-1k-id2label.json\"\n     num_labels = 1000\n     expected_shape = (1, num_labels)"
        },
        {
            "sha": "ff8b36c4528fe05f80406556d92aa0782a528159",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -333,7 +333,7 @@ def _preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n@@ -563,7 +563,7 @@ def preprocess(\n         image_grid_pinpoints: List = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "9aa09e9673e0c876c719a3ecc797ff2a7ab7248e",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -183,7 +183,7 @@ def _preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n@@ -283,7 +283,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "b8ab958e61171f21e01f1e8cace6b61105ce9773",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -577,7 +577,7 @@ def _preprocess(\n         image: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n@@ -601,7 +601,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,"
        },
        {
            "sha": "6d35d69f5faf1847403cdc1b0d2a4277d1ba743c",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -1592,7 +1592,7 @@ def with_pos_embed(self, tensor, pos: Optional[Tensor]):\n     def forward_post(\n         self,\n         hidden_states: torch.Tensor,\n-        level_index: int = None,\n+        level_index: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n         query_position_embeddings: Optional[torch.Tensor] = None,\n@@ -1651,7 +1651,7 @@ def forward_post(\n     def forward_pre(\n         self,\n         hidden_states: torch.Tensor,\n-        level_index: int = None,\n+        level_index: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n         query_position_embeddings: Optional[torch.Tensor] = None,\n@@ -1712,7 +1712,7 @@ def forward_pre(\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        level_index: int = None,\n+        level_index: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[torch.Tensor] = None,\n         query_position_embeddings: Optional[torch.Tensor] = None,\n@@ -2013,7 +2013,9 @@ def __init__(self, hidden_size: int, num_heads: int, mask_feature_size: torch.Te\n \n         self.mask_embedder = Mask2FormerMLPPredictionHead(self.hidden_size, self.hidden_size, mask_feature_size)\n \n-    def forward(self, outputs: torch.Tensor, pixel_embeddings: torch.Tensor, attention_mask_target_size: int = None):\n+    def forward(\n+        self, outputs: torch.Tensor, pixel_embeddings: torch.Tensor, attention_mask_target_size: Optional[int] = None\n+    ):\n         mask_embeddings = self.mask_embedder(outputs.transpose(0, 1))\n \n         is_tracing = torch.jit.is_tracing() or isinstance(outputs, torch.fx.Proxy) or is_torchdynamo_compiling()"
        },
        {
            "sha": "532bbaffddf7e97943354f7eb599b5274305461c",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -578,7 +578,7 @@ def _preprocess(\n         image: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n@@ -602,7 +602,7 @@ def _preprocess_image(\n         image: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,"
        },
        {
            "sha": "b77203ca0ed0a808a40c9d3468db60f2267ba0b4",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -1316,7 +1316,7 @@ def decode(self, embed_ind):\n class MimiResidualVectorQuantizer(nn.Module):\n     \"\"\"Residual Vector Quantizer.\"\"\"\n \n-    def __init__(self, config: MimiConfig, num_quantizers: int = None):\n+    def __init__(self, config: MimiConfig, num_quantizers: Optional[int] = None):\n         super().__init__()\n         self.codebook_size = config.codebook_size\n         self.frame_rate = config.frame_rate"
        },
        {
            "sha": "e2ccedb0c6eac0be2457b5cf534142eaa2c52cce",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -437,7 +437,7 @@ def forward(\n \n \n class MoonshineDecoderLayer(nn.Module):\n-    def __init__(self, config: MoonshineConfig, layer_idx: int = None):\n+    def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n "
        },
        {
            "sha": "db071b526e4214d806cc2e6d78783a8c5d51701a",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -427,7 +427,7 @@ def __init__(self, config: MoonshineConfig, layer_idx: int):\n \n \n class MoonshineDecoderLayer(nn.Module):\n-    def __init__(self, config: MoonshineConfig, layer_idx: int = None):\n+    def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n "
        },
        {
            "sha": "193f43054cc2bf578ac0fd91531cccbb3e563ed1",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -420,7 +420,7 @@ def __init__(self, config, use_flexible_linear=False):\n             self.fc1 = MoshiFlexibleLinear(hidden_size, ffn_dim, num_layers)\n             self.fc2 = MoshiFlexibleLinear(ffn_dim // 2, hidden_size, num_layers)\n \n-    def forward(self, hidden_states: torch.Tensor, layer_idx: int = None) -> torch.Tensor:\n+    def forward(self, hidden_states: torch.Tensor, layer_idx: Optional[int] = None) -> torch.Tensor:\n         hidden_states = self.fc1(hidden_states) if layer_idx is None else self.fc1(hidden_states, layer_idx)\n \n         batch_size, sequence_length, _ = hidden_states.shape\n@@ -2644,7 +2644,7 @@ def apply_delay_pattern_mask(input_ids, decoder_pad_token_mask):\n         return input_ids\n \n     def build_delay_pattern_mask(\n-        self, input_ids: torch.LongTensor, bos_token_id: int, pad_token_id: int, max_length: int = None\n+        self, input_ids: torch.LongTensor, bos_token_id: int, pad_token_id: int, max_length: Optional[int] = None\n     ):\n         \"\"\"Build a delayed pattern mask to the input_ids. Each codebook, except the first one, is offset by\n         one, giving a delayed pattern mask at the start of sequence and end of sequence. Take the example where there"
        },
        {
            "sha": "e424845245f4ece3c7437f8228eedab61d4abfe8",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -1377,7 +1377,9 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n-    def build_delay_pattern_mask(self, input_ids: torch.LongTensor, pad_token_id: int, max_length: int = None):\n+    def build_delay_pattern_mask(\n+        self, input_ids: torch.LongTensor, pad_token_id: int, max_length: Optional[int] = None\n+    ):\n         \"\"\"Build a delayed pattern mask to the input_ids. Each codebook is offset by the previous codebook by\n         one, giving a delayed pattern mask at the start of sequence and end of sequence. Take the example where there\n         are 4 codebooks and a max sequence length of 8, we have the delayed pattern mask of shape `(codebooks,\n@@ -1828,9 +1830,9 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     @classmethod\n     def from_sub_models_pretrained(\n         cls,\n-        text_encoder_pretrained_model_name_or_path: str = None,\n-        audio_encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        text_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        audio_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:\n@@ -2232,8 +2234,8 @@ def _prepare_decoder_input_ids_for_generation(\n         batch_size: int,\n         model_input_name: str,\n         model_kwargs: Dict[str, torch.Tensor],\n-        decoder_start_token_id: int = None,\n-        bos_token_id: int = None,\n+        decoder_start_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n         device: torch.device = None,\n     ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n@@ -2454,7 +2456,7 @@ def _maybe_initialize_input_ids_for_generation(\n         return torch.ones((batch_size, 1), dtype=torch.long, device=self.device) * bos_token_id\n \n     def _get_decoder_start_token_id(\n-        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: int = None\n+        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: Optional[int] = None\n     ) -> int:\n         decoder_start_token_id = (\n             decoder_start_token_id"
        },
        {
            "sha": "ec6074f48a9601550a262191368ea8bcbf5f08d8",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -1297,7 +1297,9 @@ def prepare_inputs_for_generation(\n             \"use_cache\": use_cache,\n         }\n \n-    def build_delay_pattern_mask(self, input_ids: torch.LongTensor, pad_token_id: int, max_length: int = None):\n+    def build_delay_pattern_mask(\n+        self, input_ids: torch.LongTensor, pad_token_id: int, max_length: Optional[int] = None\n+    ):\n         \"\"\"Build a delayed pattern mask to the input_ids. Each codebook is offset by the previous codebook by\n         one, giving a delayed pattern mask at the start of sequence and end of sequence. Take the example where there\n         are 4 codebooks and a max sequence length of 8, we have the delayed pattern mask of shape `(codebooks,\n@@ -1706,9 +1708,9 @@ def set_output_embeddings(self, new_embeddings):\n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration.from_sub_models_pretrained with Musicgen->MusicgenMelody, musicgen-small->musicgen-melody\n     def from_sub_models_pretrained(\n         cls,\n-        text_encoder_pretrained_model_name_or_path: str = None,\n-        audio_encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        text_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        audio_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:\n@@ -2112,8 +2114,8 @@ def _prepare_decoder_input_ids_for_generation(\n         batch_size: int,\n         model_input_name: str,\n         model_kwargs: Dict[str, torch.Tensor],\n-        decoder_start_token_id: int = None,\n-        bos_token_id: int = None,\n+        decoder_start_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n         device: torch.device = None,\n     ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n@@ -2304,7 +2306,7 @@ def freeze_text_encoder(self):\n \n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForConditionalGeneration._get_decoder_start_token_id\n     def _get_decoder_start_token_id(\n-        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: int = None\n+        self, decoder_start_token_id: Union[int, List[int]] = None, bos_token_id: Optional[int] = None\n     ) -> int:\n         decoder_start_token_id = (\n             decoder_start_token_id"
        },
        {
            "sha": "e5dc6ed1645369dd15c9c2324a82fca576e82501",
            "filename": "src/transformers/models/nougat/tokenization_nougat_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Ftokenization_nougat_fast.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,7 +19,7 @@\n import re\n from functools import partial\n from multiprocessing import Pool\n-from typing import List, Union\n+from typing import List, Optional, Union\n \n import numpy as np\n \n@@ -584,7 +584,7 @@ def post_process_generation(\n         self,\n         generation: Union[str, List[str]],\n         fix_markdown: bool = True,\n-        num_workers: int = None,\n+        num_workers: Optional[int] = None,\n     ) -> Union[str, List[str]]:\n         \"\"\"\n         Postprocess a generated text or a list of generated texts."
        },
        {
            "sha": "cbce106f3316feb4399287fdb2e9d258d5a1dc3a",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -440,7 +440,7 @@ def __init__(\n         ignore_index: Optional[int] = None,\n         do_reduce_labels: bool = False,\n         repo_path: Optional[str] = \"shi-labs/oneformer_demo\",\n-        class_info_file: str = None,\n+        class_info_file: Optional[str] = None,\n         num_text: Optional[int] = None,\n         num_labels: Optional[int] = None,\n         **kwargs,"
        },
        {
            "sha": "5ab6efd7dd39d74c3ef1c2879bcbe813a4f586e7",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -105,7 +105,7 @@ class OPTAttention(nn.Module):\n     def __init__(\n         self,\n         config: OPTConfig,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         **kwargs,\n     ):\n         super().__init__()\n@@ -369,7 +369,7 @@ def forward(\n \n \n class OPTDecoderLayer(nn.Module):\n-    def __init__(self, config: OPTConfig, layer_idx: int = None):\n+    def __init__(self, config: OPTConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n "
        },
        {
            "sha": "61061ec1f5c97cac2ea236c725c7ee43e063ca30",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -215,7 +215,7 @@ def preprocess(\n         images: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        crop_pct: int = None,\n+        crop_pct: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n         crop_size: Dict[str, int] = None,"
        },
        {
            "sha": "898475835be833cd842ffc4c4bd7a5d42b1e0edd",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -152,7 +152,7 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = False,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         prompt_scale_to_meter: float = 0.001,  # default unit is mm\n         **kwargs,\n     ):"
        },
        {
            "sha": "81136cb00a7fd5438f943d72f05abb84e0246c74",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -132,8 +132,8 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_convert_rgb: bool = True,\n-        min_pixels: int = None,\n-        max_pixels: int = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n         patch_size: int = 14,\n         temporal_patch_size: int = 2,\n         merge_size: int = 2,\n@@ -177,9 +177,9 @@ def _preprocess(\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        patch_size: int = None,\n-        temporal_patch_size: int = None,\n-        merge_size: int = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n         do_convert_rgb: bool = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -304,17 +304,17 @@ def preprocess(\n         videos: VideoInput = None,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        min_pixels: int = None,\n-        max_pixels: int = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        patch_size: int = None,\n-        temporal_patch_size: int = None,\n-        merge_size: int = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n         do_convert_rgb: bool = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,"
        },
        {
            "sha": "21084b1dd33db07f36b7e02cb0687feba60aed03",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -263,11 +263,11 @@ def preprocess(\n         do_normalize: bool = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        min_pixels: int = None,\n-        max_pixels: int = None,\n-        patch_size: int = None,\n-        temporal_patch_size: int = None,\n-        merge_size: int = None,\n+        min_pixels: Optional[int] = None,\n+        max_pixels: Optional[int] = None,\n+        patch_size: Optional[int] = None,\n+        temporal_patch_size: Optional[int] = None,\n+        merge_size: Optional[int] = None,\n         do_convert_rgb: bool = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,"
        },
        {
            "sha": "9d6664e1eb7f3a9105b2c1946f42d36886e36811",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -245,8 +245,8 @@ def from_pretrained(cls, *args, **kwargs):\n     @classmethod\n     def from_pretrained_question_encoder_generator(\n         cls,\n-        question_encoder_pretrained_model_name_or_path: str = None,\n-        generator_pretrained_model_name_or_path: str = None,\n+        question_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        generator_pretrained_model_name_or_path: Optional[str] = None,\n         retriever: RagRetriever = None,\n         **kwargs,\n     ) -> PreTrainedModel:"
        },
        {
            "sha": "5c27ad4aafe6ed5f13a88e3ce14f0375cad2ab5a",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -232,8 +232,8 @@ class TFRagPreTrainedModel(TFPreTrainedModel):\n     @classmethod\n     def from_pretrained_question_encoder_generator(\n         cls,\n-        question_encoder_pretrained_model_name_or_path: str = None,\n-        generator_pretrained_model_name_or_path: str = None,\n+        question_encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        generator_pretrained_model_name_or_path: Optional[str] = None,\n         retriever: RagRetriever = None,\n         *model_args,\n         **kwargs,"
        },
        {
            "sha": "428b5f66446c022fb37d34eecd4c360b720c0cc7",
            "filename": "src/transformers/models/rag/tokenization_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -81,7 +81,7 @@ def prepare_seq2seq_batch(\n         max_length: Optional[int] = None,\n         max_target_length: Optional[int] = None,\n         padding: str = \"longest\",\n-        return_tensors: str = None,\n+        return_tensors: Optional[str] = None,\n         truncation: bool = True,\n         **kwargs,\n     ) -> BatchEncoding:"
        },
        {
            "sha": "95bd5b854282e37f36677fc94c2fc27f89efa7bd",
            "filename": "src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -25,7 +25,7 @@\n from functools import partial\n from pathlib import Path\n from pprint import pprint\n-from typing import Dict, List, Tuple\n+from typing import Dict, List, Optional, Tuple\n \n import torch\n import torch.nn as nn\n@@ -159,7 +159,7 @@ def to_params_dict(dict_with_modules):\n     return from_to_ours_keys\n \n \n-def convert_weights_and_push(save_directory: Path, model_name: str = None, push_to_hub: bool = True):\n+def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = None, push_to_hub: bool = True):\n     filename = \"imagenet-1k-id2label.json\"\n     num_labels = 1000\n "
        },
        {
            "sha": "9544400416bd97e5011cc6c7640069ce10b7e27f",
            "filename": "src/transformers/models/regnet/convert_regnet_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,7 +19,7 @@\n from dataclasses import dataclass, field\n from functools import partial\n from pathlib import Path\n-from typing import Callable, Dict, List, Tuple\n+from typing import Callable, Dict, List, Optional, Tuple\n \n import timm\n import torch\n@@ -218,7 +218,7 @@ def convert_weight_and_push(\n         print(f\"Pushed {name}\")\n \n \n-def convert_weights_and_push(save_directory: Path, model_name: str = None, push_to_hub: bool = True):\n+def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = None, push_to_hub: bool = True):\n     filename = \"imagenet-1k-id2label.json\"\n     num_labels = 1000\n     expected_shape = (1, num_labels)"
        },
        {
            "sha": "4909f1dc670358c8da12e09e5ce53a5770a7809f",
            "filename": "src/transformers/models/resnet/convert_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -19,7 +19,7 @@\n from dataclasses import dataclass, field\n from functools import partial\n from pathlib import Path\n-from typing import List\n+from typing import List, Optional\n \n import timm\n import torch\n@@ -122,7 +122,7 @@ def convert_weight_and_push(name: str, config: ResNetConfig, save_directory: Pat\n         print(f\"Pushed {checkpoint_name}\")\n \n \n-def convert_weights_and_push(save_directory: Path, model_name: str = None, push_to_hub: bool = True):\n+def convert_weights_and_push(save_directory: Path, model_name: Optional[str] = None, push_to_hub: bool = True):\n     filename = \"imagenet-1k-id2label.json\"\n     num_labels = 1000\n     expected_shape = (1, num_labels)"
        },
        {
            "sha": "f0f38a48e51afc21f621fad4b85dfc1de75eb57d",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -770,8 +770,8 @@ def build_inputs_with_special_tokens(\n         self,\n         token_ids_0: List[int],\n         token_ids_1: Optional[List[int]] = None,\n-        cls_token_id: int = None,\n-        sep_token_id: int = None,\n+        cls_token_id: Optional[int] = None,\n+        sep_token_id: Optional[int] = None,\n     ) -> List[int]:\n         \"\"\"\n         Build model inputs from a sequence or a pair of sequence for sequence classification tasks by concatenating and"
        },
        {
            "sha": "c80a8a290b2e5cb9d3f1d6e57f2d98d8af24169d",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -127,8 +127,8 @@ def __init__(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = True,\n-        pad_size: int = None,\n-        mask_pad_size: int = None,\n+        pad_size: Optional[int] = None,\n+        mask_pad_size: Optional[int] = None,\n         do_convert_rgb: bool = True,\n         **kwargs,\n     ) -> None:"
        },
        {
            "sha": "bad72cdf2bbc1752279507febd14d86e94a3e20d",
            "filename": "src/transformers/models/segformer/modeling_tf_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_tf_segformer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -325,8 +325,8 @@ def __init__(\n         self,\n         config: SegformerConfig,\n         in_features: int,\n-        hidden_features: int = None,\n-        out_features: int = None,\n+        hidden_features: Optional[int] = None,\n+        out_features: Optional[int] = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "0d66fb9d5f9907c8850e97d8a1da11e30a7be8d1",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -52,7 +52,7 @@ def __call__(\n         images: ImageInput = None,\n         padding: Union[bool, str, PaddingStrategy] = False,\n         truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: int = None,\n+        max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n     ) -> BatchFeature:\n         \"\"\""
        },
        {
            "sha": "ddc72894af656d61aa2b9b5d9e35d64dc0c90519",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -141,7 +141,9 @@ class SmolVLMProcessor(ProcessorMixin):\n     image_processor_class = \"SmolVLMImageProcessor\"\n     tokenizer_class = \"AutoTokenizer\"\n \n-    def __init__(self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: str = None, **kwargs):\n+    def __init__(\n+        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n+    ):\n         self.fake_image_token = getattr(tokenizer, \"fake_image_token\", \"<fake_token_around_image>\")\n         self.image_token = getattr(tokenizer, \"image_token\", \"<image>\")\n         self.end_of_utterance_token = getattr(tokenizer, \"end_of_utterance_token\", \"<end_of_utterance>\")"
        },
        {
            "sha": "4375f56a87e6b0e4eefc60c571a7077d60a7e339",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -291,8 +291,8 @@ def from_pretrained(cls, *args, **kwargs):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:"
        },
        {
            "sha": "b290b4990d1e815c81b7291eaa912dfc2c3d933c",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -247,8 +247,8 @@ def __init__(\n         tokenize_chinese_chars=True,\n         strip_accents=None,\n         cell_trim_length: int = -1,\n-        max_column_id: int = None,\n-        max_row_id: int = None,\n+        max_column_id: Optional[int] = None,\n+        max_row_id: Optional[int] = None,\n         strip_column_names: bool = False,\n         update_answer_coordinates: bool = False,\n         min_question_length=None,\n@@ -2242,8 +2242,8 @@ class NumericValue:\n \n @dataclass\n class NumericValueSpan:\n-    begin_index: int = None\n-    end_index: int = None\n+    begin_index: Optional[int] = None\n+    end_index: Optional[int] = None\n     values: List[NumericValue] = None\n \n "
        },
        {
            "sha": "1f56d604495cf1fdbd5fdfc00efd88f3e1cbbcb7",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -205,10 +205,10 @@ def preprocess(\n         images: ImageInput,\n         do_resize: bool = None,\n         size: Dict[str, int] = None,\n-        size_divisor: int = None,\n+        size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,"
        },
        {
            "sha": "fcc0de5b6d8a88197db9997aa09f8461c7abb53e",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -144,8 +144,8 @@ def __init__(\n         config,\n         embed_dim: int,\n         num_heads: int,\n-        kdim: int = None,\n-        vdim: int = None,\n+        kdim: Optional[int] = None,\n+        vdim: Optional[int] = None,\n         dropout: float = 0.0,\n         is_decoder: bool = False,\n         bias: bool = True,"
        },
        {
            "sha": "c5725515793c2903cc0fcc4740c41722caa64fee",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -178,7 +178,7 @@ def preprocess(\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_rescale: bool = None,\n         rescale_factor: float = None,\n         do_normalize: bool = None,\n@@ -332,7 +332,7 @@ def _preprocess_image(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_center_crop: bool = None,\n-        crop_size: int = None,\n+        crop_size: Optional[int] = None,\n         do_convert_rgb: bool = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "b909d136653f7fb8d2fd9cde9fb1f2154396c54d",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -309,8 +309,8 @@ def tf_to_pt_weight_rename(self, tf_weight):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:"
        },
        {
            "sha": "9b18306713d2e15cf808981a6f367d7eaf88bdc8",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -380,8 +380,8 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     @classmethod\n     def from_encoder_decoder_pretrained(\n         cls,\n-        encoder_pretrained_model_name_or_path: str = None,\n-        decoder_pretrained_model_name_or_path: str = None,\n+        encoder_pretrained_model_name_or_path: Optional[str] = None,\n+        decoder_pretrained_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:"
        },
        {
            "sha": "b12327d8ca0253a8ce1a5217e23c2d6c05600906",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -414,8 +414,8 @@ def _get_features(module, pixel_values, deterministic):\n     @classmethod\n     def from_vision_text_pretrained(\n         cls,\n-        vision_model_name_or_path: str = None,\n-        text_model_name_or_path: str = None,\n+        vision_model_name_or_path: Optional[str] = None,\n+        text_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> FlaxPreTrainedModel:"
        },
        {
            "sha": "ca88d2fec95ef8f5b3933d888f6cf76ff5c4770d",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -465,8 +465,8 @@ def call(\n     @classmethod\n     def from_vision_text_pretrained(\n         cls,\n-        vision_model_name_or_path: str = None,\n-        text_model_name_or_path: str = None,\n+        vision_model_name_or_path: Optional[str] = None,\n+        text_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> TFPreTrainedModel:"
        },
        {
            "sha": "a5d3cad6016e79494d3cc2d198ff12624877ad7a",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -417,8 +417,8 @@ def from_pretrained(cls, *args, **kwargs):\n     @classmethod\n     def from_vision_text_pretrained(\n         cls,\n-        vision_model_name_or_path: str = None,\n-        text_model_name_or_path: str = None,\n+        vision_model_name_or_path: Optional[str] = None,\n+        text_model_name_or_path: Optional[str] = None,\n         *model_args,\n         **kwargs,\n     ) -> PreTrainedModel:"
        },
        {
            "sha": "aba8fec7ae41a1ebd7caf27f582b3b1913cad06c",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -14,6 +14,8 @@\n # limitations under the License.\n \"\"\"VitPose model configuration\"\"\"\n \n+from typing import Optional\n+\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n@@ -75,11 +77,11 @@ class VitPoseConfig(PretrainedConfig):\n \n     def __init__(\n         self,\n-        backbone_config: PretrainedConfig = None,\n-        backbone: str = None,\n+        backbone_config: Optional[PretrainedConfig] = None,\n+        backbone: Optional[str] = None,\n         use_pretrained_backbone: bool = False,\n         use_timm_backbone: bool = False,\n-        backbone_kwargs: dict = None,\n+        backbone_kwargs: Optional[dict] = None,\n         initializer_range: float = 0.02,\n         scale_factor: int = 4,\n         use_simple_decoder: bool = True,"
        },
        {
            "sha": "be0465d6c63c8e4a319b40228b7a68a031efa025",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -652,7 +652,7 @@ def forward(\n \n \n class WhisperDecoderLayer(nn.Module):\n-    def __init__(self, config: WhisperConfig, layer_idx: int = None):\n+    def __init__(self, config: WhisperConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.embed_dim = config.d_model\n "
        },
        {
            "sha": "b5dbb49a3639ef31f95dbaa795fcb4b8fd74167c",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -377,7 +377,9 @@ def bpe(self, token):\n         self.cache[token] = word\n         return word\n \n-    def set_prefix_tokens(self, language: str = None, task: str = None, predict_timestamps: bool = None):\n+    def set_prefix_tokens(\n+        self, language: Optional[str] = None, task: Optional[str] = None, predict_timestamps: bool = None\n+    ):\n         \"\"\"\n         Override the prefix tokens appended to the start of the label sequence. This method can be used standalone to\n         update the prefix tokens as required when fine-tuning. Example:\n@@ -1276,7 +1278,7 @@ def _collate_word_timestamps(tokenizer, tokens, token_timestamps, language, retu\n def _combine_tokens_into_words(\n     tokenizer,\n     tokens: List[int],\n-    language: str = None,\n+    language: Optional[str] = None,\n     prepend_punctuations: str = \"\\\"'([{-\",\n     append_punctuations: str = \"\\\"'.,!?:)]}\",\n ):"
        },
        {
            "sha": "5b25def5e4055eccc5723e9fa0480065b73b316f",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -451,7 +451,9 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n \n         return tuple(files) + (normalizer_file,)\n \n-    def set_prefix_tokens(self, language: str = None, task: str = None, predict_timestamps: bool = None):\n+    def set_prefix_tokens(\n+        self, language: Optional[str] = None, task: Optional[str] = None, predict_timestamps: bool = None\n+    ):\n         \"\"\"\n         Override the prefix tokens appended to the start of the label sequence. This method can be used standalone to\n         update the prefix tokens as required when fine-tuning. Example:"
        },
        {
            "sha": "7033102eeefca62e07ad1aed2db0ddc8268870fd",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -679,7 +679,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         original_hidden_states: Optional[torch.Tensor] = None,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[ZambaHybridDynamicCache] = None,\n@@ -747,7 +747,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         original_hidden_states: Optional[torch.Tensor] = None,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[ZambaHybridDynamicCache] = None,"
        },
        {
            "sha": "05502d0e4e3d82b00b76acb03b88702a5ac4a60d",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -385,8 +385,8 @@ def __init__(\n         self,\n         config: Zamba2Config,\n         layer_idx: Optional[int] = None,\n-        num_fwd_mem_blocks: int = None,\n-        block_id: int = None,\n+        num_fwd_mem_blocks: Optional[int] = None,\n+        block_id: Optional[int] = None,\n     ):\n         super().__init__()\n         self.config = config\n@@ -560,7 +560,7 @@ class Zamba2MambaMixer(nn.Module):\n     and is why Mamba is called **selective** state spaces)\n     \"\"\"\n \n-    def __init__(self, config: Zamba2Config, layer_idx: int = None):\n+    def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size\n@@ -983,7 +983,7 @@ def forward(\n \n \n class Zamba2MLP(nn.Module):\n-    def __init__(self, config: Zamba2Config, num_fwd_mem_blocks=None, block_id: int = None):\n+    def __init__(self, config: Zamba2Config, num_fwd_mem_blocks=None, block_id: Optional[int] = None):\n         \"\"\"\n         This MLP layer contributes to tied transformer blocks aimed to increasing compute without increasing model size. Because this layer\n         is tied, un-tied adapter modules (formally same as LoRA, but used in the base model) are added to the up and gate projectors to increase expressivity with a small memory overhead.\n@@ -1025,7 +1025,7 @@ def forward(self, hidden_state, layer_idx=None):\n \n \n class Zamba2AttentionDecoderLayer(nn.Module):\n-    def __init__(self, config: Zamba2Config, block_id: int = None, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Zamba2Config, block_id: Optional[int] = None, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.block_id = block_id\n         num_gs = len(config.hybrid_layer_ids)\n@@ -1099,7 +1099,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         original_hidden_states: Optional[torch.Tensor] = None,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Zamba2HybridDynamicCache] = None,\n@@ -1169,7 +1169,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         original_hidden_states: Optional[torch.Tensor] = None,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Zamba2HybridDynamicCache] = None,"
        },
        {
            "sha": "625cdb0bf5d0fd9458bd188e0915b2392a4c1548",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -199,8 +199,8 @@ def __init__(\n         self,\n         config: Zamba2Config,\n         layer_idx: Optional[int] = None,\n-        num_fwd_mem_blocks: int = None,\n-        block_id: int = None,\n+        num_fwd_mem_blocks: Optional[int] = None,\n+        block_id: Optional[int] = None,\n     ):\n         super().__init__(config, layer_idx)\n         self.num_fwd_mem_blocks = num_fwd_mem_blocks\n@@ -302,7 +302,7 @@ class Zamba2MambaMixer(nn.Module):\n     and is why Mamba is called **selective** state spaces)\n     \"\"\"\n \n-    def __init__(self, config: Zamba2Config, layer_idx: int = None):\n+    def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.hidden_size = config.hidden_size\n@@ -725,7 +725,7 @@ def forward(\n \n \n class Zamba2MLP(nn.Module):\n-    def __init__(self, config: Zamba2Config, num_fwd_mem_blocks=None, block_id: int = None):\n+    def __init__(self, config: Zamba2Config, num_fwd_mem_blocks=None, block_id: Optional[int] = None):\n         \"\"\"\n         This MLP layer contributes to tied transformer blocks aimed to increasing compute without increasing model size. Because this layer\n         is tied, un-tied adapter modules (formally same as LoRA, but used in the base model) are added to the up and gate projectors to increase expressivity with a small memory overhead.\n@@ -767,7 +767,7 @@ def forward(self, hidden_state, layer_idx=None):\n \n \n class Zamba2AttentionDecoderLayer(ZambaAttentionDecoderLayer):\n-    def __init__(self, config: Zamba2Config, block_id: int = None, layer_idx: Optional[int] = None):\n+    def __init__(self, config: Zamba2Config, block_id: Optional[int] = None, layer_idx: Optional[int] = None):\n         self.block_id = block_id\n         num_gs = len(config.hybrid_layer_ids)\n         super().__init__(config, layer_idx)\n@@ -847,7 +847,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         original_hidden_states: Optional[torch.Tensor] = None,\n-        layer_idx: int = None,\n+        layer_idx: Optional[int] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         past_key_value: Optional[Zamba2HybridDynamicCache] = None,"
        },
        {
            "sha": "c93e29c583ae531da7a2e9411628560115e331db",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -305,9 +305,9 @@ def preprocess(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_resize: bool = None,\n-        size: int = None,\n+        size: Optional[int] = None,\n         keep_aspect_ratio: bool = None,\n-        ensure_multiple_of: int = None,\n+        ensure_multiple_of: Optional[int] = None,\n         resample: PILImageResampling = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,"
        },
        {
            "sha": "460ee9329977bdb2aadc54aae72e0cd93792aab0",
            "filename": "src/transformers/onnx/config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fonnx%2Fconfig.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconfig.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -291,7 +291,7 @@ def generate_dummy_inputs(\n         sampling_rate: int = 22050,\n         time_duration: float = 5.0,\n         frequency: int = 220,\n-        tokenizer: \"PreTrainedTokenizerBase\" = None,\n+        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n     ) -> Mapping[str, Any]:\n         \"\"\"\n         Generate inputs to provide to the ONNX exporter for the specific framework\n@@ -445,7 +445,7 @@ def __init__(\n         self,\n         config: \"PretrainedConfig\",\n         task: str = \"default\",\n-        patching_specs: List[PatchingSpec] = None,\n+        patching_specs: Optional[list[PatchingSpec]] = None,\n         use_past: bool = False,\n     ):\n         super().__init__(config, task=task, patching_specs=patching_specs)\n@@ -639,7 +639,7 @@ def num_attention_heads(self) -> Tuple[int]:\n \n     def generate_dummy_inputs(\n         self,\n-        tokenizer: \"PreTrainedTokenizerBase\",\n+        tokenizer: Optional[\"PreTrainedTokenizerBase\"],\n         batch_size: int = -1,\n         seq_length: int = -1,\n         is_pair: bool = False,"
        },
        {
            "sha": "58bc51f8e801cfaeb9423342f330c269fa5c2318",
            "filename": "src/transformers/onnx/convert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconvert.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -16,7 +16,7 @@\n from inspect import signature\n from itertools import chain\n from pathlib import Path\n-from typing import TYPE_CHECKING, Iterable, List, Tuple, Union\n+from typing import TYPE_CHECKING, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n from packaging.version import Version, parse\n@@ -85,7 +85,7 @@ def export_pytorch(\n     config: OnnxConfig,\n     opset: int,\n     output: Path,\n-    tokenizer: \"PreTrainedTokenizer\" = None,\n+    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n     device: str = \"cpu\",\n ) -> Tuple[List[str], List[str]]:\n     \"\"\"\n@@ -188,7 +188,7 @@ def export_tensorflow(\n     config: OnnxConfig,\n     opset: int,\n     output: Path,\n-    tokenizer: \"PreTrainedTokenizer\" = None,\n+    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n ) -> Tuple[List[str], List[str]]:\n     \"\"\"\n     Export a TensorFlow model to an ONNX Intermediate Representation (IR)\n@@ -254,7 +254,7 @@ def export(\n     config: OnnxConfig,\n     opset: int,\n     output: Path,\n-    tokenizer: \"PreTrainedTokenizer\" = None,\n+    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n     device: str = \"cpu\",\n ) -> Tuple[List[str], List[str]]:\n     \"\"\"\n@@ -321,7 +321,7 @@ def validate_model_outputs(\n     onnx_model: Path,\n     onnx_named_outputs: List[str],\n     atol: float,\n-    tokenizer: \"PreTrainedTokenizer\" = None,\n+    tokenizer: Optional[\"PreTrainedTokenizer\"] = None,\n ):\n     from onnxruntime import InferenceSession, SessionOptions\n "
        },
        {
            "sha": "6a88acf711099200f98843826ef665d65171e7f1",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -531,7 +531,7 @@ def word_to_tokens(\n         span = self._encodings[batch_index].word_to_tokens(word_index, sequence_index)\n         return TokenSpan(*span) if span is not None else None\n \n-    def token_to_chars(self, batch_or_token_index: int, token_index: Optional[int] = None) -> CharSpan:\n+    def token_to_chars(self, batch_or_token_index: int, token_index: Optional[int] = None) -> Optional[CharSpan]:\n         \"\"\"\n         Get the character span corresponding to an encoded token in a sequence of the batch.\n \n@@ -2629,7 +2629,7 @@ def encode(\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         padding_side: Optional[str] = None,\n@@ -2810,15 +2810,15 @@ def _get_padding_truncation_strategies(\n     @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n     def __call__(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput], None] = None,\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        text_target: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput], None] = None,\n         text_pair_target: Optional[\n             Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]\n         ] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n@@ -2905,7 +2905,7 @@ def _call_one(\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n@@ -3131,7 +3131,7 @@ def batch_encode_plus(\n         ],\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Union[bool, str, TruncationStrategy, None] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n@@ -3807,7 +3807,7 @@ def batch_decode(\n         self,\n         sequences: Union[List[int], List[List[int]], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n     ) -> List[str]:\n         \"\"\"\n@@ -3841,7 +3841,7 @@ def decode(\n         self,\n         token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n     ) -> str:\n         \"\"\"\n@@ -3878,7 +3878,7 @@ def _decode(\n         self,\n         token_ids: Union[int, List[int]],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         **kwargs,\n     ) -> str:\n         raise NotImplementedError"
        },
        {
            "sha": "17ad614504550ef248f629bc55f511c8762dedd1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de77f5b1ec37ea214cda08a1ca33a841ce097d75/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=de77f5b1ec37ea214cda08a1ca33a841ce097d75",
            "patch": "@@ -414,7 +414,7 @@ class Trainer:\n     @deprecate_kwarg(\"tokenizer\", new_name=\"processing_class\", version=\"5.0.0\", raise_if_both_names=True)\n     def __init__(\n         self,\n-        model: Union[PreTrainedModel, nn.Module] = None,\n+        model: Union[PreTrainedModel, nn.Module, None] = None,\n         args: TrainingArguments = None,\n         data_collator: Optional[DataCollator] = None,\n         train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\n@@ -2139,7 +2139,7 @@ def patched_optimizer_step(optimizer, barrier=False, optimizer_args={}):\n     def train(\n         self,\n         resume_from_checkpoint: Optional[Union[str, bool]] = None,\n-        trial: Union[\"optuna.Trial\", dict[str, Any]] = None,\n+        trial: Union[\"optuna.Trial\", dict[str, Any], None] = None,\n         ignore_keys_for_eval: Optional[list[str]] = None,\n         **kwargs,\n     ):\n@@ -4920,10 +4920,10 @@ def prediction_loop(\n         logger.info(f\"  Num examples = {num_examples}\")\n         logger.info(f\"  Batch size = {batch_size}\")\n \n-        losses_host: torch.Tensor = None\n-        preds_host: Union[torch.Tensor, list[torch.Tensor]] = None\n-        labels_host: Union[torch.Tensor, list[torch.Tensor]] = None\n-        inputs_host: Union[torch.Tensor, list[torch.Tensor]] = None\n+        losses_host: Optional[torch.Tensor] = None\n+        preds_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n+        labels_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n+        inputs_host: Union[torch.Tensor, list[torch.Tensor], None] = None\n         metrics: Optional[dict] = None\n         eval_set_kwargs: dict = {}\n "
        }
    ],
    "stats": {
        "total": 520,
        "additions": 271,
        "deletions": 249
    }
}