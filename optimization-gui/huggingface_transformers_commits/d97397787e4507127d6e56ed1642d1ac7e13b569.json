{
    "author": "ssharpe42",
    "message": "Wait for main process in _save_checkpoint to ensure best checkpoint exists (#40923)\n\n* Update trainer.py\n\n* fix\n\n* fix format\n\n* move barrier, delete redundant",
    "sha": "d97397787e4507127d6e56ed1642d1ac7e13b569",
    "files": [
        {
            "sha": "b50d6664340879d6aeba37b599ac40af10b0769b",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d97397787e4507127d6e56ed1642d1ac7e13b569/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d97397787e4507127d6e56ed1642d1ac7e13b569/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=d97397787e4507127d6e56ed1642d1ac7e13b569",
            "patch": "@@ -2765,14 +2765,6 @@ def _inner_training_loop(\n \n         logger.info(\"\\n\\nTraining completed. Do not forget to share your model on huggingface.co/models =)\\n\\n\")\n         if args.load_best_model_at_end and self.state.best_model_checkpoint is not None:\n-            # Wait for everyone to get here so we are sure the model has been saved by process 0.\n-            if is_torch_xla_available():\n-                xm.rendezvous(\"load_best_model_at_end\")\n-            elif args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                dist.barrier()\n-            elif is_sagemaker_mp_enabled():\n-                smp.barrier()\n-\n             self._load_best_model()\n \n         # add remaining tr_loss\n@@ -3289,6 +3281,15 @@ def _save_checkpoint(self, model, trial):\n         self.save_model(output_dir, _internal_call=True)\n \n         if self.args.save_strategy in [SaveStrategy.STEPS, SaveStrategy.EPOCH] and self.state.best_global_step:\n+            # Wait for everyone to get here so we are sure the model has been saved by process 0\n+            # before we check if the best_checkpoint_dir exists\n+            if is_torch_xla_available():\n+                xm.rendezvous(\"load_best_model_at_end\")\n+            elif self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n+                dist.barrier()\n+            elif is_sagemaker_mp_enabled():\n+                smp.barrier()\n+\n             best_checkpoint_folder = f\"{PREFIX_CHECKPOINT_DIR}-{self.state.best_global_step}\"\n             best_checkpoint_dir = os.path.join(run_dir, best_checkpoint_folder)\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 9,
        "deletions": 8
    }
}