{
    "author": "SunMarc",
    "message": "remove `triton_kernels` dep with `kernels` instead (#39926)\n\n* remove dep\n\n* style\n\n* rm import\n\n* fix\n\n* style\n\n* simplify\n\n* style",
    "sha": "6902ffa505970a39bd90f69b1afaa7affd564ac5",
    "files": [
        {
            "sha": "b37f72e7c388e2549f5b2aff8002b4180417c9e6",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 31,
            "deletions": 10,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -49,17 +49,21 @@\n \n # Copied from GPT_OSS repo and vllm\n def quantize_to_mxfp4(w):\n-    from triton_kernels.numerics_details.mxfp import downcast_to_mxfp\n+    downcast_to_mxfp = triton_kernels_hub.numerics_details.mxfp.downcast_to_mxfp\n \n     w, w_scale = downcast_to_mxfp(w.to(torch.bfloat16), torch.uint8, axis=1)\n     w, w_scale = swizzle_mxfp4(w, w_scale)\n     return w, w_scale\n \n \n def swizzle_mxfp4(w, w_scale):\n-    from triton_kernels.tensor import FP4, convert_layout, wrap_torch_tensor\n-    from triton_kernels.tensor_details import layout\n-    from triton_kernels.tensor_details.layout import StridedLayout\n+    FP4, convert_layout, wrap_torch_tensor = (\n+        triton_kernels_hub.tensor.FP4,\n+        triton_kernels_hub.tensor.convert_layout,\n+        triton_kernels_hub.tensor.wrap_torch_tensor,\n+    )\n+    layout = triton_kernels_hub.tensor_details.layout\n+    StridedLayout = triton_kernels_hub.tensor_details.layout.StridedLayout\n \n     value_layout, value_layout_opts = layout.make_default_matmul_mxfp4_w_layout(mx_axis=1)\n     w = convert_layout(wrap_torch_tensor(w, dtype=FP4), value_layout, **value_layout_opts)\n@@ -173,8 +177,12 @@ def __init__(self, config):\n         self.down_proj_precision_config = None\n \n     def forward(self, hidden_states: torch.Tensor, routing_data, gather_idx, scatter_idx) -> torch.Tensor:\n-        from triton_kernels.matmul_ogs import FnSpecs, FusedActivation, matmul_ogs\n-        from triton_kernels.swiglu import swiglu_fn\n+        FnSpecs, FusedActivation, matmul_ogs = (\n+            triton_kernels_hub.matmul_ogs.FnSpecs,\n+            triton_kernels_hub.matmul_ogs.FusedActivation,\n+            triton_kernels_hub.matmul_ogs.matmul_ogs,\n+        )\n+        swiglu_fn = triton_kernels_hub.swiglu.swiglu_fn\n \n         with torch.cuda.device(hidden_states.device):\n             act = FusedActivation(FnSpecs(\"swiglu\", swiglu_fn, (\"alpha\", \"limit\")), (self.alpha, None), 2)\n@@ -211,7 +219,12 @@ def routing_torch_dist(\n ):\n     import os\n \n-    from triton_kernels.routing import GatherIndx, RoutingData, ScatterIndx, compute_expt_data_torch\n+    GatherIndx, RoutingData, ScatterIndx, compute_expt_data_torch = (\n+        triton_kernels_hub.routing.GatherIndx,\n+        triton_kernels_hub.routing.RoutingData,\n+        triton_kernels_hub.routing.ScatterIndx,\n+        triton_kernels_hub.routing.compute_expt_data_torch,\n+    )\n \n     with torch.cuda.device(logits.device):\n         world_size = torch.distributed.get_world_size()\n@@ -274,7 +287,7 @@ def mlp_forward(self, hidden_states):\n     if dist.is_available() and dist.is_initialized():\n         routing = routing_torch_dist\n     else:\n-        from triton_kernels.routing import routing\n+        routing = triton_kernels_hub.routing.routing\n \n         routing = routing\n     batch_size = hidden_states.shape[0]\n@@ -337,8 +350,11 @@ def dequantize(module, param_name, param_value, target_device, dq_param_name, **\n \n \n def load_and_swizzle_mxfp4(module, param_name, param_value, target_device, **kwargs):\n-    from triton_kernels.matmul_ogs import FlexCtx, InFlexData, PrecisionConfig\n-\n+    PrecisionConfig, FlexCtx, InFlexData = (\n+        triton_kernels_hub.matmul_ogs.PrecisionConfig,\n+        triton_kernels_hub.matmul_ogs.FlexCtx,\n+        triton_kernels_hub.matmul_ogs.InFlexData,\n+    )\n     from ..integrations.tensor_parallel import shard_and_distribute_module\n \n     model = kwargs.get(\"model\", None)\n@@ -450,6 +466,11 @@ def replace_with_mxfp4_linear(\n ):\n     if quantization_config.dequantize:\n         return model\n+    else:\n+        from kernels import get_kernel\n+\n+        global triton_kernels_hub\n+        triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n \n     modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n "
        },
        {
            "sha": "5281d4d76388e360a19efcd424d1bfdc81fa4632",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 14,
            "deletions": 6,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -21,9 +21,9 @@\n \n from ..utils import (\n     is_accelerate_available,\n+    is_kernels_available,\n     is_torch_available,\n     is_triton_available,\n-    is_triton_kernels_availalble,\n     logging,\n )\n from .quantizers_utils import get_module_from_name\n@@ -68,7 +68,7 @@ def validate_environment(self, *args, **kwargs):\n \n         compute_capability = torch.cuda.get_device_capability()\n         gpu_is_supported = compute_capability >= (7, 5)\n-        kernels_available = is_triton_available(\"3.4.0\") and is_triton_kernels_availalble()\n+        kernels_available = is_triton_available(\"3.4.0\") and is_kernels_available()\n \n         if self.pre_quantized:\n             # On unsupported GPUs or without kernels, we will dequantize the model to bf16\n@@ -82,7 +82,7 @@ def validate_environment(self, *args, **kwargs):\n \n             if not kernels_available:\n                 logger.warning_once(\n-                    \"MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed, we will default to dequantizing the model to bf16\"\n+                    \"MXFP4 quantization requires triton >= 3.4.0 and kernels installed, we will default to dequantizing the model to bf16\"\n                 )\n                 self.quantization_config.dequantize = True\n                 return\n@@ -95,6 +95,12 @@ def validate_environment(self, *args, **kwargs):\n             # we can't quantize the model in this case so we raise an error\n             raise ValueError(\"MXFP4 quantization requires triton >= 3.4.0 and triton_kernels installed\")\n \n+        if not self.pre_quantized:\n+            from kernels import get_kernel\n+\n+            global triton_kernels_hub\n+            triton_kernels_hub = get_kernel(\"kernels-community/triton_kernels\")\n+\n         device_map = kwargs.get(\"device_map\", None)\n         if device_map is None:\n             logger.warning_once(\n@@ -160,13 +166,15 @@ def create_quantized_param(\n         unexpected_keys: Optional[list[str]] = None,\n         **kwargs,\n     ):\n-        if is_triton_kernels_availalble() and is_triton_available(\"3.4.0\"):\n-            from triton_kernels.matmul_ogs import FlexCtx, InFlexData, PrecisionConfig\n-\n         from ..integrations import Mxfp4GptOssExperts, dequantize, load_and_swizzle_mxfp4, quantize_to_mxfp4\n         from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n \n         if not self.pre_quantized:\n+            PrecisionConfig, FlexCtx, InFlexData = (\n+                triton_kernels_hub.matmul_ogs.PrecisionConfig,\n+                triton_kernels_hub.matmul_ogs.FlexCtx,\n+                triton_kernels_hub.matmul_ogs.InFlexData,\n+            )\n             module, _ = get_module_from_name(model, param_name)\n             with torch.cuda.device(target_device):\n                 if isinstance(module, Mxfp4GptOssExperts):"
        },
        {
            "sha": "e75bbab5b0c40b94363dceba1e40f4b86b6da4d0",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -170,7 +170,6 @@\n     is_torchdynamo_available,\n     is_torchvision_available,\n     is_triton_available,\n-    is_triton_kernels_availalble,\n     is_vision_available,\n     is_vptq_available,\n     strtobool,\n@@ -471,13 +470,6 @@ def decorator(test_case):\n     return decorator\n \n \n-def require_triton_kernels(test_case):\n-    \"\"\"\n-    Decorator marking a test that requires triton_kernels. These tests are skipped when triton_kernels isn't installed.\n-    \"\"\"\n-    return unittest.skipUnless(is_triton_kernels_availalble(), \"test requires triton_kernels\")(test_case)\n-\n-\n def require_gguf(test_case, min_version: str = GGUF_MIN_VERSION):\n     \"\"\"\n     Decorator marking a test that requires ggguf. These tests are skipped when gguf isn't installed."
        },
        {
            "sha": "c28ae9a5b1441afa382f964820aa99fef70e71c8",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -270,7 +270,6 @@\n     is_torchvision_v2_available,\n     is_training_run_on_sagemaker,\n     is_triton_available,\n-    is_triton_kernels_availalble,\n     is_uroman_available,\n     is_vision_available,\n     is_vptq_available,"
        },
        {
            "sha": "da740e68de9c0f1b6c12909083b24db488dd8a50",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -238,7 +238,6 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _matplotlib_available = _is_package_available(\"matplotlib\")\n _mistral_common_available = _is_package_available(\"mistral_common\")\n _triton_available, _triton_version = _is_package_available(\"triton\", return_version=True)\n-_triton_kernels_available = _is_package_available(\"triton_kernels\")\n \n _torch_version = \"N/A\"\n _torch_available = False\n@@ -423,10 +422,6 @@ def is_triton_available(min_version: str = TRITON_MIN_VERSION):\n     return _triton_available and version.parse(_triton_version) >= version.parse(min_version)\n \n \n-def is_triton_kernels_availalble():\n-    return _triton_kernels_available\n-\n-\n def is_hadamard_available():\n     return _hadamard_available\n "
        },
        {
            "sha": "1743891f8b3fbc1b01df2c127a6921eba465533b",
            "filename": "tests/quantization/mxfp4/test_mxfp4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 11,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/6902ffa505970a39bd90f69b1afaa7affd564ac5/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6902ffa505970a39bd90f69b1afaa7affd564ac5/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py?ref=6902ffa505970a39bd90f69b1afaa7affd564ac5",
            "patch": "@@ -18,11 +18,11 @@\n \n from transformers import AutoTokenizer, GptOssForCausalLM, Mxfp4Config\n from transformers.testing_utils import (\n+    require_kernels,\n     require_torch,\n     require_torch_gpu,\n     require_torch_large_gpu,\n     require_triton,\n-    require_triton_kernels,\n     slow,\n )\n from transformers.utils import (\n@@ -194,7 +194,7 @@ def test_quantizer_validation_missing_triton(self):\n         \"\"\"Test quantizer validation when triton is not available\"\"\"\n         with (\n             patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_available\", return_value=False),\n-            patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_kernels_availalble\", return_value=False),\n+            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_availalble\", return_value=False),\n         ):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -208,7 +208,7 @@ def test_quantizer_validation_missing_triton_pre_quantized_no_dequantize(self):\n         \"\"\"Test quantizer validation when triton is not available but model is pre-quantized and dequantize is False\"\"\"\n         with (\n             patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_available\", return_value=False),\n-            patch(\"transformers.quantizers.quantizer_mxfp4.is_triton_kernels_availalble\", return_value=False),\n+            patch(\"transformers.quantizers.quantizer_mxfp4.is_kernels_availalble\", return_value=False),\n         ):\n             from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n \n@@ -348,7 +348,7 @@ def test_convert_moe_packed_tensors(self):\n         self.assertEqual(result.dtype, torch.bfloat16)\n \n     @require_triton(min_version=\"3.4.0\")\n-    @require_triton_kernels\n+    @require_kernels\n     @require_torch_gpu\n     @require_torch\n     def test_quantize_to_mxfp4(self):\n@@ -368,12 +368,14 @@ def test_quantize_to_mxfp4(self):\n \n @require_torch\n @require_torch_large_gpu\n+@require_triton(min_version=\"3.4.0\")\n+@require_kernels\n @slow\n class Mxfp4ModelTest(unittest.TestCase):\n     \"\"\"Test mxfp4 with actual models (requires specific model and hardware)\"\"\"\n \n     # These should be paths to real OpenAI MoE models for proper testing\n-    model_name_packed = \"/fsx/mohamed/oai-hf/tests/20b_converted_packed\"  # TODO: Use real packed quantized model\n+    model_name = \"openai/gpt-oss-20b\"\n \n     input_text = \"Once upon a time\"\n \n@@ -421,12 +423,12 @@ def test_gpt_oss_model_loading_quantized_with_device_map(self):\n         self.assertFalse(quantization_config.dequantize)\n \n         model = GptOssForCausalLM.from_pretrained(\n-            self.model_name_packed,\n+            self.model_name,\n             quantization_config=quantization_config,\n             torch_dtype=torch.bfloat16,\n             device_map=\"auto\",\n         )\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_name_packed)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n         self.check_inference_correctness_quantized(model, tokenizer)\n \n     def test_gpt_oss_model_loading_dequantized_with_device_map(self):\n@@ -438,12 +440,12 @@ def test_gpt_oss_model_loading_dequantized_with_device_map(self):\n         self.assertTrue(quantization_config.dequantize)\n \n         model = GptOssForCausalLM.from_pretrained(\n-            self.model_name_packed,\n+            self.model_name,\n             quantization_config=quantization_config,\n             torch_dtype=torch.bfloat16,\n             device_map=\"auto\",\n         )\n-        tokenizer = AutoTokenizer.from_pretrained(self.model_name_packed)\n+        tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n         self.check_inference_correctness_quantized(model, tokenizer)\n \n     def test_model_device_map_validation(self):\n@@ -464,12 +466,12 @@ def test_memory_footprint_comparison(self):\n         # Expected: quantized < dequantized < unquantized memory usage\n         quantization_config = Mxfp4Config(dequantize=True)\n         quantized_model = GptOssForCausalLM.from_pretrained(\n-            self.model_name_packed,\n+            self.model_name,\n             torch_dtype=torch.bfloat16,\n             device_map=\"auto\",\n         )\n         dequantized_model = GptOssForCausalLM.from_pretrained(\n-            self.model_name_packed,\n+            self.model_name,\n             torch_dtype=torch.bfloat16,\n             device_map=\"auto\",\n             quantization_config=quantization_config,"
        }
    ],
    "stats": {
        "total": 99,
        "additions": 58,
        "deletions": 41
    }
}