{
    "author": "kwen2501",
    "message": "Update doc re list of models supporting TP (#35864)\n\nUpdate doc about models' TP support",
    "sha": "f869d486d3aed92f75a9578bdcb272c55f97d30c",
    "files": [
        {
            "sha": "7f5d52363e4d4b05a4fc55fa8d623548288d02a0",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f869d486d3aed92f75a9578bdcb272c55f97d30c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f869d486d3aed92f75a9578bdcb272c55f97d30c/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=f869d486d3aed92f75a9578bdcb272c55f97d30c",
            "patch": "@@ -54,6 +54,16 @@ torchrun --nproc-per-node 4 demo.py\n \n PyTorch tensor parallel is currently supported for the following models:\n * [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n+* [Gemma](https://huggingface.co/docs/transformers/en/model_doc/gemma), [Gemma2](https://huggingface.co/docs/transformers/en/model_doc/gemma2)\n+* [Granite](https://huggingface.co/docs/transformers/en/model_doc/granite)\n+* [Mistral](https://huggingface.co/docs/transformers/en/model_doc/mistral)\n+* [Qwen2](https://huggingface.co/docs/transformers/en/model_doc/qwen2), [Qwen2MoE](https://huggingface.co/docs/transformers/en/model_doc/qwen2_moe), [Qwen2-VL](https://huggingface.co/docs/transformers/v4.48.0/en/model_doc/qwen2_vl)\n+* [Starcoder2](https://huggingface.co/docs/transformers/en/model_doc/starcoder2)\n+* [Cohere](https://huggingface.co/docs/transformers/en/model_doc/cohere), [Cohere2](https://huggingface.co/docs/transformers/en/model_doc/cohere2)\n+* [GLM](https://huggingface.co/docs/transformers/en/model_doc/glm)\n+* [Mixtral](https://huggingface.co/docs/transformers/en/model_doc/mixtral)\n+* [OLMo](https://huggingface.co/docs/transformers/en/model_doc/olmo), [OLMo2](https://huggingface.co/docs/transformers/en/model_doc/olmo2)\n+* [Phi](https://huggingface.co/docs/transformers/en/model_doc/phi), [Phi-3](https://huggingface.co/docs/transformers/en/model_doc/phi3)\n \n You can request to add tensor parallel support for another model by opening a GitHub Issue or Pull Request.\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}