{
    "author": "isaac091",
    "message": "Add SDPA support for M2M100 (#33309)\n\n* Add SDPA support for M2M100\r\n\r\n* [run_slow] m2m_100, nllb",
    "sha": "61e98cb957862d679c4a338319a386da197b8073",
    "files": [
        {
            "sha": "d64545fafb06120e1e6ee5e6155b02b51b9cc4ee",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=61e98cb957862d679c4a338319a386da197b8073",
            "patch": "@@ -163,3 +163,21 @@ Below is an expected speedup diagram that compares pure inference time between t\n <div style=\"text-align: center\">\n <img src=\"https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp\">\n </div>\n+\n+## Using Scaled Dot Product Attention (SDPA)\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```python\n+from transformers import M2M100ForConditionalGeneration\n+model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\\ No newline at end of file"
        },
        {
            "sha": "abdff7445aa34c0766886f1543fd30b02b64c4da",
            "filename": "docs/source/en/model_doc/nllb.md",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnllb.md?ref=61e98cb957862d679c4a338319a386da197b8073",
            "patch": "@@ -188,3 +188,21 @@ Below is an expected speedup diagram that compares pure inference time between t\n <div style=\"text-align: center\">\n <img src=\"https://huggingface.co/datasets/visheratin/documentation-images/resolve/main/nllb-speedup.webp\">\n </div>\n+\n+## Using Scaled Dot Product Attention (SDPA)\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```python\n+from transformers import AutoModelForSeq2SeqLM\n+model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/nllb-200-distilled-600M\", torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n+...\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\\ No newline at end of file"
        },
        {
            "sha": "7a0a3e4d250ed4a4e4137731b7e95bae75c1b9d7",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/61e98cb957862d679c4a338319a386da197b8073/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=61e98cb957862d679c4a338319a386da197b8073",
            "patch": "@@ -233,11 +233,13 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [Jamba](https://huggingface.co/docs/transformers/model_doc/jamba#transformers.JambaModel)\n * [Llama](https://huggingface.co/docs/transformers/model_doc/llama#transformers.LlamaModel)\n * [LLaVA-Onevision](https://huggingface.co/docs/transformers/model_doc/llava_onevision)\n+* [M2M100](https://huggingface.co/docs/transformers/model_doc/m2m_100#transformers.M2M100Model)\n * [Mimi](https://huggingface.co/docs/transformers/model_doc/mimi)\n * [Mistral](https://huggingface.co/docs/transformers/model_doc/mistral#transformers.MistralModel)\n * [Mixtral](https://huggingface.co/docs/transformers/model_doc/mixtral#transformers.MixtralModel)\n * [Musicgen](https://huggingface.co/docs/transformers/model_doc/musicgen#transformers.MusicgenModel)\n * [MusicGen Melody](https://huggingface.co/docs/transformers/model_doc/musicgen_melody#transformers.MusicgenMelodyModel)\n+* [NLLB](https://huggingface.co/docs/transformers/model_doc/nllb)\n * [OLMo](https://huggingface.co/docs/transformers/model_doc/olmo#transformers.OlmoModel)\n * [OLMoE](https://huggingface.co/docs/transformers/model_doc/olmoe#transformers.OlmoeModel)\n * [PaliGemma](https://huggingface.co/docs/transformers/model_doc/paligemma#transformers.PaliGemmaForConditionalGeneration)"
        },
        {
            "sha": "77386f0ff1ad9dca70203335a963738048a7639f",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 140,
            "deletions": 1,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/61e98cb957862d679c4a338319a386da197b8073/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/61e98cb957862d679c4a338319a386da197b8073/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=61e98cb957862d679c4a338319a386da197b8073",
            "patch": "@@ -24,7 +24,12 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n+from ...modeling_attn_mask_utils import (\n+    _prepare_4d_attention_mask,\n+    _prepare_4d_attention_mask_for_sdpa,\n+    _prepare_4d_causal_attention_mask,\n+    _prepare_4d_causal_attention_mask_for_sdpa,\n+)\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -428,6 +433,113 @@ def forward(\n         return attn_output, None, past_key_value\n \n \n+# Copied from transformers.models.bart.modeling_bart.BartSdpaAttention with Bart->M2M100\n+class M2M100SdpaAttention(M2M100Attention):\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+        if output_attentions or layer_head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"M2M100Model is using M2M100SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention\"\n+                ' implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                key_value_states=key_value_states,\n+                past_key_value=past_key_value,\n+                attention_mask=attention_mask,\n+                layer_head_mask=layer_head_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # get query proj\n+        query_states = self.q_proj(hidden_states)\n+        # get key, value proj\n+        # `past_key_value[0].shape[2] == key_value_states.shape[1]`\n+        # is checking that the `sequence_length` of the `past_key_value` is the same as\n+        # the provided `key_value_states` to support prefix tuning\n+        if (\n+            is_cross_attention\n+            and past_key_value is not None\n+            and past_key_value[0].shape[2] == key_value_states.shape[1]\n+        ):\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_value[0]\n+            value_states = past_key_value[1]\n+        elif is_cross_attention:\n+            # cross_attentions\n+            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n+        elif past_key_value is not None:\n+            # reuse k, v, self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n+            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n+        else:\n+            # self_attention\n+            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_states, value_states)\n+\n+        query_states = self._shape(query_states, tgt_len, bsz)\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case tgt_len == 1.\n+        is_causal = True if self.is_causal and attention_mask is None and tgt_len > 1 else False\n+\n+        # NOTE: SDPA with memory-efficient backend is currently (torch==2.1.2) bugged when using non-contiguous inputs and a custom attn_mask,\n+        # but we are fine here as `_shape` do call `.contiguous()`. Reference: https://github.com/pytorch/pytorch/issues/112577\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        if attn_output.size() != (bsz, self.num_heads, tgt_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+\n+        # Use the `embed_dim` from the config (stored in the class) rather than `hidden_state` because `attn_output` can be\n+        # partitioned across GPUs when using tensor-parallelism.\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None, past_key_value\n+\n+\n # Copied from transformers.models.mbart.modeling_mbart.MBartEncoderLayer with MBart->M2M100, MBART->M2M100\n class M2M100EncoderLayer(nn.Module):\n     def __init__(self, config: M2M100Config):\n@@ -502,6 +614,7 @@ def forward(\n M2M100_ATTENTION_CLASSES = {\n     \"eager\": M2M100Attention,\n     \"flash_attention_2\": M2M100FlashAttention2,\n+    \"sdpa\": M2M100SdpaAttention,\n }\n \n \n@@ -632,6 +745,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n     _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         std = self.config.init_std\n@@ -805,6 +919,7 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n         self.layers = nn.ModuleList([M2M100EncoderLayer(config) for _ in range(config.encoder_layers)])\n         self.layer_norm = nn.LayerNorm(config.d_model)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_sdpa = config._attn_implementation == \"sdpa\"\n \n         self.gradient_checkpointing = False\n         # Initialize weights and apply final processing\n@@ -887,6 +1002,11 @@ def forward(\n         if attention_mask is not None:\n             if self._use_flash_attention_2:\n                 attention_mask = attention_mask if 0 in attention_mask else None\n+            elif self._use_sdpa and head_mask is None and not output_attentions:\n+                # output_attentions=True & head_mask can not be supported when using SDPA, fall back to\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                attention_mask = _prepare_4d_attention_mask_for_sdpa(attention_mask, inputs_embeds.dtype)\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 attention_mask = _prepare_4d_attention_mask(attention_mask, inputs_embeds.dtype)\n@@ -981,6 +1101,7 @@ def __init__(self, config: M2M100Config, embed_tokens: Optional[nn.Embedding] =\n         )\n         self.layers = nn.ModuleList([M2M100DecoderLayer(config) for _ in range(config.decoder_layers)])\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+        self._use_sdpa = config._attn_implementation == \"sdpa\"\n         self.layer_norm = nn.LayerNorm(config.d_model)\n \n         self.gradient_checkpointing = False\n@@ -1094,6 +1215,15 @@ def forward(\n         if self._use_flash_attention_2:\n             # 2d mask is passed through the layers\n             combined_attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n+        elif self._use_sdpa and not output_attentions and cross_attn_head_mask is None:\n+            # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+            # the manual implementation that requires a 4D causal mask in all cases.\n+            combined_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                attention_mask,\n+                input_shape,\n+                inputs_embeds,\n+                past_key_values_length,\n+            )\n         else:\n             # 4d mask is passed through the layers\n             combined_attention_mask = _prepare_4d_causal_attention_mask(\n@@ -1104,6 +1234,15 @@ def forward(\n         if encoder_hidden_states is not None and encoder_attention_mask is not None:\n             if self._use_flash_attention_2:\n                 encoder_attention_mask = encoder_attention_mask if 0 in encoder_attention_mask else None\n+            elif self._use_sdpa and cross_attn_head_mask is None and not output_attentions:\n+                # output_attentions=True & cross_attn_head_mask can not be supported when using SDPA, and we fall back on\n+                # the manual implementation that requires a 4D causal mask in all cases.\n+                # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n+                encoder_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask,\n+                    inputs_embeds.dtype,\n+                    tgt_len=input_shape[-1],\n+                )\n             else:\n                 # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n                 encoder_attention_mask = _prepare_4d_attention_mask("
        }
    ],
    "stats": {
        "total": 179,
        "additions": 178,
        "deletions": 1
    }
}