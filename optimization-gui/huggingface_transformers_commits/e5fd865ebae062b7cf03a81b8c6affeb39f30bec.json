{
    "author": "yijun-lee",
    "message": "Add Gemma2 GGUF support (#34002)\n\n* initial setup for ggml.py\r\n\r\n* initial setup of GGUFGemma2Converter class\r\n\r\n* Add gemma2 model to gguf.md doc\r\n\r\n* Partial work on GGUF_TENSOR_MAPPING\r\n\r\n* initial setup of GGUF_TENSOR_MAPPING for Gemma2\r\n\r\n* refactor: rename GemmaConvert class to GemmaConverter for naming consistency\r\n\r\n* feat: complete gemma2 tensor mapping implementation\r\n\r\n* feat: add initial implementation of GGUFGemmaConverter\r\n\r\n* feat: complete GGUFGemmaConverter implementation\r\n\r\n* feat: add test code for gemma2\r\n\r\n* refactor: minor code cleanup\r\n\r\n* refactor: minor code cleanup\r\n\r\n* fix: resolve suggestions\r\n\r\n* Update tests/quantization/ggml/test_ggml.py\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Isotr0py <2037008807@qq.com>",
    "sha": "e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
    "files": [
        {
            "sha": "b1afd55c8952eeb5a16b16c35390ee8b68a022a4",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
            "patch": "@@ -88,6 +88,7 @@ For now the supported model architectures are the architectures that have been v\n - T5\n - Mamba\n - Nemotron\n+- Gemma2\n \n ## Example usage\n "
        },
        {
            "sha": "030e3a666436308a5bcbf199e466934a50258767",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
            "patch": "@@ -1271,7 +1271,7 @@ def post_processor(self):\n         )\n \n \n-class GemmaConvert(SpmConverter):\n+class GemmaConverter(SpmConverter):\n     handle_byte_fallback = True\n     SpmExtractor = GemmaSentencePieceExtractor\n     # start and end of turn tokens must be marked as special\n@@ -1601,7 +1601,7 @@ def converted(self) -> Tokenizer:\n     \"XGLMTokenizer\": XGLMConverter,\n     \"LlamaTokenizer\": LlamaConverter,\n     \"CodeLlamaTokenizer\": LlamaConverter,\n-    \"GemmaTokenizer\": GemmaConvert,\n+    \"GemmaTokenizer\": GemmaConverter,\n     \"Phi3Tokenizer\": LlamaConverter,\n }\n "
        },
        {
            "sha": "6bb939e6459239a0a8934d2c96d396522c7d559b",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 95,
            "deletions": 1,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
            "patch": "@@ -25,7 +25,7 @@\n from tokenizers.models import BPE, Unigram\n \n from .. import AddedToken\n-from ..convert_slow_tokenizer import GPT2Converter, LlamaConverter, Qwen2Converter, T5Converter\n+from ..convert_slow_tokenizer import GemmaConverter, GPT2Converter, LlamaConverter, Qwen2Converter, T5Converter\n from ..utils import logging\n from ..utils.logging import tqdm\n \n@@ -262,6 +262,22 @@\n         \"output.weight\": \"lm_head.weight\",\n         \"output_norm\": \"model.norm\",\n     },\n+    \"gemma2\": {\n+        \"token_embd\": \"model.embed_tokens\",\n+        \"blk\": \"model.layers\",\n+        \"ffn_up\": \"mlp.up_proj\",\n+        \"ffn_down\": \"mlp.down_proj\",\n+        \"ffn_gate\": \"mlp.gate_proj\",\n+        \"ffn_norm\": \"pre_feedforward_layernorm\",\n+        \"post_attention_norm\": \"post_attention_layernorm\",\n+        \"post_ffw_norm\": \"post_feedforward_layernorm\",\n+        \"attn_norm\": \"input_layernorm\",\n+        \"attn_q\": \"self_attn.q_proj\",\n+        \"attn_v\": \"self_attn.v_proj\",\n+        \"attn_k\": \"self_attn.k_proj\",\n+        \"attn_output\": \"self_attn.o_proj\",\n+        \"output_norm\": \"model.norm\",\n+    },\n }\n \n \n@@ -423,6 +439,18 @@\n         \"attention.layer_norm_rms_epsilon\": \"norm_eps\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"gemma2\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n }\n \n GGUF_TOKENIZER_MAPPING = {\n@@ -807,6 +835,71 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n+class GGUFGemmaConverter(GemmaConverter):\n+    def __init__(self, tokenizer_dict):\n+        # set dummy data to avoid unnecessary merges calculation\n+        tokenizer_dict[\"merges\"] = [\"dummy text\"]\n+\n+        self.proto = GGUFTokenizerSkeleton(tokenizer_dict)\n+        self.original_tokenizer = self.proto\n+        self.additional_kwargs = {}\n+\n+    def vocab(self, proto):\n+        original_vocab = list(zip(proto.tokens, proto.scores))\n+        updated_vocab = []\n+\n+        for token, score in original_vocab:\n+            if token == \"<0x09>\":\n+                updated_vocab.append((\"\\t\", score))\n+            elif \" \" in token and len(token.strip()) == 0:\n+                underscores = \"笆―" * len(token)\n+                updated_vocab.append((underscores, score))\n+            else:\n+                updated_vocab.append((token, score))\n+\n+        return updated_vocab\n+\n+    def normalizer(self, proto):\n+        return normalizers.Replace(\" \", \"笆―")\n+\n+    def decoder(self, replacement, add_prefix_space):\n+        sequence = [\n+            decoders.Replace(\"笆―", \" \"),\n+            decoders.ByteFallback(),\n+            decoders.Fuse(),\n+        ]\n+\n+        if add_prefix_space:\n+            sequence += [decoders.Strip(content=\" \", left=1)]\n+        return decoders.Sequence(sequence)\n+\n+    def converted(self) -> Tokenizer:\n+        vocab_scores = self.vocab(self.proto)\n+        tokenizer = Tokenizer(\n+            Unigram(\n+                vocab_scores,\n+                unk_id=self.proto.unk_token_id,\n+                byte_fallback=self.handle_byte_fallback,\n+            )\n+        )\n+\n+        normalizer = self.normalizer(self.proto)\n+        if normalizer is not None:\n+            tokenizer.normalizer = normalizer\n+\n+        replacement = \"笆―"\n+        add_prefix_space = True\n+        if hasattr(self.original_tokenizer, \"add_prefix_space\"):\n+            add_prefix_space = self.original_tokenizer.add_prefix_space\n+\n+        tokenizer.decoder = self.decoder(replacement, add_prefix_space)\n+        pre_tokenizer = self.pre_tokenizer(replacement, add_prefix_space)\n+        if pre_tokenizer is not None:\n+            tokenizer.pre_tokenizer = pre_tokenizer\n+\n+        return tokenizer\n+\n+\n GGUF_TO_FAST_CONVERTERS = {\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n@@ -820,6 +913,7 @@ def converted(self) -> Tokenizer:\n     \"t5\": GGUFT5Converter,\n     \"mamba\": GGUFGPTConverter,\n     \"nemotron\": GGUFGPTConverter,\n+    \"gemma2\": GGUFGemmaConverter,\n }\n \n "
        },
        {
            "sha": "5565fb152bc3a76fc2fd5ccec91f03469f5dd1a8",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
            "patch": "@@ -238,6 +238,18 @@ def process(self, weights, name, **kwargs):\n         return GGUFTensor(weights, name, {})\n \n \n+class Gemma2TensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    # ref: https://github.com/ggerganov/llama.cpp/blob/d79d8f39b4da6deca4aea8bf130c6034c482b320/convert_hf_to_gguf.py#L3191\n+    # ref: https://github.com/huggingface/transformers/blob/fc37f38915372c15992b540dfcbbe00a916d4fc6/src/transformers/models/gemma/modeling_gemma.py#L89\n+    def process(self, weights, name, **kwargs):\n+        if \"norm.weight\" in name:\n+            weights = weights - 1\n+        return GGUFTensor(weights, name, {})\n+\n+\n TENSOR_PROCESSORS = {\n     \"llama\": LlamaTensorProcessor,\n     \"qwen2moe\": Qwen2MoeTensorProcessor,\n@@ -246,6 +258,7 @@ def process(self, weights, name, **kwargs):\n     \"t5encoder\": T5TensorProcessor,\n     \"gpt2\": GPT2TensorProcessor,\n     \"mamba\": MambaTensorProcessor,\n+    \"gemma2\": Gemma2TensorProcessor,\n }\n \n "
        },
        {
            "sha": "1a9c126f3d0445afa22c4996fb450ad5c19382c9",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5fd865ebae062b7cf03a81b8c6affeb39f30bec/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=e5fd865ebae062b7cf03a81b8c6affeb39f30bec",
            "patch": "@@ -64,6 +64,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     mamba_model_id = \"jpodivin/mamba-2.8b-hf-GGUF\"\n     nemotron_original_model_id = \"nvidia/Nemotron-Mini-4B-Instruct\"\n     nemotron_model_id = \"bartowski/Nemotron-Mini-4B-Instruct-GGUF\"\n+    original_gemma2_model_id = \"google/gemma-2-2b-it\"\n+    gemma2_model_id = \"bartowski/gemma-2-2b-it-GGUF\"\n \n     # standard quants\n     q4_0_gguf_model_id = \"tinyllama-1.1b-chat-v1.0.Q4_0.gguf\"\n@@ -111,6 +113,9 @@ class GgufIntegrationTests(unittest.TestCase):\n     fp16_mamba_model_id = \"ggml-model-f16.gguf\"\n     q6_k_nemotron_model_id = \"Nemotron-Mini-4B-Instruct-Q6_K.gguf\"\n     fp16_nemotron_model_id = \"Nemotron-Mini-4B-Instruct-f16.gguf\"\n+    q3_k_gemma2_model_id = \"gemma-2-2b-it-Q3_K_L.gguf\"\n+    q8_0_gemma2_model_id = \"gemma-2-2b-it-Q8_0.gguf\"\n+    fp32_gemma2_model_id = \"gemma-2-2b-it-f32.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -833,6 +838,70 @@ def test_nemotron_q6_k(self):\n         EXPECTED_TEXT = \"'Hello. hotmail.com.'\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n \n+    def test_gemma2_q3_k(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma2_model_id,\n+            gguf_file=self.q3_k_gemma2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gemma2_model_id, gguf_file=self.q3_k_gemma2_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello! 汨欺\n\\nI'm trying to create a\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_gemma2_q8_0(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma2_model_id,\n+            gguf_file=self.q8_0_gemma2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gemma2_model_id, gguf_file=self.q8_0_gemma2_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello! 汨欺\n\\nI'm a large language model\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_gemma2_fp32(self):\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma2_model_id,\n+            gguf_file=self.fp32_gemma2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.gemma2_model_id, gguf_file=self.fp32_gemma2_model_id)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Hello! 汨欺\n\\nI'm a large language model\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_gemma2_weights_conversion_fp32(self):\n+        original_model = AutoModelForCausalLM.from_pretrained(\n+            self.original_gemma2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_model = AutoModelForCausalLM.from_pretrained(\n+            self.gemma2_model_id,\n+            gguf_file=self.fp32_gemma2_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        converted_state_dict = converted_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for layer_name, original_params in original_state_dict.items():\n+            if layer_name in converted_state_dict:\n+                self.assertTrue(original_params.shape == converted_state_dict[layer_name].shape)\n+                torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n+            else:\n+                raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n     def test_tokenization_xnli(self):\n         import tqdm\n         from datasets import load_dataset"
        }
    ],
    "stats": {
        "total": 183,
        "additions": 180,
        "deletions": 3
    }
}