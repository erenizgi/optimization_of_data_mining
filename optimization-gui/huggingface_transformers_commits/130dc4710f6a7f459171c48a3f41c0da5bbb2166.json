{
    "author": "marconaguib",
    "message": "fix : cast into floats AFTER all assignments (#42587)\n\nCo-authored-by: marconaguib <marco.naguib@aphp.fr>",
    "sha": "130dc4710f6a7f459171c48a3f41c0da5bbb2166",
    "files": [
        {
            "sha": "d880b7b751fe8e9a06baabc30b0e0aefb8105255",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/130dc4710f6a7f459171c48a3f41c0da5bbb2166/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/130dc4710f6a7f459171c48a3f41c0da5bbb2166/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=130dc4710f6a7f459171c48a3f41c0da5bbb2166",
            "patch": "@@ -711,9 +711,6 @@ def __post_init__(self):\n         if self.random_replace_prob < 0 or self.random_replace_prob > 1:\n             raise ValueError(\"random_replace_prob should be between 0 and 1.\")\n \n-        self.mask_replace_prob = float(self.mask_replace_prob)\n-        self.random_replace_prob = float(self.random_replace_prob)\n-\n         if self.whole_word_mask:\n             if not self.tokenizer.is_fast:\n                 warnings.warn(\n@@ -729,6 +726,9 @@ def __post_init__(self):\n                 self.mask_replace_prob = 1\n                 self.random_replace_prob = 0\n \n+        self.mask_replace_prob = float(self.mask_replace_prob)\n+        self.random_replace_prob = float(self.random_replace_prob)\n+\n         self.generator = None\n \n     def get_generator(self, seed):"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}