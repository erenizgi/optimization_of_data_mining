{
    "author": "FightingZhen",
    "message": "[bugfix] fix flash-attention2 unavailable error for Ascend NPU (#40151)\n\n* [bugfix] fix flash-attention2 unavailable error for Ascend NPU\n\n* remove redundant apply_rotary_emb usage\n\n* fix ruff check error\n\n* pad_input and unpad_input use same implementation as fa2\n\n* rollback redundant codes\n\n* fix ruff check error\n\n* optimize fa2 judgement logic",
    "sha": "41980ce93e775f6c88500c51c8db7946fc6a2add",
    "files": [
        {
            "sha": "f4c4e98da9b3b7248aed9626bff9cec7fc947ca5",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 17,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=41980ce93e775f6c88500c51c8db7946fc6a2add",
            "patch": "@@ -19,7 +19,7 @@\n \n \n if is_torch_npu_available():\n-    from torch_npu import npu_fusion_attention, npu_rotary_mul\n+    from torch_npu import npu_fusion_attention\n \n \n # FlashAttention2 is supported on Ascend NPU with down-right aligned causal mask by default.\n@@ -136,19 +136,3 @@ def npu_flash_attn_varlen_func(\n         )[0]\n \n     return output\n-\n-\n-def npu_apply_rotary_emb(x, cos, sin, **kwargs):\n-    # cos tensor after chunk should be repeated through chunked dimension to original shape on Ascend NPU\n-    if len(cos.shape) == 2 and cos.shape[-1] == x.shape[-1] // 2:\n-        cos = cos.repeat(1, 2)\n-        # cos tensor with [S,D] shape should be unsqueezed to 4-d tensor with shape [1,S,1,D]\n-        cos = cos.unsqueeze(0).unsqueeze(2)\n-\n-    # sin tensor after chunk should be repeated through chunked dimension to original shape on Ascend NPU\n-    if len(sin.shape) == 2 and sin.shape[-1] == x.shape[-1] // 2:\n-        sin = sin.repeat(1, 2)\n-        # sin tensor with [S,D] shape should be unsqueezed to 4-d tensor with shape [1,S,1,D]\n-        sin = sin.unsqueeze(0).unsqueeze(2)\n-\n-    return npu_rotary_mul(x, cos, sin)"
        },
        {
            "sha": "6b9e091c1a697ce9e53ce079c78929be1adeacce",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=41980ce93e775f6c88500c51c8db7946fc6a2add",
            "patch": "@@ -77,16 +77,20 @@ def _lazy_imports(implementation: Optional[str]):\n     \"\"\"\n     is_fa2 = is_flash_attn_2_available()\n     is_fa3 = is_flash_attn_3_available()\n-    if implementation == \"flash_attention_2\" or (implementation is None and is_fa2 and not is_fa3):\n+\n+    pad_input, unpad_input = _pad_input, _unpad_input\n+\n+    if (implementation == \"flash_attention_2\" and is_fa2) or (implementation is None and is_fa2 and not is_fa3):\n         from flash_attn import flash_attn_func, flash_attn_varlen_func\n         from flash_attn.bert_padding import pad_input, unpad_input\n+    elif is_torch_npu_available():\n+        # Package `flash-attn` is unavailable on Ascend NPU, which will cause ImportError\n+        # Flash-Attention2 related apis for Ascend NPU must be imported from `.integrations.npu_flash_attention` module\n+        from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n+        from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n     else:\n-        pad_input, unpad_input = _pad_input, _unpad_input\n         if implementation == \"flash_attention_3\" or (implementation is None and is_fa3):\n             from flash_attn_interface import flash_attn_func, flash_attn_varlen_func\n-        elif is_torch_npu_available():\n-            from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n-            from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n         # Kernels fallback\n         else:\n             flash_attn_func = getattr(implementation, \"flash_attn_func\", None)"
        },
        {
            "sha": "498f8db1d6106aa4866952266f17ac9382d2c3b5",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/41980ce93e775f6c88500c51c8db7946fc6a2add/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=41980ce93e775f6c88500c51c8db7946fc6a2add",
            "patch": "@@ -44,7 +44,6 @@\n from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -58,13 +57,6 @@\n from ...utils.hub import cached_file\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n-else:\n-    flash_attn_varlen_func = None\n-    apply_rotary_emb = None\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        }
    ],
    "stats": {
        "total": 40,
        "additions": 10,
        "deletions": 30
    }
}