{
    "author": "zucchini-nlp",
    "message": "Any to any pipeline and auto-mapping (#40884)\n\n* initial commit\n\n* fix tests\n\n* fix copies, tests and rename pipe\n\n* another rename\n\n* fix copies again\n\n* activate pipeline mixin in some models\n\n* audio loading\n\n* typo\n\n* fix the test\n\n* stupid typo in filename\n\n* fix copies\n\n* docs\n\n* forgot\n\n* fix pipe tests\n\n* fix copies\n\n* fix test\n\n* lets not pass it explicitly\n\n* final fix\n\n* rename in test files as well\n\n* fix again after reordering...\n\n* add qwen2 audio\n\n* add qwen3-omni\n\n* wait, I didn't push it last time?\n\n* it's only torch from now on\n\n* how was the model merged with docstring issues?\n\n* make style\n\n* requires backend depends on input modalities\n\n* add repr\n\n* fix copies\n\n* fox copies, new models were added\n\n* and now fix copies",
    "sha": "55b1400b74b7329f050f8e2bf7ca8534f2019295",
    "files": [
        {
            "sha": "8a3bd5617ef0a871c2b571aec8b8331346fc9415",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -302,6 +302,8 @@\n         title: Image tasks with IDEFICS\n       - local: tasks/image_text_to_text\n         title: Image-text-to-text\n+      - local: tasks/any_to_any\n+        title: Any-to-any\n       - local: tasks/video_text_to_text\n         title: Video-text-to-text\n       - local: tasks/visual_document_retrieval"
        },
        {
            "sha": "442ccd91998ddba621481aa3394d58f1dbb4c52e",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -485,6 +485,12 @@ Pipelines available for multimodal tasks include the following.\n     - __call__\n     - all\n \n+### AnyToAnyPipeline\n+\n+[[autodoc]] AnyToAnyPipeline\n+    - __call__\n+    - all\n+\n ### MaskGenerationPipeline\n \n [[autodoc]] MaskGenerationPipeline"
        },
        {
            "sha": "575c3081899565fe51ef8a40288cc8b7160685ba",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -241,6 +241,10 @@ The following auto classes are available for the following audio tasks.\n \n The following auto classes are available for the following multimodal tasks.\n \n+### AutoModelForMultimodalLM\n+\n+[[autodoc]] AutoModelForMultimodalLM\n+\n ### AutoModelForTableQuestionAnswering\n \n [[autodoc]] AutoModelForTableQuestionAnswering"
        },
        {
            "sha": "5a21c4422359ae1d29601f71cec4343fb3e601d0",
            "filename": "docs/source/en/tasks/any_to_any.md",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fany_to_any.md?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -0,0 +1,134 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Multimodal Generation\n+\n+[[open-in-colab]]\n+\n+Multimodal (any-to-any) models are language models capable of processing diverse types of input data (e.g., text, images, audio, or video) and generating outputs in any of these modalities. Unlike traditional unimodal or fixed-modality models, they allow flexible combinations of input and output, enabling a single system to handle a wide range of tasks: from text-to-image generation to audio-to-text transcription, image captioning, video understanding, and so on. This task shares many similarities with image-text-to-text, but supports a wider range of input and output modalities.\n+\n+In this guide, we provide a brief overview of any-to-any models and show how to use them with Transformers for inference. Unlike Vision LLMs, which are typically limited to vision-and-language tasks, omni-modal models can accept any combination of modalities (e.g., text, images, audio, video) as input, and generate outputs in different modalities, such as text or images.\n+\n+Let’s begin by installing dependencies:\n+\n+```bash\n+pip install -q transformers accelerate flash_attn\n+```\n+\n+Let's initialize the model and the processor.\n+\n+```python\n+from transformers import AutoProcessor, AutoModelForMultimodalLM, infer_device\n+import torch\n+\n+device = torch.device(infer_device())\n+model = AutoModelForMultimodalLM.from_pretrained(\n+    \"Qwen/Qwen2.5-Omni-3B\",\n+    dtype=torch.bfloat16,\n+    attn_implementation=\"flash_attention_2\",\n+).to(device)\n+\n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-3B\")\n+```\n+\n+These models typically include a [chat template](./chat_templating) to structure conversations across modalities. Inputs can mix images, text, audio, or other supported formats in a single turn. Outputs may also vary (e.g., text generation or audio generation), depending on the configuration.\n+\n+Below is an example providing a \"text + audio\" input and requesting a text response.\n+\n+```python\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"audio\", \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"},\n+            {\"type\": \"text\", \"text\": \"What do you hear in this audio?\"},\n+        ]\n+    },\n+]\n+```\n+\n+We will now call the processors' [`~ProcessorMixin.apply_chat_template`] method to preprocess its output along with the image inputs.\n+\n+```python\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    add_generation_prompt=True,\n+)\n+```\n+\n+We can now pass the preprocessed inputs to the model.\n+\n+```python\n+with torch.no_grad():\n+    generated_ids = model.generate(**inputs, max_new_tokens=100)\n+generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+print(generated_texts)\n+```\n+\n+## Pipeline\n+\n+The fastest way to get started is to use the [`Pipeline`] API. Specify the `\"any-to-any\"` task and the model you want to use.\n+\n+```python\n+from transformers import pipeline\n+pipe = pipeline(\"any-to-any\", model=\"mistralai/Voxtral-Mini-3B-2507\")\n+```\n+\n+The example below uses chat templates to format the text inputs and uses audio modality as an multimodal data.\n+\n+```python\n+messages = [\n+     {\n+         \"role\": \"user\",\n+         \"content\": [\n+             {\n+                 \"type\": \"audio\",\n+                 \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n+             },\n+             {\"type\": \"text\", \"text\": \"What do you hear in this audio?\"},\n+         ],\n+     },\n+ ]\n+```\n+\n+Pass the chat template formatted text and image to [`Pipeline`] and set `return_full_text=False` to remove the input from the generated output.\n+\n+```python\n+outputs = pipe(text=messages, max_new_tokens=20, return_full_text=False)\n+outputs[0][\"generated_text\"]\n+```\n+\n+Any-to-any pipeline also supports generating audio or images with any-to-any models. For that you need to set `generation_mode` parameter. Do not forget to set video sampling to the desired FPS, otherwise the whole video will be loaded without sampling. Here is an example code:\n+\n+```python\n+import soundfile as sf\n+pipe = pipeline(\"any-to-any\", model=\"Qwen/Qwen2.5-Omni-3B\")\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"path\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Cooking_cake.mp4\"},\n+            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n+        ],\n+    },\n+]\n+output = pipe(text=messages, fps=1, load_audio_from_video=True, max_new_tokens=20, generation_mode=\"audio\")\n+sf.write(\"generated_audio.wav\", out[0][\"generated_audio\"])\n+```\n+"
        },
        {
            "sha": "de7f684e8addde7389ac8b6dac73c9e870d580b7",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -130,6 +130,7 @@\n     \"loss\": [],\n     \"modelcard\": [\"ModelCard\"],\n     \"pipelines\": [\n+        \"AnyToAnyPipeline\",\n         \"AudioClassificationPipeline\",\n         \"AutomaticSpeechRecognitionPipeline\",\n         \"CsvPipelineDataFormat\",\n@@ -636,6 +637,7 @@\n     from .optimization import get_wsd_schedule as get_wsd_schedule\n \n     # Pipelines\n+    from .pipelines import AnyToAnyPipeline as AnyToAnyPipeline\n     from .pipelines import AudioClassificationPipeline as AudioClassificationPipeline\n     from .pipelines import AutomaticSpeechRecognitionPipeline as AutomaticSpeechRecognitionPipeline\n     from .pipelines import CsvPipelineDataFormat as CsvPipelineDataFormat"
        },
        {
            "sha": "c90b428acd972a47a3d6afa30babfa0ad61dc3f7",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -19,6 +19,7 @@\n \n import numpy as np\n \n+from .audio_utils import is_valid_audio, load_audio\n from .feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n from .utils import PaddingStrategy, TensorType, is_torch_tensor, logging, to_numpy\n \n@@ -366,3 +367,19 @@ def _get_padding_strategies(self, padding=False, max_length=None):\n             )\n \n         return padding_strategy\n+\n+    def fetch_audio(self, audio_url_or_urls: Union[str, list[str], list[list[str]]]):\n+        \"\"\"\n+        Convert a single or a list of urls into the corresponding `np.ndarray` objects.\n+\n+        If a single url is passed, the return value will be a single object. If a list is passed a list of objects is\n+        returned.\n+        \"\"\"\n+        if isinstance(audio_url_or_urls, list):\n+            return [self.fetch_audio(x) for x in audio_url_or_urls]\n+        elif isinstance(audio_url_or_urls, str):\n+            return load_audio(audio_url_or_urls)\n+        elif is_valid_audio(audio_url_or_urls):\n+            return audio_url_or_urls\n+        else:\n+            raise TypeError(f\"only a single or a list of entries is supported but got type={type(audio_url_or_urls)}\")"
        },
        {
            "sha": "b64f455178b755e9f92dd40d5f671196861aadac",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -367,7 +367,7 @@ class GenerationMixin(ContinuousMixin):\n     \"\"\"\n \n     # Should be overwritten by models that can generate non-text output\n-    output_modalities = \"text\"\n+    output_modalities = (\"text\",)\n \n     def adjust_generation_fn(\n         self,"
        },
        {
            "sha": "2ce372c1040a125f7b3e56b0facd6d2ed560668e",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -394,7 +394,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n \n     config: Aimv2Config\n     base_model_prefix = \"aimv2\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Aimv2EncoderLayer\","
        },
        {
            "sha": "87d46fae9b349ce32b9c7d6310e3c6c30f142401",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -437,7 +437,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n \n     config: Aimv2Config\n     base_model_prefix = \"aimv2\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Aimv2EncoderLayer\","
        },
        {
            "sha": "d136a10010755dc2eabf94a6e15e437e7bce3971",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -821,7 +821,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class AlignPreTrainedModel(PreTrainedModel):\n     config: AlignConfig\n     base_model_prefix = \"align\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -853,7 +853,7 @@ def _init_weights(self, module: nn.Module):\n )\n class AlignTextModel(AlignPreTrainedModel):\n     config: AlignTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     _no_split_modules = [\"AlignTextEmbeddings\"]\n \n     def __init__(self, config: AlignTextConfig, add_pooling_layer: bool = True):\n@@ -974,7 +974,7 @@ def forward(\n class AlignVisionModel(AlignPreTrainedModel):\n     config: AlignVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n \n     def __init__(self, config: AlignVisionConfig):"
        },
        {
            "sha": "254a4fc294d5a48c8f60b675c61fa9fc13e26365",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -767,7 +767,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n class AltCLIPPreTrainedModel(PreTrainedModel):\n     config: AltCLIPConfig\n     base_model_prefix = \"altclip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_module = []\n \n@@ -872,7 +872,7 @@ def forward(\n class AltCLIPVisionModel(AltCLIPPreTrainedModel):\n     config: AltCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: AltCLIPVisionConfig):\n         super().__init__(config)\n@@ -1031,7 +1031,7 @@ def forward(\n \n class AltCLIPTextModel(AltCLIPPreTrainedModel):\n     config: AltCLIPTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "801b1ae989fa4551f19d352beeb2516295ead37f",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -573,7 +573,7 @@ def forward(\n class AriaTextPreTrainedModel(PreTrainedModel):\n     config: AriaTextConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "d2e15336832f5852c07f83ff4a7ece604ccb513c",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1184,7 +1184,7 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n class AriaTextPreTrainedModel(PreTrainedModel):\n     config: AriaTextConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "72f3356c154468b8a34ce33e096513693eefc032",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -257,7 +257,7 @@ def forward(\n class AudioFlamingo3PreTrainedModel(PreTrainedModel):\n     config: AudioFlamingo3Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"AudioFlamingo3Attention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "55b6a1147ecadbf6faa353ada3088909aa085111",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1029,6 +1029,21 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n     ]\n )\n \n+# Models that accept text and optionally multimodal data in inputs\n+# and can generate text and optionally multimodal data.\n+MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES = OrderedDict(\n+    [\n+        *list(MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES.items()),\n+        (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),\n+        (\"kyutai_speech_to_text\", \"KyutaiSpeechToTextForConditionalGeneration\"),\n+        (\"phi4_multimodal\", \"Phi4MultimodalForCausalLM\"),\n+        (\"qwen2_5_omni\", \"Qwen2_5OmniForConditionalGeneration\"),\n+        (\"qwen2_audio\", \"Qwen2AudioForConditionalGeneration\"),\n+        (\"qwen3_omni_moe\", \"Qwen3OmniMoeForConditionalGeneration\"),\n+        (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n+    ]\n+)\n+\n MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Masked LM mapping\n@@ -1782,6 +1797,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING = _LazyAutoMapping(\n     CONFIG_MAPPING_NAMES, MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\n )\n+MODEL_FOR_MULTIMODAL_LM_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES)\n MODEL_FOR_RETRIEVAL_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_RETRIEVAL_MAPPING_NAMES)\n MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n     CONFIG_MAPPING_NAMES, MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING_NAMES\n@@ -2126,6 +2142,13 @@ def from_pretrained(\n AutoModelForImageTextToText = auto_class_update(AutoModelForImageTextToText, head_doc=\"image-text-to-text modeling\")\n \n \n+class AutoModelForMultimodalLM(_BaseAutoModelClass):\n+    _model_mapping = MODEL_FOR_MULTIMODAL_LM_MAPPING\n+\n+\n+AutoModelForMultimodalLM = auto_class_update(AutoModelForMultimodalLM, head_doc=\"multimodal generation\")\n+\n+\n class AutoModelForAudioClassification(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n \n@@ -2276,6 +2299,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"MODEL_FOR_VISION_2_SEQ_MAPPING\",\n     \"MODEL_FOR_RETRIEVAL_MAPPING\",\n     \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\",\n+    \"MODEL_FOR_MULTIMODAL_LM_MAPPING\",\n     \"MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\",\n     \"MODEL_MAPPING\",\n     \"MODEL_WITH_LM_HEAD_MAPPING\",\n@@ -2303,6 +2327,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n     \"AutoModelForMaskedImageModeling\",\n     \"AutoModelForMaskedLM\",\n     \"AutoModelForMultipleChoice\",\n+    \"AutoModelForMultimodalLM\",\n     \"AutoModelForNextSentencePrediction\",\n     \"AutoModelForObjectDetection\",\n     \"AutoModelForPreTraining\","
        },
        {
            "sha": "843ea4a358a11339921b63be93bed6cedbfe593f",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -823,7 +823,7 @@ def forward(\n class AutoformerPreTrainedModel(PreTrainedModel):\n     config: AutoformerConfig\n     base_model_prefix = \"model\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     main_input_name = \"past_values\"\n     supports_gradient_checkpointing = True\n "
        },
        {
            "sha": "0e21032c38c326109fed7aab63d363a055d26492",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -91,7 +91,7 @@ def pixel_shuffle(self, image_features):  # B, S, D\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config: AyaVisionConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "ee0c5d6f81ad4ac7b012afd11f0c1cce31463831",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -353,7 +353,7 @@ def device(self) -> torch.device:\n # GPT2-like autoregressive model\n class BarkCausalModel(BarkPreTrainedModel, GenerationMixin):\n     config: BarkSubModelConfig\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "8698ad95913c11bc71c65ed148b793b9ea70b4b1",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -668,7 +668,7 @@ def forward(\n class BeitPreTrainedModel(PreTrainedModel):\n     config: BeitConfig\n     base_model_prefix = \"beit\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BeitLayer\"]"
        },
        {
            "sha": "9074afe35a89c5e0ae4375534c6415af62fdf43f",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -625,7 +625,7 @@ def forward(\n class BitPreTrainedModel(PreTrainedModel):\n     config: BitConfig\n     base_model_prefix = \"bit\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     main_input_name = \"pixel_values\"\n     _no_split_modules = [\"BitEmbeddings\"]\n "
        },
        {
            "sha": "e0268b73f6920ca50a0a6b3b13035f8bfe573e5d",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -415,7 +415,7 @@ def forward(\n class BlipPreTrainedModel(PreTrainedModel):\n     config: BlipConfig\n     base_model_prefix = \"blip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BlipEncoderLayer\", \"BlipTextEmbeddings\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n@@ -466,7 +466,7 @@ def forward(\n \n class BlipVisionModel(BlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     config: BlipVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": BlipEncoderLayer,"
        },
        {
            "sha": "d38112f1fbf895bafe2acfbbefc0874283b50321",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -392,7 +392,7 @@ def forward(\n class Blip2PreTrainedModel(PreTrainedModel):\n     config: Blip2Config\n     base_model_prefix = \"blip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True\n@@ -467,7 +467,7 @@ def forward(\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->Blip2, BLIP->BLIP_2\n class Blip2VisionModel(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     config: Blip2VisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": Blip2EncoderLayer,\n@@ -1441,7 +1441,7 @@ def forward(\n @auto_docstring\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n \n@@ -1904,7 +1904,7 @@ def generate(\n )\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n     _supports_flash_attn = False  # because self.qformer does not support FA2\n "
        },
        {
            "sha": "03fb85630050cc8e0ba5f3c7fa7abeda37fa0c7c",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -431,7 +431,7 @@ def forward(\n class BltPreTrainedModel(PreTrainedModel):\n     config: BltConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BltTransformerLayer\"]\n     _can_compile_fullgraph = False  # static cache cannot have different shapes for each layer"
        },
        {
            "sha": "b83fbf4826073404d46aca324f8639de9120b44d",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -915,7 +915,7 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n class BridgeTowerPreTrainedModel(PreTrainedModel):\n     config: BridgeTowerConfig\n     base_model_prefix = \"bridgetower\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"BridgeTowerSelfAttention\", \"BridgeTowerResidualAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -950,7 +950,7 @@ def _init_weights(self, module: nn.Module):\n \n class BridgeTowerVisionModel(BridgeTowerPreTrainedModel):\n     config: BridgeTowerVisionConfig\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config):\n         super().__init__(config)\n@@ -980,7 +980,7 @@ def forward(self, image, image_mask=None, interpolate_pos_encoding=False):\n )\n class BridgeTowerTextModel(BridgeTowerPreTrainedModel):\n     config: BridgeTowerTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\""
        },
        {
            "sha": "2e816ad63b4d54867d8e998b2683de2f12912340",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -772,7 +772,7 @@ def convert_img2bpe(self, img_batch: torch.Tensor) -> torch.Tensor:\n class ChameleonPreTrainedModel(PreTrainedModel):\n     config: ChameleonConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ChameleonDecoderLayer\", \"ChameleonSwinDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]"
        },
        {
            "sha": "957615c923ec0fd199cff7aa4f1763305f2d7eaa",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -560,7 +560,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class ChineseCLIPPreTrainedModel(PreTrainedModel):\n     config: ChineseCLIPConfig\n     base_model_prefix = \"chinese_clip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -798,7 +798,7 @@ class ChineseCLIPTextModel(ChineseCLIPPreTrainedModel):\n     \"\"\"\n \n     config: ChineseCLIPTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     _no_split_modules = [\"ChineseCLIPTextEmbeddings\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n@@ -906,7 +906,7 @@ def forward(\n class ChineseCLIPVisionModel(ChineseCLIPPreTrainedModel):\n     config: ChineseCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"ChineseCLIPVisionEmbeddings\", \"ChineseCLIPVisionAttention\"]\n \n     def __init__(self, config: ChineseCLIPVisionConfig):"
        },
        {
            "sha": "1421bdb83c9d2dac6a74fdf254ffd8fd56b48760",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1306,7 +1306,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class ClapPreTrainedModel(PreTrainedModel):\n     config: ClapConfig\n     base_model_prefix = \"clap\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = False\n \n     @torch.no_grad()\n@@ -1410,7 +1410,7 @@ def forward(\n )\n class ClapTextModel(ClapPreTrainedModel):\n     config: ClapTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config, add_pooling_layer=True):\n         r\"\"\"\n@@ -1715,7 +1715,7 @@ def forward(\n @auto_docstring\n class ClapTextModelWithProjection(ClapPreTrainedModel):\n     config: ClapTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: ClapTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "05109a841cfab149245dfab729cc951c1e725f98",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -398,7 +398,7 @@ def forward(\n class CLIPPreTrainedModel(PreTrainedModel):\n     config: CLIPConfig\n     base_model_prefix = \"clip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n@@ -597,7 +597,7 @@ def forward(\n )\n class CLIPTextModel(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n@@ -693,7 +693,7 @@ def forward(\n class CLIPVisionModel(CLIPPreTrainedModel):\n     config: CLIPVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"CLIPEncoderLayer\"]\n \n     def __init__(self, config: CLIPVisionConfig):\n@@ -942,7 +942,7 @@ def forward(\n @auto_docstring\n class CLIPTextModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     _no_split_modules = [\"CLIPTextEmbeddings\", \"CLIPEncoderLayer\"]\n \n@@ -1008,7 +1008,7 @@ def forward(\n class CLIPVisionModelWithProjection(CLIPPreTrainedModel):\n     config: CLIPVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: CLIPVisionConfig):\n         super().__init__(config)\n@@ -1075,7 +1075,7 @@ def forward(\n )\n class CLIPForImageClassification(CLIPPreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: CLIPConfig) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "af7f0324142ad627c7f2fb8eb3b0464584255f45",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -425,7 +425,7 @@ def forward(\n class CLIPSegPreTrainedModel(PreTrainedModel):\n     config: CLIPSegConfig\n     base_model_prefix = \"clip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -651,7 +651,7 @@ def forward(\n \n class CLIPSegTextModel(CLIPSegPreTrainedModel):\n     config: CLIPSegTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     _no_split_modules = [\"CLIPSegTextEmbeddings\", \"CLIPSegEncoderLayer\"]\n \n@@ -757,7 +757,7 @@ def forward(\n class CLIPSegVisionModel(CLIPSegPreTrainedModel):\n     config: CLIPSegVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: CLIPSegVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "45ebf5af45166d2be250955b426ba470980fdfd1",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -130,7 +130,7 @@ class Cohere2VisionCausalLMOutputWithPast(ModelOutput):\n class Cohere2VisionPreTrainedModel(PreTrainedModel):\n     config: Cohere2VisionConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "c31195f249a1330da7af9bcc17521d2342e2fba7",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -33,7 +33,7 @@\n class ColPaliPreTrainedModel(PreTrainedModel):\n     config: ColPaliConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "383cde0b8cb974e695a102e44b904e5e386a154f",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -41,7 +41,7 @@\n class ColQwen2PreTrainedModel(PreTrainedModel):\n     config: ColQwen2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _no_split_modules = []\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "0b0336505db75ce4dfed5ea24f43bbdee81694ba",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -968,7 +968,7 @@ class ConditionalDetrPreTrainedModel(PreTrainedModel):\n     config: ConditionalDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"ConditionalDetrConvEncoder\", r\"ConditionalDetrEncoderLayer\", r\"ConditionalDetrDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "8d1188dea3b8ee9c036fc376f2b80f221c29c509",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -237,7 +237,7 @@ class ConvNextPreTrainedModel(PreTrainedModel):\n     config: ConvNextConfig\n     base_model_prefix = \"convnext\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"ConvNextLayer\"]\n     _can_record_outputs = {}  # hidden states are collected explicitly\n "
        },
        {
            "sha": "b7774fcd4d1137d5940cb04c7ec9a80feb7c83dc",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -258,7 +258,7 @@ class ConvNextV2PreTrainedModel(PreTrainedModel):\n     config: ConvNextV2Config\n     base_model_prefix = \"convnextv2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"ConvNextV2Layer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "2ba3394b4416d958675810992e569b4e1c728c85",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -395,7 +395,7 @@ def forward(\n class CsmPreTrainedModel(PreTrainedModel):\n     config: CsmConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "cbff1111587b66cb59b0684c5d8d87d4abf47a03",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -126,7 +126,7 @@ class CsmDecoderLayer(LlamaDecoderLayer):\n class CsmPreTrainedModel(PreTrainedModel):\n     config: CsmConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]"
        },
        {
            "sha": "9c1f0d25de3ed52a0b6039fb2f10bd87ef1ee02a",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -441,7 +441,7 @@ class DFinePreTrainedModel(PreTrainedModel):\n     config: DFineConfig\n     base_model_prefix = \"d_fine\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"DFineHybridEncoder\", r\"DFineDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "f2ac2916d7570e3e7c7010c429484a9537a79414",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -813,7 +813,7 @@ class DabDetrPreTrainedModel(PreTrainedModel):\n     config: DabDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"DabDetrConvEncoder\", r\"DabDetrEncoderLayer\", r\"DabDetrDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "16d6bb4ff685691c5ed43eed1ba2637a9a31f089",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -682,7 +682,7 @@ def forward(\n class Data2VecVisionPreTrainedModel(PreTrainedModel):\n     config: Data2VecVisionConfig\n     base_model_prefix = \"data2vec_vision\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Data2VecVisionLayer\"]"
        },
        {
            "sha": "f568477303c045c90a22079285e30331133d739a",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -122,7 +122,7 @@ def forward(self, vision_encodings: torch.Tensor) -> torch.Tensor:\n class DeepseekVLPreTrainedModel(PreTrainedModel):\n     config: DeepseekVLConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -235,7 +235,7 @@ def forward(\n \n class DeepseekVLForConditionalGeneration(DeepseekVLPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n-    output_modalities = \"text\"\n+    output_modalities = (\"text\",)\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLConfig):"
        },
        {
            "sha": "f251b393f089b1a4dac65ef6d4ae244ca6428483",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -157,7 +157,7 @@ def __init__(self, config):\n \n \n class DeepseekVLForConditionalGeneration(JanusForConditionalGeneration):\n-    output_modalities = \"text\"\n+    output_modalities = (\"text\",)\n \n     def prepare_embeddings_for_image_generation(self):\n         raise AttributeError(\"Not needed for DeepseekVL\")"
        },
        {
            "sha": "c0204fe94382a3b2b93a174973277bb1521224d1",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -205,7 +205,7 @@ def forward(\n class DeepseekVLHybridPreTrainedModel(PreTrainedModel):\n     config: DeepseekVLHybridConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -390,7 +390,7 @@ def get_high_res_image_features(self, pixel_values):\n \n class DeepseekVLHybridForConditionalGeneration(DeepseekVLHybridPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n-    output_modalities = \"text\"\n+    output_modalities = (\"text\",)\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: DeepseekVLHybridConfig):"
        },
        {
            "sha": "68cb0456ddac0e5b2fb21180aedc94a1031a1b8d",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -919,7 +919,7 @@ class DeformableDetrPreTrainedModel(PreTrainedModel):\n     config: DeformableDetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         r\"DeformableDetrConvEncoder\","
        },
        {
            "sha": "7f7cffa4f786d2cded3c88062594f48c74322e56",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -355,7 +355,7 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     config: DeiTConfig\n     base_model_prefix = \"deit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "e13499b24fbd946f784e848b08077462dc820c3f",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -213,7 +213,7 @@ class DepthAnythingPreTrainedModel(PreTrainedModel):\n     config: DepthAnythingConfig\n     base_model_prefix = \"depth_anything\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "75b8c213e1f0f261e4491cb49e3d8b647e0a744f",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -603,7 +603,7 @@ class DepthProPreTrainedModel(PreTrainedModel):\n     config: DepthProConfig\n     base_model_prefix = \"depth_pro\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _no_split_modules = [\"DepthProPreActResidualLayer\"]"
        },
        {
            "sha": "7f5aa1a458e688be15c094ea20b34283274003db",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -725,7 +725,7 @@ class DetrPreTrainedModel(PreTrainedModel):\n     config: DetrConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"DetrConvEncoder\", r\"DetrEncoderLayer\", r\"DetrDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "c2e96fba9fb30ddac49dd1c52e1670e3fe2e2ca7",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -805,7 +805,7 @@ def forward(\n )\n class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n     base_model_prefix = \"model\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: DiaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "b15ba8b0dbb3e412b8562959767ede999b716b54",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -597,7 +597,7 @@ def forward(\n )\n class DiaForConditionalGeneration(DiaPreTrainedModel, DiaGenerationMixin):\n     base_model_prefix = \"model\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: DiaConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c9133497f5bc4c7dff61c74c4783ebeede14a9b3",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -559,7 +559,7 @@ class DinatPreTrainedModel(PreTrainedModel):\n     config: DinatConfig\n     base_model_prefix = \"dinat\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n \n @auto_docstring"
        },
        {
            "sha": "be5c46d776150debd88dad7905cc6bc1a64ceb1a",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -404,7 +404,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     config: Dinov2Config\n     base_model_prefix = \"dinov2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2Layer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "5049969f437b75e5703395b07cdaa0ff8b0b9f30",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -421,7 +421,7 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     config: Dinov2WithRegistersConfig\n     base_model_prefix = \"dinov2_with_registers\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2WithRegistersLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "49bc6fc17e4d7f93c17800780a45dc386b0ecd3a",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -189,7 +189,7 @@ class DINOv3ConvNextPreTrainedModel(PreTrainedModel):\n     config: DINOv3ConvNextConfig\n     base_model_prefix = \"dinov3_convnext\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"DINOv3ConvNextLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "473590315431b9e2c9410a938ce51579cccf1d75",
            "filename": "src/transformers/models/dinov3_vit/modeling_dinov3_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fmodeling_dinov3_vit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -437,7 +437,7 @@ class DINOv3ViTPreTrainedModel(PreTrainedModel):\n     config: DINOv3ViTConfig\n     base_model_prefix = \"dinov3_vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DINOv3ViTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "9886488c0542332c31fabb4d55cfc6ca5adb7358",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -786,7 +786,7 @@ class DonutSwinPreTrainedModel(PreTrainedModel):\n     config: DonutSwinConfig\n     base_model_prefix = \"donut\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DonutSwinStage\"]\n "
        },
        {
            "sha": "fb871c90d1e3b8fe82bd60d92757b4f106faa504",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -723,7 +723,7 @@ class DPTPreTrainedModel(PreTrainedModel):\n     config: DPTConfig\n     base_model_prefix = \"dpt\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "11c2e130ce6e763b64c53d2f0e5141c91cb4fd5e",
            "filename": "src/transformers/models/edgetam/modeling_edgetam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodeling_edgetam.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -304,7 +304,7 @@ class EdgeTamPreTrainedModel(PreTrainedModel):\n     config_class = EdgeTamConfig\n     base_model_prefix = \"edgetam\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -911,7 +911,7 @@ def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n     \"\"\"\n )\n class EdgeTamModel(EdgeTamPreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(EdgeTamTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = [\n         r\"^memory_.*\","
        },
        {
            "sha": "756a14397d0de327f1436bf9660b693519b039fc",
            "filename": "src/transformers/models/edgetam_video/modeling_edgetam_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam_video%2Fmodeling_edgetam_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1973,7 +1973,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class EdgeTamVideoModel(EdgeTamVideoPreTrainedModel):\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(EdgeTamVideoTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = []\n     _tied_weights_keys = {"
        },
        {
            "sha": "bcf0da0da2af015857bc36975c21b073003ba1d2",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -668,7 +668,7 @@ class EfficientLoFTRPreTrainedModel(PreTrainedModel):\n     config_class = EfficientLoFTRConfig\n     base_model_prefix = \"efficientloftr\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "dc535f052208259b015aff5b5db6fbd8e9895840",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -434,7 +434,7 @@ class EfficientNetPreTrainedModel(PreTrainedModel):\n     config: EfficientNetConfig\n     base_model_prefix = \"efficientnet\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = []\n \n     @torch.no_grad()"
        },
        {
            "sha": "2a3e06b0fd6ca958a25027a40fdffb46ad95cc41",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -927,7 +927,7 @@ class Emu3VQVAE(PreTrainedModel):\n     config: Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -1095,7 +1095,7 @@ def convert_bpe2img(self, img_batch: torch.Tensor) -> torch.Tensor:\n class Emu3PreTrainedModel(PreTrainedModel):\n     config: Emu3Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Emu3DecoderLayer\",\n@@ -1484,7 +1484,7 @@ def forward(\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    output_modalities = [\"image\", \"text\"]\n+    output_modalities = (\"image\", \"text\")\n     _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\","
        },
        {
            "sha": "bd6b3013c6442af80ebc95287992dbb6702ebac3",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -677,7 +677,7 @@ class Emu3VQVAE(PreTrainedModel):\n     config: Emu3VQVAEConfig\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -1038,7 +1038,7 @@ def forward(\n \n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    output_modalities = [\"image\", \"text\"]\n+    output_modalities = (\"image\", \"text\")\n     _tied_weights_keys = {\"lm_head.weight\": \"model.text_model.embed_tokens.weight\"}\n     _checkpoint_conversion_mapping = {\n         \"^text_model.model\": \"model.text_model\","
        },
        {
            "sha": "568190295d533302dc7980dcd361a4793e90d14f",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -230,5 +230,40 @@ def calculate_generate_size(self, ratio, image_area, spatial_factor):\n     def postprocess(self, images: ImageInput, **kwargs):\n         return self.image_processor.postprocess(images, **kwargs)\n \n+    def post_process_multimodal_output(\n+        self, generated_outputs, skip_special_tokens=True, generation_mode=None, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of a multimodal model to return the requested modality output.\n+        If the model cannot generated the requested modality, an error will be raised.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            generation_mode (`str`, *optional*):\n+                Generation mode indicated which modality to output and can be one of `[\"text\", \"image\", \"audio\"]`.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[Union[str, PIL.Image.Image]]`: The decoded text or generated image.\n+        \"\"\"\n+        if generation_mode is None or generation_mode == \"text\":\n+            return self.post_process_image_text_to_text(\n+                generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs\n+            )\n+\n+        elif generation_mode == \"image\":\n+            images = self.postprocess(generated_outputs, return_tensors=\"PIL.Image.Image\")\n+            return images[\"pixel_values\"]\n+\n+        else:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} got an unexpected generation_mode={generation_mode}. Supported options are only `text` and `image\"\n+            )\n+\n \n __all__ = [\"Emu3Processor\"]"
        },
        {
            "sha": "bf0bf12ed7c4dd1392562c1ea6872022a466637d",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -988,7 +988,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     config: EomtConfig\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "2d45abe42d93a0df733719cd1099f08638160f33",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -393,7 +393,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     config: EomtConfig\n     base_model_prefix = \"eomt\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "c6ffb49febd052a5cc0230dfee4386a9ac32d369",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -663,7 +663,7 @@ def forward(self, hidden_states: torch.Tensor):\n class FlavaPreTrainedModel(PreTrainedModel):\n     config: FlavaConfig\n     base_model_prefix = \"flava\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -690,7 +690,7 @@ class FlavaImageModel(FlavaPreTrainedModel):\n     # This override allows us to load FlavaImageModel from FlavaModel/FlavaForPreTraining checkpoints.\n     base_model_prefix = \"flava.image_model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: FlavaImageConfig, add_pooling_layer: bool = True):\n         r\"\"\"\n@@ -770,7 +770,7 @@ class FlavaTextModel(FlavaPreTrainedModel):\n     config: FlavaTextConfig\n     # This override allows us to load FlavaTextModel from FlavaModel/FlavaForPreTraining checkpoints.\n     base_model_prefix = \"flava.text_model\"\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: FlavaTextConfig, add_pooling_layer: bool = True):\n         r\"\"\"\n@@ -1301,7 +1301,7 @@ class FlavaImageCodebook(FlavaPreTrainedModel):\n     base_model_prefix = \"model\"\n     config: FlavaImageCodebookConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n \n     def __init__("
        },
        {
            "sha": "9212cd5d8f5f01ab2926e76c1aea0b89fc36eca6",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -485,7 +485,7 @@ def forward(self, hidden_states: torch.Tensor):\n class Florence2VisionPreTrainedModel(PreTrainedModel):\n     config_class = Florence2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -616,7 +616,7 @@ class Florence2Seq2SeqLMOutput(Seq2SeqLMOutput):\n class Florence2PreTrainedModel(PreTrainedModel):\n     config: Florence2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "74ace79943e712f870c37d901c8197638e7761cf",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1366,7 +1366,7 @@ def forward(self, hidden_states: torch.Tensor):\n class Florence2VisionPreTrainedModel(PreTrainedModel):\n     config_class = Florence2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "2bd58b0e6f380867aea043815d3521db4e98e48b",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -35,7 +35,7 @@\n class FuyuPreTrainedModel(PreTrainedModel):\n     config: FuyuConfig\n     base_model_prefix = \"fuyu\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "97a881c5edc8f67815810da1693f9c7ecd160f68",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -465,7 +465,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n         \"hidden_states\": Gemma3DecoderLayer,\n         \"attentions\": Gemma3Attention,\n     }\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n@@ -493,7 +493,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n @auto_docstring\n class Gemma3TextModel(Gemma3PreTrainedModel):\n     config: Gemma3TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)\n@@ -1348,7 +1348,7 @@ class Gemma3TextForSequenceClassification(GenericForSequenceClassification, Gemm\n     \"\"\"\n \n     config: Gemma3TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n \n __all__ = ["
        },
        {
            "sha": "31f25550df032499a1c1f1ca1145b2ebf28b77eb",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -562,7 +562,7 @@ def forward(\n \n class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n         \"SiglipVisionEmbeddings\",\n@@ -595,7 +595,7 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n \n class Gemma3TextModel(Gemma2Model):\n     config: Gemma3TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Gemma3TextConfig):\n         super().__init__(config)\n@@ -1192,7 +1192,7 @@ class Gemma3TextForSequenceClassification(GenericForSequenceClassification, Gemm\n     \"\"\"\n \n     config: Gemma3TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n \n __all__ = ["
        },
        {
            "sha": "7d1fbb9623b6829aa832f76af0f7ccf3ea9b4672",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1600,7 +1600,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n         \"hidden_states\": Gemma3nDecoderLayer,\n         \"attentions\": Gemma3nAttention,\n     }\n-    input_modalities = [\"image\", \"text\", \"audio\"]\n+    input_modalities = (\"image\", \"text\", \"audio\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n@@ -1697,7 +1697,7 @@ def forward(self, x, position_ids, layer_type=None):\n @auto_docstring(custom_intro=\"The base Gemma 3n language model without a language modeling head.\")\n class Gemma3nTextModel(Gemma3nPreTrainedModel):\n     config: Gemma3nTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Gemma3nTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "cc79bf00cdc5f2358344dbdc65c097fb61f4d7e4",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1873,7 +1873,7 @@ def forward(\n \n class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     config: Gemma3nConfig\n-    input_modalities = [\"image\", \"text\", \"audio\"]\n+    input_modalities = (\"image\", \"text\", \"audio\")\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "682134b09c654ee304b00297af8bdc233fdbff4f",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -386,7 +386,7 @@ def forward(\n class GitPreTrainedModel(PreTrainedModel):\n     config: GitConfig\n     base_model_prefix = \"git\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -807,7 +807,7 @@ def forward(\n class GitVisionModel(GitPreTrainedModel):\n     config: GitVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP->Git\n     def __init__(self, config: GitVisionConfig):"
        },
        {
            "sha": "5950d5d7a2ab0b89eeb19fd6c43637fba47a6d8b",
            "filename": "src/transformers/models/glm46v/modeling_glm46v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -40,7 +40,7 @@\n class Glm46VPreTrainedModel(PreTrainedModel):\n     config: Glm46VConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = None\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "c843774c5242200f6d7ed91d40884a58c938ab02",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -691,7 +691,7 @@ class Glm4vModelOutputWithPast(ModelOutput):\n class Glm4vPreTrainedModel(PreTrainedModel):\n     config: Glm4vConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -708,7 +708,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Glm4vVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -820,7 +820,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n @auto_docstring\n class Glm4vTextModel(Glm4vPreTrainedModel):\n     config: Glm4vTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Glm4vTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "5db661e318ffb36499800e6c0049ca8098d342f8",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -733,7 +733,7 @@ class Glm4vPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n \n class Glm4vVisionModel(Glm4vPreTrainedModel):\n     config: Glm4vVisionConfig\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Glm4vVisionBlock\"]\n \n     def __init__(self, config) -> None:"
        },
        {
            "sha": "fbb167c762be6f8fdc388c515d469cbd4cda3bf1",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -552,7 +552,7 @@ class Glm4vMoePreTrainedModel(PreTrainedModel):\n         \"attentions\": Glm4vMoeTextAttention,\n         \"router_logits\": OutputRecorder(nn.Linear, layer_name=\"mlp.gate\", index=0),\n     }\n-    input_modalities = [\"text\", \"image\", \"video\"]\n+    input_modalities = (\"text\", \"image\", \"video\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n@@ -873,7 +873,7 @@ def forward(\n @auto_docstring\n class Glm4vMoeVisionModel(Glm4vMoePreTrainedModel):\n     config: Glm4vMoeVisionConfig\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Glm4vMoeVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -985,7 +985,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n @auto_docstring\n class Glm4vMoeTextModel(Glm4vMoePreTrainedModel):\n     config: Glm4vMoeTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Glm4vMoeTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "c94ad0a9a8f6ecaf596de05e1ad2ca6abfb8ff5d",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -476,7 +476,7 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: int):\n class Glm4vMoePreTrainedModel(Glm4MoePreTrainedModel):\n     config: Glm4vMoeConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"text\", \"image\", \"video\"]\n+    input_modalities = (\"text\", \"image\", \"video\")\n     _no_split_modules = [\"Glm4vMoeTextDecoderLayer\", \"Glm4vMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "cea58e797ef153234dcf1fe0ba5f944351902835",
            "filename": "src/transformers/models/glpn/modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglpn%2Fmodeling_glpn.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -386,7 +386,7 @@ class GLPNPreTrainedModel(PreTrainedModel):\n     config: GLPNConfig\n     base_model_prefix = \"glpn\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = []\n \n "
        },
        {
            "sha": "a05bb0e259855eae5663677750162a21f3f3179c",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -278,7 +278,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n class GotOcr2PreTrainedModel(PreTrainedModel):\n     config: GotOcr2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn = False\n@@ -402,7 +402,7 @@ def forward(self, hidden_states):\n \n class GotOcr2VisionEncoder(GotOcr2PreTrainedModel):\n     _can_record_outputs = {\"hidden_states\": GotOcr2VisionLayer, \"attentions\": GotOcr2VisionAttention}\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: GotOcr2VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "e4fa64918bf678a89ee3ed7ddd90e90f4677975b",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -249,11 +249,11 @@ def __init__(self, config, window_size):\n \n \n class GotOcr2PreTrainedModel(SamPreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n \n class GotOcr2VisionEncoder(SamVisionEncoder, GotOcr2PreTrainedModel):\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n \n class GotOcr2MultiModalProjector(nn.Module):"
        },
        {
            "sha": "b1c124931d46bd7e71e8d8afaf547783a6c3a8e7",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -282,7 +282,7 @@ def forward(self, hidden_states: torch.Tensor):\n @auto_docstring\n class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config: GraniteSpeechConfig\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n \n     _supports_flash_attn = False  # `blip_2_qformer` dependency does not allow for this\n     _supports_sdpa = True"
        },
        {
            "sha": "8a0acf97ec5c7f3b4bd7c133011b109771149f35",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1368,7 +1368,7 @@ class GroundingDinoPreTrainedModel(PreTrainedModel):\n     config: GroundingDinoConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "dfb43d94bd4996852b3692f8116516645fd2ad23",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -746,7 +746,7 @@ def forward(\n class GroupViTPreTrainedModel(PreTrainedModel):\n     config: GroupViTConfig\n     base_model_prefix = \"groupvit\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -1022,7 +1022,7 @@ def forward(\n \n class GroupViTTextModel(GroupViTPreTrainedModel):\n     config: GroupViTTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: GroupViTTextConfig):\n         super().__init__(config)\n@@ -1127,7 +1127,7 @@ def forward(\n class GroupViTVisionModel(GroupViTPreTrainedModel):\n     config: GroupViTVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: GroupViTVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "9b93243a5077943f969222bbdb473a84255527b2",
            "filename": "src/transformers/models/hgnet_v2/modeling_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodeling_hgnet_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -42,7 +42,7 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     config: HGNetV2Config\n     base_model_prefix = \"hgnetv2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n "
        },
        {
            "sha": "f152072635c86a31fa6747ee5e04689d03032999",
            "filename": "src/transformers/models/hgnet_v2/modular_hgnet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhgnet_v2%2Fmodular_hgnet_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -167,7 +167,7 @@ class HGNetV2PreTrainedModel(PreTrainedModel):\n     config: HGNetV2Config\n     base_model_prefix = \"hgnetv2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"HGNetV2BasicLayer\"]\n \n "
        },
        {
            "sha": "c0318128f17c7012fef732d95b7d782ab2a747e2",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -774,7 +774,7 @@ class HieraPreTrainedModel(PreTrainedModel):\n     config: HieraConfig\n     base_model_prefix = \"hiera\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "38e87cf17f5266929f534d6d3ba26b3d7dc143b4",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -818,7 +818,7 @@ def forward(\n class IdeficsPreTrainedModel(PreTrainedModel):\n     config: IdeficsConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "214dcef450816e03b39e58ca2d109f00c99dc862",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -408,7 +408,7 @@ def forward(\n class Idefics2PreTrainedModel(PreTrainedModel):\n     config: Idefics2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -434,7 +434,7 @@ def _init_weights(self, module):\n )\n class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n     config: Idefics2VisionConfig\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n@@ -691,7 +691,7 @@ def forward(\n )\n class Idefics2PerceiverResampler(Idefics2PreTrainedModel):\n     config: Idefics2PerceiverConfig\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attention_2 = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "996f5573115cf08cc14ffd0d98f0bfd290e2a8ab",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -423,7 +423,7 @@ def forward(self, image_hidden_states):\n class Idefics3PreTrainedModel(PreTrainedModel):\n     config: Idefics3Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics3VisionAttention\", \"Idefics3DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -440,7 +440,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n )\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config: Idefics3VisionConfig\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "709268e940a23126dd33c5bb838f03ab4d241450",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -313,7 +313,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     config: IJepaConfig\n     base_model_prefix = \"ijepa\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "5bb8e9d6fc3018002578dd9a9ff18afe8274047c",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -363,7 +363,7 @@ class ImageGPTPreTrainedModel(PreTrainedModel):\n     config: ImageGPTConfig\n     base_model_prefix = \"transformer\"\n     main_input_name = \"input_ids\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ImageGPTBlock\"]\n "
        },
        {
            "sha": "9849f333051f652c4dcc674b3e8170e98a7b1612",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -248,7 +248,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     config: InformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "8df8afbaaa00bdd04a6e046485545734a7963bfd",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -84,7 +84,7 @@ class InformerPreTrainedModel(PreTrainedModel):\n     config: InformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "32f5d3cf968f6316dd5a9365c3e5a9e95719b173",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -309,7 +309,7 @@ def forward(\n class InstructBlipPreTrainedModel(PreTrainedModel):\n     config: InstructBlipConfig\n     base_model_prefix = \"blip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True\n@@ -373,7 +373,7 @@ def forward(\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP\n class InstructBlipVisionModel(InstructBlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     config: InstructBlipVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": InstructBlipEncoderLayer,"
        },
        {
            "sha": "2c49be5599d28fe13b0c576c43c2a26cf187835f",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -132,7 +132,7 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config: InstructBlipVideoConfig\n     base_model_prefix = \"blip\"\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "591cff7e692b3907584c9f4b23e6e3fa954c0c7a",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -160,7 +160,7 @@ def __init__(\n \n \n class InstructBlipVideoPreTrainedModel(InstructBlipPreTrainedModel):\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n \n \n class InstructBlipVideoVisionModel(InstructBlipVisionModel):"
        },
        {
            "sha": "e2ed5fa9b375a3bbbae436aa8108f048a41bbf78",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -399,7 +399,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     config: InternVLVisionConfig\n     base_model_prefix = \"internvl_vision\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n@@ -474,7 +474,7 @@ def forward(\n class InternVLPreTrainedModel(PreTrainedModel):\n     config: InternVLConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\", \"video\"]\n+    input_modalities = (\"image\", \"text\", \"video\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "d7a75e5d1677cb6365cfc079f878ac5f24540da8",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -356,7 +356,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     config: InternVLVisionConfig\n     base_model_prefix = \"internvl_vision\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n@@ -428,7 +428,7 @@ def forward(\n \n \n class InternVLPreTrainedModel(LlavaPreTrainedModel):\n-    input_modalities = [\"image\", \"text\", \"video\"]\n+    input_modalities = (\"image\", \"text\", \"video\")\n \n \n INTERNVL_INPUTS_DOCSTRING = None"
        },
        {
            "sha": "8bf106074dda40d555dbe37d6b66d9e123d502d7",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -49,7 +49,7 @@\n class JanusPreTrainedModel(PreTrainedModel):\n     config: JanusConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -545,7 +545,7 @@ def forward(\n @auto_docstring\n class JanusVisionModel(JanusPreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     config: JanusVisionConfig\n     _can_record_outputs = {\n         \"hidden_states\": JanusEncoderLayer,\n@@ -1164,7 +1164,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n-    output_modalities = [\"image\", \"text\"]\n+    output_modalities = (\"image\", \"text\")\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):"
        },
        {
            "sha": "4aa52f27468757371046d02390d6845f37d1472e",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -382,7 +382,7 @@ def __init__(\n class JanusPreTrainedModel(PreTrainedModel):\n     config: JanusConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n@@ -980,7 +980,7 @@ def forward(\n \n class JanusForConditionalGeneration(JanusPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n-    output_modalities = [\"image\", \"text\"]\n+    output_modalities = (\"image\", \"text\")\n     _can_compile_fullgraph = True\n \n     def __init__(self, config: JanusConfig):"
        },
        {
            "sha": "26752f699e9334d580b540f638ad6c66794f3924",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -152,5 +152,41 @@ def postprocess(self, images: ImageInput, **kwargs):\n         \"\"\"\n         return self.image_processor.postprocess(images, **kwargs)\n \n+    def post_process_multimodal_output(\n+        self, generated_outputs, skip_special_tokens=True, generation_mode=None, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of a multimodal model to return the requested modality output.\n+        If the model cannot generated the requested modality, an error will be raised.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            generation_mode (`str`, *optional*):\n+                Generation mode indicated which modality to output and can be one of `[\"text\", \"image\", \"audio\"]`.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[Union[str, PIL.Image.Image]]`: The decoded text or generated image.\n+        \"\"\"\n+        if generation_mode is None or generation_mode == \"text\":\n+            return self.post_process_image_text_to_text(\n+                generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs\n+            )\n+\n+        elif generation_mode == \"image\":\n+            generated_outputs = list(generated_outputs.float())\n+            images = self.postprocess(generated_outputs, return_tensors=\"PIL.Image.Image\")\n+            return images[\"pixel_values\"]\n+\n+        else:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} got an unexpected generation_mode={generation_mode}. Supported options are only `text` and `image\"\n+            )\n+\n \n __all__ = [\"JanusProcessor\"]"
        },
        {
            "sha": "e5c62aa7e1c1a5197f47706e56d13626c522c1fa",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1114,7 +1114,7 @@ def forward(\n @auto_docstring\n class Kosmos2PreTrainedModel(PreTrainedModel):\n     config: Kosmos2Config\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2VisionEncoderLayer\", \"Kosmos2TextBlock\"]\n     _supports_attention_backend = True\n@@ -1178,7 +1178,7 @@ def _init_weights(self, module: nn.Module):\n class Kosmos2VisionModel(Kosmos2PreTrainedModel):\n     config: Kosmos2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     # Copied from transformers.models.clip.modeling_clip.CLIPVisionModel.__init__ with CLIP_VISION->KOSMOS2_VISION,CLIP->Kosmos2,self.vision_model->self.model\n     def __init__(self, config: Kosmos2VisionConfig):\n@@ -1211,7 +1211,7 @@ def forward(\n \n class Kosmos2TextModel(Kosmos2PreTrainedModel):\n     config: Kosmos2TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Kosmos2TextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "bad3cfa305302bd71fc62031da41d2a7278176c5",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1220,7 +1220,7 @@ class Kosmos2_5PreTrainedModel(PreTrainedModel):\n     \"\"\"\n \n     config_class = Kosmos2_5Config\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2_5VisionLayer\", \"Kosmos2_5TextBlock\"]\n     _supports_flash_attn_2 = True\n@@ -1257,7 +1257,7 @@ def _init_weights(self, module):\n \n class Kosmos2_5VisionModel(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5VisionConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     # Copied from transformers.models.pix2struct.modeling_pix2struct.Pix2StructVisionModel.__init__ with Pix2Struct->Kosmos2_5\n     def __init__(self, config: Kosmos2_5VisionConfig):\n@@ -1319,7 +1319,7 @@ def forward(\n # Adapted from transformers.models.kosmos2.modeling_kosmos2.Kosmos2TextModel with KOSMOS2->KOSMOS2_5\n class Kosmos2_5TextModel(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Kosmos2_5TextConfig):\n         super().__init__(config)\n@@ -1505,7 +1505,7 @@ def forward(\n )\n class Kosmos2_5TextForCausalLM(Kosmos2_5PreTrainedModel):\n     config_class = Kosmos2_5TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     _tied_weights_keys = {\"lm_head.weight\": \"model.embed_tokens.weight\"}\n \n     def __init__(self, config: Kosmos2_5TextConfig):"
        },
        {
            "sha": "c0170f45ac431155caa41ea7d893bd8962b5d3c3",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -98,7 +98,7 @@ def forward(self, x, layer_idx=None):\n class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n     config: KyutaiSpeechToTextConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n@@ -1073,7 +1073,7 @@ class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedMod\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n-    output_modalities = [\"audio\", \"text\"]\n+    output_modalities = (\"audio\", \"text\")\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "790482c33727e7e5ce2e6f9a76734ff78805f238",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -252,7 +252,7 @@ def __init__(self, config):\n \n class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin):\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n-    output_modalities = [\"audio\", \"text\"]\n+    output_modalities = (\"audio\", \"text\")\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "d88c109f61615833d4f8d7ab75688c5ede06d491",
            "filename": "src/transformers/models/layoutlmv2/modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fmodeling_layoutlmv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -457,7 +457,7 @@ def forward(\n class LayoutLMv2PreTrainedModel(PreTrainedModel):\n     config: LayoutLMv2Config\n     base_model_prefix = \"layoutlmv2\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "681406ecd6150226aae11c77849db13e9ca8fd78",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -202,7 +202,7 @@ def forward(\n class LayoutLMv3PreTrainedModel(PreTrainedModel):\n     config: LayoutLMv3Config\n     base_model_prefix = \"layoutlmv3\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "2919728b7355d21db391c28be114206703f39139",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -469,7 +469,7 @@ class LevitPreTrainedModel(PreTrainedModel):\n     config: LevitConfig\n     base_model_prefix = \"levit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"LevitResidualLayer\"]\n \n "
        },
        {
            "sha": "4ef98a251bfa684a7fb70ab405b64d958a82d825",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -77,7 +77,7 @@ def pixel_unshuffle(self, hidden_states: torch.Tensor):\n class Lfm2VlPreTrainedModel(PreTrainedModel):\n     config: Lfm2VlConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "75504bc4dd57b169ece800416f2e5022819e58c3",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -423,7 +423,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     config: LightGlueConfig\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "7db4adc110cddfbd18e71d4ffa2c8cc61b97beec",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -481,7 +481,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     config: LightGlueConfig\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "98082e7a45c6ffdf50b8e549f428184874847f32",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -463,7 +463,7 @@ def forward(\n @auto_docstring\n class Llama4PreTrainedModel(PreTrainedModel):\n     config: Llama4Config\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = False\n@@ -493,7 +493,7 @@ def _init_weights(self, module):\n class Llama4TextModel(Llama4PreTrainedModel):\n     _no_split_modules = [\"Llama4TextDecoderLayer\"]\n     base_model_prefix = \"model\"\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     config: Llama4TextConfig\n     _can_record_outputs = {\n         \"attentions\": Llama4TextAttention,\n@@ -1030,7 +1030,7 @@ def forward(self, hidden_states):\n \n class Llama4VisionModel(Llama4PreTrainedModel):\n     base_model_prefix = \"vision_model\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"Llama4VisionEncoderLayer\"]\n     config: Llama4VisionConfig\n "
        },
        {
            "sha": "df80dd6716d2bd48b3e4a0331c30ad4cda2787c9",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -111,7 +111,7 @@ def forward(self, image_features):\n class LlavaPreTrainedModel(PreTrainedModel):\n     config: LlavaConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "cf0ebf1ce869e6c7d9f096e22318d996de8bc22c",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -224,7 +224,7 @@ def forward(self, image_features):\n class LlavaNextPreTrainedModel(PreTrainedModel):\n     config: LlavaNextConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "eed31b38096daa0abf74ba8f5a1598ea74d14554",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -165,7 +165,7 @@ def forward(self, image_features):\n class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     config: LlavaNextVideoConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "4fec99df3b3005c5c7601fe0ae166bde4bf9b695",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -260,7 +260,7 @@ class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n \n \n class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n \n \n class LlavaNextVideoModel(LlavaNextModel):"
        },
        {
            "sha": "260177a63796679c79632c9c61ded0c42b9c0e70",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -106,7 +106,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     config: LlavaOnevisionConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "8e022806bf3b1ce145e4fdd6b51974245fe24240",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -672,7 +672,7 @@ def forward(self, sequence_output, pooled_output):\n class LxmertPreTrainedModel(PreTrainedModel):\n     config: LxmertConfig\n     base_model_prefix = \"lxmert\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "24c2652dd5c85c5d2d2cdf465fa7ac78be322fef",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -2101,7 +2101,7 @@ class Mask2FormerPreTrainedModel(PreTrainedModel):\n     config: Mask2FormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "799ffe4ca51b89a598044515074f2ce86b142e69",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1435,7 +1435,7 @@ class MaskFormerPreTrainedModel(PreTrainedModel):\n     config: MaskFormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "19d91b8d7e7ad43ab0319ce4a6f899c2e21ddfb6",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -698,7 +698,7 @@ class MaskFormerSwinPreTrainedModel(PreTrainedModel):\n     config: MaskFormerSwinConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MaskFormerSwinStage\"]\n "
        },
        {
            "sha": "b8dc9663364a4ba13ead6bd97d7b425a15eed849",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -288,7 +288,7 @@ def forward(\n class MetaClip2PreTrainedModel(PreTrainedModel):\n     config: MetaClip2Config\n     base_model_prefix = \"metaclip_2\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True\n@@ -500,7 +500,7 @@ class MetaClip2TextModel(MetaClip2PreTrainedModel):\n     ```\"\"\"\n \n     config: MetaClip2TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n@@ -600,7 +600,7 @@ class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n     ```\"\"\"\n \n     config: MetaClip2TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n \n@@ -1032,7 +1032,7 @@ class MetaClip2VisionModel(MetaClip2PreTrainedModel):\n \n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"MetaClip2EncoderLayer\"]\n \n     def __init__(self, config: MetaClip2VisionConfig):\n@@ -1137,7 +1137,7 @@ class MetaClip2VisionModelWithProjection(MetaClip2PreTrainedModel):\n \n     config: MetaClip2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: MetaClip2VisionConfig):\n         super().__init__(config)\n@@ -1203,7 +1203,7 @@ def forward(\n )\n class MetaClip2ForImageClassification(MetaClip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: MetaClip2Config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "a944ff4f90554c5fc7350311dfc25ab7eb4f6ce2",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -177,7 +177,7 @@ class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n class Mistral3PreTrainedModel(PreTrainedModel):\n     config: Mistral3Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "3c95edfd467ccf1d3bdd28987eed5dec7bf71de8",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -506,7 +506,7 @@ def forward(\n class MLCDVisionModel(MLCDPreTrainedModel):\n     config: MLCDVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"MLCDEncoderLayer\"]\n \n     def __init__(self, config: MLCDVisionConfig):"
        },
        {
            "sha": "83efa15f2ab496bca53cad97355b0f94a993d4ef",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -794,7 +794,7 @@ def forward(self, x, position_ids):\n class MllamaPreTrainedModel(PreTrainedModel):\n     config: MllamaConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"MllamaVisionEncoderLayer\",\n@@ -982,7 +982,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class MllamaVisionModel(MllamaPreTrainedModel):\n     config: MllamaVisionConfig\n     base_model_prefix = \"vision_model\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: MllamaVisionConfig):\n         super().__init__(config)\n@@ -1180,7 +1180,7 @@ def forward(\n class MllamaTextModel(MllamaPreTrainedModel):\n     config: MllamaTextConfig\n     base_model_prefix = \"language_model.model\"\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: MllamaTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "8f0f326426e0135c79ff402340a86eff429d7149",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -505,7 +505,7 @@ class MMGroundingDinoPreTrainedModel(PreTrainedModel):\n     config: MMGroundingDinoConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "e8c9dbdee15d13c9f0e718263d65ee4b6406546b",
            "filename": "src/transformers/models/mobilenet_v1/modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fmodeling_mobilenet_v1.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -128,7 +128,7 @@ class MobileNetV1PreTrainedModel(PreTrainedModel):\n     config: MobileNetV1Config\n     base_model_prefix = \"mobilenet_v1\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n "
        },
        {
            "sha": "4f22a17d1b9083762993a7e1142e9f055f22ce54",
            "filename": "src/transformers/models/mobilenet_v2/modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fmodeling_mobilenet_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -254,7 +254,7 @@ class MobileNetV2PreTrainedModel(PreTrainedModel):\n     config: MobileNetV2Config\n     base_model_prefix = \"mobilenet_v2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     _no_split_modules = []\n "
        },
        {
            "sha": "3dbe653c7670c2a6136dc5e244b44f1e7e3a1513",
            "filename": "src/transformers/models/mobilevit/modeling_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fmodeling_mobilevit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -604,7 +604,7 @@ class MobileViTPreTrainedModel(PreTrainedModel):\n     config: MobileViTConfig\n     base_model_prefix = \"mobilevit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MobileViTLayer\"]\n "
        },
        {
            "sha": "80800a3bf6a717c6ffeabd6e41af4d3081ed25f5",
            "filename": "src/transformers/models/mobilevitv2/modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fmodeling_mobilevitv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -571,7 +571,7 @@ class MobileViTV2PreTrainedModel(PreTrainedModel):\n     config: MobileViTV2Config\n     base_model_prefix = \"mobilevitv2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MobileViTV2Layer\"]\n "
        },
        {
            "sha": "973e0b57225cdd68cd56ad5818e77fd4066cc9cc",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -819,7 +819,7 @@ def forward(\n class MoshiPreTrainedModel(PreTrainedModel):\n     config: MoshiConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoshiDecoderLayer\", \"MimiTransformerLayer\"]\n     _supports_flash_attn = True\n@@ -1464,7 +1464,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     \"\"\"\n )\n class MoshiForCausalLM(MoshiPreTrainedModel, GenerationMixin):\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     # Copied from transformers.models.gemma.modeling_gemma.GemmaForCausalLM.__init__ with Gemma->Moshi\n     def __init__(self, config):\n@@ -1582,7 +1582,7 @@ def forward(\n )\n class MoshiForConditionalGeneration(MoshiPreTrainedModel, GenerationMixin):\n     config: MoshiConfig\n-    output_modalities = [\"audio\", \"text\"]\n+    output_modalities = (\"audio\", \"text\")\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "c053adab8eb5c51b3fcc10b91990696f370ef9a2",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -788,7 +788,7 @@ def forward(\n     \"\"\"\n )\n class MusicgenForCausalLM(MusicgenPreTrainedModel, GenerationMixin):\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: MusicgenDecoderConfig):\n         super().__init__(config)\n@@ -1284,7 +1284,7 @@ def generate(\n )\n class MusicgenForConditionalGeneration(MusicgenPreTrainedModel, GenerationMixin):\n     config: MusicgenConfig\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "f4ed374428ada0879820d4404396a02f73c08733",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -742,7 +742,7 @@ def forward(\n )\n # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenForCausalLM with MUSICGEN->MUSICGEN_MELODY,Musicgen->MusicgenMelody,MusicGen->Musicgen Melody\n class MusicgenMelodyForCausalLM(MusicgenMelodyPreTrainedModel, GenerationMixin):\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: MusicgenMelodyDecoderConfig):\n         super().__init__(config)\n@@ -1228,7 +1228,7 @@ def generate(\n class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     config: MusicgenMelodyConfig\n     main_input_name = \"input_ids\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "1c4cb425b9f284a5a41b10f9b4d87025e603531f",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -986,7 +986,7 @@ class OmDetTurboPreTrainedModel(PreTrainedModel):\n     config: OmDetTurboConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "cf04d775e4c89d72ea3cafc09116f002c9580ca6",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -2765,7 +2765,7 @@ class OneFormerPreTrainedModel(PreTrainedModel):\n     config: OneFormerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "c7480492d38100f98018fa42ac9624ab167c4d6e",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -418,7 +418,7 @@ def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n class Ovis2PreTrainedModel(PreTrainedModel):\n     config: Ovis2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Ovis2VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "6faf887eb333715efb22ba5a0de36f15eade48c4",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -147,7 +147,7 @@ def forward(self, visual_tokens: torch.Tensor) -> torch.Tensor:\n class Ovis2PreTrainedModel(PreTrainedModel):\n     config: Ovis2Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Ovis2VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "5426d37b73c31658c122d4079bf44a5078cb429a",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -564,7 +564,7 @@ def forward(\n class Owlv2PreTrainedModel(PreTrainedModel):\n     config: Owlv2Config\n     base_model_prefix = \"owlv2\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Owlv2EncoderLayer\"]\n \n@@ -771,7 +771,7 @@ def forward(\n # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTTextModel with google/owlvit-base-patch32->google/owlv2-base-patch16, OWLVIT->OWLV2,OwlViT->Owlv2\n class Owlv2TextModel(Owlv2PreTrainedModel):\n     config: Owlv2TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Owlv2TextConfig):\n         super().__init__(config)\n@@ -884,7 +884,7 @@ def forward(\n class Owlv2VisionModel(Owlv2PreTrainedModel):\n     config: Owlv2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: Owlv2VisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a4619e6b11f17bc7d7b5a730c29454c5bf348c23",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -551,7 +551,7 @@ def forward(\n class OwlViTPreTrainedModel(PreTrainedModel):\n     config: OwlViTConfig\n     base_model_prefix = \"owlvit\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OwlViTEncoderLayer\"]\n \n@@ -755,7 +755,7 @@ def forward(\n \n class OwlViTTextModel(OwlViTPreTrainedModel):\n     config: OwlViTTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: OwlViTTextConfig):\n         super().__init__(config)\n@@ -866,7 +866,7 @@ def forward(\n class OwlViTVisionModel(OwlViTPreTrainedModel):\n     config: OwlViTVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: OwlViTVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "d147f0b953ac3329bd35b54ced696ce91a0959fc",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -215,7 +215,7 @@ def create_causal_mask_mapping(\n class PaliGemmaPreTrainedModel(PreTrainedModel):\n     config: PaliGemmaConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "61e29c17b6bbbc510378d265f7ab9a3b3e38ac01",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -683,7 +683,7 @@ class PatchTSMixerPreTrainedModel(PreTrainedModel):\n     config: PatchTSMixerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     supports_gradient_checkpointing = False\n \n     @torch.no_grad()"
        },
        {
            "sha": "d482efa5b8324608d7e3d9cd304ab41696ba5513",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -553,7 +553,7 @@ class PatchTSTPreTrainedModel(PreTrainedModel):\n     config: PatchTSTConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     supports_gradient_checkpointing = False\n \n     @torch.no_grad()"
        },
        {
            "sha": "9c4edf03c979eade7bffb71bb14c20683b121d15",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -530,7 +530,7 @@ class PerceiverPreTrainedModel(PreTrainedModel):\n     config: PerceiverConfig\n     base_model_prefix = \"perceiver\"\n     main_input_name = \"inputs\"\n-    input_modalities = \"image\"  # techinically can be anything but HF impl has only image processor\n+    input_modalities = (\"image\",)  # techinically can be anything but HF impl has only image processor\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "cb41dfbb86aa2f3bcad594c9f34dbccacabf7203",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -90,7 +90,7 @@ def forward(self, features):\n class PerceptionLMPreTrainedModel(PreTrainedModel):\n     config: PerceptionLMConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "4d4bdab8ce44919dc5f253972059d9a590ca5edd",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -243,7 +243,7 @@ def default_flax_embed_init(tensor):\n class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n     config: Phi4MultimodalVisionConfig\n     base_model_prefix = \"phi4_vision\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n@@ -1432,7 +1432,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n         \"attentions\": Phi4MultimodalAttention,\n     }\n     _version = \"0.0.5\"\n-    input_modalities = [\"image\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"audio\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "728a5244468a345e4c540a93140c958388905d1c",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -534,7 +534,7 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n     config: Phi4MultimodalVisionConfig\n     base_model_prefix = \"phi4_vision\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n@@ -1442,7 +1442,7 @@ def forward(\n \n \n class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n-    input_modalities = [\"image\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"audio\", \"text\")\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "02215827e33c3931ed5bebb1c964b27dd6d5c8f4",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -336,7 +336,7 @@ def forward(\n @auto_docstring\n class Pix2StructPreTrainedModel(PreTrainedModel):\n     config: Pix2StructConfig\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n \n     _can_compile_fullgraph = False\n \n@@ -454,7 +454,7 @@ def _shift_right(self, input_ids):\n class Pix2StructVisionModel(Pix2StructPreTrainedModel):\n     config: Pix2StructVisionConfig\n     main_input_name = \"flattened_patches\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Pix2StructVisionLayer\"]\n \n@@ -956,7 +956,7 @@ def forward(\n )\n class Pix2StructTextModel(Pix2StructPreTrainedModel):\n     config: Pix2StructTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     _no_split_modules = [\"Pix2StructTextBlock\"]\n     _tied_weights_keys = {\"lm_head.weight\": \"embed_tokens.weight\"}\n     supports_gradient_checkpointing = True"
        },
        {
            "sha": "ef861b9ebdc179169fd8ff882b17cad808c50c94",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -433,7 +433,7 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     config: PixtralVisionConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "0ab490c97344b5f173a64b351ed5577b8b49a5fe",
            "filename": "src/transformers/models/poolformer/modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fmodeling_poolformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -243,7 +243,7 @@ class PoolFormerPreTrainedModel(PreTrainedModel):\n     config: PoolFormerConfig\n     base_model_prefix = \"poolformer\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"PoolFormerLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "9a68fea87c7b17084dbfbc0430f06d62ff5af9fa",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -538,7 +538,7 @@ def forward(\n class Pop2PianoPreTrainedModel(PreTrainedModel):\n     config: Pop2PianoConfig\n     base_model_prefix = \"transformer\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False"
        },
        {
            "sha": "1fa57eef92adc89331de18e0e77c6528ecbd0d0b",
            "filename": "src/transformers/models/prompt_depth_anything/modeling_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodeling_prompt_depth_anything.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -243,7 +243,7 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     config: PromptDepthAnythingConfig\n     base_model_prefix = \"prompt_depth_anything\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "c61aeeb5343c812465fd9fcd4aaa3e10d65f45e5",
            "filename": "src/transformers/models/prompt_depth_anything/modular_prompt_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fmodular_prompt_depth_anything.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -162,7 +162,7 @@ class PromptDepthAnythingPreTrainedModel(PreTrainedModel):\n     config: PromptDepthAnythingConfig\n     base_model_prefix = \"prompt_depth_anything\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "846d6755fb5ac801a15d40b64bcc21d30f5833c6",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -419,7 +419,7 @@ class PvtPreTrainedModel(PreTrainedModel):\n     config: PvtConfig\n     base_model_prefix = \"pvt\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = []\n \n     @torch.no_grad()"
        },
        {
            "sha": "8f9b7216be69435625c0e7947f2efed2cbc0b8d0",
            "filename": "src/transformers/models/pvt_v2/modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fmodeling_pvt_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -366,7 +366,7 @@ class PvtV2PreTrainedModel(PreTrainedModel):\n     config: PvtV2Config\n     base_model_prefix = \"pvt_v2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "9b7070ce8bfcd4845bf9347d698702bae8f74e53",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -42,6 +42,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_omni import (\n@@ -64,7 +65,7 @@\n class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5OmniConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\", \"Qwen2_5OmniVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -75,7 +76,7 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n \n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n@@ -1075,7 +1076,7 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n class Qwen2_5OmniVisionEncoder(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:\n         super().__init__(config, *inputs, **kwargs)\n@@ -1530,7 +1531,7 @@ def forward(\n @auto_docstring\n class Qwen2_5OmniThinkerTextModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\"]\n \n     def __init__(self, config: Qwen2_5OmniTextConfig):\n@@ -1825,7 +1826,7 @@ def get_placeholder_mask(\n         special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n             raise ValueError(\n-                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n             )\n \n         special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n@@ -2105,7 +2106,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n @auto_docstring\n class Qwen2_5OmniTalkerModel(Qwen2_5OmniPreTrainedModel):\n     config: Qwen2_5OmniTalkerConfig\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n \n     _no_split_modules = [\"Qwen2_5OmniTalkerDecoderLayer\"]\n \n@@ -2263,7 +2264,7 @@ def forward(\n class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniTalkerConfig\n     base_model_prefix = \"talker\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         super().__init__(config)\n@@ -3765,7 +3766,7 @@ def forward(\n )\n class Qwen2_5OmniForConditionalGeneration(Qwen2_5OmniPreTrainedModel, GenerationMixin):\n     config: Qwen2_5OmniConfig\n-    output_modalities = [\"audio\", \"text\"]\n+    output_modalities = (\"audio\", \"text\")\n     _no_split_modules = [\n         \"Qwen2_5OmniTalkerForConditionalGeneration\",\n         \"Qwen2_5OmniToken2WavModel\",\n@@ -3849,13 +3850,13 @@ def from_pretrained(\n         return model\n \n     @torch.no_grad()\n+    @deprecate_kwarg(\"return_audio\", version=\"v5\", new_name=\"generation_mode\")\n     # TODO: raushan, defaults should be saved in generation config\n     def generate(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         speaker: str = \"Chelsie\",\n         use_audio_in_video: bool = False,\n-        return_audio: Optional[bool] = None,\n         thinker_max_new_tokens: int = 1024,\n         talker_max_new_tokens: int = 4096,\n         talker_do_sample: bool = True,\n@@ -3876,8 +3877,8 @@ def generate(\n                 Which speaker should be used in audio response.\n             use_audio_in_video (`bool`, defaults to False):\n                 Whether or not use audio track in video, should same as the parameter in `process_audio_info`.\n-            return_audio (`Optional[bool]`, *optional*):\n-                Whether or not return response in audio format. When `return_audio=None`, this parameter is same as `config.enable_audio_output`.\n+            generation_mode (`Optional[str]`, *optional*):\n+                Whether or not return response in audio format. When `generation_mode=\"audio\"`, this parameter is same as `config.enable_audio_output`.\n             kwargs (*optional*):\n                 - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model.\n                 - With a *thinker_*, *talker_*, *token2wav_* prefix, they will be input for the `generate` method of the\n@@ -3889,6 +3890,10 @@ def generate(\n                 - **Text** (`torch.Tensor`): Generated text token sequence.\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n+        # check `False` on purpose because the paramter can be `str/bool`. This is needed for BC\n+        generation_mode = kwargs.pop(\"generation_mode\", None)\n+        return_audio = generation_mode != \"text\" and generation_mode is not False\n+\n         if speaker not in self.speaker_map:\n             raise ValueError(f\"{speaker} is not available, available speakers: {self.speaker_map.keys()}\")\n         if return_audio and not self.has_talker:"
        },
        {
            "sha": "553213af7e7703ca6cc79b2c54dba4d54f698f98",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -54,6 +54,7 @@\n     check_torch_load_is_safe,\n     logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n \n \n@@ -1064,12 +1065,12 @@ def get_text_config(self, *args, **kwargs):\n \n class Qwen2_5OmniPreTrainedModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5OmniConfig\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n     _can_compile_fullgraph = False\n \n \n class Qwen2_5OmniPreTrainedModelForConditionalGeneration(Qwen2_5OmniPreTrainedModel):\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n \n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n@@ -1933,7 +1934,7 @@ def forward(\n \n class Qwen2_5OmniVisionEncoder(Qwen2_5_VisionTransformerPretrainedModel):\n     config: Qwen2_5OmniVisionEncoderConfig\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Qwen2_5OmniVisionBlock\"]\n \n     def __init__(self, config: Qwen2_5OmniVisionEncoderConfig, *inputs, **kwargs) -> None:\n@@ -2189,7 +2190,7 @@ def get_placeholder_mask(\n         special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n             raise ValueError(\n-                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n             )\n \n         special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n@@ -2468,7 +2469,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n \n class Qwen2_5OmniTalkerModel(Qwen2_5_VLTextModel):\n     config: Qwen2_5OmniTalkerConfig\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n \n     _no_split_modules = [\"Qwen2_5OmniTalkerDecoderLayer\"]\n \n@@ -2480,7 +2481,7 @@ def __init__(self, config: Qwen2_5OmniTalkerConfig):\n class Qwen2_5OmniTalkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniTalkerConfig\n     base_model_prefix = \"talker\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n \n     def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         super().__init__(config)\n@@ -3939,7 +3940,7 @@ def forward(\n )\n class Qwen2_5OmniForConditionalGeneration(Qwen2_5OmniPreTrainedModel, GenerationMixin):\n     config: Qwen2_5OmniConfig\n-    output_modalities = [\"audio\", \"text\"]\n+    output_modalities = (\"audio\", \"text\")\n     _no_split_modules = [\n         \"Qwen2_5OmniTalkerForConditionalGeneration\",\n         \"Qwen2_5OmniToken2WavModel\",\n@@ -4023,13 +4024,13 @@ def from_pretrained(\n         return model\n \n     @torch.no_grad()\n+    @deprecate_kwarg(\"return_audio\", version=\"v5\", new_name=\"generation_mode\")\n     # TODO: raushan, defaults should be saved in generation config\n     def generate(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         speaker: str = \"Chelsie\",\n         use_audio_in_video: bool = False,\n-        return_audio: Optional[bool] = None,\n         thinker_max_new_tokens: int = 1024,\n         talker_max_new_tokens: int = 4096,\n         talker_do_sample: bool = True,\n@@ -4050,8 +4051,8 @@ def generate(\n                 Which speaker should be used in audio response.\n             use_audio_in_video (`bool`, defaults to False):\n                 Whether or not use audio track in video, should same as the parameter in `process_audio_info`.\n-            return_audio (`Optional[bool]`, *optional*):\n-                Whether or not return response in audio format. When `return_audio=None`, this parameter is same as `config.enable_audio_output`.\n+            generation_mode (`Optional[str]`, *optional*):\n+                Whether or not return response in audio format. When `generation_mode=\"audio\"`, this parameter is same as `config.enable_audio_output`.\n             kwargs (*optional*):\n                 - Without a prefix, they will be entered as `**kwargs` for the `generate` method of each sub-model.\n                 - With a *thinker_*, *talker_*, *token2wav_* prefix, they will be input for the `generate` method of the\n@@ -4063,6 +4064,10 @@ def generate(\n                 - **Text** (`torch.Tensor`): Generated text token sequence.\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n+        # check `False` on purpose because the paramter can be `str/bool`. This is needed for BC\n+        generation_mode = kwargs.pop(\"generation_mode\", None)\n+        return_audio = generation_mode != \"text\" and generation_mode is not False\n+\n         if speaker not in self.speaker_map:\n             raise ValueError(f\"{speaker} is not available, available speakers: {self.speaker_map.keys()}\")\n         if return_audio and not self.has_talker:"
        },
        {
            "sha": "55906c8f8364c37dc318daf268490f4bfd6b1799",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -330,6 +330,60 @@ def apply_chat_template(self, conversations, chat_template=None, **kwargs):\n \n         return super().apply_chat_template(conversations, chat_template, **kwargs)\n \n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n+        \"\"\"\n+        Post-process the output of a vlm to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(generated_outputs[0], skip_special_tokens=skip_special_tokens, **kwargs)\n+\n+    def post_process_multimodal_output(\n+        self, generated_outputs, skip_special_tokens=True, generation_mode=None, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of a multimodal model to return the requested modality output.\n+        If the model cannot generated the requested modality, an error will be raised.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            generation_mode (`str`, *optional*):\n+                Generation mode indicated which modality to output and can be one of `[\"text\", \"image\", \"audio\"]`.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[Inion[str, np.ndarray]]`: The decoded text or generated audio.\n+        \"\"\"\n+        if generation_mode is None or generation_mode == \"text\":\n+            return self.post_process_image_text_to_text(\n+                generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs\n+            )\n+\n+        elif generation_mode == \"audio\":\n+            # model supports only bs=1, so we will never get several audio outputs\n+            audio = generated_outputs[1].reshape(-1).detach().cpu().numpy()\n+            return [audio]\n+\n+        else:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} got an unexpected generation_mode={generation_mode}. Supported options are only `text` and `audio\"\n+            )\n+\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "c0dd5b983cd6ef9004643f59b0da0cf205a1aa3b",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -293,7 +293,7 @@ def forward(\n class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     config: Qwen2_5_VLConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -794,7 +794,7 @@ def forward(\n @auto_docstring\n class Qwen2_5_VLTextModel(Qwen2_5_VLPreTrainedModel):\n     config: Qwen2_5_VLTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Qwen2_5_VLTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "142a8f76d816988c5dea0a4db125c6bdd0f6bdb4",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -250,7 +250,7 @@ def forward(\n class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     config: Qwen2AudioConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2AudioAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "593a160ec7997496886d60c6cb0e97f0c9db0b4a",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -655,7 +655,7 @@ def forward(\n class Qwen2VLPreTrainedModel(PreTrainedModel):\n     config: Qwen2VLConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -669,7 +669,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n @auto_docstring\n class Qwen2VisionTransformerPretrainedModel(Qwen2VLPreTrainedModel):\n     config: Qwen2VLVisionConfig\n-    input_modalities = [\"image\", \"video\"]\n+    input_modalities = (\"image\", \"video\")\n     _no_split_modules = [\"Qwen2VLVisionBlock\"]\n \n     def __init__(self, config) -> None:\n@@ -767,7 +767,7 @@ def forward(\n @auto_docstring\n class Qwen2VLTextModel(Qwen2VLPreTrainedModel):\n     config: Qwen2VLTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Qwen2VLTextConfig):\n         super().__init__(config)"
        },
        {
            "sha": "69b6be3f4fcc15939a81ba0d2131a5756bc9b44d",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -68,7 +68,7 @@\n class Qwen3OmniMoePreTrainedModel(PreTrainedModel):\n     config: Qwen3OmniMoeConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3OmniMoeDecoderLayer\", \"Qwen3OmniMoeVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -99,7 +99,7 @@ def _get_feat_extract_output_lengths(input_lengths):\n \n \n class Qwen3OmniMoePreTrainedModelForConditionalGeneration(Qwen3OmniMoePreTrainedModel):\n-    input_modalities = [\"image\", \"video\", \"audio\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"audio\", \"text\")\n \n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         self,\n@@ -2011,7 +2011,7 @@ def get_placeholder_mask(\n         special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         if video_features is not None and inputs_embeds[special_video_mask].numel() != video_features.numel():\n             raise ValueError(\n-                f\"Videos features and image tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n+                f\"Videos features and video tokens do not match: tokens: {n_video_tokens}, features {video_features.shape[0]}\"\n             )\n \n         special_audio_mask = special_audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n@@ -3787,7 +3787,7 @@ def chunked_decode(self, codes, chunk_size=300, left_context_size=25):\n \n class Qwen3OmniMoeForConditionalGeneration(Qwen3OmniMoePreTrainedModel, GenerationMixin):\n     config_class = Qwen3OmniMoeConfig\n-    output_modalities = [\"text\", \"audio\"]\n+    output_modalities = (\"text\", \"audio\")\n \n     def __init__(self, config: Qwen3OmniMoeConfig):\n         super().__init__(config)"
        },
        {
            "sha": "cd9f94681b9de6d6ec3e239e6209c043af17da29",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -2304,7 +2304,7 @@ def chunked_decode(self, codes, chunk_size=300, left_context_size=25):\n \n class Qwen3OmniMoeForConditionalGeneration(Qwen3OmniMoePreTrainedModel, GenerationMixin):\n     config_class = Qwen3OmniMoeConfig\n-    output_modalities = [\"text\", \"audio\"]\n+    output_modalities = (\"text\", \"audio\")\n \n     def __init__(self, config: Qwen3OmniMoeConfig):\n         super().__init__(config)"
        },
        {
            "sha": "5d8b35f744c6b5b7860ffc3915c80ae3d6f7dfca",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -329,6 +329,60 @@ def _iter():\n     def apply_chat_template(self, conversations, chat_template=None, **kwargs):\n         return super().apply_chat_template(conversations, chat_template, **kwargs)\n \n+    def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n+        \"\"\"\n+        Post-process the output of a vlm to decode the text.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(generated_outputs[0], skip_special_tokens=skip_special_tokens, **kwargs)\n+\n+    def post_process_multimodal_output(\n+        self, generated_outputs, skip_special_tokens=True, generation_mode=None, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of a multimodal model to return the requested modality output.\n+        If the model cannot generated the requested modality, an error will be raised.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            generation_mode (`str`, *optional*):\n+                Generation mode indicated which modality to output and can be one of `[\"text\", \"image\", \"audio\"]`.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[Inion[str, np.ndarray]]`: The decoded text or generated audio.\n+        \"\"\"\n+        if generation_mode is None or generation_mode == \"text\":\n+            return self.post_process_image_text_to_text(\n+                generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs\n+            )\n+\n+        elif generation_mode == \"audio\":\n+            # model supports only bs=1, so we will never get several audio outputs\n+            audio = generated_outputs[1].reshape(-1).detach().cpu().numpy()\n+            return [audio]\n+\n+        else:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} got an unexpected generation_mode={generation_mode}. Supported options are only `text` and `audio\"\n+            )\n+\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "1b41f5eee01d89918ae2f876630b02dadfed2091",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -576,7 +576,7 @@ class Qwen3VLModelOutputWithPast(ModelOutput):\n class Qwen3VLPreTrainedModel(PreTrainedModel):\n     config: Qwen3VLConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3VLTextDecoderLayer\", \"Qwen3VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "aa3be5d54be3423f62b247cd41c78f74ef7c193f",
            "filename": "src/transformers/models/resnet/modeling_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_resnet.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -248,7 +248,7 @@ class ResNetPreTrainedModel(PreTrainedModel):\n     config: ResNetConfig\n     base_model_prefix = \"resnet\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"ResNetConvLayer\", \"ResNetShortCut\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "b5f02de709a1b17285170fa89c6f2f06f5dc8485",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1007,7 +1007,7 @@ class RTDetrPreTrainedModel(PreTrainedModel):\n     config: RTDetrConfig\n     base_model_prefix = \"rt_detr\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"RTDetrHybridEncoder\", r\"RTDetrDecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "80908a1bcca94a44f633d250257cfb5ebee08ac6",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr_resnet.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -302,7 +302,7 @@ class RTDetrResNetPreTrainedModel(PreTrainedModel):\n     config: RTDetrResNetConfig\n     base_model_prefix = \"resnet\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"RTDetrResNetConvLayer\", \"RTDetrResNetShortCut\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "4136b6a9f851202db02d2f56b2c06f61c05c5817",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -454,7 +454,7 @@ class RTDetrV2PreTrainedModel(PreTrainedModel):\n     config: RTDetrV2Config\n     base_model_prefix = \"rt_detr_v2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n \n     @torch.no_grad()"
        },
        {
            "sha": "aef70f382ff006f8823faf06e8339bc2d95842c7",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -999,7 +999,7 @@ class SamPreTrainedModel(PreTrainedModel):\n     config: SamConfig\n     base_model_prefix = \"sam\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"SamVisionAttention\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n@@ -1103,7 +1103,7 @@ def forward(\n     \"\"\"\n )\n class SamModel(SamPreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(SamTwoWayAttentionBlock, index=2)}\n \n     def __init__(self, config: SamConfig):"
        },
        {
            "sha": "e739468e2270eea74fbe0d9fecf9902dbe0f4996",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -552,7 +552,7 @@ class Sam2PreTrainedModel(PreTrainedModel):\n     config_class = Sam2Config\n     base_model_prefix = \"sam2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -1268,7 +1268,7 @@ def _dynamic_multimask_via_stability(self, all_mask_logits, all_iou_scores):\n     \"\"\"\n )\n class Sam2Model(Sam2PreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam2TwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = [\n         r\"^memory_.*\","
        },
        {
            "sha": "178734a6a3979c7cc6e92bfb3935d14ac67fcf68",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -668,7 +668,7 @@ class Sam2PreTrainedModel(PreTrainedModel):\n     config_class = Sam2Config\n     base_model_prefix = \"sam2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "d491e10d5692d025b995217606f56325d1d19385",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1556,7 +1556,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class Sam2VideoModel(Sam2VideoPreTrainedModel):\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam2VideoTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = []\n     _tied_weights_keys = {"
        },
        {
            "sha": "65ff51bdc3e3e2e8ef0a45c379deaf114c4e3e4c",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1445,7 +1445,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class Sam2VideoModel(Sam2Model):\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     _tied_weights_keys = {\n         \"prompt_encoder.shared_embedding.positional_embedding\": \"shared_image_embedding.positional_embedding\"\n     }"
        },
        {
            "sha": "829fe01b9436808be8cd64127601601804ffd04d",
            "filename": "src/transformers/models/sam3_tracker/modeling_sam3_tracker.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fmodeling_sam3_tracker.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -112,7 +112,7 @@ class Sam3TrackerPreTrainedModel(PreTrainedModel):\n     config_class = Sam3TrackerConfig\n     base_model_prefix = \"sam3_tracker\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n     _supports_attention_backend = True\n@@ -755,7 +755,7 @@ class Sam3TrackerVisionEncoderOutput(ModelOutput):\n     \"\"\"\n )\n class Sam3TrackerModel(Sam3TrackerPreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam3TrackerTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = [\n         r\"^detector_model.\","
        },
        {
            "sha": "d31ff21f80d36e68184822e905b60b57310ba0c8",
            "filename": "src/transformers/models/sam3_tracker_video/modeling_sam3_tracker_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fmodeling_sam3_tracker_video.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1564,7 +1564,7 @@ def get_1d_sine_pe(pos_inds, dim, temperature=10000):\n \n @auto_docstring\n class Sam3TrackerVideoModel(Sam3TrackerVideoPreTrainedModel):\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(Sam3TrackerVideoTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_unexpected = [r\"^detector_model.\"]\n     _tied_weights_keys = {}"
        },
        {
            "sha": "5c74dfde9c94477df4478e1fe2c0b38f79ce7f4b",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -418,7 +418,7 @@ class SamHQPreTrainedModel(PreTrainedModel):\n     config: SamHQConfig\n     base_model_prefix = \"sam_hq\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\"SamHQVisionAttention\"]\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n@@ -1230,7 +1230,7 @@ def forward(\n     \"\"\"\n )\n class SamHQModel(SamHQPreTrainedModel):\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _can_record_outputs = {\"mask_decoder_attentions\": OutputRecorder(SamHQTwoWayAttentionBlock, index=2)}\n     _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n "
        },
        {
            "sha": "acd95ec9f0d4e7f330e0bd5e0502fa08d4225e22",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -2940,7 +2940,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TForTextToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3259,7 +3259,7 @@ def generate(\n )\n class SeamlessM4TForSpeechToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n     input_modalities = \"audio\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3581,8 +3581,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4TModel(SeamlessM4TPreTrainedModel, GenerationMixin):\n-    input_modalities = [\"audio\", \"text\"]\n-    output_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n+    output_modalities = (\"audio\", \"text\")\n     _tied_weights_keys = {\n         \"lm_head.weight\": \"shared.weight\",\n         \"text_encoder.embed_tokens.weight\": \"shared.weight\","
        },
        {
            "sha": "bbb5b13bc9c724ccccac4beffa7e43d4f97d4e66",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -3152,7 +3152,7 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2ForTextToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3508,7 +3508,7 @@ def generate(\n )\n class SeamlessM4Tv2ForSpeechToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     input_modalities = \"audio\"\n-    output_modalities = \"audio\"\n+    output_modalities = (\"audio\",)\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3866,8 +3866,8 @@ def generate(\n     \"\"\"\n )\n class SeamlessM4Tv2Model(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n-    input_modalities = [\"audio\", \"text\"]\n-    output_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n+    output_modalities = (\"audio\", \"text\")\n     _tied_weights_keys = {\n         \"lm_head.weight\": \"shared.weight\",\n         \"text_encoder.embed_tokens.weight\": \"shared.weight\","
        },
        {
            "sha": "7bf80b7643c6084348856a2746f5c2a170671a0c",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -412,7 +412,7 @@ class SegformerPreTrainedModel(PreTrainedModel):\n     config: SegformerConfig\n     base_model_prefix = \"segformer\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n \n @auto_docstring"
        },
        {
            "sha": "17ccbda1ca514be9d3da43e43028c6f620bd9e40",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -592,7 +592,7 @@ class SegGptPreTrainedModel(PreTrainedModel):\n     config: SegGptConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SegGptEmbeddings\", \"SegGptLayer\"]\n "
        },
        {
            "sha": "11c0a77d22f6541198c6ac04504a26c01ecb15b7",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -44,7 +44,7 @@ class ShieldGemma2ImageClassifierOutputWithNoAttention(ImageClassifierOutputWith\n @auto_docstring\n class ShieldGemma2ForImageClassification(PreTrainedModel):\n     config: ShieldGemma2Config\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _checkpoint_conversion_mapping = {\n         \"model.language_model.model\": \"model.model.language_model\",\n         \"model.vision_tower\": \"model.model.vision_tower\","
        },
        {
            "sha": "875814cef197b52fba7e7f43ef0c1dfa150f01c3",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -401,7 +401,7 @@ def forward(\n class SiglipPreTrainedModel(PreTrainedModel):\n     config: SiglipConfig\n     base_model_prefix = \"siglip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\n@@ -565,7 +565,7 @@ def forward(\n )\n class SiglipTextModel(SiglipPreTrainedModel):\n     config: SiglipTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: SiglipTextConfig):\n         super().__init__(config)\n@@ -689,7 +689,7 @@ def forward(self, hidden_state):\n class SiglipVisionModel(SiglipPreTrainedModel):\n     config: SiglipVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: SiglipVisionConfig):\n         super().__init__(config)\n@@ -949,7 +949,7 @@ def forward(\n )\n class SiglipForImageClassification(SiglipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: SiglipConfig) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "9b24f5a33f931c0c17f26017765ac826a1047b0d",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -383,7 +383,7 @@ def default_flax_embed_init(tensor):\n class Siglip2PreTrainedModel(PreTrainedModel):\n     config: Siglip2Config\n     base_model_prefix = \"siglip2\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\n@@ -651,7 +651,7 @@ def forward(\n )\n class Siglip2TextModel(Siglip2PreTrainedModel):\n     config: Siglip2TextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__(config)\n@@ -738,7 +738,7 @@ def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Ten\n class Siglip2VisionModel(Siglip2PreTrainedModel):\n     config: Siglip2VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: Siglip2VisionConfig):\n         super().__init__(config)\n@@ -1028,7 +1028,7 @@ def forward(\n )\n class Siglip2ForImageClassification(Siglip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: Siglip2Config) -> None:\n         super().__init__(config)"
        },
        {
            "sha": "02a080385aa6842c038cd47ede59c3221bd06e5f",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -53,7 +53,7 @@\n class SmolVLMPreTrainedModel(PreTrainedModel):\n     config: SmolVLMConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SmolVLMVisionAttention\", \"SmolVLMDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -312,7 +312,7 @@ def forward(\n )\n class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     config: SmolVLMVisionConfig\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True"
        },
        {
            "sha": "c0f56a8ed32fc2641145df9b6550049bc312b950",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1007,7 +1007,7 @@ def forward(\n     \"\"\"\n )\n class Speech2TextForConditionalGeneration(Speech2TextPreTrainedModel, GenerationMixin):\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     base_model_prefix = \"model\"\n     _tied_weights_keys = {\"lm_head.weight\": \"model.decoder.embed_tokens.weight\"}\n "
        },
        {
            "sha": "1d4e4a1e3c139a9e005d3515baeff6e19a9a54de",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -2309,7 +2309,7 @@ def _generate_speech(\n     \"\"\"\n )\n class SpeechT5ForTextToSpeech(SpeechT5PreTrainedModel):\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n     main_input_name = \"input_ids\"\n \n     def __init__(self, config: SpeechT5Config):"
        },
        {
            "sha": "831118db592bdae43afd843c57e00a9373a0e9fe",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -468,7 +468,7 @@ class SuperGluePreTrainedModel(PreTrainedModel):\n     config: SuperGlueConfig\n     base_model_prefix = \"superglue\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     @torch.no_grad()\n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "570a5d63894e14047c593cc0ed461bb44d663d14",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -325,7 +325,7 @@ class SuperPointPreTrainedModel(PreTrainedModel):\n     config: SuperPointConfig\n     base_model_prefix = \"superpoint\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n \n     def extract_one_channel_pixel_values(self, pixel_values: torch.FloatTensor) -> torch.FloatTensor:"
        },
        {
            "sha": "006c8a0ec0010a6c35f26993e913ab8efcfa8b6e",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -386,7 +386,7 @@ class SwiftFormerPreTrainedModel(PreTrainedModel):\n     config: SwiftFormerConfig\n     base_model_prefix = \"swiftformer\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SwiftFormerEncoderBlock\"]\n "
        },
        {
            "sha": "5fbfda4248105bdfa2fbe5e7d38c7d1308c67afe",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -808,7 +808,7 @@ class SwinPreTrainedModel(PreTrainedModel):\n     config: SwinConfig\n     base_model_prefix = \"swin\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SwinStage\"]\n "
        },
        {
            "sha": "3ba0823f904c07125b1aeb1e0703c82f4a51bb8e",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -689,7 +689,7 @@ class Swin2SRPreTrainedModel(PreTrainedModel):\n     config: Swin2SRConfig\n     base_model_prefix = \"swin2sr\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "91e68c318d651b6cc457880ee6ee2abb0b7f43cd",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -883,7 +883,7 @@ class Swinv2PreTrainedModel(PreTrainedModel):\n     config: Swinv2Config\n     base_model_prefix = \"swinv2\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Swinv2Stage\"]\n "
        },
        {
            "sha": "09c5a1d9c02931c2456328e8d6fe6771cedbac4a",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -688,7 +688,7 @@ class TableTransformerPreTrainedModel(PreTrainedModel):\n     config: TableTransformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = [\n         r\"TableTransformerConvEncoder\",\n         r\"TableTransformerEncoderLayer\","
        },
        {
            "sha": "114387714c67ce21c6dfcab2955cabcbcbb8642c",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -608,7 +608,7 @@ class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n     config: TimeSeriesTransformerConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     supports_gradient_checkpointing = True\n     # TODO: tests would need a rewrite to check for correct implementation\n     # Current tests always assume certain inputs to be passed"
        },
        {
            "sha": "e9251102556ef009e36ef0c98ea83665d09a8ceb",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -304,7 +304,7 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"timesfm\"\n     _no_split_modules = [\"TimesFmDecoderLayer\"]\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     _supports_sdpa = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "a265fb54f4219949318bcaf47a2ad882e61440d2",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -260,7 +260,7 @@ class TimesFmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"timesfm\"\n     _no_split_modules = [\"TimesFmDecoderLayer\"]\n     main_input_name = \"past_values\"\n-    input_modalities = \"time\"\n+    input_modalities = (\"time\",)\n     _supports_sdpa = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "147b2a663d772a59a0a32abd86918d083c7fdeeb",
            "filename": "src/transformers/models/timesformer/modeling_timesformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fmodeling_timesformer.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -452,7 +452,7 @@ class TimesformerPreTrainedModel(PreTrainedModel):\n     config: TimesformerConfig\n     base_model_prefix = \"timesformer\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"TimesformerLayer\"]\n "
        },
        {
            "sha": "4b90ea27f120686f5b263ac1fb2cebd80592d670",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -39,7 +39,7 @@ class TimmBackbone(PreTrainedModel, BackboneMixin):\n     \"\"\"\n \n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = False\n     config: TimmBackboneConfig\n "
        },
        {
            "sha": "aa541e0b15fb70edb7983668ca02d342bfb6d4d2",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -82,7 +82,7 @@ def _create_timm_model_with_error_handling(config: \"TimmWrapperConfig\", **model_\n class TimmWrapperPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"timm_model\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     config: TimmWrapperConfig\n     _no_split_modules = []\n     model_tags = [\"timm\"]"
        },
        {
            "sha": "24775baf7ab0e2b9a3ccdabbfe61f6f0c79b5fb0",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -520,7 +520,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n class TvpPreTrainedModel(PreTrainedModel):\n     config: TvpConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"video\", \"text\"]\n+    input_modalities = (\"video\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "bce23ba287264ee0e7cd94af073e0c4423354313",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -252,7 +252,7 @@ def forward(self, pixel_values):\n class UdopPreTrainedModel(PreTrainedModel):\n     config: UdopConfig\n     base_model_prefix = \"transformer\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     _can_compile_fullgraph = False"
        },
        {
            "sha": "af212b57c68df5e19c4e1efa095c5f9111e4265f",
            "filename": "src/transformers/models/upernet/modeling_upernet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fmodeling_upernet.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -269,7 +269,7 @@ def forward(self, encoder_hidden_states: torch.Tensor) -> torch.Tensor:\n class UperNetPreTrainedModel(PreTrainedModel):\n     config: UperNetConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _no_split_modules = []\n \n "
        },
        {
            "sha": "d635977ca151bb47f2f4a4914f182ce7ea5847b7",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -370,7 +370,7 @@ def forward(\n class VideoLlama3PreTrainedModel(PreTrainedModel):\n     config: VideoLlama3Config\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlama3VisionEncoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -384,7 +384,7 @@ class VideoLlama3PreTrainedModel(PreTrainedModel):\n class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n     config: VideoLlama3VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _can_record_outputs = {\n         \"hidden_states\": VideoLlama3VisionEncoderLayer,\n         \"attentions\": VideoLlama3VisionAttention,"
        },
        {
            "sha": "9389b9112ff9351bb208b4e25b9365eb480167b5",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -437,7 +437,7 @@ class VideoLlama3PreTrainedModel(Qwen2VLPreTrainedModel):\n class VideoLlama3VisionModel(VideoLlama3PreTrainedModel):\n     config: VideoLlama3VisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     _can_record_outputs = {\n         \"hidden_states\": VideoLlama3VisionEncoderLayer,\n         \"attentions\": VideoLlama3VisionAttention,"
        },
        {
            "sha": "dfa2c6559f8c917bfb97c06f461a8b20cc7fce4a",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -126,7 +126,7 @@ def forward(self, image_features):\n class VideoLlavaPreTrainedModel(PreTrainedModel):\n     config: VideoLlavaConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"video\", \"text\"]\n+    input_modalities = (\"image\", \"video\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "5fc0fe2b9e15ec65cbdac3482c876e692eabe98e",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -512,7 +512,7 @@ def forward(\n class ViltPreTrainedModel(PreTrainedModel):\n     config: ViltConfig\n     base_model_prefix = \"vilt\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViltEmbeddings\", \"ViltSelfAttention\"]\n "
        },
        {
            "sha": "dcd50dcb02b65417e89200c0fa8beadf56d803af",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -114,7 +114,7 @@ def forward(self, hidden_states):\n class VipLlavaPreTrainedModel(PreTrainedModel):\n     config: VipLlavaConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n "
        },
        {
            "sha": "d91633ed8809e6a9f11622af1032ebcaf9e58728",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -65,7 +65,7 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     config: VisionEncoderDecoderConfig\n     base_model_prefix = \"vision_encoder_decoder\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n     _supports_flash_attn = True\n     _supports_sdpa = True"
        },
        {
            "sha": "ee1ddfa0f6e481121175b2d9c2e840a50fcbd890",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -47,7 +47,7 @@ def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n class VisionTextDualEncoderModel(PreTrainedModel):\n     config: VisionTextDualEncoderConfig\n     base_model_prefix = \"vision_text_dual_encoder\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     _supports_flash_attn = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "3364428a0cf22caefd6dd4e0e0418cc7c51ea06d",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -458,7 +458,7 @@ def forward(self, sequence_output, pooled_output):\n class VisualBertPreTrainedModel(PreTrainedModel):\n     config: VisualBertConfig\n     base_model_prefix = \"visual_bert\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "85fc66fe0b761bacb295819bf9eaa6bfb6299f8e",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -354,7 +354,7 @@ class ViTPreTrainedModel(PreTrainedModel):\n     config: ViTConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "7f2a3fb5ff44dd7771a7b5ef93d6c6d3eab8de13",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -522,7 +522,7 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     config: ViTMAEConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn = True"
        },
        {
            "sha": "b0e6b44491a8b76d1a40ba713300103d5087a39c",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -357,7 +357,7 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     config: ViTMSNConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "ba255295dac52e58cd3563d91d43e710c5a05b40",
            "filename": "src/transformers/models/vitdet/modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitdet%2Fmodeling_vitdet.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -576,7 +576,7 @@ class VitDetPreTrainedModel(PreTrainedModel):\n     config: VitDetConfig\n     base_model_prefix = \"vitdet\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n "
        },
        {
            "sha": "dfa408e126333d5e8e24a57b2a44ef8e4c8c5713",
            "filename": "src/transformers/models/vitmatte/modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -55,7 +55,7 @@ class ImageMattingOutput(ModelOutput):\n class VitMattePreTrainedModel(PreTrainedModel):\n     config: VitMatteConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n "
        },
        {
            "sha": "b2d24979625eee11163f6a068d909a6ce12634de",
            "filename": "src/transformers/models/vitpose/modeling_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -64,7 +64,7 @@ class VitPosePreTrainedModel(PreTrainedModel):\n     config: VitPoseConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()"
        },
        {
            "sha": "52a8c78a267b5dcfd4ae00ce6ec22530ce5f61e7",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -349,7 +349,7 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     config: VitPoseBackboneConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n     _supports_sdpa = True"
        },
        {
            "sha": "c2edced1a11973ac4089a69dd981ba4753c5abd7",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -220,7 +220,7 @@ def forward(\n class VoxtralPreTrainedModel(PreTrainedModel):\n     config: VoxtralConfig\n     base_model_prefix = \"model\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = None\n     _skip_keys_device_placement = \"past_key_values\""
        },
        {
            "sha": "f32d984e5899ab52290e0687414d547c97c15d4b",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -530,7 +530,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     config: WhisperConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"input_features\"\n-    input_modalities = [\"audio\", \"text\"]\n+    input_modalities = (\"audio\", \"text\")\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"]\n     _supports_flash_attn = True"
        },
        {
            "sha": "81f5404602280f7db46e6ad7b8bdce83d90a2228",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -502,7 +502,7 @@ def forward(\n class XCLIPPreTrainedModel(PreTrainedModel):\n     config: XCLIPConfig\n     base_model_prefix = \"x_clip\"\n-    input_modalities = [\"image\", \"text\"]\n+    input_modalities = (\"image\", \"text\")\n     supports_gradient_checkpointing = True\n \n     @torch.no_grad()\n@@ -714,7 +714,7 @@ def forward(\n \n class XCLIPTextModel(XCLIPPreTrainedModel):\n     config: XCLIPTextConfig\n-    input_modalities = \"text\"\n+    input_modalities = (\"text\",)\n \n     def __init__(self, config: XCLIPTextConfig):\n         super().__init__(config)\n@@ -909,7 +909,7 @@ def forward(\n class XCLIPVisionModel(XCLIPPreTrainedModel):\n     config: XCLIPVisionConfig\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n \n     def __init__(self, config: XCLIPVisionConfig):\n         super().__init__(config)"
        },
        {
            "sha": "a296174b8b7478ff4880421ca44a4f96ebf76e3a",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -433,7 +433,7 @@ class YolosPreTrainedModel(PreTrainedModel):\n     config: YolosConfig\n     base_model_prefix = \"vit\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True"
        },
        {
            "sha": "6faabf639bd3931d548092d0173f741b10ffcb7c",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1208,7 +1208,7 @@ class ZoeDepthPreTrainedModel(PreTrainedModel):\n     config: ZoeDepthConfig\n     base_model_prefix = \"zoedepth\"\n     main_input_name = \"pixel_values\"\n-    input_modalities = \"image\"\n+    input_modalities = (\"image\",)\n     supports_gradient_checkpointing = True\n \n "
        },
        {
            "sha": "573a102d9fc3c9fb2ca9a5c6c92e1f77a2417bfe",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -45,6 +45,7 @@\n     is_torch_available,\n     logging,\n )\n+from .any_to_any import AnyToAnyPipeline\n from .audio_classification import AudioClassificationPipeline\n from .automatic_speech_recognition import AutomaticSpeechRecognitionPipeline\n from .base import (\n@@ -107,6 +108,7 @@\n         AutoModelForKeypointMatching,\n         AutoModelForMaskedLM,\n         AutoModelForMaskGeneration,\n+        AutoModelForMultimodalLM,\n         AutoModelForObjectDetection,\n         AutoModelForQuestionAnswering,\n         AutoModelForSemanticSegmentation,\n@@ -328,6 +330,17 @@\n         \"default\": {\"model\": (\"magic-leap-community/superglue_outdoor\", \"f4041f8\")},\n         \"type\": \"image\",\n     },\n+    \"any-to-any\": {\n+        \"impl\": AnyToAnyPipeline,\n+        \"tf\": (),\n+        \"pt\": (AutoModelForMultimodalLM,) if is_torch_available() else (),\n+        \"default\": {\n+            \"model\": {\n+                \"pt\": (\"google/gemma-3n-E4B-it\", \"c1221e9\"),\n+            }\n+        },\n+        \"type\": \"multimodal\",\n+    },\n }\n \n PIPELINE_REGISTRY = PipelineRegistry(supported_tasks=SUPPORTED_TASKS, task_aliases=TASK_ALIASES)\n@@ -433,6 +446,8 @@ def clean_custom_task(task_info):\n @overload\n def pipeline(task: Literal[None], model: Optional[Union[str, \"PreTrainedModel\"]] = None, config: Optional[Union[str, PreTrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> Pipeline: ...\n @overload\n+def pipeline(task: Literal[\"any-to-any\"], model: Optional[Union[str, \"PreTrainedModel\"]] = None, config: Optional[Union[str, PreTrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> AnyToAnyPipeline: ...\n+@overload\n def pipeline(task: Literal[\"audio-classification\"], model: Optional[Union[str, \"PreTrainedModel\"]] = None, config: Optional[Union[str, PreTrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> AudioClassificationPipeline: ...\n @overload\n def pipeline(task: Literal[\"automatic-speech-recognition\"], model: Optional[Union[str, \"PreTrainedModel\"]] = None, config: Optional[Union[str, PreTrainedConfig]] = None, tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None, feature_extractor: Optional[Union[str, PreTrainedFeatureExtractor]] = None, image_processor: Optional[Union[str, BaseImageProcessor]] = None, processor: Optional[Union[str, ProcessorMixin]] = None, revision: Optional[str] = None, use_fast: bool = True, token: Optional[Union[str, bool]] = None, device: Optional[Union[int, str, \"torch.device\"]] = None, device_map: Optional[Union[str, dict[str, Union[int, str]]]] = None, dtype: Optional[Union[str, \"torch.dtype\"]] = \"auto\", trust_remote_code: Optional[bool] = None, model_kwargs: Optional[dict[str, Any]] = None, pipeline_class: Optional[Any] = None, **kwargs: Any) -> AutomaticSpeechRecognitionPipeline: ..."
        },
        {
            "sha": "e5febf875d7e9632767bee8b2cab6a809696b40f",
            "filename": "src/transformers/pipelines/any_to_any.py",
            "status": "added",
            "additions": 505,
            "deletions": 0,
            "changes": 505,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2Fany_to_any.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2Fany_to_any.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fany_to_any.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -0,0 +1,505 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import enum\n+from typing import Any, Optional, Union, overload\n+\n+import numpy as np\n+\n+from ..audio_utils import AudioInput\n+from ..generation import GenerationConfig\n+from ..image_utils import ImageInput\n+from ..processing_utils import ProcessingKwargs, Unpack\n+from ..utils import (\n+    add_end_docstrings,\n+    is_torch_available,\n+    is_vision_available,\n+    logging,\n+    requires_backends,\n+)\n+from ..video_utils import VideoInput\n+from .base import Pipeline, build_pipeline_init_args\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from ..models.auto.modeling_auto import MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES\n+    from .pt_utils import KeyDataset\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ReturnType(enum.Enum):\n+    TENSORS = 0\n+    NEW_TEXT = 1\n+    FULL_TEXT = 2\n+\n+\n+class Chat:\n+    \"\"\"This class is intended to just be used internally in this pipeline and not exposed to users. We convert chats\n+    to this format because the rest of the pipeline code tends to assume that lists of messages are\n+    actually a batch of samples rather than messages in the same conversation.\"\"\"\n+\n+    def __init__(self, messages: list[dict]):\n+        for message in messages:\n+            if not (\"role\" in message and \"content\" in message):\n+                raise ValueError(\"When passing chat dicts as input, each dict must have a 'role' and 'content' key.\")\n+        self.messages = messages\n+\n+\n+@add_end_docstrings(build_pipeline_init_args(has_processor=True))\n+class AnyToAnyPipeline(Pipeline):\n+    \"\"\"\n+    Multimodal Generation pipeline using an `AutoModelForMultimodalLM`. This pipeline generates text given any\n+    combination of multimodal data and text.When the underlying model is a conversational model, it can also\n+    accept one or more chats, in which case the pipeline will operate in chat mode and will continue the\n+    chat(s) by adding its response(s). Each chat takes the form of a list of dicts, where each dict contains\n+    \"role\" and \"content\" keys.\n+\n+    Unless the model you're using explicitly sets these generation parameters in its configuration files\n+    (`generation_config.json`), the following default values will be used:\n+    - max_new_tokens: 256\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import pipeline\n+\n+    >>> pipe = pipeline(task=\"any-to-any\", model=\"google/gemma-3n-E4B-it\")\n+    >>> pipe(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\", text=\"A photo of\")\n+    [{'generated_text': 'a photo of two birds'}]\n+    ```\n+\n+    ```python\n+    >>> from transformers import pipeline\n+\n+    >>> pipe = pipeline(\"any-to-any\", model=\"google/gemma-3n-E4B-it\")\n+    >>> messages = [\n+    >>>     {\n+    >>>         \"role\": \"user\",\n+    >>>         \"content\": [\n+    >>>             {\n+    >>>                 \"type\": \"image\",\n+    >>>                 \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n+    >>>             },\n+    >>>             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n+    >>>         ],\n+    >>>     },\n+    >>>     {\n+    >>>         \"role\": \"assistant\",\n+    >>>         \"content\": [\n+    >>>             {\"type\": \"text\", \"text\": \"There is a dog and\"},\n+    >>>         ],\n+    >>>     },\n+    >>> ]\n+    >>> pipe(text=messages, max_new_tokens=20, return_full_text=False)\n+    [{'input_text': [{'role': 'user',\n+        'content': [{'type': 'image',\n+        'url': 'https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg'},\n+        {'type': 'text', 'text': 'Describe this image.'}]},\n+    {'role': 'assistant',\n+        'content': [{'type': 'text', 'text': 'There is a dog and'}]}],\n+    'generated_text': ' a person in the image. The dog is sitting on the sand, and the person is sitting on'}]\n+    ```\n+\n+    Learn more about the basics of using a pipeline in the [pipeline tutorial](../pipeline_tutorial)\n+\n+    This multimodal pipeline can currently be loaded from pipeline() using the following task identifier:\n+    \"any-to-any\".\n+\n+    See the list of available models on\n+    [huggingface.co/models](https://huggingface.co/models?pipeline_tag=any-to-any).\n+    \"\"\"\n+\n+    _load_processor = True\n+    _load_image_processor = False\n+    _load_feature_extractor = False\n+    _load_tokenizer = False\n+\n+    _pipeline_calls_generate = True\n+    # Make sure the docstring is updated when the default generation config is changed\n+    _default_generation_config = GenerationConfig(\n+        max_new_tokens=256,\n+    )\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        if \"image\" in self.model.input_modalities or \"video\" in self.model.input_modalities:\n+            requires_backends(self, \"vision\")\n+            requires_backends(self, \"torchvision\")\n+        if \"audio\" in self.model.input_modalities:\n+            requires_backends(self, \"librosa\")\n+        self.check_model_type(MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES)\n+\n+    def _sanitize_parameters(\n+        self,\n+        max_new_tokens=None,\n+        generate_kwargs=None,\n+        timeout=None,\n+        return_full_text=None,\n+        return_tensors=None,\n+        return_type=None,\n+        clean_up_tokenization_spaces=None,\n+        stop_sequence=None,\n+        continue_final_message=None,\n+        skip_special_tokens=None,\n+        generation_mode=None,\n+        **kwargs: Unpack[ProcessingKwargs],\n+    ):\n+        forward_kwargs = {}\n+        preprocess_params = {}\n+        postprocess_params = {}\n+\n+        # Preprocess params\n+        preprocess_params.update(kwargs)\n+        if timeout is not None:\n+            preprocess_params[\"timeout\"] = timeout\n+        if continue_final_message is not None:\n+            preprocess_params[\"continue_final_message\"] = continue_final_message\n+\n+        # Forward kwargs\n+        forward_kwargs[\"generate_kwargs\"] = generate_kwargs or {}\n+        if generation_mode is not None and generation_mode != \"text\":\n+            forward_kwargs[\"generate_kwargs\"][\"generation_mode\"] = generation_mode\n+        if kwargs.get(\"load_audio_from_video\"):\n+            forward_kwargs[\"generate_kwargs\"][\"use_audio_in_video\"] = True\n+        if stop_sequence is not None:\n+            if isinstance(stop_sequence, str):\n+                stop_sequence = [stop_sequence]\n+            forward_kwargs[\"generate_kwargs\"][\"stop_strings\"] = stop_sequence\n+            forward_kwargs[\"generate_kwargs\"][\"tokenizer\"] = self.processor.tokenizer\n+\n+        if max_new_tokens is not None:\n+            if generate_kwargs is not None and \"max_new_tokens\" in generate_kwargs:\n+                raise ValueError(\n+                    \"'max_new_tokens' is defined twice, once in 'generate_kwargs' and \"\n+                    \"once as a direct argument. Please use only one.\"\n+                )\n+            forward_kwargs[\"generate_kwargs\"][\"max_new_tokens\"] = max_new_tokens\n+\n+        if return_full_text is not None and return_type is None:\n+            if return_tensors is not None:\n+                raise ValueError(\"`return_full_text` is mutually exclusive with `return_tensors`\")\n+            return_type = ReturnType.FULL_TEXT if return_full_text else ReturnType.NEW_TEXT\n+        elif return_tensors is not None and return_type is None:\n+            return_type = ReturnType.TENSORS\n+        # We don't want to set the global default to FULLTEXT at init time. That is why\n+        # `_postprocess_params` is checked before setting the default value\n+        elif return_type is None and generation_mode in [None, \"text\"] and hasattr(self, \"_postprocess_params\"):\n+            return_type = ReturnType.FULL_TEXT\n+\n+        # Postprocess params\n+        if generation_mode not in [None, \"text\"] and return_type is not None:\n+            raise ValueError(\n+                f\"`return_type` cannot be set to {return_type} when generation_mode={generation_mode}. \"\n+                \"Set `return_type=None` or generation_mode='text'\"\n+            )\n+        if generation_mode not in [None, \"text\", \"image\", \"audio\"]:\n+            raise ValueError(\n+                f\"`generation_mode` can be only one of the `text`, `audio`, `image` but got generation_mode[={generation_mode}]\"\n+            )\n+        elif generation_mode is not None and generation_mode not in self.model.output_modalities:\n+            raise ValueError(\n+                f\"`generation_mode={generation_mode}` is not supported for {self.model.__class__.__name__}. \"\n+                f\"The model can only output the following modalities: {self.model.output_modalities}\"\n+            )\n+\n+        if return_type is not None:\n+            postprocess_params[\"return_type\"] = return_type\n+        if continue_final_message is not None:\n+            postprocess_params[\"continue_final_message\"] = continue_final_message\n+        if clean_up_tokenization_spaces is not None:\n+            postprocess_params[\"clean_up_tokenization_spaces\"] = clean_up_tokenization_spaces\n+        if skip_special_tokens is not None:\n+            postprocess_params[\"skip_special_tokens\"] = skip_special_tokens\n+        postprocess_params[\"generation_mode\"] = generation_mode\n+        return preprocess_params, forward_kwargs, postprocess_params\n+\n+    @overload\n+    def __call__(\n+        self,\n+        text: Optional[str] = None,\n+        images: Optional[Union[str, \"Image.Image\"]] = None,\n+        videos: Optional[Union[str, \"np.ndarray\", \"torch.Tensor\"]] = None,\n+        audio: Optional[Union[str, \"np.ndarray\"]] = None,\n+        **kwargs: Any,\n+    ) -> list[dict[str, Any]]: ...\n+\n+    @overload\n+    def __call__(\n+        self,\n+        text: Optional[list[str]] = None,\n+        images: Optional[Union[list[str], list[\"Image.Image\"]]] = None,\n+        videos: Optional[Union[list[str], list[\"np.ndarray\"], list[\"torch.Tensor\"]]] = None,\n+        audio: Optional[Union[list[str], list[\"np.ndarray\"]]] = None,\n+        **kwargs: Any,\n+    ) -> list[list[dict[str, Any]]]: ...\n+\n+    def __call__(\n+        self,\n+        text: Union[str, list[str], list[dict]],\n+        images: Optional[\n+            Union[\n+                str,\n+                list[str],\n+                list[list[str]],\n+                ImageInput,\n+            ]\n+        ] = None,\n+        videos: Optional[Union[str, list[str], VideoInput]] = None,\n+        audio: Optional[Union[str, list[str], AudioInput]] = None,\n+        **kwargs,\n+    ) -> Union[list[dict[str, Any]], list[list[dict[str, Any]]]]:\n+        \"\"\"\n+        Generate a text given text and optionally multimodal data passed as inputs.\n+\n+        Args:\n+            text (`str`, `list[str]`, `list[dict]`):\n+                The text to be used for generation. If a list of strings is passed, the length of the list should be\n+                the same as the number of images. Text can also follow the chat format: a list of dictionaries where\n+                each dictionary represents a message in a conversation. Each dictionary should have two keys: 'role'\n+                and 'content'. 'role' should be one of 'user', 'system' or 'assistant'. 'content' should be a list of\n+                dictionary containing the text of the message and the type of the message.\n+            images (`str`, `list[str]`, `ImageInput`):\n+                The pipeline handles three types of images:\n+\n+                - A string containing a HTTP(s) link pointing to an image\n+                - A string containing a local path to an image\n+                - An image loaded in PIL directly\n+\n+                The pipeline accepts either a single image or a batch of images. Finally, this pipeline also supports\n+                the chat format (see `text`) containing images and text in this argument.\n+            videos (`str`, `list[str]`, `VideoInput`):\n+                The pipeline handles three types of videos:\n+\n+                - A string containing a HTTP(s) link pointing to a video\n+                - A string containing a local path to a video\n+                - A video loaded and decoded to array format\n+\n+                The pipeline accepts either a single video or a batch of videos. Finally, this pipeline also supports\n+                the chat format (see `text`) containing videos and text in this argument.\n+            audio (`str`, `list[str]`, `AudioInput`):\n+                The pipeline handles three types of audios:\n+\n+                - A string containing a HTTP(s) link pointing to an audio\n+                - A string containing a local path to an audio\n+                - An audio loaded in PIL directly\n+\n+                The pipeline accepts either a single audios or a batch of audios. Finally, this pipeline also supports\n+                the chat format (see `text`) containing audios and text in this argument.\n+            return_tensors (`bool`, *optional*, defaults to `False`):\n+                Returns the tensors of predictions (as token indices) in the outputs. If set to\n+                `True`, the decoded text is not returned.\n+            return_text (`bool`, *optional*):\n+                Returns the decoded texts in the outputs.\n+            return_full_text (`bool`, *optional*, defaults to `True`):\n+                If set to `False` only added text is returned, otherwise the full text is returned. Cannot be\n+                specified at the same time as `return_text`.\n+            clean_up_tokenization_spaces (`bool`, *optional*, defaults to `True`):\n+                Whether or not to clean up the potential extra spaces in the text output.\n+            continue_final_message( `bool`, *optional*): This indicates that you want the model to continue the\n+                last message in the input chat rather than starting a new one, allowing you to \"prefill\" its response.\n+                By default this is `True` when the final message in the input chat has the `assistant` role and\n+                `False` otherwise, but you can manually override that behaviour by setting this flag.\n+\n+        Return:\n+            A list or a list of list of `dict`: Each result comes as a dictionary with the following key (cannot\n+            return a combination of both `generated_text` and `generated_token_ids`):\n+\n+            - **generated_text** (`str`, present when `return_text=True` and `generation_mode=\"text\") -- The generated text.\n+            - **generated_audio** (`np.ndarray`, present when `generation_mode=\"audio\") -- The generated audio.\n+            - **generated_image** (`PIL.Image.Image`, present when `generation_mode=\"image\") -- The generated image.\n+            - **generated_token_ids** (`torch.Tensor`, present when `return_tensors=True` and `generation_mode=\"text\") -- The token\n+                ids of the generated text.\n+            - **input_text** (`str`) -- The input text.\n+        \"\"\"\n+        if images is None and text is None:\n+            raise ValueError(\"You must at least provide either text or images.\")\n+\n+        if isinstance(text, (list, tuple, KeyDataset)) and isinstance(text[0], (list, tuple, dict)):\n+            # We have one or more prompts in list-of-dicts format, so this is chat mode\n+            if isinstance(text[0], dict) and \"role\" in text[0]:\n+                return super().__call__(Chat(text), **kwargs)\n+            elif isinstance(text[0], (list, tuple)) and isinstance(text[0][0], dict) and \"role\" in text[0][0]:\n+                chats = [Chat(chat) for chat in text]  # 🐈 🐈 🐈\n+                return super().__call__(chats, **kwargs)\n+\n+        if text is not None and not (isinstance(text, str) or (isinstance(text, list) and isinstance(text[0], str))):\n+            \"\"\"\n+            Supports the following format\n+            - {\"text\": text, \"image\": image, \"video\": video, \"audio\": audio}\n+            - [{\"text\": text, \"image\": image, \"video\": video, \"audio\": audio}]\n+            - Generator and datasets\n+            This is a common pattern in other multimodal pipelines, so we support it here as well.\n+            \"\"\"\n+            return super().__call__(text, **kwargs)\n+\n+        # encourage the user to use the chat format if supported\n+        if getattr(self.processor, \"chat_template\", None) is not None:\n+            logger.warning_once(\n+                \"The input data was not formatted as a chat with dicts containing 'role' and 'content' keys, even \"\n+                \"though this model supports chat. Consider using the chat format for better results. For more \"\n+                \"information, see https://huggingface.co/docs/transformers/en/chat_templating\"\n+            )\n+\n+        return super().__call__({\"text\": text, \"images\": images, \"video\": videos, \"audio\": audio}, **kwargs)\n+\n+    def preprocess(self, inputs=None, timeout=None, continue_final_message=None, **processing_kwargs):\n+        if isinstance(inputs, Chat):\n+            # If the user passes a chat that ends in an assistant message, we treat it as a prefill by default\n+            # because very few models support multiple separate, consecutive assistant messages\n+            if continue_final_message is None:\n+                continue_final_message = inputs.messages[-1][\"role\"] == \"assistant\"\n+\n+            # Handle Mistral tokenizer which does not accept processing kwargs\n+            chat_template_kwargs = {\"add_generation_prompt\": not continue_final_message, **processing_kwargs}\n+            if self.processor.tokenizer.__class__.__name__ == \"MistralCommonTokenizer\":\n+                chat_template_kwargs = {\n+                    k: v for k, v in chat_template_kwargs.items() if k in [\"padding\", \"truncation\", \"max_length\"]\n+                }\n+\n+            model_inputs = self.processor.apply_chat_template(\n+                inputs.messages,\n+                continue_final_message=continue_final_message,\n+                return_tensors=\"pt\",\n+                tokenize=True,\n+                return_dict=True,\n+                **chat_template_kwargs,\n+            ).to(dtype=self.dtype)\n+            model_inputs[\"text\"] = inputs\n+            return model_inputs\n+\n+        # In case we only have text inputs\n+        if isinstance(inputs, (list, tuple, str)):\n+            text = inputs\n+            inputs = {}\n+        else:\n+            inputs = inputs.copy()  # avoid in-place changes if users passed dict\n+            text = inputs.pop(\"text\")\n+\n+            # Feature extractor do not load audio files and expect a decode array\n+            if \"audio\" in inputs and hasattr(self.processor, \"feature_extractor\"):\n+                inputs[\"audio\"] = self.processor.feature_extractor.fetch_audio(inputs[\"audio\"])\n+\n+        # If batched text inputs, we set padding to True unless specified otherwise\n+        if isinstance(text, (list, tuple)) and len(text) > 1:\n+            processing_kwargs.setdefault(\"padding\", True)\n+\n+        # Multimodal data is loaded in preprocessors so we pass all ipnuts directly to `self.processor`\n+        model_inputs = self.processor(text=text, **inputs, return_tensors=\"pt\", **processing_kwargs).to(\n+            dtype=self.dtype\n+        )\n+        model_inputs[\"text\"] = text\n+        return model_inputs\n+\n+    def _forward(self, model_inputs, generate_kwargs=None):\n+        generate_kwargs = {} if generate_kwargs is None else generate_kwargs\n+        prompt_text = model_inputs.pop(\"text\")\n+        input_ids = model_inputs.get(\"input_ids\", model_inputs.get(\"decoder_input_ids\"))\n+\n+        # User-defined `generation_config` passed to the pipeline call take precedence\n+        if \"generation_config\" not in generate_kwargs:\n+            generate_kwargs[\"generation_config\"] = self.generation_config\n+\n+        generated_sequence = self.model.generate(**model_inputs, **generate_kwargs)\n+        return {\"generated_sequence\": generated_sequence, \"prompt_text\": prompt_text, \"input_ids\": input_ids}\n+\n+    def postprocess(\n+        self,\n+        model_outputs,\n+        return_type=None,\n+        continue_final_message=None,\n+        skip_special_tokens=None,\n+        **postprocess_kwargs,\n+    ):\n+        input_texts = model_outputs[\"prompt_text\"]\n+        input_texts = [input_texts] if isinstance(input_texts, (str, Chat)) else input_texts\n+        generated_sequence = model_outputs[\"generated_sequence\"]\n+        input_ids = model_outputs[\"input_ids\"]\n+        if return_type == ReturnType.TENSORS:\n+            return [\n+                {\"input_text\": input_texts[i], \"generated_token_ids\": generated_sequence[i]}\n+                for i in range(len(input_texts))\n+            ]\n+\n+        # Decode inputs and outputs the same way to remove input text from generated text if present\n+        skip_special_tokens = skip_special_tokens if skip_special_tokens is not None else True\n+        generation_mode = postprocess_kwargs[\"generation_mode\"] or \"text\"\n+        if generation_mode == \"image\" and hasattr(self.model, \"decode_image_tokens\"):\n+            generated_sequence = self.model.decode_image_tokens(generated_sequence.to(self.model.device))\n+        generated_outputs = self.processor.post_process_multimodal_output(\n+            generated_sequence, skip_special_tokens=skip_special_tokens, **postprocess_kwargs\n+        )\n+\n+        # Force consistent behavior for including the input text in the output\n+        if return_type in {ReturnType.NEW_TEXT, ReturnType.FULL_TEXT}:\n+            # Remove the input text from the generated text if the generated text starts with the input text\n+            # (accounting for the possibility of a space between the input and generated text)\n+            new_generated_texts = []\n+            postprocess_kwargs[\"generation_mode\"] = \"text\"\n+            decoded_inputs = self.processor.post_process_multimodal_output(\n+                input_ids, skip_special_tokens=skip_special_tokens, **postprocess_kwargs\n+            )\n+            for text_generated, decoded_input in zip(generated_outputs, decoded_inputs):\n+                # There can be added characters before the input text, so we need to find the beginning of the input text in the generated text\n+                index_input_text = text_generated.find(decoded_input)\n+                # Limit the search to 2 residual characters, like spaces or new lines, to avoid removing a large part of the answer\n+                if 0 <= index_input_text <= 2:\n+                    # If the input text is found, we remove it\n+                    new_generated_texts.append(text_generated[index_input_text + len(decoded_input) :])\n+                else:\n+                    new_generated_texts.append(text_generated)\n+            generated_outputs = new_generated_texts\n+        if return_type == ReturnType.FULL_TEXT:\n+            full_texts = []\n+            for prompt_text, generated_text in zip(input_texts, generated_outputs):\n+                if isinstance(prompt_text, str):\n+                    generated_text = prompt_text + generated_text\n+                elif isinstance(prompt_text, Chat):\n+                    if continue_final_message is None:\n+                        # If the user passes a chat ending in an assistant message, we treat it as a prefill by\n+                        # default because very few models support multiple separate, consecutive assistant messages\n+                        continue_final_message = prompt_text.messages[-1][\"role\"] == \"assistant\"\n+                    if continue_final_message:\n+                        # With assistant prefill, concat onto the end of the last message\n+                        new_text = dict(prompt_text.messages[-1][\"content\"][-1].items())\n+                        new_text[\"text\"] += generated_text\n+                        generated_text = list(prompt_text.messages)[:-1] + [\n+                            {\n+                                \"role\": prompt_text.messages[-1][\"role\"],\n+                                \"content\": prompt_text.messages[-1][\"content\"][:-1] + [new_text],\n+                            }\n+                        ]\n+                    else:\n+                        # When we're not starting from a prefill, the output is a new assistant message\n+                        generated_text = list(prompt_text.messages) + [\n+                            {\"role\": \"assistant\", \"content\": generated_text}\n+                        ]\n+                full_texts.append(generated_text)\n+            generated_outputs = full_texts\n+\n+        records = [\n+            {\n+                \"input_text\": input_text.messages if isinstance(input_text, Chat) else input_text,\n+                f\"generated_{generation_mode}\": generated_output,\n+            }\n+            for input_text, generated_output in zip(input_texts, generated_outputs)\n+        ]\n+\n+        return records"
        },
        {
            "sha": "58677f86273880d5c83148a6a7cc8f7c14fddb5a",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -937,6 +937,17 @@ def __init__(\n                 # then we should keep working\n                 self.image_processor = self.feature_extractor\n \n+    def __repr__(self):\n+        pipe_information = {\n+            \"model\": self.model.__class__.__name__,\n+            \"dtype\": str(self.dtype).split(\".\")[-1],\n+            \"device\": self.device.type,\n+            \"input_modalities\": self.model.input_modalities,\n+        }\n+        if self.model.can_generate():\n+            pipe_information[\"output_modalities\"] = self.model.output_modalities\n+        return f\"{self.__class__.__name__}: {pipe_information}\"\n+\n     def save_pretrained(\n         self,\n         save_directory: str | os.PathLike,"
        },
        {
            "sha": "9d268f93c6037d984d46324c5784c377d0ce4980",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1749,6 +1749,35 @@ def apply_chat_template(\n                 return out[\"input_ids\"]\n         return prompt\n \n+    def post_process_multimodal_output(\n+        self, generated_outputs, skip_special_tokens=True, generation_mode=None, **kwargs\n+    ):\n+        \"\"\"\n+        Post-process the output of a multimodal model to return the requested modality output.\n+        If the model cannot generated the requested modality, an error will be raised.\n+\n+        Args:\n+            generated_outputs (`torch.Tensor` or `np.ndarray`):\n+                The output of the model `generate` function. The output is expected to be a tensor of shape `(batch_size, sequence_length)`\n+                or `(sequence_length,)`.\n+            skip_special_tokens (`bool`, *optional*, defaults to `True`):\n+                Whether or not to remove special tokens in the output. Argument passed to the tokenizer's `batch_decode` method.\n+            generation_mode (`str`, *optional*):\n+                Generation mode indicated which modality to output and can be one of `[\"text\", \"image\", \"audio\"]`.\n+            **kwargs:\n+                Additional arguments to be passed to the tokenizer's `batch_decode method`.\n+\n+        Returns:\n+            `list[str]`: The decoded text.\n+        \"\"\"\n+        if generation_mode is not None and generation_mode != \"text\":\n+            raise ValueError(\n+                f\"{self.__class__.__name__} got an unexpected generation_mode={generation_mode}. Supported options are only [`text`]\"\n+            )\n+        return self.post_process_image_text_to_text(\n+            generated_outputs, skip_special_tokens=skip_special_tokens, **kwargs\n+        )\n+\n     def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-process the output of a vlm to decode the text."
        },
        {
            "sha": "d1385bbcc9a6a90b88b389db0abf2e9295b26ac7",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -1457,12 +1457,13 @@ def apply_chat_template(\n         def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             \"\"\"Adapt message to `mistral-common` format and leave validation to `mistral-common`.\"\"\"\n             if not isinstance(message, dict):\n-                return\n+                return message\n             maybe_list_content: str | list[dict[str, str | dict[str, Any]]] | None = message.get(\"content\")\n             if not maybe_list_content or isinstance(maybe_list_content, str):\n-                return\n+                return message\n \n             normalized_content: list[dict[str, str | dict[str, Any]]] = []\n+            message = message.copy()\n             for content in maybe_list_content:\n                 content_type = content.get(\"type\", None)\n                 if not content_type:\n@@ -1498,6 +1499,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n                 else:\n                     normalized_content.append(content)\n             message[\"content\"] = normalized_content\n+            return message\n \n         outputs = []\n         images: list[np.ndarray] = []\n@@ -1506,7 +1508,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n         for conversation in conversations:\n             messages: list[dict[str, str | list[dict[str, str | dict[str, Any]]]]] = []\n             for message in conversation:\n-                _maybe_adapt_message(message)\n+                message = _maybe_adapt_message(message)\n                 messages.append(message)\n \n             chat_request = ChatCompletionRequest.from_openai(\n@@ -1758,12 +1760,11 @@ def from_pretrained(\n         if init_inputs:\n             raise ValueError(\"`init_inputs` are not supported by `MistralCommonTokenizer.from_pretrained`.\")\n \n-        # Handle kwargs and AutoTokenizer case\n-        ignore_subset = {\"_from_auto\", \"trust_remote_code\"}\n-        if kwargs and not (set_kwargs := set(kwargs.keys())).issubset(ignore_subset):\n-            raise ValueError(\n-                f\"Kwargs {list(set_kwargs - ignore_subset)} are not supported by `MistralCommonTokenizer.from_pretrained`.\"\n-            )\n+        # Handle kwargs and AutoTokenizer/AutoProcessor case\n+        if kwargs and not set(kwargs.keys()).issubset(\n+            {\"trust_remote_code\", \"_from_pipeline\", \"_commit_hash\", \"dtype\", \"_from_auto\"}\n+        ):\n+            raise ValueError(f\"Some kwargs in {kwargs} are not supported by `MistralCommonTokenizer.from_pretrained`.\")\n \n         if not os.path.isdir(pretrained_model_name_or_path):\n             tokenizer_path = download_tokenizer_from_hf_hub("
        },
        {
            "sha": "1856ea982a54b5f03d7a871faf650b4511c4ed40",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -166,6 +166,7 @@ class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n     pipeline_model_mapping = (\n         {\n             \"image-text-to-text\": AyaVisionForConditionalGeneration,\n+            \"any-to-any\": AyaVisionForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "2448d3221cf776fd5cda0ca930a82a51451ff34d",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -788,6 +788,7 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n             \"image-to-text\": Blip2ForConditionalGeneration,\n             \"visual-question-answering\": Blip2ForConditionalGeneration,\n             \"image-text-to-text\": Blip2ForConditionalGeneration,\n+            \"any-to-any\": Blip2ForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "50050dd3b4694ba799bcbb1e2f0efddf8fe30c75",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -246,11 +246,12 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class ChameleonVision2SeqModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class ChameleonVision2SeqModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ChameleonModel, ChameleonForConditionalGeneration) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"image-text-to-text\": ChameleonForConditionalGeneration,\n+            \"any-to-any\": ChameleonForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "0aabcf5f2e6a4b7c8192e0f8a6d9c1b413935aa2",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -153,6 +153,7 @@ class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     pipeline_model_mapping = (\n         {\n             \"image-text-to-text\": Cohere2VisionForConditionalGeneration,\n+            \"any-to-any\": Cohere2VisionForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "9639a25430f423804bf7fa8fe701fe89bc7627ac",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -35,6 +35,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -127,12 +128,13 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class DeepseekVLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class DeepseekVLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DeepseekVLModel, DeepseekVLForConditionalGeneration) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": DeepseekVLModel,\n             \"image-text-to-text\": DeepseekVLForConditionalGeneration,\n+            \"any-to-any\": DeepseekVLForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "e4b74b0620f93dbc372d1bc0b53a0ae01918cd56",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -35,6 +35,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -154,14 +155,15 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class DeepseekVLHybridModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class DeepseekVLHybridModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (DeepseekVLHybridModel, DeepseekVLHybridForConditionalGeneration) if is_torch_available() else ()\n     )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": DeepseekVLHybridModel,\n             \"image-text-to-text\": DeepseekVLHybridForConditionalGeneration,\n+            \"any-to-any\": DeepseekVLHybridForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "5803b0be37b6dc9ea3da3c01632ff5a2075237da",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -284,7 +284,11 @@ class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n         if is_torch_available()\n         else ()\n     )\n-    pipeline_model_mapping = {}\n+    pipeline_model_mapping = (\n+        {\"any-to-any\": Emu3ForConditionalGeneration, \"image-text-to-text\": Emu3ForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n \n     def setUp(self):\n         self.model_tester = Emu3Vision2TextModelTester(self)"
        },
        {
            "sha": "c9ee2e46f8731565455669978f8542144ff26e5e",
            "filename": "tests/models/florence2/test_modeling_florence2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflorence2%2Ftest_modeling_florence2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -36,6 +36,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -221,7 +222,9 @@ def test_sdpa_can_dispatch_on_flash(self):\n \n \n @require_torch\n-class Florence2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Florence2ForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `Florence2ForConditionalGeneration`.\n     \"\"\"\n@@ -231,6 +234,7 @@ class Florence2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n         {\n             \"image-to-text\": Florence2ForConditionalGeneration,\n             \"image-text-to-text\": Florence2ForConditionalGeneration,\n+            \"any-to-any\": Florence2ForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "28b4b134e59cea3cea2795aa3c78b94371294ec7",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -376,6 +376,7 @@ class GitModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n             \"image-to-text\": GitForCausalLM,\n             \"text-generation\": GitForCausalLM,\n             \"image-text-to-text\": GitForCausalLM,\n+            \"any-to-any\": GitForCausalLM,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "7cf71c7c023490aecd48609a287dfb17ffc14f64",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -149,6 +149,7 @@ class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         {\n             \"image-to-text\": GotOcr2ForConditionalGeneration,\n             \"image-text-to-text\": GotOcr2ForConditionalGeneration,\n+            \"any-to-any\": GotOcr2ForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "86feec01543b5bf05f03a181ae4f5619bb111a0f",
            "filename": "tests/models/granite_speech/test_modeling_granite_speech.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite_speech%2Ftest_modeling_granite_speech.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -43,6 +43,7 @@\n     floats_tensor,\n     ids_tensor,\n )\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -211,13 +212,15 @@ def create_and_check_granite_speech_model_fp16_autocast_forward(\n \n \n @require_torch\n-class GraniteSpeechForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class GraniteSpeechForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `GraniteSpeechForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (GraniteSpeechForConditionalGeneration,) if is_torch_available() else ()\n-\n+    pipeline_model_mapping = {\"any-to-any\": GraniteSpeechForConditionalGeneration} if is_torch_available() else {}\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "2148861e2ad31c9c05354fe642664a5c6be4afa2",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -319,7 +319,11 @@ def prepare_pixel_values(self):\n class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsModel, IdeficsForVisionText2Text) if is_torch_available() else ()\n     pipeline_model_mapping = (\n-        {\"feature-extraction\": IdeficsModel, \"image-text-to-text\": IdeficsForVisionText2Text}\n+        {\n+            \"feature-extraction\": IdeficsModel,\n+            \"image-text-to-text\": IdeficsForVisionText2Text,\n+            \"any-to-any\": IdeficsForVisionText2Text,\n+        }\n         if is_torch_available()\n         else {}\n     )"
        },
        {
            "sha": "84fd2e13cf6b7797a64ea94c52caba3ba5192069",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -45,6 +45,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -190,10 +191,14 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class JanusVisionText2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class JanusVisionText2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (JanusModel, JanusForConditionalGeneration) if is_torch_available() else ()\n     all_generative_model_classes = (JanusForConditionalGeneration,) if is_torch_available() else ()\n-\n+    pipeline_model_mapping = (\n+        {\"any-to-any\": JanusForConditionalGeneration, \"image-text-to-text\": JanusForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "9265375731f02c4814eda36966fd2ccd21fa91c6",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -248,6 +248,7 @@ class KyutaiSpeechToTextModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n         {\n             \"feature-extraction\": KyutaiSpeechToTextModel,\n             \"automatic-speech-recognition\": KyutaiSpeechToTextForConditionalGeneration,\n+            \"any-to-any\": KyutaiSpeechToTextForConditionalGeneration,\n         }\n         if is_torch_available()\n         else {}"
        },
        {
            "sha": "9db92220b8887dce89bc5633b257f47da512b1cf",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -42,6 +42,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -167,7 +168,9 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class LlavaForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `LlavaForConditionalGeneration`.\n     \"\"\"\n@@ -181,7 +184,11 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n         else ()\n     )\n     pipeline_model_mapping = (\n-        {\"image-to-text\": LlavaForConditionalGeneration, \"image-text-to-text\": LlavaForConditionalGeneration}\n+        {\n+            \"image-to-text\": LlavaForConditionalGeneration,\n+            \"image-text-to-text\": LlavaForConditionalGeneration,\n+            \"any-to-any\": LlavaForConditionalGeneration,\n+        }\n         if is_torch_available()\n         else {}\n     )"
        },
        {
            "sha": "cb6a6c934803badc90f888030017f6494cb6e878",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -191,8 +191,14 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n         if is_torch_available()\n         else ()\n     )\n-    pipeline_model_mapping = {\"image-text-to-text\": LlavaNextForConditionalGeneration} if is_torch_available() else {}\n-\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": LlavaNextForConditionalGeneration,\n+            \"any-to-any\": LlavaNextForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "603bd260ad75f74656e6381a72195c007eb4bb44",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -193,7 +193,12 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n         else ()\n     )\n     pipeline_model_mapping = (\n-        {\"image-text-to-text\": LlavaOnevisionForConditionalGeneration} if is_torch_available() else {}\n+        {\n+            \"image-text-to-text\": LlavaOnevisionForConditionalGeneration,\n+            \"any-to-any\": LlavaOnevisionForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n     )\n \n     # MP works but offload doesn't work when the MultiheadAttention is offloaded"
        },
        {
            "sha": "8e6931420390b8b9574883bb74117425a5072fe4",
            "filename": "tests/models/ovis2/test_modeling_ovis2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -39,6 +39,7 @@\n     floats_tensor,\n     ids_tensor,\n )\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -160,7 +161,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Ovis2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Ovis2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `Ovis2ForConditionalGeneration`.\n     \"\"\"\n@@ -173,7 +174,11 @@ class Ovis2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n-    pipeline_model_mapping = {\"image-text-to-text\": Ovis2ForConditionalGeneration} if is_torch_available() else {}\n+    pipeline_model_mapping = (\n+        {\"image-text-to-text\": Ovis2ForConditionalGeneration, \"any-to-any\": Ovis2ForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "a2a131fee64ff850e01c84e67e696ccd4678674c",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -49,6 +49,7 @@\n     floats_tensor,\n     ids_tensor,\n )\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -249,14 +250,25 @@ def create_and_check_qwenomnithinker_model_fp16_forward(self, config, input_ids,\n \n \n @require_torch\n-class Qwen2_5OmniThinkerForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Qwen2_5OmniThinkerForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `Qwen2_5OmniThinkerForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (Qwen2_5OmniThinkerForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (Qwen2_5OmniThinkerForConditionalGeneration,) if is_torch_available() else ()\n-\n+    # pipeline_model_mapping = (\n+    #     {\n+    #         \"any-to-any\": Qwen2_5OmniForConditionalGeneration,\n+    #         \"image-text-to-text\": Qwen2_5OmniThinkerForConditionalGeneration,\n+    #     }\n+    #     if is_torch_available()\n+    #     else {}\n+    # )\n+    # FIXME @raushan Omni tests take ages because the model is big. Try to make it even smaller\n+    pipeline_model_mapping = {}\n     _is_composite = True\n     model_split_percents = [0.5, 0.9]\n "
        },
        {
            "sha": "4df16b9f6f4b6051984683f195fb19dc21eed4d5",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -37,6 +37,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -134,13 +135,15 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Qwen2AudioForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `Qwen2AudioForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (Qwen2AudioForConditionalGeneration,) if is_torch_available() else ()\n-\n+    pipeline_model_mapping = {\"any-to-any\": Qwen2AudioForConditionalGeneration} if is_torch_available() else {}\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "d9dcc335cdba23ecd247b57cc84b7d7ff09af921",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -46,6 +46,7 @@\n     floats_tensor,\n     ids_tensor,\n )\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -165,7 +166,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     \"\"\"\n     Model tester for `Qwen2VLForConditionalGeneration`.\n     \"\"\"\n@@ -178,8 +179,10 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n         if is_torch_available()\n         else ()\n     )\n-    pipeline_model_mapping = {\"image-text-to-text\": Qwen2VLForConditionalGeneration}\n-\n+    pipeline_model_mapping = {\n+        \"image-text-to-text\": Qwen2VLForConditionalGeneration,\n+        \"any-to-any\": Qwen2VLForConditionalGeneration,\n+    }\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "8005b7f88a87559c917290a53943f97e075fcd58",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -38,6 +38,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -321,15 +322,23 @@ def test_resize_embeddings_untied(self):\n \n \n @require_torch\n-class SmolVLMForConditionalGenerationModelTest(GenerationTesterMixin, ModelTesterMixin, unittest.TestCase):\n+class SmolVLMForConditionalGenerationModelTest(\n+    GenerationTesterMixin, ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `SmolVLMForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (SmolVLMForConditionalGeneration,) if is_torch_available() else ()\n     all_generative_model_classes = (SmolVLMForConditionalGeneration,) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"image-text-to-text\": SmolVLMForConditionalGeneration} if is_torch_available() else ()\n-\n+    pipeline_model_mapping = (\n+        {\n+            \"image-text-to-text\": SmolVLMForConditionalGeneration,\n+            \"any-to-any\": SmolVLMForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else ()\n+    )\n     test_resize_embeddings = True\n \n     def setUp(self):"
        },
        {
            "sha": "0cff2a66779b710fc93aff2e246b4ad71062f8dd",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -33,6 +33,7 @@\n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n@@ -129,14 +130,16 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class VoxtralForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class VoxtralForConditionalGenerationModelTest(\n+    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n+):\n     \"\"\"\n     Model tester for `VoxtralForConditionalGeneration`.\n     \"\"\"\n \n     all_model_classes = (VoxtralForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n-        {\"text-to-speech\": VoxtralForConditionalGeneration, \"audio-text-to-text\": VoxtralForConditionalGeneration}\n+        {\"text-to-speech\": VoxtralForConditionalGeneration, \"any-to-any\": VoxtralForConditionalGeneration}\n         if is_torch_available()\n         else {}\n     )"
        },
        {
            "sha": "e047b6393d68a888bd32055b05ac0e392824f12f",
            "filename": "tests/pipelines/test_pipelines_any_to_any.py",
            "status": "added",
            "additions": 373,
            "deletions": 0,
            "changes": 373,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fpipelines%2Ftest_pipelines_any_to_any.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Fpipelines%2Ftest_pipelines_any_to_any.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_any_to_any.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -0,0 +1,373 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import sys\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import MODEL_FOR_MULTIMODAL_LM_MAPPING, is_vision_available\n+from transformers.pipelines import AnyToAnyPipeline, pipeline\n+from transformers.testing_utils import (\n+    is_pipeline_test,\n+    require_librosa,\n+    require_torch,\n+    require_vision,\n+    slow,\n+)\n+\n+from .test_pipelines_common import ANY\n+\n+\n+sys.path.append(\".\")\n+from utils.fetch_hub_objects_for_ci import url_to_local_path\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+@is_pipeline_test\n+@require_vision\n+@require_librosa\n+@require_torch\n+class AnyToAnyPipelineTests(unittest.TestCase):\n+    model_mapping = MODEL_FOR_MULTIMODAL_LM_MAPPING\n+\n+    # We only need `processor` but the Mixin will pass all possible preprocessing classes for a model.\n+    # So we add them all in signature\n+    def get_test_pipeline(\n+        self, model, tokenizer, processor, image_processor=None, feature_extractor=None, dtype=\"float32\"\n+    ):\n+        _is_images_supported = hasattr(processor, \"image_processor\")\n+        _is_videos_supported = hasattr(processor, \"video_processor\")\n+        _is_audios_supported = hasattr(processor, \"feature_extractor\")\n+\n+        image_token = getattr(processor.tokenizer, \"image_token\", \"\")\n+        video_token = getattr(processor.tokenizer, \"video_token\", \"\")\n+        audio_token = getattr(processor.tokenizer, \"audio_token\", \"\")\n+\n+        images_examples = [\n+            {\n+                \"images\": \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n+                \"text\": f\"{image_token}This is a \",\n+            },\n+            {\n+                \"images\": \"./tests/fixtures/tests_samples/COCO/000000039769.png\",\n+                \"text\": f\"{image_token}Here I see a \",\n+            },\n+        ]\n+\n+        videos_examples = [\n+            {\n+                \"videos\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\"\n+                ),\n+                \"text\": f\"{video_token}This video shows a \",\n+            },\n+            {\n+                \"video\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\"\n+                ),\n+                \"text\": f\"{video_token}In the video I see a \",\n+            },\n+        ]\n+\n+        audio_examples = [\n+            {\n+                \"audio\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\"\n+                ),\n+                \"text\": f\"{audio_token}This is sound of a \",\n+            },\n+            {\n+                \"audio\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"\n+                ),\n+                \"text\": f\"{audio_token}Here I hear a \",\n+            },\n+        ]\n+\n+        examples = []\n+        if _is_images_supported:\n+            examples.extend(images_examples)\n+        if _is_videos_supported:\n+            examples.extend(videos_examples)\n+        if _is_audios_supported:\n+            examples.extend(audio_examples)\n+\n+        pipe = AnyToAnyPipeline(model=model, processor=processor, dtype=dtype, max_new_tokens=10)\n+\n+        return pipe, examples\n+\n+    def run_pipeline_test(self, pipe, examples):\n+        # Single\n+        outputs = pipe(examples[0])\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+            ],\n+        )\n+\n+        # Batched but limit to last 2 examples\n+        outputs = pipe(examples[:2])\n+        self.assertEqual(\n+            outputs,\n+            [\n+                [\n+                    {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+                ],\n+                [\n+                    {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+                ],\n+            ],\n+        )\n+\n+        # `generation_mode` raises errors when dosn't match with other params\n+        with self.assertRaises(ValueError):\n+            pipe(examples, generation_mode=\"video\")\n+\n+        with self.assertRaises(ValueError):\n+            pipe(examples, generation_mode=\"audio\", return_full_text=True)\n+\n+        with self.assertRaises(ValueError):\n+            pipe(examples, generation_mode=\"image\", return_type=1)\n+\n+        # Chat template\n+        if getattr(pipe.processor, \"chat_template\", None) is not None:\n+            messages = []\n+            for example in examples[:2]:\n+                example.pop(\"text\")\n+                modality_type, modality_data = list(example.items())[0]\n+                message = {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"This is a \"},\n+                        {\"type\": modality_type, \"path\": modality_data},\n+                    ],\n+                }\n+                messages.append([message])\n+            outputs = pipe(messages, return_full_text=True, max_new_tokens=10)\n+\n+            self.assertEqual(\n+                outputs,\n+                [\n+                    [\n+                        {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+                    ],\n+                    [\n+                        {\"input_text\": ANY(str), \"generated_text\": ANY(str)},\n+                    ],\n+                ],\n+            )\n+\n+    @slow\n+    def test_small_model_pt_token_text_only(self):\n+        pipe = pipeline(\"any-to-any\", model=\"google/gemma-3n-E4B-it\")\n+        text = \"What is the capital of France? Assistant:\"\n+\n+        outputs = pipe(text=text, generate_kwargs={\"do_sample\": False})\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": \"What is the capital of France? Assistant:\",\n+                    \"generated_None\": \"What is the capital of France? Assistant: The capital of France is Paris.\\n\",\n+                }\n+            ],\n+        )\n+\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"},\n+                    ],\n+                },\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+                    ],\n+                },\n+            ],\n+        ]\n+        outputs = pipe(text=messages, generate_kwargs={\"do_sample\": False})\n+        self.assertEqual(\n+            outputs,\n+            [\n+                [\n+                    {\n+                        \"input_text\": [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"}],\n+                            }\n+                        ],\n+                        \"generated_None\": [\n+                            {\n+                                \"role\": \"user\",\n+                                \"content\": [{\"type\": \"text\", \"text\": \"Write a poem on Hugging Face, the company\"}],\n+                            },\n+                            {\n+                                \"role\": \"assistant\",\n+                                \"content\": \"A digital embrace, a friendly face,\\nHugging Face rises, setting the pace.\\nFor AI's heart, a vibrant core,\\nOpen source models, and so much more.\\n\\nFrom transformers deep, a powerful might,\\nNLP's future, shining so bright.\\nDatasets curated, a treasure trove found,\\nFor researchers and builders, on fertile ground.\\n\\nA community thriving, a collaborative art,\\nSharing knowledge, playing a vital part.\\nSpaces to showcase, creations unfold,\\nStories in code, bravely told.\\n\\nWith libraries sleek, and tools so refined,\\nDemocratizing AI, for all humankind.\\nFrom sentiment analysis to text generation's grace,\\nHugging Face empowers, at a rapid pace.\\n\\nA platform of learning, a place to explore,\\nUnlocking potential, and asking for more.\\nSo let's give a cheer, for this innovative team,\\nHugging Face's vision, a beautiful dream. \\n\",\n+                            },\n+                        ],\n+                    }\n+                ],\n+                [\n+                    {\n+                        \"input_text\": [\n+                            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]}\n+                        ],\n+                        \"generated_None\": [\n+                            {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the capital of France?\"}]},\n+                            {\"role\": \"assistant\", \"content\": \"The capital of France is **Paris**. \\n\"},\n+                        ],\n+                    }\n+                ],\n+            ],\n+        )\n+\n+    @slow\n+    def test_small_model_pt_token_audio_input(self):\n+        pipe = pipeline(\"any-to-any\", model=\"google/gemma-3n-E4B-it\")\n+\n+        audio_path = url_to_local_path(\n+            \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What do you hear in this audio?\"},\n+                    {\"type\": \"audio\", \"url\": audio_path},\n+                ],\n+            },\n+        ]\n+        outputs = pipe(text=messages, return_type=1, generate_kwargs={\"do_sample\": False})  # return new text\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"What do you hear in this audio?\"},\n+                                {\n+                                    \"type\": \"audio\",\n+                                    \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n+                                },\n+                            ],\n+                        }\n+                    ],\n+                    \"generated_None\": \"user\\nWhat do you hear in this audio?\\n\\n\\n\\n\\nmodel\\nThe audio contains the repeated sound of someone **coughing**. It's a fairly consistent, forceful cough throughout the duration.\",\n+                }\n+            ],\n+        )\n+\n+    @slow\n+    def test_small_model_pt_token_audio_gen(self):\n+        pipe = pipeline(\"any-to-any\", model=\"Qwen/Qwen2.5-Omni-3B\", dtype=\"bfloat16\")\n+\n+        video_path = url_to_local_path(\n+            \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Cooking_cake.mp4\"\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Describe this video.\"},\n+                    {\"type\": \"video\", \"video\": video_path},\n+                ],\n+            },\n+        ]\n+        outputs = pipe(\n+            text=messages,\n+            num_frames=16,\n+            max_new_tokens=50,\n+            load_audio_from_video=True,\n+            generate_kwargs={\"use_audio_in_video\": True, \"talker_do_sample\": False, \"do_sample\": False},\n+        )\n+        self.assertEqual(\n+            outputs,\n+            [\n+                {\n+                    \"input_text\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"Describe this video.\"},\n+                                {\n+                                    \"type\": \"video\",\n+                                    \"video\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Cooking_cake.mp4\",\n+                                },\n+                            ],\n+                        }\n+                    ],\n+                    \"generated_None\": [\n+                        {\n+                            \"role\": \"user\",\n+                            \"content\": [\n+                                {\"type\": \"text\", \"text\": \"Describe this video.\"},\n+                                {\n+                                    \"type\": \"video\",\n+                                    \"video\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Cooking_cake.mp4\",\n+                                },\n+                            ],\n+                        },\n+                        {\n+                            \"role\": \"assistant\",\n+                            \"content\": \"system\\nYou are a helpful assistant.\\nuser\\nDescribe this video.\\nassistant\\nThe video begins with a man standing in a kitchen, wearing a black shirt. He is holding a large glass bowl filled with flour and a spoon. The man starts to mix the flour in the bowl, creating a dough. As he mixes, he continues to talk to the camera, explaining the process. The kitchen has wooden cabinets and a white refrigerator in the background. The man's movements are deliberate and focused as he works with the dough. The video ends with the man still mixing the dough in the bowl. Overall, the video provides a clear and detailed demonstration of how to make dough using flour and a spoon.\",\n+                        },\n+                    ],\n+                }\n+            ],\n+        )\n+\n+        outputs = pipe(text=messages, generation_mode=\"audio\", num_frames=16, max_new_tokens=20)\n+\n+        self.assertEqual(len(outputs), len(messages))\n+        self.assertIsInstance(outputs[0], dict)\n+        for out in outputs:\n+            self.assertTrue(\"input_text\" in out)\n+            self.assertTrue(\"generated_audio\" in out)\n+            self.assertIsInstance(out[\"generated_audio\"], np.ndarray)\n+\n+    @slow\n+    def test_small_model_pt_image_gen(self):\n+        pipe = pipeline(\"any-to-any\", model=\"deepseek-community/Janus-Pro-1B\")\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"A dog running under the rain.\"},\n+                ],\n+            },\n+        ]\n+        outputs = pipe(text=messages, generation_mode=\"image\")\n+\n+        self.assertEqual(len(outputs), len(messages))\n+        self.assertIsInstance(outputs[0], dict)\n+        for out in outputs:\n+            self.assertTrue(\"input_text\" in out)\n+            self.assertTrue(\"generated_image\" in out)\n+            self.assertIsInstance(out[\"generated_image\"], PIL.Image.Image)"
        },
        {
            "sha": "a99445c487c13878d2da03600bea238fec2be4a5",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -60,6 +60,7 @@\n )\n from transformers.utils import direct_transformers_import, logging\n \n+from .pipelines.test_pipelines_any_to_any import AnyToAnyPipelineTests\n from .pipelines.test_pipelines_audio_classification import AudioClassificationPipelineTests\n from .pipelines.test_pipelines_automatic_speech_recognition import AutomaticSpeechRecognitionPipelineTests\n from .pipelines.test_pipelines_depth_estimation import DepthEstimationPipelineTests\n@@ -105,6 +106,7 @@\n     \"image-to-image\": {\"test\": ImageToImagePipelineTests},\n     \"image-to-text\": {\"test\": ImageToTextPipelineTests},\n     \"mask-generation\": {\"test\": MaskGenerationPipelineTests},\n+    \"any-to-any\": {\"test\": AnyToAnyPipelineTests},\n     \"object-detection\": {\"test\": ObjectDetectionPipelineTests},\n     \"question-answering\": {\"test\": QAPipelineTests},\n     \"summarization\": {\"test\": SummarizationPipelineTests},\n@@ -590,6 +592,18 @@ def test_pipeline_image_text_to_text(self):\n     def test_pipeline_image_text_to_text_fp16(self):\n         self.run_task_tests(task=\"image-text-to-text\", dtype=\"float16\")\n \n+    @is_pipeline_test\n+    @require_vision\n+    @require_torch\n+    def test_pipeline_any_to_any(self):\n+        self.run_task_tests(task=\"any-to-any\")\n+\n+    @is_pipeline_test\n+    @require_vision\n+    @require_torch\n+    def test_pipeline_any_to_any_fp16(self):\n+        self.run_task_tests(task=\"any-to-any\", dtype=\"float16\")\n+\n     @is_pipeline_test\n     @require_vision\n     def test_pipeline_image_to_text(self):"
        },
        {
            "sha": "134c6df34b1822402759c3317e481b2db1924d9e",
            "filename": "tests/utils/tiny_model_summary.json",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Futils%2Ftiny_model_summary.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/tests%2Futils%2Ftiny_model_summary.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftiny_model_summary.json?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -5000,6 +5000,17 @@\n         ],\n         \"sha\": \"c40765c382515ae627652d60e9077b6478448d48\"\n     },\n+    \"Qwen2_5OmniForConditionalGeneration\": {\n+        \"tokenizer_classes\": [\n+            \"Qwen2Tokenizer\",\n+            \"Qwen2TokenizerFast\"\n+        ],\n+        \"processor_classes\": [\"Qwen2_5OmniProcessor\"],\n+        \"model_classes\": [\n+            \"Qwen2_5OmniForConditionalGeneration\"\n+        ],\n+        \"sha\": \"9bc7a812cc447b430acb994f8d42e8e64c7b61f6\"\n+    },\n     \"ReformerForMaskedLM\": {\n         \"tokenizer_classes\": [\n             \"ReformerTokenizer\","
        },
        {
            "sha": "66e3d75266b099d2d67e96adbe4a132fcd177cd3",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -249,6 +249,7 @@ class DecoratedItem:\n     \"ImageGPTConfig\",\n     \"ImageSegmentationPipeline\",\n     \"ImageTextToTextPipeline\",\n+    \"AnyToAnyPipeline\",\n     \"ImageToImagePipeline\",\n     \"ImageToTextPipeline\",\n     \"InformerConfig\","
        },
        {
            "sha": "f4fda1b78cfb6f815a3f0212a1c1825f1b719cd7",
            "filename": "utils/update_metadata.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/55b1400b74b7329f050f8e2bf7ca8534f2019295/utils%2Fupdate_metadata.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55b1400b74b7329f050f8e2bf7ca8534f2019295/utils%2Fupdate_metadata.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fupdate_metadata.py?ref=55b1400b74b7329f050f8e2bf7ca8534f2019295",
            "patch": "@@ -66,6 +66,7 @@\n     (\"image-classification\", \"MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES\", \"AutoModelForImageClassification\"),\n     (\"image-segmentation\", \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES\", \"AutoModelForImageSegmentation\"),\n     (\"image-text-to-text\", \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\", \"AutoModelForImageTextToText\"),\n+    (\"any-to-any\", \"MODEL_FOR_MULTIMODAL_LM_MAPPING_NAMES\", \"AutoModelForMultimodalLM\"),\n     (\"image-to-image\", \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES\", \"AutoModelForImageToImage\"),\n     (\"fill-mask\", \"MODEL_FOR_MASKED_LM_MAPPING_NAMES\", \"AutoModelForMaskedLM\"),\n     (\"object-detection\", \"MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES\", \"AutoModelForObjectDetection\"),"
        }
    ],
    "stats": {
        "total": 2180,
        "additions": 1802,
        "deletions": 378
    }
}