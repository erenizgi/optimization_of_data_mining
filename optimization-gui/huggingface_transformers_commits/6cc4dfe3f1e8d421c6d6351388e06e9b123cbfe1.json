{
    "author": "SunMarc",
    "message": "Fix the initialization of the cache when we have multi gpu (#33303)\n\n* init cache multi-gpu\r\n\r\n* Update src/transformers/generation/utils.py\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\n\r\n* switch to execution device map\r\n\r\n* naming more consistant\r\n\r\n* fix\r\n\r\n* mutually exclusive device\r\n\r\n* added an integration example\r\n\r\n* remove useless check\r\n\r\n* suggestion from joao + typing\r\n\r\n* fix couple of typo and add test\r\n\r\n* revert check\r\n\r\n---------\r\n\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1",
    "files": [
        {
            "sha": "0671157e44703805811d609d9b75af1045226a69",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 11,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1",
            "patch": "@@ -1030,6 +1030,9 @@ class StaticCache(Cache):\n             The device on which the cache should be initialized. Should be the same as the layer.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n+        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1060,6 +1063,7 @@ def __init__(\n         device: torch.device = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         if max_batch_size is not None:\n@@ -1088,16 +1092,20 @@ def __init__(\n         # Note: There will be significant perf decrease if switching to use 5D tensors instead.\n         cache_shape = (self.batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n         for idx in range(config.num_hidden_layers):\n-            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n-            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n+            if layer_device_map is not None:\n+                layer_device = layer_device_map[idx]\n+            else:\n+                layer_device = device\n+            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             # Notes:\n             # 1. `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             #     breaks when updating the cache. It can't be used if the cache code is being compiled (but in that case\n             #     it is not needed anyway)\n             # 2. `torch.export()` requires mutations to be registered as buffers.\n             if not is_torchdynamo_compiling():\n-                self.register_buffer(f\"key_cache_{idx}\", torch.zeros(cache_shape, dtype=dtype, device=device))\n-                self.register_buffer(f\"value_cache_{idx}\", torch.zeros(cache_shape, dtype=dtype, device=device))\n+                self.register_buffer(f\"key_cache_{idx}\", torch.zeros(cache_shape, dtype=dtype, device=layer_device))\n+                self.register_buffer(f\"value_cache_{idx}\", torch.zeros(cache_shape, dtype=dtype, device=layer_device))\n                 new_layer_key_cache = getattr(self, f\"key_cache_{idx}\")\n                 new_layer_value_cache = getattr(self, f\"value_cache_{idx}\")\n                 torch._dynamo.mark_static_address(new_layer_key_cache)\n@@ -1130,9 +1138,9 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n+\n         cache_position = cache_kwargs.get(\"cache_position\")\n-        self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device=key_states.device)\n-        self.value_cache[layer_idx] = self.value_cache[layer_idx].to(device=value_states.device)\n+\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n \n@@ -1201,6 +1209,9 @@ class SlidingWindowCache(StaticCache):\n             The device on which the cache should be initialized. Should be the same as the layer.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n+        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1231,6 +1242,7 @@ def __init__(\n         device: torch.device = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n@@ -1247,6 +1259,7 @@ def __init__(\n             device=device,\n             dtype=dtype,\n             max_batch_size=max_batch_size,\n+            layer_device_map=layer_device_map,\n         )\n \n     def update(\n@@ -1280,7 +1293,6 @@ def update(\n         v_out = v_out[:, :, indices]\n \n         try:\n-            cache_position.to(device=k_out.device)\n             k_out.index_copy_(2, cache_position, key_states)\n             v_out.index_copy_(2, cache_position, value_states)\n         except NotImplementedError:\n@@ -1495,6 +1507,9 @@ class HybridCache(Cache):\n             The device on which the cache should be initialized. Should be the same as the layer.\n         dtype (torch.dtype, *optional*, defaults to `torch.float32`):\n             The default `dtype` to use when initializing the layer.\n+        layer_device_map(`Dict[int, Union[str, torch.device, int]]]`, `optional`):\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Example:\n \n@@ -1525,6 +1540,7 @@ def __init__(\n         device: Union[torch.device, str] = \"cpu\",\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n         if max_batch_size is not None:\n@@ -1562,11 +1578,15 @@ def __init__(\n             self.head_dim,\n         )\n         for i in range(config.num_hidden_layers):\n+            if layer_device_map is not None:\n+                layer_device = layer_device_map[i]\n+            else:\n+                layer_device = device\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n             cache_shape = global_cache_shape if not self.is_sliding[i] else sliding_cache_shape\n-            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n-            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=device)\n+            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n             torch._dynamo.mark_static_address(new_layer_value_cache)\n             self.key_cache.append(new_layer_key_cache)\n@@ -1617,8 +1637,6 @@ def update(\n     ) -> Tuple[torch.Tensor]:\n         cache_position = cache_kwargs.get(\"cache_position\")\n         sliding_window = cache_kwargs.get(\"sliding_window\")\n-        self.key_cache[layer_idx] = self.key_cache[layer_idx].to(device=key_states.device)\n-        self.value_cache[layer_idx] = self.value_cache[layer_idx].to(device=value_states.device)\n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n         if sliding_window:"
        },
        {
            "sha": "019eb6c27f18cc897904a0b74937f77b06192892",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1",
            "patch": "@@ -1446,12 +1446,39 @@ def _get_cache(\n                     # models. May cause trobles with non-text modalities.\n                     cache_dtype = self.get_output_embeddings().weight.dtype\n \n+            def get_layer_device_map(execution_device_map: Optional[dict] = None):\n+                if execution_device_map is None or len(execution_device_map) <= 1:\n+                    return None\n+                layer_device_map = {}\n+                for layer in execution_device_map:\n+                    for idx in range(self.config.num_hidden_layers):\n+                        if f\".{idx}.\" in f\"{layer}.\":\n+                            layer_device_map[idx] = execution_device_map[layer]\n+                            break\n+                for idx in range(self.config.num_hidden_layers):\n+                    if idx not in layer_device_map:\n+                        raise RuntimeError(f\"layer {idx} has not been mapped to a device.\")\n+                return layer_device_map\n+\n+            execution_device_map = None\n+            # Taken from dispatch_model from accelerate.\n+            # This is needed here if we don't want to make changes in accelerate in order to save execution_device\n+            # For offloaded case, we need to get the execution device, not just the device where it is offloaded\n+            if hasattr(self, \"hf_device_map\"):\n+                main_device = [d for d in self.hf_device_map.values() if d not in [\"cpu\", \"disk\"]][0]\n+                execution_device_map = {\n+                    name: main_device if device in [\"cpu\", \"disk\"] else device\n+                    for name, device in self.hf_device_map.items()\n+                }\n+            layer_device_map = get_layer_device_map(execution_device_map)\n+\n             cache_kwargs = {\n                 \"config\": self.config if hasattr(self.config, \"text_config\") else self.config,\n                 \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"device\": device,\n                 \"dtype\": cache_dtype,\n+                \"layer_device_map\": layer_device_map,\n             }\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:"
        },
        {
            "sha": "0ed054ad58696e395d509c6231e757fabeafc742",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 85,
            "deletions": 0,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=6cc4dfe3f1e8d421c6d6351388e06e9b123cbfe1",
            "patch": "@@ -3444,6 +3444,91 @@ def test_special_tokens_fall_back_to_model_default(self):\n         self.assertTrue(test_bos_id == gen_output[0, 0])\n         self.assertTrue(generation_config.bos_token_id is None)\n \n+    @pytest.mark.generate\n+    @require_torch_multi_gpu\n+    def test_generate_with_static_cache_multi_gpu(self):\n+        \"\"\"\n+        Tests if the static cache has been set correctly and if generate works correctly when we are using multi-gpus.\n+        \"\"\"\n+        # need to split manually as auto doesn't work well with unbalanced model\n+        device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=device_map\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+\n+        text = \"Hello world\"\n+        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n+        input_ids = tokenized_inputs.input_ids.to(torch_device)\n+\n+        generation_kwargs = {\n+            \"max_new_tokens\": 20,\n+            \"cache_implementation\": \"static\",\n+            \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+        }\n+\n+        results = model.generate(input_ids, **generation_kwargs)\n+        self.assertTrue(isinstance(results.past_key_values, StaticCache))\n+\n+        # check device of each layer\n+        key_cache_0 = results.past_key_values.key_cache[0]\n+        value_cache_0 = results.past_key_values.value_cache[0]\n+        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n+\n+        key_cache_1 = results.past_key_values.key_cache[1]\n+        value_cache_1 = results.past_key_values.value_cache[1]\n+        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n+\n+    @pytest.mark.generate\n+    @require_torch_multi_gpu\n+    def test_init_static_cache_multi_gpu(self):\n+        \"\"\"\n+        Tests if the static cache has been set correctly when we initialize it manually in a multi-gpu setup.\n+        \"\"\"\n+        # need to split manually as auto doesn't work well with unbalanced model\n+        device_map = {\"model.embed_tokens\": 0, \"model.layers.0\": 0, \"model.layers.1\": 1, \"model.norm\": 1, \"lm_head\": 0}\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-MistralForCausalLM\", device_map=device_map\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-MistralForCausalLM\")\n+\n+        text = \"Hello world\"\n+        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n+        input_ids = tokenized_inputs.input_ids.to(torch_device)\n+\n+        generation_kwargs = {\n+            \"max_new_tokens\": 20,\n+            \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+        }\n+\n+        # TODO: We need to raise a warning in case the cache is not set correctly\n+        # with self.assertRaisesRegex(ValueError, \"If you are manually initializing the cache\"):\n+        #     past_key_values = StaticCache(\n+        #         config=model.config, batch_size=1, max_cache_len=30, device=torch_device, dtype=model.dtype\n+        #     )\n+        #     results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n+\n+        # deduced from the device_map : layer 0 on device 0 and layer 1 on device 1\n+        layer_device_map = {0: 0, 1: 1}\n+        past_key_values = StaticCache(\n+            config=model.config,\n+            batch_size=1,\n+            max_cache_len=30,\n+            device=torch_device,\n+            dtype=model.dtype,\n+            layer_device_map=layer_device_map,\n+        )\n+        results = model.generate(input_ids, past_key_values=past_key_values, **generation_kwargs)\n+\n+        # check device of each layer\n+        key_cache_0 = results.past_key_values.key_cache[0]\n+        value_cache_0 = results.past_key_values.value_cache[0]\n+        self.assertTrue(key_cache_0.device == value_cache_0.device == torch.device(0))\n+\n+        key_cache_1 = results.past_key_values.key_cache[1]\n+        value_cache_1 = results.past_key_values.value_cache[1]\n+        self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 152,
        "additions": 141,
        "deletions": 11
    }
}