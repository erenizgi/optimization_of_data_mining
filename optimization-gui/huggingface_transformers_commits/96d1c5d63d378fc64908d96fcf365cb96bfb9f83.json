{
    "author": "yonigozlan",
    "message": "Add image processor fast vitpose (#42021)\n\n* Add pix2struct fast image processor\n\n* Add vit image processor fast",
    "sha": "96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
    "files": [
        {
            "sha": "8d8c156d2255f831bbcc67beacfcfc9c9ca5fb17",
            "filename": "docs/source/en/model_doc/vitpose.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvitpose.md?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -295,6 +295,12 @@ Refer to resources below to learn more about using ViTPose.\n     - preprocess\n     - post_process_pose_estimation\n \n+## VitPoseImageProcessorFast\n+\n+[[autodoc]] VitPoseImageProcessorFast\n+    - preprocess\n+    - post_process_pose_estimation\n+\n ## VitPoseConfig\n \n [[autodoc]] VitPoseConfig"
        },
        {
            "sha": "c4d6eb4a4c960060bc027b8924bd51245c112a64",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -206,6 +206,7 @@\n             (\"vit_mae\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"vitmatte\", (\"VitMatteImageProcessor\", \"VitMatteImageProcessorFast\")),\n+            (\"vitpose\", (\"VitPoseImageProcessor\", \"VitPoseImageProcessorFast\")),\n             (\"xclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"yolos\", (\"YolosImageProcessor\", \"YolosImageProcessorFast\")),\n             (\"zoedepth\", (\"ZoeDepthImageProcessor\", \"ZoeDepthImageProcessorFast\")),"
        },
        {
            "sha": "bf3ad9e75eeb845c9aa3787322971ab04e944b94",
            "filename": "src/transformers/models/vitpose/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2F__init__.py?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_vitpose import *\n     from .image_processing_vitpose import *\n+    from .image_processing_vitpose_fast import *\n     from .modeling_vitpose import *\n else:\n     import sys"
        },
        {
            "sha": "5acd058bffa0c39b259d836196bf2799e4379235",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose.py?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -33,6 +33,7 @@\n     to_numpy_array,\n     valid_images,\n )\n+from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, is_scipy_available, is_torch_available, is_vision_available, logging\n \n \n@@ -52,6 +53,18 @@\n logger = logging.get_logger(__name__)\n \n \n+class VitPoseImageProcessorKwargs(ImagesKwargs, total=False):\n+    r\"\"\"\n+    do_affine_transform (`bool`, *optional*):\n+        Whether to apply an affine transformation to the input images based on the bounding boxes.\n+    normalize_factor (`float`, *optional*, defaults to `200.0`):\n+        Width and height scale factor used for normalization when computing center and scale from bounding boxes.\n+    \"\"\"\n+\n+    do_affine_transform: Optional[bool]\n+    normalize_factor: Optional[float]\n+\n+\n # inspired by https://github.com/ViTAE-Transformer/ViTPose/blob/d5216452796c90c6bc29f5c5ec0bdba94366768a/mmpose/datasets/datasets/base/kpt_2d_sview_rgb_img_top_down_dataset.py#L132\n def box_to_center_and_scale(\n     box: Union[tuple, list, np.ndarray],\n@@ -348,6 +361,7 @@ class VitPoseImageProcessor(BaseImageProcessor):\n             The sequence of standard deviations for each channel, to be used when normalizing images.\n     \"\"\"\n \n+    valid_kwargs = VitPoseImageProcessorKwargs\n     model_input_names = [\"pixel_values\"]\n \n     def __init__("
        },
        {
            "sha": "ec5fadbfd6c16e1d78f1af49d7641756dcf522b6",
            "filename": "src/transformers/models/vitpose/image_processing_vitpose_fast.py",
            "status": "added",
            "additions": 292,
            "deletions": 0,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fimage_processing_vitpose_fast.py?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for VitPose.\"\"\"\n+\n+import itertools\n+from typing import TYPE_CHECKING, Optional, Union\n+\n+import numpy as np\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_transforms import group_images_by_shape, reorder_images\n+from ...image_utils import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD, ImageInput, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import TensorType, auto_docstring\n+from .image_processing_vitpose import (\n+    VitPoseImageProcessorKwargs,\n+    box_to_center_and_scale,\n+    coco_to_pascal_voc,\n+    get_keypoint_predictions,\n+    get_warp_matrix,\n+    post_dark_unbiased_data_processing,\n+    scipy_warp_affine,\n+    transform_preds,\n+)\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_vitpose import VitPoseEstimatorOutput\n+\n+\n+@auto_docstring\n+class VitPoseImageProcessorFast(BaseImageProcessorFast):\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"height\": 256, \"width\": 192}\n+    do_rescale = True\n+    do_normalize = True\n+    do_affine_transform = True\n+    normalize_factor = 200.0\n+    valid_kwargs = VitPoseImageProcessorKwargs\n+    model_input_names = [\"pixel_values\"]\n+\n+    def torch_affine_transform(\n+        self,\n+        image: torch.Tensor,\n+        center: tuple[float],\n+        scale: tuple[float],\n+        rotation: float,\n+        size: SizeDict,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Apply an affine transformation to a torch tensor image.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image tensor of shape (C, H, W) to transform.\n+            center (`tuple[float]`):\n+                Center of the bounding box (x, y).\n+            scale (`tuple[float]`):\n+                Scale of the bounding box with respect to height/width.\n+            rotation (`float`):\n+                Rotation angle in degrees.\n+            size (`SizeDict`):\n+                Size of the destination image.\n+\n+        Returns:\n+            `torch.Tensor`: The transformed image.\n+        \"\"\"\n+        transformation = get_warp_matrix(\n+            rotation, center * 2.0, np.array((size.width, size.height)) - 1.0, scale * 200.0\n+        )\n+        # Convert tensor to numpy (channels last) for scipy_warp_affine\n+        image_np = image.permute(1, 2, 0).cpu().numpy()\n+        transformed_np = scipy_warp_affine(src=image_np, M=transformation, size=(size.height, size.width))\n+\n+        # Convert back to torch tensor (channels first)\n+        transformed = torch.from_numpy(transformed_np).permute(2, 0, 1).to(image.device)\n+\n+        return transformed\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        boxes: Union[list[list[float]], np.ndarray],\n+        **kwargs: Unpack[VitPoseImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        boxes (`list[list[list[float]]]` or `np.ndarray`):\n+            List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the\n+            bounding box coordinates in COCO format (top_left_x, top_left_y, width, height).\n+        \"\"\"\n+        return super().preprocess(images, boxes, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[torch.Tensor],\n+        boxes: Union[list, np.ndarray],\n+        do_affine_transform: bool,\n+        size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, tuple[float]],\n+        image_std: Union[float, tuple[float]],\n+        disable_grouping: bool,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess images with affine transformations based on bounding boxes.\n+        \"\"\"\n+        if len(images) != len(boxes):\n+            raise ValueError(f\"Number of images and boxes must match: {len(images)} != {len(boxes)}\")\n+\n+        # Apply affine transformation for each image and each box\n+        if do_affine_transform:\n+            transformed_images = []\n+            for image, image_boxes in zip(images, boxes):\n+                for box in image_boxes:\n+                    center, scale = box_to_center_and_scale(\n+                        box,\n+                        image_width=size.width,\n+                        image_height=size.height,\n+                        normalize_factor=self.normalize_factor,\n+                    )\n+                    transformed_image = self.torch_affine_transform(image, center, scale, rotation=0, size=size)\n+                    transformed_images.append(transformed_image)\n+            images = transformed_images\n+\n+        # Group images by shape for efficient batch processing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Apply rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        # Stack into batch tensor\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    def keypoints_from_heatmaps(\n+        self,\n+        heatmaps: np.ndarray,\n+        center: np.ndarray,\n+        scale: np.ndarray,\n+        kernel: int = 11,\n+    ):\n+        \"\"\"\n+        Get final keypoint predictions from heatmaps and transform them back to\n+        the image.\n+\n+        Args:\n+            heatmaps (`np.ndarray` of shape `(batch_size, num_keypoints, height, width])`):\n+                Model predicted heatmaps.\n+            center (`np.ndarray` of shape `(batch_size, 2)`):\n+                Center of the bounding box (x, y).\n+            scale (`np.ndarray` of shape `(batch_size, 2)`):\n+                Scale of the bounding box wrt original images of width and height.\n+            kernel (int, *optional*, defaults to 11):\n+                Gaussian kernel size (K) for modulation, which should match the heatmap gaussian sigma when training.\n+                K=17 for sigma=3 and k=11 for sigma=2.\n+\n+        Returns:\n+            tuple: A tuple containing keypoint predictions and scores.\n+\n+            - preds (`np.ndarray` of shape `(batch_size, num_keypoints, 2)`):\n+                Predicted keypoint location in images.\n+            - scores (`np.ndarray` of shape `(batch_size, num_keypoints, 1)`):\n+                Scores (confidence) of the keypoints.\n+        \"\"\"\n+        batch_size, _, height, width = heatmaps.shape\n+\n+        coords, scores = get_keypoint_predictions(heatmaps)\n+\n+        preds = post_dark_unbiased_data_processing(coords, heatmaps, kernel=kernel)\n+\n+        # Transform back to the image\n+        for i in range(batch_size):\n+            preds[i] = transform_preds(preds[i], center=center[i], scale=scale[i], output_size=[height, width])\n+\n+        return preds, scores\n+\n+    def post_process_pose_estimation(\n+        self,\n+        outputs: \"VitPoseEstimatorOutput\",\n+        boxes: Union[list[list[list[float]]], np.ndarray],\n+        kernel_size: int = 11,\n+        threshold: Optional[float] = None,\n+        target_sizes: Optional[Union[TensorType, list[tuple]]] = None,\n+    ):\n+        \"\"\"\n+        Transform the heatmaps into keypoint predictions and transform them back to the image.\n+\n+        Args:\n+            outputs (`VitPoseEstimatorOutput`):\n+                VitPoseForPoseEstimation model outputs.\n+            boxes (`list[list[list[float]]]` or `np.ndarray`):\n+                List or array of bounding boxes for each image. Each box should be a list of 4 floats representing the bounding\n+                box coordinates in COCO format (top_left_x, top_left_y, width, height).\n+            kernel_size (`int`, *optional*, defaults to 11):\n+                Gaussian kernel size (K) for modulation.\n+            threshold (`float`, *optional*, defaults to None):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `list[tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. If unset, predictions will be resize with the default value.\n+        Returns:\n+            `list[list[Dict]]`: A list of dictionaries, each dictionary containing the keypoints and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+\n+        # First compute centers and scales for each bounding box\n+        batch_size, num_keypoints, _, _ = outputs.heatmaps.shape\n+\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        centers = np.zeros((batch_size, 2), dtype=np.float32)\n+        scales = np.zeros((batch_size, 2), dtype=np.float32)\n+        flattened_boxes = list(itertools.chain(*boxes))\n+        for i in range(batch_size):\n+            if target_sizes is not None:\n+                image_width, image_height = target_sizes[i][0], target_sizes[i][1]\n+                scale_factor = np.array([image_width, image_height, image_width, image_height])\n+                flattened_boxes[i] = flattened_boxes[i] * scale_factor\n+            width, height = self.size[\"width\"], self.size[\"height\"]\n+            center, scale = box_to_center_and_scale(flattened_boxes[i], image_width=width, image_height=height)\n+            centers[i, :] = center\n+            scales[i, :] = scale\n+\n+        preds, scores = self.keypoints_from_heatmaps(\n+            outputs.heatmaps.cpu().numpy(), centers, scales, kernel=kernel_size\n+        )\n+\n+        all_boxes = np.zeros((batch_size, 4), dtype=np.float32)\n+        all_boxes[:, 0:2] = centers[:, 0:2]\n+        all_boxes[:, 2:4] = scales[:, 0:2]\n+\n+        poses = torch.tensor(preds)\n+        scores = torch.tensor(scores)\n+        labels = torch.arange(0, num_keypoints)\n+        bboxes_xyxy = torch.tensor(coco_to_pascal_voc(all_boxes))\n+\n+        results: list[list[dict[str, torch.Tensor]]] = []\n+\n+        pose_bbox_pairs = zip(poses, scores, bboxes_xyxy)\n+\n+        for image_bboxes in boxes:\n+            image_results: list[dict[str, torch.Tensor]] = []\n+            for _ in image_bboxes:\n+                # Unpack the next pose and bbox_xyxy from the iterator\n+                pose, score, bbox_xyxy = next(pose_bbox_pairs)\n+                score = score.squeeze()\n+                keypoints_labels = labels\n+                if threshold is not None:\n+                    keep = score > threshold\n+                    pose = pose[keep]\n+                    score = score[keep]\n+                    keypoints_labels = keypoints_labels[keep]\n+                pose_result = {\"keypoints\": pose, \"scores\": score, \"labels\": keypoints_labels, \"bbox\": bbox_xyxy}\n+                image_results.append(pose_result)\n+            results.append(image_results)\n+\n+        return results\n+\n+\n+__all__ = [\"VitPoseImageProcessorFast\"]"
        },
        {
            "sha": "49df757f186e3748ffcda28ed724457f4316f6d4",
            "filename": "tests/models/vitpose/test_image_processing_vitpose.py",
            "status": "modified",
            "additions": 181,
            "deletions": 112,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96d1c5d63d378fc64908d96fcf365cb96bfb9f83/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py?ref=96d1c5d63d378fc64908d96fcf365cb96bfb9f83",
            "patch": "@@ -11,14 +11,14 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n-\n+import io\n import unittest\n \n+import httpx\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -32,6 +32,9 @@\n \n     from transformers import VitPoseImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import VitPoseImageProcessorFast\n+\n \n class VitPoseImageProcessingTester:\n     def __init__(\n@@ -95,6 +98,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class VitPoseImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = VitPoseImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = VitPoseImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -105,124 +109,189 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_affine_transform\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_affine_transform\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 20, \"width\": 20})\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n-        )\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, Image.Image)\n-\n-        # Test not batched input\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n-        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n-\n-        # Test batched\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n-        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+            encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+            # Test batched\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+            encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+            )\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image, np.ndarray)\n-\n-        # Test not batched input\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n-        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n-\n-        # Test batched\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n-        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+            encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+            # Test batched\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+            encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+            )\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n-\n-        for image in image_inputs:\n-            self.assertIsInstance(image, torch.Tensor)\n-\n-        # Test not batched input\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n-        encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n-\n-        # Test batched\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n-        encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n-        )\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+            encoded_images = image_processing(image_inputs[0], boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (2, *expected_output_image_shape))\n+\n+            # Test batched\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+            encoded_images = image_processing(image_inputs, boxes=boxes, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape), (self.image_processor_tester.batch_size * 2, *expected_output_image_shape)\n+            )\n \n     def test_call_numpy_4_channels(self):\n-        # Test that can process images which have an arbitrary number of channels\n-        # Initialize image_processing\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-\n-        # create random numpy tensors\n-        self.image_processor_tester.num_channels = 4\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n-        # Test not batched input\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n-        encoded_images = image_processor(\n-            image_inputs[0],\n-            boxes=boxes,\n-            return_tensors=\"pt\",\n-            input_data_format=\"channels_last\",\n-            image_mean=(0.0, 0.0, 0.0, 0.0),\n-            image_std=(1.0, 1.0, 1.0, 1.0),\n-        ).pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n-        self.assertEqual(tuple(encoded_images.shape), (len(boxes[0]), *expected_output_image_shape))\n-\n-        # Test batched\n-        boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n-        encoded_images = image_processor(\n-            image_inputs,\n-            boxes=boxes,\n-            return_tensors=\"pt\",\n-            input_data_format=\"channels_last\",\n-            image_mean=(0.0, 0.0, 0.0, 0.0),\n-            image_std=(1.0, 1.0, 1.0, 1.0),\n-        ).pixel_values\n-        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n-        self.assertEqual(\n-            tuple(encoded_images.shape),\n-            (self.image_processor_tester.batch_size * len(boxes[0]), *expected_output_image_shape),\n+        for image_processing_class in self.image_processor_list:\n+            # Test that can process images which have an arbitrary number of channels\n+            # Initialize image_processing\n+            image_processor = image_processing_class(**self.image_processor_dict)\n+\n+            # create random numpy tensors\n+            self.image_processor_tester.num_channels = 4\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            # Test not batched input\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]]\n+            encoded_images = image_processor(\n+                image_inputs[0],\n+                boxes=boxes,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n+            ).pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n+            self.assertEqual(tuple(encoded_images.shape), (len(boxes[0]), *expected_output_image_shape))\n+\n+            # Test batched\n+            boxes = [[[0, 0, 1, 1], [0.5, 0.5, 0.5, 0.5]]] * self.image_processor_tester.batch_size\n+            encoded_images = image_processor(\n+                image_inputs,\n+                boxes=boxes,\n+                return_tensors=\"pt\",\n+                input_data_format=\"channels_last\",\n+                image_mean=(0.0, 0.0, 0.0, 0.0),\n+                image_std=(1.0, 1.0, 1.0, 1.0),\n+            ).pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs)\n+            self.assertEqual(\n+                tuple(encoded_images.shape),\n+                (self.image_processor_tester.batch_size * len(boxes[0]), *expected_output_image_shape),\n+            )\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            io.BytesIO(\n+                httpx.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", follow_redirects=True).content\n+            )\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        boxes = [[[0, 0, 1, 1]]]\n+        encoding_slow = image_processor_slow(dummy_image, boxes=boxes, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, boxes=boxes, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        boxes = [[[0, 0, 1, 1]]] * len(dummy_images)\n+        encoding_slow = image_processor_slow(dummy_images, boxes=boxes, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, boxes=boxes, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    def test_can_compile_fast_image_processor(self):\n+        from packaging import version\n+\n+        from transformers.testing_utils import torch_device\n+\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = torch.randint(0, 255, (3, 224, 224), dtype=torch.uint8)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        boxes = [[[0, 0, 1, 1]]]\n+        output_eager = image_processor(input_image, boxes=boxes, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, boxes=boxes, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n         )"
        }
    ],
    "stats": {
        "total": 607,
        "additions": 495,
        "deletions": 112
    }
}