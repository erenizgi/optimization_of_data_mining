{
    "author": "Cyrilvallez",
    "message": "Fix cache update! (#38046)\n\n* fix slicing\n\n* better fix",
    "sha": "aaed2f557718b7b762a88bd0fd700cfca9851193",
    "files": [
        {
            "sha": "6ecb60f1cfdd04056e140b52be59f848a8b4c8db",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaed2f557718b7b762a88bd0fd700cfca9851193/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaed2f557718b7b762a88bd0fd700cfca9851193/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=aaed2f557718b7b762a88bd0fd700cfca9851193",
            "patch": "@@ -1402,7 +1402,7 @@ def update(\n         value_states = value_states.to(v_out.dtype)\n \n         # assume this only happens in prefill phase when prompt length > sliding_window_size (= max_cache_len)\n-        if cache_position.shape[0] > self.max_cache_len:\n+        if cache_position.shape[0] >= self.max_cache_len:\n             k_out = key_states[:, :, -self.max_cache_len :, :]\n             v_out = value_states[:, :, -self.max_cache_len :, :]\n             # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n@@ -1413,8 +1413,8 @@ def update(\n             return key_states, value_states\n \n         slicing = torch.ones(self.max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n-        cache_position = cache_position.clamp(0, self.max_cache_len - 1)\n         to_shift = cache_position > self.max_cache_len - 1\n+        cache_position = cache_position.clamp(0, self.max_cache_len - 1)\n         indices = (slicing + to_shift[-1].int() - 1) % self.max_cache_len\n \n         k_out = k_out[:, :, indices]\n@@ -1725,7 +1725,7 @@ def __init__(\n             self.value_cache.append(new_layer_value_cache)\n \n     def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k_out, v_out, max_cache_len):\n-        if cache_position.shape[0] > max_cache_len:\n+        if cache_position.shape[0] >= max_cache_len:\n             k_out = key_states[:, :, -max_cache_len:, :]\n             v_out = value_states[:, :, -max_cache_len:, :]\n             # Assumption: caches are all zeros at this point, `+=` is equivalent to `=` but compile-friendly\n@@ -1736,8 +1736,8 @@ def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k\n             return key_states, value_states\n \n         slicing = torch.ones(max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n-        cache_position = cache_position.clamp(0, max_cache_len - 1)\n         to_shift = cache_position > max_cache_len - 1\n+        cache_position = cache_position.clamp(0, max_cache_len - 1)\n         indices = (slicing + to_shift[-1].int() - 1) % max_cache_len\n         k_out = k_out[:, :, indices]\n         v_out = v_out[:, :, indices]"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}