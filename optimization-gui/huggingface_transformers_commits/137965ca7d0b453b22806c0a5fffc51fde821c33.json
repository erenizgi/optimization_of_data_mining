{
    "author": "zucchini-nlp",
    "message": "Don't show warning for `inv_freq` buffers (#35255)\n\ndont show warning",
    "sha": "137965ca7d0b453b22806c0a5fffc51fde821c33",
    "files": [
        {
            "sha": "40892f0cdc8d9af1f4d525eaf6df8092ff51543d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/137965ca7d0b453b22806c0a5fffc51fde821c33/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/137965ca7d0b453b22806c0a5fffc51fde821c33/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=137965ca7d0b453b22806c0a5fffc51fde821c33",
            "patch": "@@ -4479,6 +4479,11 @@ def _load_pretrained_model(\n             model_buffers = {\".\".join([prefix, key]) for key in model_buffers}\n         unexpected_keys = sorted(unexpected_keys - model_buffers)\n \n+        # Clean up buffer for `inv-freq` because RoPE embedding moved under base model (https://github.com/huggingface/transformers/pull/34858)\n+        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer in model_buffers)\n+        if has_inv_freq_buffers:\n+            unexpected_keys = {k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k}\n+\n         model.tie_weights()\n         if device_map is None and not is_fsdp_enabled() and not is_deepspeed_zero3_enabled():\n             ptrs = collections.defaultdict(list)"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}