{
    "author": "shawntan",
    "message": "GraniteMoeHybrid: Allow for only shared expert case. (#38801)\n\n* Allow for only shared expert case.\n\n* Style",
    "sha": "5ab0f447ab6aa988f7117e0b529115b2071befde",
    "files": [
        {
            "sha": "37519fe435a04157e0c66febf753d1d5f6486c28",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=5ab0f447ab6aa988f7117e0b529115b2071befde",
            "patch": "@@ -504,7 +504,8 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = GraniteMoeAttention(config=config, layer_idx=layer_idx)\n-        self.block_sparse_moe = GraniteMoeMoE(config)\n+        if config.num_local_experts > 0:\n+            self.block_sparse_moe = GraniteMoeMoE(config)\n         self.input_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n "
        },
        {
            "sha": "b1480085601a7dd61d0ed77130ab7fbdc3236e78",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=5ab0f447ab6aa988f7117e0b529115b2071befde",
            "patch": "@@ -1051,7 +1051,8 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n         self.hidden_size = config.hidden_size\n         # Either attention or mamba will be initialized, depending on the layer type.\n         self.self_attn = None\n-        self.block_sparse_moe = GraniteMoeHybridMoE(config)\n+        if config.num_local_experts > 0:\n+            self.block_sparse_moe = GraniteMoeHybridMoE(config)\n         self.input_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeHybridRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n@@ -1065,6 +1066,9 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n             self.self_attn = GraniteMoeHybridAttention(config, layer_idx)\n         self.layer_type = config.layers_block_type[layer_idx]\n \n+        # Accept 0 experts: skip MoE if num_local_experts == 0\n+        self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1131,9 +1135,14 @@ def forward(\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n \n-        hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n+        if self.has_experts:\n+            moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+            hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n+        else:\n+            hidden_states = self.shared_mlp(hidden_states)\n+            router_logits = None\n+\n         hidden_states = residual + hidden_states * self.residual_multiplier\n \n         outputs = (hidden_states,)"
        },
        {
            "sha": "160e0aa1bf38abcc824da39ea32230f624a4042b",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=5ab0f447ab6aa988f7117e0b529115b2071befde",
            "patch": "@@ -71,6 +71,9 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n             self.self_attn = GraniteMoeHybridAttention(config, layer_idx)\n         self.layer_type = config.layers_block_type[layer_idx]\n \n+        # Accept 0 experts: skip MoE if num_local_experts == 0\n+        self.has_experts = getattr(config, \"num_local_experts\", 0) > 0\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -137,9 +140,14 @@ def forward(\n         # Fully Connected\n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n-        moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n \n-        hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n+        if self.has_experts:\n+            moe_hidden_states, router_logits = self.block_sparse_moe(hidden_states)\n+            hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n+        else:\n+            hidden_states = self.shared_mlp(hidden_states)\n+            router_logits = None\n+\n         hidden_states = residual + hidden_states * self.residual_multiplier\n \n         outputs = (hidden_states,)"
        },
        {
            "sha": "da7ade3cf489627af9d3ecd1a450d2f4b3235acd",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5ab0f447ab6aa988f7117e0b529115b2071befde/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=5ab0f447ab6aa988f7117e0b529115b2071befde",
            "patch": "@@ -412,7 +412,8 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = GraniteMoeSharedAttention(config=config, layer_idx=layer_idx)\n-        self.block_sparse_moe = GraniteMoeSharedMoE(config)\n+        if config.num_local_experts > 0:\n+            self.block_sparse_moe = GraniteMoeSharedMoE(config)\n         self.input_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GraniteMoeSharedRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n "
        }
    ],
    "stats": {
        "total": 33,
        "additions": 26,
        "deletions": 7
    }
}