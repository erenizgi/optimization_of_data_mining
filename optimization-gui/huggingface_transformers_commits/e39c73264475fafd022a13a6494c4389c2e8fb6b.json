{
    "author": "Kh4L",
    "message": "Handle torch ver in flexattn (#37400)\n\n* Handle torch ver in flexattn\n\n* update",
    "sha": "e39c73264475fafd022a13a6494c4389c2e8fb6b",
    "files": [
        {
            "sha": "b4d3b6fc3e01b37ec53bd50f1fc03e40670d64f6",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e39c73264475fafd022a13a6494c4389c2e8fb6b/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e39c73264475fafd022a13a6494c4389c2e8fb6b/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=e39c73264475fafd022a13a6494c4389c2e8fb6b",
            "patch": "@@ -29,6 +29,7 @@\n from typing import Optional, Tuple, Union\n \n import torch\n+from packaging import version\n \n from ..utils import is_torch_flex_attn_available\n from ..utils.import_utils import _torch_version\n@@ -66,7 +67,7 @@ def __init__(self, training):\n             # cause errors. The suggested fix is to compile with \"max-autotune-no-cudagraphs\"\n             # see https://github.com/pytorch/pytorch/issues/146260 for training\n             self.training = training\n-            if _torch_version.split(\"+\")[0] == \"2.6.0\" and training:\n+            if version.parse(_torch_version).base_version == \"2.6.0\" and training:\n                 self._compiled_flex_attention = torch.compile(\n                     flex_attention, dynamic=False, mode=\"max-autotune-no-cudagraphs\"\n                 )"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}