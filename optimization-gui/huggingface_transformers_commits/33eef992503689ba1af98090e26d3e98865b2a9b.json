{
    "author": "aymeric-roucher",
    "message": "Agents: Small fixes in streaming to gradio + add tests (#34549)\n\n* Better support transformers.agents in gradio: small fixes and additional tests",
    "sha": "33eef992503689ba1af98090e26d3e98865b2a9b",
    "files": [
        {
            "sha": "c461c50f29592c533c3146821779fe54d83b42f2",
            "filename": "src/transformers/agents/agents.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fagents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fagents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagents.py?ref=33eef992503689ba1af98090e26d3e98865b2a9b",
            "patch": "@@ -1141,11 +1141,10 @@ def step(self):\n             )\n             self.logger.warning(\"Print outputs:\")\n             self.logger.log(32, self.state[\"print_outputs\"])\n+            observation = \"Print outputs:\\n\" + self.state[\"print_outputs\"]\n             if result is not None:\n                 self.logger.warning(\"Last output from code snippet:\")\n                 self.logger.log(32, str(result))\n-            observation = \"Print outputs:\\n\" + self.state[\"print_outputs\"]\n-            if result is not None:\n                 observation += \"Last output from code snippet:\\n\" + str(result)[:100000]\n             current_step_logs[\"observation\"] = observation\n         except Exception as e:"
        },
        {
            "sha": "755418d35a56a3598edf195aa6da37cb8d81a97b",
            "filename": "src/transformers/agents/monitoring.py",
            "status": "modified",
            "additions": 30,
            "deletions": 12,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fmonitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fmonitoring.py?ref=33eef992503689ba1af98090e26d3e98865b2a9b",
            "patch": "@@ -18,11 +18,19 @@\n from .agents import ReactAgent\n \n \n-def pull_message(step_log: dict):\n+def pull_message(step_log: dict, test_mode: bool = True):\n     try:\n         from gradio import ChatMessage\n     except ImportError:\n-        raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n+        if test_mode:\n+\n+            class ChatMessage:\n+                def __init__(self, role, content, metadata=None):\n+                    self.role = role\n+                    self.content = content\n+                    self.metadata = metadata\n+        else:\n+            raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n \n     if step_log.get(\"rationale\"):\n         yield ChatMessage(role=\"assistant\", content=step_log[\"rationale\"])\n@@ -46,30 +54,40 @@ def pull_message(step_log: dict):\n         )\n \n \n-def stream_to_gradio(agent: ReactAgent, task: str, **kwargs):\n+def stream_to_gradio(agent: ReactAgent, task: str, test_mode: bool = False, **kwargs):\n     \"\"\"Runs an agent with the given task and streams the messages from the agent as gradio ChatMessages.\"\"\"\n \n     try:\n         from gradio import ChatMessage\n     except ImportError:\n-        raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n+        if test_mode:\n+\n+            class ChatMessage:\n+                def __init__(self, role, content, metadata=None):\n+                    self.role = role\n+                    self.content = content\n+                    self.metadata = metadata\n+        else:\n+            raise ImportError(\"Gradio should be installed in order to launch a gradio demo.\")\n \n     for step_log in agent.run(task, stream=True, **kwargs):\n         if isinstance(step_log, dict):\n-            for message in pull_message(step_log):\n+            for message in pull_message(step_log, test_mode=test_mode):\n                 yield message\n \n-    if isinstance(step_log, AgentText):\n-        yield ChatMessage(role=\"assistant\", content=f\"**Final answer:**\\n```\\n{step_log.to_string()}\\n```\")\n-    elif isinstance(step_log, AgentImage):\n+    final_answer = step_log  # Last log is the run's final_answer\n+\n+    if isinstance(final_answer, AgentText):\n+        yield ChatMessage(role=\"assistant\", content=f\"**Final answer:**\\n```\\n{final_answer.to_string()}\\n```\")\n+    elif isinstance(final_answer, AgentImage):\n         yield ChatMessage(\n             role=\"assistant\",\n-            content={\"path\": step_log.to_string(), \"mime_type\": \"image/png\"},\n+            content={\"path\": final_answer.to_string(), \"mime_type\": \"image/png\"},\n         )\n-    elif isinstance(step_log, AgentAudio):\n+    elif isinstance(final_answer, AgentAudio):\n         yield ChatMessage(\n             role=\"assistant\",\n-            content={\"path\": step_log.to_string(), \"mime_type\": \"audio/wav\"},\n+            content={\"path\": final_answer.to_string(), \"mime_type\": \"audio/wav\"},\n         )\n     else:\n-        yield ChatMessage(role=\"assistant\", content=str(step_log))\n+        yield ChatMessage(role=\"assistant\", content=str(final_answer))"
        },
        {
            "sha": "6e90f356cb928e3dc62968bcd39fdefa7d7133cf",
            "filename": "src/transformers/agents/python_interpreter.py",
            "status": "modified",
            "additions": 15,
            "deletions": 21,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fpython_interpreter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Fpython_interpreter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fpython_interpreter.py?ref=33eef992503689ba1af98090e26d3e98865b2a9b",
            "patch": "@@ -848,6 +848,13 @@ def evaluate_ast(\n         raise InterpreterError(f\"{expression.__class__.__name__} is not supported.\")\n \n \n+def truncate_print_outputs(print_outputs: str, max_len_outputs: int = MAX_LEN_OUTPUT) -> str:\n+    if len(print_outputs) < max_len_outputs:\n+        return print_outputs\n+    else:\n+        return f\"Print outputs:\\n{print_outputs[:max_len_outputs]}\\n_Print outputs have been truncated over the limit of {max_len_outputs} characters._\\n\"\n+\n+\n def evaluate_python_code(\n     code: str,\n     static_tools: Optional[Dict[str, Callable]] = None,\n@@ -890,25 +897,12 @@ def evaluate_python_code(\n     PRINT_OUTPUTS = \"\"\n     global OPERATIONS_COUNT\n     OPERATIONS_COUNT = 0\n-    for node in expression.body:\n-        try:\n+    try:\n+        for node in expression.body:\n             result = evaluate_ast(node, state, static_tools, custom_tools, authorized_imports)\n-        except InterpreterError as e:\n-            msg = \"\"\n-            if len(PRINT_OUTPUTS) > 0:\n-                if len(PRINT_OUTPUTS) < MAX_LEN_OUTPUT:\n-                    msg += f\"Print outputs:\\n{PRINT_OUTPUTS}\\n====\\n\"\n-                else:\n-                    msg += f\"Print outputs:\\n{PRINT_OUTPUTS[:MAX_LEN_OUTPUT]}\\n_Print outputs were over {MAX_LEN_OUTPUT} characters, so they have been truncated._\\n====\\n\"\n-            msg += f\"EXECUTION FAILED:\\nEvaluation stopped at line '{ast.get_source_segment(code, node)}' because of the following error:\\n{e}\"\n-            raise InterpreterError(msg)\n-        finally:\n-            if len(PRINT_OUTPUTS) < MAX_LEN_OUTPUT:\n-                state[\"print_outputs\"] = PRINT_OUTPUTS\n-            else:\n-                state[\"print_outputs\"] = (\n-                    PRINT_OUTPUTS[:MAX_LEN_OUTPUT]\n-                    + f\"\\n_Print outputs were over {MAX_LEN_OUTPUT} characters, so they have been truncated._\"\n-                )\n-\n-    return result\n+        state[\"print_outputs\"] = truncate_print_outputs(PRINT_OUTPUTS, max_len_outputs=MAX_LEN_OUTPUT)\n+        return result\n+    except InterpreterError as e:\n+        msg = truncate_print_outputs(PRINT_OUTPUTS, max_len_outputs=MAX_LEN_OUTPUT)\n+        msg += f\"EXECUTION FAILED:\\nEvaluation stopped at line '{ast.get_source_segment(code, node)}' because of the following error:\\n{e}\"\n+        raise InterpreterError(msg)"
        },
        {
            "sha": "84bcf0fde61f18a945cfc908283a366fb3aea700",
            "filename": "src/transformers/agents/tools.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33eef992503689ba1af98090e26d3e98865b2a9b/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=33eef992503689ba1af98090e26d3e98865b2a9b",
            "patch": "@@ -14,6 +14,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import ast\n import base64\n import importlib\n import inspect\n@@ -141,15 +142,19 @@ def validate_arguments(self, do_validate_forward: bool = True):\n         required_attributes = {\n             \"description\": str,\n             \"name\": str,\n-            \"inputs\": Dict,\n+            \"inputs\": dict,\n             \"output_type\": str,\n         }\n         authorized_types = [\"string\", \"integer\", \"number\", \"image\", \"audio\", \"any\", \"boolean\"]\n \n         for attr, expected_type in required_attributes.items():\n             attr_value = getattr(self, attr, None)\n+            if attr_value is None:\n+                raise TypeError(f\"You must set an attribute {attr}.\")\n             if not isinstance(attr_value, expected_type):\n-                raise TypeError(f\"You must set an attribute {attr} of type {expected_type.__name__}.\")\n+                raise TypeError(\n+                    f\"Attribute {attr} should have type {expected_type.__name__}, got {type(attr_value)} instead.\"\n+                )\n         for input_name, input_content in self.inputs.items():\n             assert isinstance(input_content, dict), f\"Input '{input_name}' should be a dictionary.\"\n             assert (\n@@ -248,7 +253,6 @@ def save(self, output_dir):\n     def from_hub(\n         cls,\n         repo_id: str,\n-        model_repo_id: Optional[str] = None,\n         token: Optional[str] = None,\n         **kwargs,\n     ):\n@@ -266,9 +270,6 @@ def from_hub(\n         Args:\n             repo_id (`str`):\n                 The name of the repo on the Hub where your tool is defined.\n-            model_repo_id (`str`, *optional*):\n-                If your tool uses a model and you want to use a different model than the default, you can pass a second\n-                repo ID or an endpoint url to this argument.\n             token (`str`, *optional*):\n                 The token to identify you on hf.co. If unset, will use the token generated when running\n                 `huggingface-cli login` (stored in `~/.huggingface`).\n@@ -354,6 +355,9 @@ def from_hub(\n         if tool_class.output_type != custom_tool[\"output_type\"]:\n             tool_class.output_type = custom_tool[\"output_type\"]\n \n+        if not isinstance(tool_class.inputs, dict):\n+            tool_class.inputs = ast.literal_eval(tool_class.inputs)\n+\n         return tool_class(**kwargs)\n \n     def push_to_hub("
        },
        {
            "sha": "c43c9cb8bf86dd7e44db2828ee90f7c93a74633e",
            "filename": "tests/agents/test_monitoring.py",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/33eef992503689ba1af98090e26d3e98865b2a9b/tests%2Fagents%2Ftest_monitoring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/33eef992503689ba1af98090e26d3e98865b2a9b/tests%2Fagents%2Ftest_monitoring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_monitoring.py?ref=33eef992503689ba1af98090e26d3e98865b2a9b",
            "patch": "@@ -0,0 +1,82 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.agents.agent_types import AgentImage\n+from transformers.agents.agents import AgentError, ReactCodeAgent, ReactJsonAgent\n+from transformers.agents.monitoring import stream_to_gradio\n+\n+\n+class MonitoringTester(unittest.TestCase):\n+    def test_streaming_agent_text_output(self):\n+        def dummy_llm_engine(prompt, **kwargs):\n+            return \"\"\"\n+Code:\n+````\n+final_answer('This is the final answer.')\n+```\"\"\"\n+\n+        agent = ReactCodeAgent(\n+            tools=[],\n+            llm_engine=dummy_llm_engine,\n+            max_iterations=1,\n+        )\n+\n+        # Use stream_to_gradio to capture the output\n+        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n+\n+        self.assertEqual(len(outputs), 3)\n+        final_message = outputs[-1]\n+        self.assertEqual(final_message.role, \"assistant\")\n+        self.assertIn(\"This is the final answer.\", final_message.content)\n+\n+    def test_streaming_agent_image_output(self):\n+        def dummy_llm_engine(prompt, **kwargs):\n+            return 'Action:{\"action\": \"final_answer\", \"action_input\": {\"answer\": \"image\"}}'\n+\n+        agent = ReactJsonAgent(\n+            tools=[],\n+            llm_engine=dummy_llm_engine,\n+            max_iterations=1,\n+        )\n+\n+        # Use stream_to_gradio to capture the output\n+        outputs = list(stream_to_gradio(agent, task=\"Test task\", image=AgentImage(value=\"path.png\"), test_mode=True))\n+\n+        self.assertEqual(len(outputs), 2)\n+        final_message = outputs[-1]\n+        self.assertEqual(final_message.role, \"assistant\")\n+        self.assertIsInstance(final_message.content, dict)\n+        self.assertEqual(final_message.content[\"path\"], \"path.png\")\n+        self.assertEqual(final_message.content[\"mime_type\"], \"image/png\")\n+\n+    def test_streaming_with_agent_error(self):\n+        def dummy_llm_engine(prompt, **kwargs):\n+            raise AgentError(\"Simulated agent error\")\n+\n+        agent = ReactCodeAgent(\n+            tools=[],\n+            llm_engine=dummy_llm_engine,\n+            max_iterations=1,\n+        )\n+\n+        # Use stream_to_gradio to capture the output\n+        outputs = list(stream_to_gradio(agent, task=\"Test task\", test_mode=True))\n+\n+        self.assertEqual(len(outputs), 3)\n+        final_message = outputs[-1]\n+        self.assertEqual(final_message.role, \"assistant\")\n+        self.assertIn(\"Simulated agent error\", final_message.content)"
        }
    ],
    "stats": {
        "total": 179,
        "additions": 138,
        "deletions": 41
    }
}