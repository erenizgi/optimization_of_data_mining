{
    "author": "cyyever",
    "message": "Fix outdated version checks of accelerator (#40969)\n\n* Fix outdated version checks of accelerator\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix outdated version checks of accelerator\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "f6104189fda98a39757fd456b65261edfd183e65",
    "files": [
        {
            "sha": "10fa8cad13cbef05954ab39bd3fd2138f6fee781",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6104189fda98a39757fd456b65261edfd183e65/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6104189fda98a39757fd456b65261edfd183e65/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=f6104189fda98a39757fd456b65261edfd183e65",
            "patch": "@@ -241,10 +241,9 @@\n     DATA_SAMPLERS = [RandomSampler]\n     if version.parse(accelerate_version) > version.parse(\"1.3.0\"):\n         from accelerate.utils import TorchTensorParallelPlugin\n-    if version.parse(accelerate_version) > version.parse(\"0.23.0\"):\n-        from accelerate.data_loader import SeedableRandomSampler\n+    from accelerate.data_loader import SeedableRandomSampler\n \n-        DATA_SAMPLERS += [SeedableRandomSampler]\n+    DATA_SAMPLERS += [SeedableRandomSampler]\n \n     if is_deepspeed_available():\n         from accelerate.utils import DeepSpeedSchedulerWrapper\n@@ -4196,9 +4195,7 @@ def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = Fa\n         elif (tp_size := getattr(self.model, \"_tp_size\", 0)) is not None and tp_size > 1:\n             self._save(output_dir)\n         elif self.is_fsdp_enabled:\n-            if (\"FULL_STATE_DICT\" in str(self.accelerator.state.fsdp_plugin.state_dict_type)) and (\n-                version.parse(accelerate_version) > version.parse(\"0.24.1\")\n-            ):\n+            if \"FULL_STATE_DICT\" in str(self.accelerator.state.fsdp_plugin.state_dict_type):\n                 state_dict = self.accelerator.get_state_dict(self.model)\n                 if self.args.should_save:\n                     self._save(output_dir, state_dict=state_dict)"
        },
        {
            "sha": "6a4060b0a7318490eb79272cfd7db328c6faab97",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 13,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6104189fda98a39757fd456b65261edfd183e65/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6104189fda98a39757fd456b65261edfd183e65/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=f6104189fda98a39757fd456b65261edfd183e65",
            "patch": "@@ -88,22 +88,11 @@ def get_master_port(real_launcher=False):\n \n \n if is_torch_available():\n-    from tests.trainer.test_trainer import (  # noqa\n-        RegressionModelConfig,\n-        RegressionPreTrainedModel,\n-    )\n-\n     # hack to restore original logging level pre #21700\n     get_regression_trainer = partial(tests.trainer.test_trainer.get_regression_trainer, log_level=\"info\")\n \n-require_fsdp_version = require_fsdp\n if is_accelerate_available():\n-    from accelerate.utils.constants import (\n-        FSDP_PYTORCH_VERSION,\n-        FSDP_SHARDING_STRATEGY,\n-    )\n-\n-    require_fsdp_version = partial(require_fsdp, min_version=FSDP_PYTORCH_VERSION)\n+    from accelerate.utils.constants import FSDP_SHARDING_STRATEGY\n \n \n FSDP2_ACCELERATE_VERSION = \"1.6.0\"\n@@ -142,7 +131,6 @@ def _parameterized_custom_name_func(func, param_num, param):\n \n @require_accelerate\n @require_torch_accelerator\n-@require_fsdp_version\n class TrainerIntegrationFSDP(TestCasePlus, TrainerIntegrationCommon):\n     def setUp(self):\n         super().setUp()"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 4,
        "deletions": 19
    }
}