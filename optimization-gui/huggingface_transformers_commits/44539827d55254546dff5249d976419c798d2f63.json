{
    "author": "vasqu",
    "message": "[`Executorch`] Simplify for encoder models (#41627)\n\n* Trigger Build\n\n* revert extra treatment for executorch as we default to no vmapping now",
    "sha": "44539827d55254546dff5249d976419c798d2f63",
    "files": [
        {
            "sha": "312cbe2ca8b8761b430f65c62dc8c74b7753f799",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -26,7 +26,6 @@\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n-    _ignore_bidirectional_mask_sdpa,\n     _ignore_causal_mask_sdpa,\n     _is_torch_greater_or_equal_than_2_5,\n     prepare_padding_mask,\n@@ -193,101 +192,6 @@ def generate(\n         pass\n \n \n-class TorchExportableModuleForEncoderOnlyLM(torch.nn.Module):\n-    \"\"\"\n-    A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n-    specifically for encoder-only LM. This module ensures that the exported model is compatible\n-    with further lowering and execution in `ExecuTorch`.\n-    \"\"\"\n-\n-    def __init__(self, model: PreTrainedModel) -> None:\n-        \"\"\"\n-        Initializes the exportable module.\n-\n-        Args:\n-            model (`PreTrainedModel`): The pretrained model to wrap.\n-        \"\"\"\n-        super().__init__()\n-\n-        self.model = model\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\n-            \"sdpa_bidirectional_mask_without_vmap\", sdpa_bidirectional_mask_without_vmap\n-        )\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_bidirectional_mask_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.config._attn_implementation = \"sdpa_bidirectional_mask_without_vmap\"\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Forward pass of the module, which is compatible with the ExecuTorch llm runner.\n-\n-        Args:\n-            input_ids (`torch.Tensor`): Tensor representing current input token id to the module.\n-            inputs_embeds (`torch.Tensor`): Tensor representing current input embeddings to the module.\n-            cache_position (`torch.Tensor`): Tensor representing current input position in the cache.\n-\n-        Returns:\n-            torch.Tensor: Logits output from the model.\n-        \"\"\"\n-        return self.model.forward(\n-            input_ids=input_ids,\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-        )\n-\n-    def export(\n-        self,\n-        input_ids: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        strict: Optional[bool] = None,\n-    ) -> torch.export.ExportedProgram:\n-        \"\"\"\n-        Export the wrapped module using `torch.export`.\n-\n-        Args:\n-            input_ids (`Optional[torch.Tensor]`):\n-                Tensor representing current input token id to the module. Must specify either this or inputs_embeds.\n-            inputs_embeds (`Optional[torch.Tensor]`):\n-                Tensor representing current input embeddings to the module. Must specify either this or input_ids.\n-            strict(`Optional[bool]`):\n-                Flag to instruct `torch.export` to use `torchdynamo`.\n-\n-        Returns:\n-            torch.export.ExportedProgram: The exported program that can be used for inference.\n-\n-        \"\"\"\n-        if not (input_ids is None) ^ (inputs_embeds is None):\n-            raise ValueError(\"Need to specify either input_ids or inputs_embeds.\")\n-\n-        if input_ids is not None:\n-            input_kwargs = {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask if attention_mask is not None else torch.ones_like(input_ids),\n-            }\n-        else:\n-            input_kwargs = {\n-                \"inputs_embeds\": inputs_embeds,\n-                \"attention_mask\": attention_mask\n-                if attention_mask is not None\n-                else torch.ones_like(inputs_embeds)[..., 0],\n-            }\n-\n-        exported_program = torch.export.export(\n-            self.model,\n-            args=(),\n-            kwargs=input_kwargs,\n-            strict=strict if strict is not None else True,\n-        )\n-\n-        return exported_program\n-\n-\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -1296,51 +1200,3 @@ def sdpa_mask_without_vmap(\n     if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n         causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n     return causal_mask\n-\n-\n-def sdpa_bidirectional_mask_without_vmap(\n-    kv_length: int,\n-    kv_offset: int = 0,\n-    attention_mask: Optional[torch.Tensor] = None,\n-    allow_torch_fix: bool = True,\n-    allow_is_bidirectional_skip: bool = True,\n-    **kwargs,\n-) -> Optional[torch.Tensor]:\n-    \"\"\"\n-    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n-    the element should take part in the attention computation, and False that it should not.\n-\n-    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n-    Additionally, surrounding logic for causal masks is omitted for simplicity.\n-\n-    Args:\n-        kv_length (`int`):\n-            The size that the key and value states will have during the attention computation.\n-        kv_offset (`int`, optional):\n-            An optional offset to indicate at which first position the key and values states will refer to.\n-        attention_mask (`torch.Tensor`, optional):\n-            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n-        allow_torch_fix (`bool`, optional):\n-            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n-            versions. We need an arg to skip it when using eager. By default `True`.\n-        allow_is_bidirectional_skip (`bool`, optional):\n-            Whether to allow to return `None` for the mask under conditions where we do not have to add any bias,\n-            i.e. full attention without any padding. Default to `True`.\n-    \"\"\"\n-    # Potentially pad the 2D mask, and slice it correctly\n-    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n-\n-    # Under specific conditions, we can avoid materializing the mask\n-    if allow_is_bidirectional_skip and _ignore_bidirectional_mask_sdpa(padding_mask):\n-        return None\n-\n-    bidirectional_mask = None\n-    if padding_mask is not None:\n-        bidirectional_mask = padding_mask[:, None, None, :]\n-\n-    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n-    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n-    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix and bidirectional_mask is not None:\n-        bidirectional_mask |= torch.all(~bidirectional_mask, dim=-1, keepdim=True)\n-\n-    return bidirectional_mask"
        },
        {
            "sha": "6e0d5ef5603c96cd9d94569e601108f314caaf0b",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -337,8 +337,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"albert/albert-base-v2\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -365,15 +363,13 @@ def test_export(self):\n             [\"capital\", \"capitol\", \"comune\", \"arrondissement\", \"bastille\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "ad876e4e54efe9da6d0208d23658720f331813e2",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -709,8 +709,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         bert_model = \"google-bert/bert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -735,15 +733,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"barber\", \"mechanic\", \"salesman\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "b5e82a822a0f7cd5b26548d3a584380ddbc4a488",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -404,8 +404,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         distilbert_model = \"distilbert-base-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -432,15 +430,13 @@ def test_export(self):\n             [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n         )\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        },
        {
            "sha": "db2215696fd7a142a8bcf761eb6dae4ca749bc5b",
            "filename": "tests/models/mobilebert/test_modeling_mobilebert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_mobilebert.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -395,8 +395,6 @@ def test_export(self):\n         if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         mobilebert_model = \"google/mobilebert-uncased\"\n         device = \"cpu\"\n         attn_implementation = \"eager\"\n@@ -420,15 +418,13 @@ def test_export(self):\n         eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask.split(), [\"carpenter\", \"waiter\", \"mechanic\", \"teacher\", \"clerk\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        },
        {
            "sha": "99032b83e8ed74eeec6ea207ce98b7ef16c084f8",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/44539827d55254546dff5249d976419c798d2f63/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=44539827d55254546dff5249d976419c798d2f63",
            "patch": "@@ -691,8 +691,6 @@ def test_export(self):\n         if not is_torch_greater_or_equal_than_2_4:\n             self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n \n-        from transformers.integrations.executorch import TorchExportableModuleForEncoderOnlyLM\n-\n         roberta_model = \"FacebookAI/roberta-base\"\n         device = \"cpu\"\n         attn_implementation = \"sdpa\"\n@@ -717,15 +715,13 @@ def test_export(self):\n         eager_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask.split(), [\"happiness\", \"love\", \"peace\", \"freedom\", \"simplicity\"])\n \n-        exportable_module = TorchExportableModuleForEncoderOnlyLM(model)\n-        exported_program = exportable_module.export(\n-            input_ids=inputs[\"input_ids\"],\n-            attention_mask=inputs[\"attention_mask\"],\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n             strict=True,\n         )\n \n-        result = exported_program.module().forward(\n-            input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]\n-        )\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         exported_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        }
    ],
    "stats": {
        "total": 214,
        "additions": 25,
        "deletions": 189
    }
}