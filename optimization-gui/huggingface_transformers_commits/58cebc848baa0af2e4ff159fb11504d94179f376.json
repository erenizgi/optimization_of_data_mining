{
    "author": "pcuenca",
    "message": "flash_paged: s_aux may not exist (#40434)\n\nSome implementations (i.e.,\nhttps://huggingface.co/kernels-community/vllm-flash-attn3) support an\n`s_aux` arg for attention sinks, but others\n(https://huggingface.co/kernels-community/flash-attn) do not. If s_aux\nis present in the kwargs, we forward it, otherwise we don't.\n\nThe user will still get an error if they use a model like gpt-oss-20b\nwith an implementation that does not support `s_aux`, but models that\ndon't use it won't error out. For example, [this is currently\nfailing](https://github.com/huggingface/transformers/blob/399cd5c04b11ba3f740b4f76e8067326786405cc/examples/pytorch/continuous_batching.py#L16)\nbecause we are sending `s_aux: None` in the dict.",
    "sha": "58cebc848baa0af2e4ff159fb11504d94179f376",
    "files": [
        {
            "sha": "352bc82a1e40c2f25c24e5e7a2d37b11237aff4b",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/58cebc848baa0af2e4ff159fb11504d94179f376/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/58cebc848baa0af2e4ff159fb11504d94179f376/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=58cebc848baa0af2e4ff159fb11504d94179f376",
            "patch": "@@ -53,7 +53,7 @@ def paged_attention_forward(\n     sliding_window = (-1, -1) if not getattr(module, \"sliding_window\", False) else (module.sliding_window, 0)\n     if implementation is not None:\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func\n-    custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")}\n+    custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n     attn_output = flash_attn_varlen_func(\n         q.transpose(1, 2).squeeze(0).contiguous(),\n         k.transpose(1, 2).squeeze(0).contiguous(),"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}