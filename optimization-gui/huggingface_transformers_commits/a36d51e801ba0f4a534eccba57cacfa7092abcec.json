{
    "author": "manueldeprada",
    "message": "ðŸš¨ Always return Cache objects in modelings (to align with generate) (#39765)\n\n* watch the world burn\n\n* fix models, pipelines\n\n* make the error a warning\n\n* remove kwargs and return_legacy_cache\n\n* fix reformer",
    "sha": "a36d51e801ba0f4a534eccba57cacfa7092abcec",
    "files": [
        {
            "sha": "49146cae31b257fbc4012fa2ae4b7a61831c2ad6",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -13,7 +13,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n@@ -540,14 +540,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -576,9 +577,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "f0b976a5be71482e6295488bf6461d041c859a78",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -13,7 +13,7 @@\n from packaging import version\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n@@ -543,14 +543,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -579,9 +580,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "56a9e7a4b5a90ce8e4735d5f076882be0d4365e2",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -1063,6 +1063,8 @@ def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tens\n         backward compatibility.\n         \"\"\"\n         cache = cls()\n+        if past_key_values is None:\n+            logger.warning_once(\"past_key_values should not be None in from_legacy_cache()\")\n         if past_key_values is not None:\n             for layer_idx in range(len(past_key_values)):\n                 key_states, value_states = past_key_values[layer_idx]\n@@ -1528,6 +1530,8 @@ def from_legacy_cache(\n         cls, past_key_values: tuple[tuple[torch.FloatTensor, torch.FloatTensor], ...]\n     ) -> \"EncoderDecoderCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `EncoderDecoderCache`.\"\"\"\n+        if past_key_values is None:\n+            logger.warning_once(\"past_key_values should not be None in from_legacy_cache()\")\n         cache = cls(\n             self_attention_cache=DynamicCache(),\n             cross_attention_cache=DynamicCache(),"
        },
        {
            "sha": "b2006ad72ffad412f9ee5aa53c9a796f409e2201",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1154,14 +1154,14 @@ def forward(\n             )\n             use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         # expand encoder attention mask\n@@ -1229,9 +1229,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "3703af30b28ea9140081f158bd044cca24351edd",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n from torch.nn import functional as F\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...generation.logits_process import (\n     AlternatingCodebooksLogitsProcessor,\n@@ -498,14 +498,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n                 \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -569,9 +569,6 @@ def forward(\n \n         logits = self.lm_head(hidden_states)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v for v in [None, logits, past_key_values, all_hidden_states, all_self_attentions] if v is not None"
        },
        {
            "sha": "82931e2eb7850300382d935f75f91f8726d32e73",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -1042,9 +1042,13 @@ def forward(\n             inputs_embeds = self.embed_tokens(input)\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1138,9 +1142,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "edc511315805d06f04d3b3c9550634a61db1a549",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -28,7 +28,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -639,14 +639,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -675,9 +676,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "b3e6fa053e9d596ea428c5c182903e91d484e2c5",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n@@ -380,14 +380,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -416,9 +416,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "1861bb525c846db1276ec8628f04c2dcf9cad235",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -1568,14 +1568,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n                 \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -1608,9 +1608,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "6e4a9e100b6f340f55f2f2468e8dd7fea25b550f",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -2200,9 +2200,13 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -2297,9 +2301,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "bdf0729f48a532ba3e08c6f98c92f80ea5ecd89c",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -556,15 +556,15 @@ def forward(\n                 use_cache = False\n \n         # initialize past_key_values\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -578,11 +578,7 @@ def forward(\n             mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        self_attn_cache = (\n-            past_key_values.self_attention_cache\n-            if isinstance(past_key_values, EncoderDecoderCache)\n-            else past_key_values\n-        )\n+        self_attn_cache = past_key_values\n \n         causal_mask = self._update_causal_mask(\n             attention_mask,\n@@ -646,9 +642,6 @@ def forward(\n \n         hidden_states = self.layer_norm(hidden_states)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "d500fe9ec7ec375b0782895b92c3d625a78e634d",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 15,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -380,15 +380,15 @@ def forward(\n                 use_cache = False\n \n         # initialize past_key_values\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n-                \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n+                \"You should pass an instance of `DynamicCache` instead, e.g. \"\n+                \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         batch_size, seq_length = inputs_embeds.size()[:-1]\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -402,11 +402,7 @@ def forward(\n             mask_seq_length = past_key_values_length + seq_length\n             attention_mask = torch.ones(batch_size, mask_seq_length, device=inputs_embeds.device)\n \n-        self_attn_cache = (\n-            past_key_values.self_attention_cache\n-            if isinstance(past_key_values, EncoderDecoderCache)\n-            else past_key_values\n-        )\n+        self_attn_cache = past_key_values\n \n         causal_mask = self._update_causal_mask(\n             attention_mask,\n@@ -470,9 +466,6 @@ def forward(\n \n         hidden_states = self.layer_norm(hidden_states)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "22a4faccd71a14db709825fc131d07c1f67ac296",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -998,9 +998,13 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1096,9 +1100,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "c705be8ab556469f56b3053940398bfd4e75de3c",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -984,9 +984,13 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1081,9 +1085,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "4aa44a9afb4f52aeaa2a04119e9532dc0a1b4e09",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -433,20 +433,22 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n         if use_cache:\n-            if not isinstance(past_key_values, Cache):\n+            if isinstance(past_key_values, tuple):\n                 logger.warning_once(\n                     \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                     \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                     \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n                 )\n-                return_legacy_cache = True\n                 past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n             # The model acts as encoder decoder but is not an encoder decoder. So we cast all cache objects to\n             # `EncoderDecoderCache` type assuming that the incoming cache is from `self_attention`\n             elif isinstance(past_key_values, DynamicCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+            elif past_key_values is None:\n+                past_key_values = EncoderDecoderCache(\n+                    self_attention_cache=DynamicCache(), cross_attention_cache=DynamicCache()\n+                )\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n@@ -479,9 +481,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "50318d34dfd85d9045609604779b4bf7444f3785",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN, QuickGELUActivation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -767,14 +767,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -803,9 +804,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "be5904a6293a01a2c596b1e9504a82ac3aa5dca1",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -596,14 +596,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -632,9 +633,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "40490b23d2eff3e821eef838c7e4ca70cf2fdfe4",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -1069,14 +1069,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n                 \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1155,9 +1155,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "c8a61d6bd0ee8221243a14b03c0a6a44024af4ad",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -636,14 +636,14 @@ def forward(\n         position = torch.arange(seq_length, dtype=dtype, device=device).repeat(batch, 1)\n         span = torch.full((batch, seq_length), 0, dtype=dtype, device=device)\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n                 \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -687,9 +687,6 @@ def forward(\n                     new_hidden_states += (hidden_state[:, self.prompt_length :, :],)\n                 all_hidden_states = new_hidden_states\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None"
        },
        {
            "sha": "1dd5d8ad2aedfa541443c616a80ceaf9062edcd1",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n-from ...cache_utils import Cache, DynamicCache\n+from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -338,14 +338,14 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n                 \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -422,9 +422,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None"
        },
        {
            "sha": "cbe35c7b10701befed9262eaaeaca64cde31c304",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -484,14 +484,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -520,9 +521,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "3b2e3b70726b09e11b93af6181f95908d75a7fe3",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -567,19 +567,18 @@ def forward(\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n         # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder and similar addition in GPT2Model\n-        return_legacy_cache = False\n         if use_cache:\n             if past_key_values is None:\n-                return_legacy_cache = True\n                 past_key_values = DynamicCache()\n-            elif not isinstance(past_key_values, Cache):\n-                return_legacy_cache = True\n+            elif isinstance(past_key_values, tuple):\n                 logger.warning_once(\n                     \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n                     \"You should pass an instance of `Cache` instead, e.g. \"\n                     \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n                 )\n                 past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            elif past_key_values is None:\n+                past_key_values = DynamicCache()\n \n             if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n                 past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n@@ -697,12 +696,7 @@ def forward(\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n         past_key_values = past_key_values if use_cache else None\n-        if return_legacy_cache:\n-            past_key_values = (\n-                past_key_values.self_attention_cache.to_legacy_cache()\n-                if self.config.add_cross_attention\n-                else past_key_values.to_legacy_cache()\n-            )\n+        # no return to legacy cache\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "ee921b6813266662780ad62bfbd9a0aa7c559399",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -539,14 +539,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -575,9 +576,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "b21267322c164dc70d069f9b28e7e732e28eedab",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -468,14 +468,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -504,9 +505,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "6c0ca07d26826b6cabe115c27d84955d4af6eccd",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 27,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu, get_activation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -858,7 +858,10 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        if not isinstance(cache, Cache):\n+        if cache is None:\n+            cache = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if isinstance(cache, tuple):\n             cache = EncoderDecoderCache.from_legacy_cache(cache)\n \n         if lengths is None:\n@@ -1144,12 +1147,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1255,12 +1254,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n@@ -1352,12 +1347,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1495,12 +1486,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         is_impossible (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels whether a question has an answer or no answer (SQuAD 2.0)\n         cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1636,12 +1623,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the"
        },
        {
            "sha": "36fcb173ece5d713ac638ed7d56a591ffbcfefe5",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -35,7 +35,7 @@\n from torch.nn import CrossEntropyLoss, LayerNorm\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_outputs import (\n@@ -649,9 +649,9 @@ def forward(\n             raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -718,9 +718,6 @@ def forward(\n \n         x = self.output_projection(x)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v for v in [x, past_key_values, all_hidden_states, all_self_attns, all_cross_attns] if v is not None"
        },
        {
            "sha": "3b2159a4ede03c72435227abac0ebf1594ce1304",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -839,13 +839,10 @@ def forward(\n                 use_cache = False\n \n         # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder\n-        return_legacy_cache = False\n         if use_cache:\n             if past_key_values is None:\n-                return_legacy_cache = True\n                 past_key_values = DynamicCache()\n-            elif not isinstance(past_key_values, Cache):\n-                return_legacy_cache = True\n+            elif isinstance(past_key_values, tuple):\n                 logger.warning_once(\n                     \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n                     \"You should pass an instance of `Cache` instead, e.g. \"\n@@ -961,12 +958,6 @@ def forward(\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n         past_key_values = past_key_values if use_cache else None\n-        if return_legacy_cache:\n-            past_key_values = (\n-                past_key_values.self_attention_cache.to_legacy_cache()\n-                if self.config.add_cross_attention\n-                else past_key_values.to_legacy_cache()\n-            )\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "5d743413792865927386a03e0124a1db00399549",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n@@ -487,14 +487,14 @@ def forward(\n         if batch_size <= 0:\n             raise ValueError(\"batch_size has to be defined and > 0\")\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         if inputs_embeds is None:\n@@ -589,9 +589,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "7472f669b226e1e2e04d9310c08ac9a9f7361134",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -646,14 +646,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_length = past_key_values.get_seq_length() if past_key_values is not None else past_key_values\n@@ -760,9 +760,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "ae0e3019360e39093c31e6451dda04f6cb12e44b",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -1216,9 +1216,9 @@ def forward(\n \n         input_shape = inputs_embeds.size()[:-1]\n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1304,9 +1304,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "1e4110fb196a70b40b88640745b994ea6719d90b",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1050,14 +1050,18 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1134,9 +1138,6 @@ def forward(\n         # add final layer norm\n         hidden_states = self.layer_norm(hidden_states)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)"
        },
        {
            "sha": "4b162cfdd957cea8cc4bad68b91ee875fe180295",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1760,14 +1760,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1844,9 +1844,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "4a0a804a844974a3746cc16b3ea17610ff5c3d7b",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -1055,9 +1055,9 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1158,9 +1158,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "5ed87c86f31e3f6864a4bdfdc3fd77ff5f86c22b",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -997,9 +997,13 @@ def forward(\n         inputs_embeds = inputs_embeds * self.embed_scale\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1090,9 +1094,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "499a626f4440a8749b1f93548500e6bb22d6e989",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -1038,9 +1038,13 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1134,9 +1138,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "69239a0b468f89c2e50e054b69d5dc3bdcd287f8",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -509,14 +509,14 @@ def forward(\n                     \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                 )\n                 use_cache = False\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -555,9 +555,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "45db56a9ee117e3709a29ee34e32750b04f68e1c",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -636,9 +636,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            self_attention_cache = DynamicCache()\n-            cross_attention_cache = DynamicCache()\n-            past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "c095abdefed5f4dff9e170e48e18069089af7ce3",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -658,9 +658,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None:\n-            self_attention_cache = DynamicCache()\n-            cross_attention_cache = DynamicCache()\n-            past_key_values = EncoderDecoderCache(self_attention_cache, cross_attention_cache)\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0"
        },
        {
            "sha": "8dbd8235b86b37d9a3a2155eca8cd4bc855087fb",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -354,9 +354,9 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `DynamicCache` instead, e.g. \"\n@@ -405,9 +405,6 @@ def forward(\n         # Add last hidden state\n         hidden_states = self.norm_f(hidden_states)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n "
        },
        {
            "sha": "0df49dae8a35a149c70772ddaa553d2666e0619f",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import (\n     ClassifierFreeGuidanceLogitsProcessor,\n     GenerationConfig,\n@@ -559,14 +559,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -638,9 +638,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "559f7375977b7a80e1c017acae95d41b90042096",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import (\n     ClassifierFreeGuidanceLogitsProcessor,\n     GenerationConfig,\n@@ -521,14 +521,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -601,9 +601,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None"
        },
        {
            "sha": "e2d1072f6a71a35387a73fdf2ffb01e86b10055a",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n@@ -855,14 +855,18 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -939,9 +943,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "f5b786b9767cf685f598a0398edfa4c90d6ea1a1",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -1233,9 +1233,9 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1326,9 +1326,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "78e2dc0cb4eb03f9d9f1aa046af6b7921fdc51c8",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -1047,9 +1047,13 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1141,9 +1145,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "578cc9958e07fafb47b882dab3d8b72cb3f14d91",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -1305,9 +1305,9 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1391,9 +1391,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "b348b7a88f7f0d5eb71c8cbf35d138f2c249f8d0",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -958,9 +958,13 @@ def forward(\n             inputs_embeds = self.embed_tokens(input)\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1054,9 +1058,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "4085cecaea379a2c6bdab93c333a2446992a710b",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import LayerNorm\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n@@ -1238,14 +1238,18 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1357,9 +1361,6 @@ def forward(\n             if self.config.ngram > 0:\n                 all_ngram_stream_hidden_states += (hidden_states[:, sequence_length:],)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         # split last_hidden_state for return\n         last_hidden_state = hidden_states[:, :sequence_length]\n         last_hidden_state_ngram = hidden_states[:, sequence_length:] if self.config.ngram > 0 else None"
        },
        {
            "sha": "367af66923575f9caf2324fb54770258f258e18e",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -30,7 +30,6 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutput, MaskedLMOutput, QuestionAnsweringModelOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -62,13 +61,12 @@\n )\n \n \n-class ReformerDynamicCache(DynamicCache):\n+class ReformerDynamicCache:\n     \"\"\"\n     A dynamic cache that stores past buckets instead of key/values.\n     \"\"\"\n \n     def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n-        super().__init__()\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n         self.buckets_cache: list[torch.Tensor] = []\n         self.states_cache: list[torch.Tensor] = []\n@@ -1827,14 +1825,14 @@ def forward(\n         all_attentions = []\n \n         # init cached hidden states if necessary\n-        return_legacy_cache = False\n-        if use_cache or not isinstance(past_buckets_states, ReformerDynamicCache):\n+        if use_cache and past_buckets_states is None:\n+            past_buckets_states = ReformerDynamicCache()\n+        elif use_cache and isinstance(past_buckets_states, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `ReformerDynamicCache` instead, e.g. \"\n                 \"`past_key_values=ReformerDynamicCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_buckets_states = ReformerDynamicCache.from_legacy_cache(past_buckets_states)\n \n         # concat same tensor for reversible ResNet\n@@ -1861,8 +1859,6 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n \n         next_cache = past_buckets_states if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = past_buckets_states.to_legacy_cache()\n \n         return ReformerEncoderOutput(\n             hidden_states=hidden_states,\n@@ -2360,15 +2356,15 @@ def prepare_inputs_for_generation(\n \n     def _reorder_cache(self, past_key_values, beam_idx):\n         reord_past_buckets_states = []\n-        for layer_past in past_key_values:\n+        for buckets, hidden_states in past_key_values:\n             # buckets\n-            if layer_past[0] is not None:\n-                reord_buckets = layer_past[0].index_select(0, beam_idx.to(layer_past[0].device))\n+            if buckets is not None and buckets.numel() > 0:\n+                reord_buckets = buckets.index_select(0, beam_idx.to(buckets.device))\n             else:\n                 reord_buckets = None\n \n             # hidden states\n-            reord_hidden_states = layer_past[1].index_select(0, beam_idx.to(layer_past[1].device))\n+            reord_hidden_states = hidden_states.index_select(0, beam_idx.to(hidden_states.device))\n             reord_past_buckets_states.append((reord_buckets, reord_hidden_states))\n \n         if isinstance(past_key_values, ReformerDynamicCache):"
        },
        {
            "sha": "10530f4949eabfffe056fa1eb40321071d73d6fd",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -505,14 +505,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         hidden_states = self.embedding_hidden_mapping_in(hidden_states)\n@@ -545,9 +545,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "de8fd818cbd6d62013167101385b9cbcdbee72ac",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -595,14 +595,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -631,9 +632,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "6f6adeeb0937163c1211d8939f3c588598481b7d",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -476,14 +476,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -512,9 +513,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "990331044bed3f41322ce01960e584c3b9c84d28",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -595,14 +595,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -631,9 +632,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "2a3490e3438013b4de07f60db4317939cb6592b8",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -540,14 +540,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -586,9 +586,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "6a84b884fba5050380a7317ed9b1d5f12d430b9c",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -1800,9 +1800,9 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1866,9 +1866,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "e475b5a9f81f46e080da9f4d85df25f2a777e2b2",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -1843,9 +1843,9 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1910,9 +1910,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "7469e684613a624300060e4fce95e3d5b8d5f10f",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n@@ -873,9 +873,9 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -949,9 +949,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "fefbb8b6604f647f4a4086578899199794eb2848",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, L1Loss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n@@ -1582,14 +1582,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1655,9 +1655,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "7ad3fd845af6e3ace117ee97c9e6db608c345150",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -812,10 +812,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if not self.training and use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(\n-                self_attention_cache=DynamicCache(config=self.config),\n-                cross_attention_cache=DynamicCache(config=self.config),\n-            )\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange("
        },
        {
            "sha": "988e24e65365e5512e45bf1e464a470a218129ac",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -675,10 +675,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if not self.training and use_cache and past_key_values is None:\n-            past_key_values = EncoderDecoderCache(\n-                self_attention_cache=DynamicCache(config=self.config),\n-                cross_attention_cache=DynamicCache(config=self.config),\n-            )\n+            past_key_values = EncoderDecoderCache(DynamicCache(config=self.config), DynamicCache(config=self.config))\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange("
        },
        {
            "sha": "6b3ae41e67d3cf71dd073a6bcdbf3ff0a6ac3253",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -26,7 +26,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, MaskedLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n@@ -577,7 +577,9 @@ def forward(\n         return_dict=True,\n         cache_position=None,\n     ):\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \""
        },
        {
            "sha": "53f1ffed9bfb5b5664fca08d9cf511d924677809",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n     _prepare_4d_attention_mask_for_sdpa,\n@@ -993,9 +993,9 @@ def forward(\n \n         input_shape = inputs_embeds.size()[:-1]\n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -1081,9 +1081,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "d357643af0ee1ac62fa5641783cd813429fb54fd",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     _prepare_4d_attention_mask,\n@@ -582,14 +582,18 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -668,9 +672,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "b0de0327c657c60b6601642fdc1bdc7c9a3b13ed",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -494,9 +494,13 @@ def forward(\n                 use_cache = False\n \n         # initialize `past_key_values`\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n+        if use_cache and past_key_values is None:\n+            past_key_values = (\n+                EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                if encoder_hidden_states is not None\n+                else DynamicCache()\n+            )\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n@@ -578,9 +582,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "5655195102bfc9a3cbc0527c86688f9a09a51f61",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 37,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -27,7 +27,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import gelu, get_activation\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -811,12 +811,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -831,7 +827,10 @@ def forward(\n \n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        if not isinstance(cache, Cache):\n+        if cache is None:\n+            cache = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if isinstance(cache, tuple):\n             cache = EncoderDecoderCache.from_legacy_cache(cache)\n \n         if lengths is None:\n@@ -1033,12 +1032,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n@@ -1126,12 +1121,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1240,12 +1231,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1347,12 +1334,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         is_impossible (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels whether a question has an answer or no answer (SQuAD 2.0)\n         cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1468,12 +1451,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\n         \"\"\"\n@@ -1579,12 +1558,8 @@ def forward(\n             also use *attention_mask* for the same result (see above), kept here for compatibility. Indices selected in\n             `[0, ..., input_ids.size(-1)]`.\n         cache (`dict[str, torch.FloatTensor]`, *optional*):\n-            Dictionary string to `torch.FloatTensor` that contains precomputed hidden states (key and values in the\n-            attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n+            Instance of `EncoderDecoderCache` that contains precomputed KV states. Can be used to speed up sequential\n             decoding.\n-\n-            The dictionary object will be modified in-place during the forward pass to add newly computed\n-            hidden-states.\n         inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, hidden_size)`, *optional*):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the"
        },
        {
            "sha": "a0e7075cd8bc9ca644ea4ac0f956a876edf80fb8",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -596,14 +596,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and self.config.is_decoder and not isinstance(past_key_values, Cache):\n+        if use_cache and self.config.is_decoder and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+\n+        if use_cache and self.config.is_decoder and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         for i, layer_module in enumerate(self.layer):\n@@ -632,9 +633,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "054aed9ee07429e59e35aa6a7ed1d103ddf891c9",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -585,14 +585,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -627,9 +627,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "e2bb9e2e74b9db0d745d4895c5ba042dcaeb3c85",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, gelu\n-from ...cache_utils import Cache, EncoderDecoderCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -534,14 +534,14 @@ def forward(\n                 )\n                 use_cache = False\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n+        if use_cache and past_key_values is None:\n+            past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if use_cache and isinstance(past_key_values, tuple):\n             logger.warning_once(\n                 \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.58.0. \"\n                 \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n                 \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n             )\n-            return_legacy_cache = True\n             past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n \n         all_hidden_states = () if output_hidden_states else None\n@@ -578,9 +578,6 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        if return_legacy_cache:\n-            past_key_values = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v"
        },
        {
            "sha": "3857805962a93f796e407ed88c89495b46fd0bd0",
            "filename": "src/transformers/pipelines/pt_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fpipelines%2Fpt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/src%2Ftransformers%2Fpipelines%2Fpt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fpt_utils.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -86,13 +86,15 @@ def loader_batch_item(self):\n                     elif isinstance(element[0], np.ndarray):\n                         loader_batched[k] = tuple(np.expand_dims(el[self._loader_batch_index], 0) for el in element)\n                     continue\n-                if k in {\"hidden_states\", \"past_key_values\", \"attentions\"} and isinstance(element, tuple):\n+                if k in {\"hidden_states\", \"attentions\"} and isinstance(element, tuple):\n                     # Those are stored as lists of tensors so need specific unbatching.\n                     if isinstance(element[0], torch.Tensor):\n                         loader_batched[k] = tuple(el[self._loader_batch_index].unsqueeze(0) for el in element)\n                     elif isinstance(element[0], np.ndarray):\n                         loader_batched[k] = tuple(np.expand_dims(el[self._loader_batch_index], 0) for el in element)\n                     continue\n+                if k == \"past_key_values\":\n+                    continue\n                 if element is None:\n                     # This can happen for optional data that get passed around\n                     loader_batched[k] = None"
        },
        {
            "sha": "8d33d9dc1b223c898de48dd6803d0e50b02881ce",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -17,6 +17,7 @@\n from packaging import version\n \n from transformers import AutoTokenizer, BertConfig, is_torch_available\n+from transformers.cache_utils import EncoderDecoderCache\n from transformers.models.auto import get_values\n from transformers.testing_utils import (\n     CaptureLogger,\n@@ -716,8 +717,8 @@ def test_sdpa_ignored_mask(self):\n             # Case where query length != kv_length. Note that model needs to be a decoder so we can use cache\n             model.config.is_decoder = True\n             model_sdpa.config.is_decoder = True\n-            res_eager = model(**inp, past_key_values=pkv, use_cache=True)\n-            res_sdpa = model_sdpa(**inp, past_key_values=pkv, use_cache=True)\n+            res_eager = model(**inp, past_key_values=EncoderDecoderCache.from_legacy_cache(pkv), use_cache=True)\n+            res_sdpa = model_sdpa(**inp, past_key_values=EncoderDecoderCache.from_legacy_cache(pkv), use_cache=True)\n             self.assertTrue(\n                 torch.allclose(res_eager.last_hidden_state, res_sdpa.last_hidden_state, atol=1e-5, rtol=1e-4)\n             )"
        },
        {
            "sha": "2b3c887f5c3a1b5b5834d850045289358392d757",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -12,13 +12,12 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import math\n import unittest\n \n import pytest\n \n-from transformers import GPT2Config, is_torch_available\n+from transformers import DynamicCache, GPT2Config, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n@@ -443,9 +442,15 @@ def create_and_check_cached_forward_with_and_without_attention_mask(self, config\n \n         # Cached forward once with the attention mask provided and the other time without it (which should assume full attention)\n         cache_outputs = model(**cache_inputs)\n-        full_outputs_with_attention_mask = model(\n-            **non_cache_inputs, past_key_values=cache_outputs.past_key_values\n-        ).last_hidden_state\n+        # Caches are mutable (unlike legacy tuples), so we need to copy them before using multiple times\n+        pkv_copy = DynamicCache()\n+        pkv_copy.update(\n+            cache_outputs.past_key_values.layers[0].keys, cache_outputs.past_key_values.layers[0].values, 0\n+        )\n+        pkv_copy.update(\n+            cache_outputs.past_key_values.layers[1].keys, cache_outputs.past_key_values.layers[1].values, 1\n+        )\n+        full_outputs_with_attention_mask = model(**non_cache_inputs, past_key_values=pkv_copy).last_hidden_state\n         full_outputs_without_attention_mask = model(\n             non_cache_inputs[\"input_ids\"], past_key_values=cache_outputs.past_key_values\n         ).last_hidden_state"
        },
        {
            "sha": "a609b75ea0c76ea252a979cd19ae24aff3a13b98",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -1316,21 +1316,6 @@ def test_trainer_works_with_dict(self):\n         _ = trainer.evaluate()\n         _ = trainer.predict(eval_dataset)\n \n-    def test_evaluation_with_keys_to_drop(self):\n-        config = GPT2Config(vocab_size=100, n_positions=128, n_embd=32, n_layer=3, n_head=4)\n-        tiny_gpt2 = GPT2LMHeadModel(config)\n-        x = torch.randint(0, 100, (128,))\n-        eval_dataset = RepeatDataset(x)\n-        args = TrainingArguments(self.get_auto_remove_tmp_dir(), report_to=\"none\")\n-        trainer = Trainer(tiny_gpt2, args, eval_dataset=eval_dataset)\n-        # By default the past_key_values are removed\n-        result = trainer.predict(eval_dataset)\n-        self.assertTrue(isinstance(result.predictions, np.ndarray))\n-        # We can still get them by setting ignore_keys to []\n-        result = trainer.predict(eval_dataset, ignore_keys=[])\n-        self.assertTrue(isinstance(result.predictions, tuple))\n-        self.assertEqual(len(result.predictions), 2)\n-\n     def test_training_arguments_are_left_untouched(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         trainer = get_regression_trainer(output_dir=tmp_dir)"
        },
        {
            "sha": "043a43865212c3995475c58b01733b399f7daa90",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a36d51e801ba0f4a534eccba57cacfa7092abcec/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=a36d51e801ba0f4a534eccba57cacfa7092abcec",
            "patch": "@@ -1197,6 +1197,28 @@ def test_dynamic_cache(self):\n             \"DynamicCache Scenario 2 layer 1 failed\",\n         )\n \n+    def test_dynamic_cache_batch_select_indices(self):\n+        \"\"\"Select a subset of batches in-place using batch_select_indices.\"\"\"\n+        cache = DynamicCache()\n+        # Shape: (batch=3, heads=1, seq_len=2, head_dim=1)\n+        prefill = torch.tensor(\n+            [\n+                [[[1.0], [2.0]]],\n+                [[[10.0], [20.0]]],\n+                [[[100.0], [200.0]]],\n+            ]\n+        )\n+        cache.update(prefill, prefill, 0)\n+        self.assertEqual(cache.layers[0].keys.shape[0], 3)\n+\n+        # Keep batches 0 and 2\n+        cache.batch_select_indices((0, 2))\n+        self.assertEqual(cache.layers[0].keys.shape[0], 2)\n+        self.assertEqual(\n+            cache.layers[0].keys[:, 0, :, 0].tolist(),\n+            [[1.0, 2.0], [100.0, 200.0]],\n+        )\n+\n     def test_hybrid_cache(self):\n         \"\"\"\n         Test HybridCache with a mix of static and sliding layers,"
        }
    ],
    "stats": {
        "total": 916,
        "additions": 378,
        "deletions": 538
    }
}