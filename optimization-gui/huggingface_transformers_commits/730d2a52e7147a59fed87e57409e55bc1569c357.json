{
    "author": "inkcherry",
    "message": "DeepSpeed tensor parallel+ZeRO (#36825)\n\nadd ds tp change",
    "sha": "730d2a52e7147a59fed87e57409e55bc1569c357",
    "files": [
        {
            "sha": "bf47511345fcc623053e3b252e39bb7da46d01b0",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/730d2a52e7147a59fed87e57409e55bc1569c357/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/730d2a52e7147a59fed87e57409e55bc1569c357/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=730d2a52e7147a59fed87e57409e55bc1569c357",
            "patch": "@@ -464,6 +464,11 @@ def deepspeed_init(trainer, num_training_steps, inference=False):\n         model_parameters = None\n     else:\n         trainer.optimizer = None  # important for when deepspeed_init is used as re-init\n+        tp_size = hf_deepspeed_config.config.get(\"tensor_parallel\", {}).get(\"autotp_size\", 0)\n+        if tp_size > 1:\n+            import deepspeed\n+\n+            model = deepspeed.tp_model_init(model=model, tp_size=tp_size, dtype=hf_deepspeed_config.dtype())\n         model_parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n         optimizer, lr_scheduler = deepspeed_optim_sched(\n             trainer, hf_deepspeed_config, args, num_training_steps, model_parameters"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}