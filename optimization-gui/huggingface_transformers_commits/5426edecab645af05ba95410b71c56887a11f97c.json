{
    "author": "Cyrilvallez",
    "message": "Make quantizers good citizens loading-wise (#41138)\n\n* fix param_needs_quantization\n\n* rewrite most hqq\n\n* clean\n\n* fix\n\n* comment\n\n* remove it from exception of safetensors\n\n* start on bnb 4bits\n\n* post-rebase fix\n\n* make bnb4 bit a good citizen\n\n* remove forgotten print\n\n* make bnb 8bits a good citizen\n\n* better hqq\n\n* fix\n\n* clean\n\n* remove state dict from signature\n\n* switch method\n\n* make torchao a good citizen\n\n* fixes\n\n* fix torchao\n\n* add check\n\n* typo",
    "sha": "5426edecab645af05ba95410b71c56887a11f97c",
    "files": [
        {
            "sha": "884b943f696b1b4d75e2c670753fd975954ef3ca",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 12,
            "deletions": 16,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -159,24 +159,13 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n             pre_quantized=self.pre_quantized,\n         )\n \n-    def param_needs_quantization(\n-        self,\n-        model,\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def param_needs_quantization(self, model, param_name: str, **kwargs) -> bool:\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         if isinstance(module, Int8SymmetricLinear):\n             if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n                 return False\n             else:\n-                if tensor_name == \"weight_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n         return False\n \n@@ -186,11 +175,18 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Quantizes weights to INT8 symmetric format.\n-        \"\"\"\n+        # Sanity check\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, Int8SymmetricLinear):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n+                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n+            else:\n+                if tensor_name == \"weight_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+\n         abs_max_per_row = torch.max(torch.abs(param_value), dim=1, keepdim=True)[0].clamp(min=1e-5)\n \n         weight_scale = abs_max_per_row / 127.0"
        },
        {
            "sha": "698d7fbd5c23184ce7483a736ff4829f279d313e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 55,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -104,7 +104,6 @@\n     is_torch_npu_available,\n     is_torch_xla_available,\n     is_torch_xpu_available,\n-    is_torchao_available,\n     logging,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n@@ -119,9 +118,6 @@\n from .utils.quantization_config import BitsAndBytesConfig, QuantizationMethod\n \n \n-if is_torchao_available():\n-    from torchao.quantization import Int4WeightOnlyConfig\n-\n if is_accelerate_available():\n     from accelerate import dispatch_model, infer_auto_device_map\n     from accelerate.hooks import add_hook_to_module\n@@ -644,6 +640,7 @@ def _infer_parameter_dtype(\n             QuantizationMethod.HQQ,\n             QuantizationMethod.QUARK,\n             QuantizationMethod.MXFP4,\n+            QuantizationMethod.BITS_AND_BYTES,\n         }:\n             return True, None\n         else:\n@@ -698,13 +695,8 @@ def _load_state_dict_into_meta_model(\n         device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n \n     is_quantized = hf_quantizer is not None\n-    is_hqq_or_bnb_or_ao = is_quantized and hf_quantizer.quantization_config.quant_method in {\n-        QuantizationMethod.HQQ,\n-        QuantizationMethod.BITS_AND_BYTES,\n-        QuantizationMethod.TORCHAO,\n-    }\n     is_safetensors = shard_file.endswith(\".safetensors\")\n-    is_meta_state_dict = is_safetensors and not is_hqq_or_bnb_or_ao\n+    is_meta_state_dict = is_safetensors\n     file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device) if is_meta_state_dict else None\n     params_to_load = list(state_dict.keys())\n \n@@ -726,9 +718,7 @@ def _load_state_dict_into_meta_model(\n         )\n \n         if device_mesh is not None:\n-            if not is_quantized or not hf_quantizer.param_needs_quantization(\n-                model, param, param_name, state_dict, device_map=device_map\n-            ):\n+            if not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n                 # In this case, the param is already on the correct device!\n                 shard_and_distribute_module(\n                     model,\n@@ -740,7 +730,8 @@ def _load_state_dict_into_meta_model(\n                     device_mesh.get_local_rank(),\n                     device_mesh,\n                 )\n-            else:  # we have a device mesh but the param needs to be quantized, so we shard inside create_quantized_param:\n+            else:\n+                # we have a device mesh but the param needs to be quantized, so we shard inside create_quantized_param\n                 sharding_kwargs = {\n                     \"empty_param\": empty_param,\n                     \"casting_dtype\": casting_dtype,\n@@ -753,7 +744,6 @@ def _load_state_dict_into_meta_model(\n                     param,\n                     param_name,\n                     device_mesh.get_local_rank(),\n-                    state_dict,\n                     **sharding_kwargs,\n                 )\n         else:\n@@ -775,17 +765,15 @@ def _load_state_dict_into_meta_model(\n             if param_device == \"disk\":\n                 if not is_safetensors:\n                     disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n-            elif not is_quantized or not hf_quantizer.param_needs_quantization(\n-                model, param, param_name, state_dict, param_device=param_device, device_map=device_map\n-            ):\n+            elif not is_quantized or not hf_quantizer.param_needs_quantization(model, param_name):\n                 if is_fsdp_enabled():\n                     param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n \n                 _load_parameter_into_model(model, param_name, param.to(param_device))\n \n             else:\n                 # TODO naming is stupid it loads it as well\n-                hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict)\n+                hf_quantizer.create_quantized_param(model, param, param_name, param_device)\n \n                 # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n@@ -823,7 +811,6 @@ def load_shard_file(args):\n         shard_file,\n         state_dict,\n         disk_only_shard_files,\n-        is_hqq_or_bnb_or_ao,\n         is_quantized,\n         device_map,\n         hf_quantizer,\n@@ -842,22 +829,8 @@ def load_shard_file(args):\n         return [], disk_offload_index\n \n     map_location = \"cpu\"\n-    if (\n-        shard_file.endswith(\".safetensors\")\n-        and not is_hqq_or_bnb_or_ao\n-        and not (is_deepspeed_zero3_enabled() and not is_quantized)\n-    ):\n+    if shard_file.endswith(\".safetensors\") and not (is_deepspeed_zero3_enabled() and not is_quantized):\n         map_location = \"meta\"\n-    elif (\n-        device_map is not None\n-        and hf_quantizer is not None\n-        and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO\n-        and (\n-            hf_quantizer.quantization_config.quant_type in [\"int4_weight_only\", \"autoquant\"]\n-            or isinstance(hf_quantizer.quantization_config.quant_type, Int4WeightOnlyConfig)\n-        )\n-    ):\n-        map_location = torch.device([d for d in device_map.values() if d not in [\"disk\"]][0])\n \n     # If shard_file is \"\", we use the existing state_dict instead of loading it\n     if shard_file != \"\":\n@@ -868,14 +841,7 @@ def load_shard_file(args):\n     # Fix the key names\n     state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n \n-    if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO:\n-        if shard_file.endswith(\".safetensors\") and is_safetensors_available():\n-            with safe_open(shard_file, framework=\"pt\") as f:\n-                metadata = f.metadata()\n-            state_dict = hf_quantizer.update_state_dict_with_metadata(state_dict, metadata)\n-\n     error_msgs = []\n-\n     if is_deepspeed_zero3_enabled() and not is_quantized:\n         error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n     # Skip it with fsdp on ranks other than 0\n@@ -1384,6 +1350,7 @@ def _find_missing_and_unexpected_keys(\n \n     if hf_quantizer is not None:\n         missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n+        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys)\n \n     return missing_keys, unexpected_keys\n \n@@ -4398,9 +4365,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                 cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n@@ -4931,6 +4895,10 @@ def _assign_original_dtype(module):\n             config._pre_quantization_dtype = original_dtype\n             _assign_original_dtype(model)\n \n+            # Torchao needs access to all metadata later\n+            if hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO:\n+                hf_quantizer.set_metadata(checkpoint_files)\n+\n         if _torch_distributed_available and device_mesh is not None:\n             model = distribute_model(model, distributed_config, device_mesh, tp_size)\n \n@@ -5201,11 +5169,6 @@ def _load_pretrained_model(\n             QuantizationMethod.HQQ,\n             QuantizationMethod.QUARK,\n         }\n-        is_hqq_or_bnb_or_ao = is_quantized and hf_quantizer.quantization_config.quant_method in {\n-            QuantizationMethod.HQQ,\n-            QuantizationMethod.BITS_AND_BYTES,\n-            QuantizationMethod.TORCHAO,\n-        }\n \n         # Get all the keys of the state dicts that we have to initialize the model\n         if sharded_metadata is not None:\n@@ -5338,7 +5301,6 @@ def _load_pretrained_model(\n                 shard_file,\n                 state_dict,\n                 disk_only_shard_files,\n-                is_hqq_or_bnb_or_ao,\n                 is_quantized,\n                 device_map,\n                 hf_quantizer,\n@@ -5709,12 +5671,10 @@ def _move_missing_keys_from_meta_to_cpu(\n             # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n             if param.device == torch.device(\"meta\"):\n                 value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n-                if not is_quantized or not hf_quantizer.param_needs_quantization(\n-                    self, param_value=value, param_name=key, state_dict={}\n-                ):\n+                if not is_quantized or not hf_quantizer.param_needs_quantization(self, key):\n                     _load_parameter_into_model(self, key, value)\n                 else:\n-                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict)\n+                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\")\n \n     def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:\n         \"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to"
        },
        {
            "sha": "b9dd7ae10f9e07b604de7900b1c309989cfd2d34",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -140,6 +140,9 @@ def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: lis\n         \"\"\"\n         return expected_keys\n \n+    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n+        return unexpected_keys\n+\n     def get_special_dtypes_update(self, model, dtype: \"torch.dtype\") -> dict[str, \"torch.dtype\"]:\n         \"\"\"\n         returns dtypes for modules that are not quantized - used for the computation of the device_map in case\n@@ -175,10 +178,12 @@ def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **\n         \"\"\"\n         return False\n \n-    def create_quantized_param(self, *args, **kwargs) -> \"torch.nn.Parameter\":\n+    def create_quantized_param(self, *args, **kwargs):\n         \"\"\"\n-        takes needed components from state_dict and creates quantized param; only applicable if\n-        requires_parameters_quantization == True\n+        Take needed components from state_dict (those from which `param_needs_quantization` is True) and create\n+        quantized param.\n+        It usually also load the new param directly in the `model`.\n+        Note: only applicable if requires_parameters_quantization == True.\n         \"\"\"\n         if not self.requires_parameters_quantization:\n             raise AttributeError("
        },
        {
            "sha": "57e393ccda1767626922c66d9f10eaeee61226ca",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 52,
            "deletions": 79,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -12,8 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib\n+from collections import defaultdict\n from functools import cached_property\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from packaging import version\n \n@@ -67,6 +68,15 @@ def __init__(self, quantization_config, **kwargs):\n         if self.quantization_config.llm_int8_skip_modules is not None:\n             self.modules_to_not_convert = self.quantization_config.llm_int8_skip_modules\n \n+        # This describes the additional items that are saved on the state dict (on the params themselves)\n+        self.bnb_keys = [\n+            f\"quant_state.bitsandbytes__{self.quantization_config.bnb_4bit_quant_type}\",\n+            \"absmax\",\n+            \"quant_map\",\n+        ]\n+        if self.quantization_config.bnb_4bit_use_double_quant:\n+            self.bnb_keys.extend([\"nested_absmax\", \"nested_quant_map\"])\n+\n     def validate_environment(self, *args, **kwargs):\n         if not is_accelerate_available():\n             raise ImportError(\n@@ -132,105 +142,69 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n                 \"calculation. You may encounter unexpected behavior, or pass your own device map\"\n             )\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n+        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.bnb_keys)]\n+\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb\n \n-        module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module._parameters.get(tensor_name, None), bnb.nn.Params4bit):\n-            # Add here check for loaded components' dtypes once serialization is implemented\n+        # They are on the params themselves, so we cannot easily extract the module from the name\n+        if any(param_name.endswith(x) for x in self.bnb_keys):\n             return True\n-        elif isinstance(module, bnb.nn.Linear4bit) and tensor_name == \"bias\":\n-            # bias could be loaded by regular set_module_tensor_to_device() from accelerate,\n-            # but it would wrongly use uninitialized weight there.\n-            return True\n-        else:\n-            return False\n+        module, name = get_module_from_name(model, param_name)\n+        return isinstance(module, bnb.nn.Linear4bit) and name != \"bias\"\n \n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()\n-        \"\"\"\n         import bitsandbytes as bnb\n \n+        is_quant_stat = any(param_name.endswith(x) for x in self.bnb_keys)\n+        full_name = param_name\n+        if is_quant_stat:\n+            param_name = (\n+                param_name.rsplit(\".\", 1)[0] if \"quant_state.\" not in param_name else param_name.rsplit(\".\", 2)[0]\n+            )\n         module, tensor_name = get_module_from_name(model, param_name)\n \n-        if tensor_name not in module._parameters:\n-            raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n-\n-        old_value = getattr(module, tensor_name)\n-\n         # `torch.Tensor.to(<int num>)` is not supported by `torch_npu` (see this [issue](https://github.com/Ascend/pytorch/issues/16)).\n         if isinstance(target_device, int) and is_torch_npu_available():\n             target_device = f\"npu:{target_device}\"\n-        if tensor_name == \"bias\":\n-            if param_value is None:\n-                new_value = old_value.to(target_device)\n-            else:\n-                new_value = param_value.to(target_device)\n-\n-            new_value = torch.nn.Parameter(new_value, requires_grad=old_value.requires_grad)\n-            module._parameters[tensor_name] = new_value\n-            return\n \n-        if not isinstance(module._parameters[tensor_name], bnb.nn.Params4bit):\n-            raise ValueError(\"this function only loads `Linear4bit components`\")\n-        if (\n-            old_value.device == torch.device(\"meta\")\n-            and target_device not in [\"meta\", torch.device(\"meta\")]\n-            and param_value is None\n-        ):\n-            raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.\")\n-\n-        # construct `new_value` for the module._parameters[tensor_name]:\n+        # construct `new_value` for the module._parameters[tensor_name]\n         if self.pre_quantized:\n-            # 4bit loading. Collecting components for restoring quantized weight\n-            # This can be expanded to make a universal call for any quantized weight loading\n-\n-            if not self.is_serializable:\n-                raise ValueError(\n-                    \"Detected int4 weights but the version of bitsandbytes is not compatible with int4 serialization. \"\n-                    \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n-                )\n-\n-            if (param_name + \".quant_state.bitsandbytes__fp4\" not in state_dict) and (\n-                param_name + \".quant_state.bitsandbytes__nf4\" not in state_dict\n-            ):\n-                raise ValueError(\n-                    f\"Supplied state dict for {param_name} does not contain `bitsandbytes__*` and possibly other `quantized_stats` components.\"\n+            module_name = param_name.rsplit(\".\", 1)[0]\n+            # Save the states for later quantization when they are all gathered\n+            if not hasattr(self, \"param_quant_stats\"):\n+                self.param_quant_stats = defaultdict(dict)\n+            self.param_quant_stats[module_name].update({full_name: param_value})\n+\n+            # We are ready for quantization in this case (note, the +1 is for the weight itself)\n+            if len(self.param_quant_stats[module_name]) == len(self.bnb_keys) + 1:\n+                param_kwargs = {}\n+                if self.is_bnb_supports_quant_storage_module:\n+                    param_kwargs[\"module\"] = module\n+\n+                weight = self.param_quant_stats[module_name].pop(f\"{module_name}.weight\")\n+                new_value = bnb.nn.Params4bit.from_prequantized(\n+                    data=weight,\n+                    quantized_stats=self.param_quant_stats[module_name],\n+                    requires_grad=False,\n+                    device=target_device,\n+                    **param_kwargs,\n                 )\n-\n-            quantized_stats = {}\n-            for k, v in state_dict.items():\n-                if param_name + \".\" in k:\n-                    quantized_stats[k] = v\n-\n-            param_kwargs = {}\n-            if self.is_bnb_supports_quant_storage_module:\n-                param_kwargs[\"module\"] = module\n-\n-            new_value = bnb.nn.Params4bit.from_prequantized(\n-                data=param_value,\n-                quantized_stats=quantized_stats,\n-                requires_grad=False,\n-                device=target_device,\n-                **param_kwargs,\n-            )\n+                # Set it\n+                module._parameters[tensor_name] = new_value\n+                # Delete the states\n+                del self.param_quant_stats[module_name]\n         else:\n             new_value = param_value.to(\"cpu\")\n+            old_value = getattr(module, tensor_name)\n \n             # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n             # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n@@ -241,7 +215,7 @@ def create_quantized_param(\n             kwargs.pop(\"_is_hf_initialized\", None)\n             new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)\n \n-        module._parameters[tensor_name] = new_value\n+            module._parameters[tensor_name] = new_value\n \n     # Copied from transformers.quantizers.quantizer_bnb_8bit.Bnb8BitHfQuantizer.adjust_max_memory\n     def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str, Union[int, str]]:\n@@ -313,7 +287,6 @@ def _process_model_before_weight_loading(\n         model = replace_with_bnb_linear(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n-        # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandbyter.py to here\n \n         model.config.quantization_config = self.quantization_config\n "
        },
        {
            "sha": "08a0fcd9269ce6be03def89bbbae2c4295364c8e",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 22,
            "deletions": 49,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from packaging import version\n \n@@ -158,80 +158,54 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             logger.info(\"target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\")\n         return torch.int8\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n+        bnb_keys = [\"SCB\", \"weight_format\"]\n+        return [k for k in unexpected_keys if not any(k.endswith(x) for x in bnb_keys)]\n+\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         import bitsandbytes as bnb\n \n-        module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module._parameters.get(tensor_name, None), bnb.nn.Int8Params):\n-            if self.pre_quantized:\n-                if param_name.replace(\"weight\", \"SCB\") not in state_dict:\n-                    raise ValueError(\"Missing quantization component `SCB`\")\n-                if param_value.dtype != torch.int8:\n-                    raise ValueError(\n-                        f\"Incompatible dtype `{param_value.dtype}` when loading 8-bit prequantized weight. Expected `torch.int8`.\"\n-                    )\n-            return True\n-        return False\n+        module, name = get_module_from_name(model, param_name)\n+        return isinstance(module, bnb.nn.Linear8bitLt) and name != \"bias\"\n \n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()\n-        needs aux items from state dicts, if found\n-        \"\"\"\n         import bitsandbytes as bnb\n \n-        fp16_statistics_key = param_name.replace(\"weight\", \"SCB\")\n-        fp16_statistics = state_dict.get(fp16_statistics_key)\n-\n         module, tensor_name = get_module_from_name(model, param_name)\n-        if tensor_name not in module._parameters:\n-            raise ValueError(f\"{module} does not have a parameter or a buffer named {tensor_name}.\")\n-\n-        old_value = getattr(module, tensor_name)\n-\n-        if not isinstance(module._parameters[tensor_name], bnb.nn.Int8Params):\n-            raise TypeError(f\"Parameter `{tensor_name}` should only be a `bnb.nn.Int8Params` instance.\")\n-        if (\n-            old_value.device == torch.device(\"meta\")\n-            and target_device not in [\"meta\", torch.device(\"meta\")]\n-            and param_value is None\n-        ):\n-            raise ValueError(f\"{tensor_name} is on the meta device, we need a `value` to put in on {target_device}.\")\n \n-        new_value = param_value.to(\"cpu\")\n         if self.pre_quantized and not self.is_serializable():\n             raise ValueError(\n                 \"Detected int8 weights but the version of bitsandbytes is not compatible with int8 serialization. \"\n                 \"Make sure to download the latest `bitsandbytes` version. `pip install --upgrade bitsandbytes`.\"\n             )\n \n+        # Those 2 can only happen when self.pre_quantized == True\n+        if tensor_name == \"SCB\":\n+            setattr(module.weight, \"SCB\", param_value.to(target_device))\n+            return\n+        # It's not used, but it's getting serialized for BC reason...\n+        elif tensor_name == \"weight_format\":\n+            return\n+\n         # Support models using `Conv1D` in place of `nn.Linear` (e.g. openai-community/gpt2) by transposing the weight matrix prior to quantization.\n         # Since weights are saved in the correct \"orientation\", we skip transposing when loading.\n-        if issubclass(module.source_cls, Conv1D):\n-            if fp16_statistics is None:\n-                new_value = new_value.T\n+        if issubclass(module.source_cls, Conv1D) and not self.pre_quantized:\n+            param_value = param_value.T\n \n+        old_value = getattr(module, tensor_name)\n         kwargs = old_value.__dict__\n         kwargs.pop(\"_is_hf_initialized\", None)\n-        new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)\n+        new_value = bnb.nn.Int8Params(param_value.to(\"cpu\"), requires_grad=False, **kwargs).to(target_device)\n \n+        # Set it to the module\n         module._parameters[tensor_name] = new_value\n-        if fp16_statistics is not None:\n-            setattr(module.weight, \"SCB\", fp16_statistics.to(target_device))\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         model.is_loaded_in_8bit = True\n@@ -268,7 +242,6 @@ def _process_model_before_weight_loading(\n         model = replace_with_bnb_linear(\n             model, modules_to_not_convert=self.modules_to_not_convert, quantization_config=self.quantization_config\n         )\n-        # TODO: consider bringing replace_with_bnb_linear() code from ..integrations/bitsandbyter.py to here\n \n         model.config.quantization_config = self.quantization_config\n "
        },
        {
            "sha": "1401f893da312e13e8bf7d83baad9164031c2c65",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 13,
            "deletions": 18,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from .base import HfQuantizer\n \n@@ -100,26 +100,15 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with EETQ.\")\n         return dtype\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from eetq import EetqLinear\n \n         module, tensor_name = get_module_from_name(model, param_name)\n \n         if isinstance(module, EetqLinear):\n             if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n                 return False\n             else:\n-                if tensor_name == \"weight_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n         return False\n \n@@ -129,16 +118,22 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        quantizes weights into qweight and weight_scales\n-        \"\"\"\n-        from eetq import quantize_and_preprocess_weights\n+        from eetq import EetqLinear, quantize_and_preprocess_weights\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         new_value, weight_scale = quantize_and_preprocess_weights(param_value)\n \n+        # Samity check\n+        if isinstance(module, EetqLinear):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n+                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n+            else:\n+                if tensor_name == \"weight_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+\n         module._buffers[tensor_name] = new_value.to(target_device)\n         module.register(\"weight_scales\", weight_scale.to(target_device))\n "
        },
        {
            "sha": "22c90aa446dd0ea0652279b892ea69269b84c0b4",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 18,
            "deletions": 21,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from .base import HfQuantizer\n \n@@ -105,33 +105,20 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         return dtype\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n \n         module, tensor_name = get_module_from_name(model, param_name)\n \n         if isinstance(module, FbgemmFp8Linear):\n             if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n                 return False\n             else:\n-                if tensor_name == \"weight_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n         if isinstance(module, FbgemmFp8Llama4TextExperts):\n             if self.pre_quantized or tensor_name == \"bias\":\n                 return False\n             else:\n-                if tensor_name == \"gate_up_proj_scale\" or tensor_name == \"down_proj_scale\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n         return False\n \n@@ -141,15 +128,25 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Quantizes weights into weight and weight_scale\n-        \"\"\"\n-\n-        from ..integrations import FbgemmFp8Llama4TextExperts\n+        from ..integrations import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts\n \n         module, tensor_name = get_module_from_name(model, param_name)\n+\n+        # Sanity checks\n+        if isinstance(module, FbgemmFp8Linear):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n+                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n+            else:\n+                if tensor_name == \"weight_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+        if isinstance(module, FbgemmFp8Llama4TextExperts):\n+            if not (self.pre_quantized or tensor_name == \"bias\"):\n+                if tensor_name == \"gate_up_proj_scale\" or tensor_name == \"down_proj_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+\n         if isinstance(module, FbgemmFp8Llama4TextExperts):\n             if tensor_name == \"gate_up_proj\":\n                 # Process each expert separately"
        },
        {
            "sha": "dc0123c1b0076fba46c4ed6ec7f9144c422a6dd2",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from ..utils import is_accelerate_available, is_torch_available, is_torch_xpu_available, logging\n from .base import HfQuantizer\n@@ -81,13 +81,21 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Quantizes weights to FP8 format using Block-wise quantization\n-        \"\"\"\n+        from ..integrations.finegrained_fp8 import FP8Linear\n         from ..modeling_utils import _load_parameter_into_model\n \n+        # Sanity checks\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        if isinstance(module, FP8Linear):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n+                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n+            else:\n+                if tensor_name == \"weight_scale_inv\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+\n         param_value = param_value.to(target_device)\n \n         # Get FP8 min/max values\n@@ -128,26 +136,14 @@ def create_quantized_param(\n         _load_parameter_into_model(model, param_name, quantized_param)\n         _load_parameter_into_model(model, param_name.rsplit(\".\", 1)[0] + \".weight_scale_inv\", scale)\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations.finegrained_fp8 import FP8Linear\n \n         module, tensor_name = get_module_from_name(model, param_name)\n-\n         if isinstance(module, FP8Linear):\n             if self.pre_quantized or tensor_name == \"bias\":\n-                if tensor_name == \"weight\" and param_value.dtype != torch.float8_e4m3fn:\n-                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n                 return False\n             else:\n-                if tensor_name == \"weight_scale_inv\":\n-                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n                 return True\n         return False\n "
        },
        {
            "sha": "58c5619774f725296ba1884c0aa68026e32e028c",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n@@ -89,7 +89,7 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n         module, _ = get_module_from_name(model, param_name)\n \n@@ -159,14 +159,7 @@ def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n     def is_serializable(self, safe_serialization=None):\n         return True\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from fp_quant import FPQuantLinear\n \n         module, tensor_name = get_module_from_name(model, param_name)"
        },
        {
            "sha": "41e2d86cf1ec448cb2a9acce3e8f0c3ce220636c",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from ..utils.logging import tqdm\n from .base import HfQuantizer\n@@ -87,13 +87,10 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n         from ..integrations import quantize_with_higgs\n \n-        \"\"\"\n-        Quantizes weights into weight and weight_scale\n-        \"\"\"\n         flute_dict = quantize_with_higgs(\n             param_value.to(target_device),\n             self.quantization_config.bits,\n@@ -180,18 +177,11 @@ def is_trainable(self) -> bool:\n     def is_serializable(self, safe_serialization=None):\n         return True\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations import HiggsLinear\n \n         module, tensor_name = get_module_from_name(model, param_name)\n-        if isinstance(module, HiggsLinear) and tensor_name == \"weight\" and param_value.dtype != torch.int16:\n+        if isinstance(module, HiggsLinear) and tensor_name == \"weight\":\n             # Only quantize weights of HiggsLinear modules that are not already quantized\n             return True\n         else:"
        },
        {
            "sha": "94907c3b48fce959c58116c08b4b2165a5bb8a98",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 108,
            "deletions": 157,
            "changes": 265,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -12,10 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import TYPE_CHECKING, Any\n+from collections import defaultdict\n+from typing import TYPE_CHECKING\n \n from ..integrations import prepare_for_hqq_linear\n-from ..utils import is_accelerate_available, is_hqq_available, is_torch_available, logging\n+from ..utils import is_hqq_available, is_torch_available, logging\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -24,22 +25,22 @@\n     from ..modeling_utils import PreTrainedModel\n \n \n-if is_accelerate_available():\n-    from accelerate.hooks import remove_hook_from_module\n-\n if is_torch_available():\n     import torch\n \n-logger = logging.get_logger(__name__)\n+if is_hqq_available():\n+    from hqq.core.quantize import HQQLinear\n \n+    # This is a compatibility hack. HQQ-quantized linear layers do not have a `weight` attribute,\n+    # but some models attempt to access `weight.dtype` during the forward pass. To prevent runtime errors,\n+    # we patch HQQLinear with a dummy `weight` property that returns an empty tensor with the correct dtype and device.\n+    @property\n+    def weight(self):\n+        return torch.empty(0, dtype=self.compute_dtype, device=self.device)\n \n-# Finds the parent of a node module named \"name\"\n-def find_parent(model, name):\n-    module_tree = name.split(\".\")[:-1]\n-    parent = model\n-    for m in module_tree:\n-        parent = parent._modules[m]\n-    return parent\n+    HQQLinear.weight = weight\n+\n+logger = logging.get_logger(__name__)\n \n \n class HqqHfQuantizer(HfQuantizer):\n@@ -54,16 +55,17 @@ class HqqHfQuantizer(HfQuantizer):\n     required_packages = [\"hqq\"]\n \n     def __init__(self, quantization_config, **kwargs):\n+        if not is_hqq_available():\n+            raise ImportError(\n+                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n+            )\n         super().__init__(quantization_config, **kwargs)\n         self.dtype = None\n         self.using_multi_gpu = False\n+        # Keys that are serialized specifically by hqq\n+        self.hqq_keys = HQQLinear(None, None).state_dict_keys() - {\"bias\"}\n \n     def validate_environment(self, *args, **kwargs):\n-        if not (is_hqq_available()):\n-            raise ImportError(\n-                \"A valid HQQ version (>=0.2.1) is not available. Please follow the instructions to install it: `https://github.com/mobiusml/hqq/`.\"\n-            )\n-\n         if self.dtype is None:\n             if \"dtype\" in kwargs:\n                 self.dtype = kwargs[\"dtype\"]\n@@ -104,165 +106,123 @@ def _find_hqq_quantizable_layers(model, layers):\n                 _find_hqq_quantizable_layers(module, layers)\n \n         new_keys = set(expected_keys)\n-        if is_hqq_available():\n-            from hqq.core.quantize import HQQLinear\n-\n-            # Name modules\n-            for name, module in model.named_modules():\n-                module.name = name\n-\n-            # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n-            _valid_modules = set()\n-            _find_hqq_quantizable_layers(model, _valid_modules)\n-\n-            # Remove skipped modules\n-            _skipped_modules = set()\n-            for _module in _valid_modules:\n-                for _skip_module in model.config.quantization_config[\"skip_modules\"]:\n-                    if _skip_module in _module:\n-                        _skipped_modules.add(_module)\n-            _valid_modules -= _skipped_modules\n-\n-            # Append new expected layers based on _ref_keys\n-            _ref_keys = HQQLinear(\n-                linear_layer=None,\n-                quant_config=None,\n-                compute_dtype=torch.float16,\n-                device=\"cpu\",\n-                del_orig=False,\n-            ).state_dict_keys() - {\"bias\"}\n-\n-            # Clean-up\n-            _rm_keys = set()\n-            for key in new_keys:\n-                if any(_module in key for _module in _valid_modules):\n-                    _rm_keys.add(key)\n-            new_keys -= _rm_keys\n-            # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n-\n-            # Re-populate Linear/HQQLinear\n-            for _module in _valid_modules:\n-                if _module + \".weight\" in loaded_keys:\n-                    new_keys.add(_module + \".weight\")\n-                else:\n-                    new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n-                if _module + \".bias\" in loaded_keys:\n-                    new_keys.add(_module + \".bias\")\n \n-        return list(new_keys)\n+        # Name modules\n+        for name, module in model.named_modules():\n+            module.name = name\n+\n+        # valid modules are Linear layers that have HQQLinear state_dict. We ignore skip_modules and any layers with Linear state_dict() params\n+        _valid_modules = set()\n+        _find_hqq_quantizable_layers(model, _valid_modules)\n+\n+        # Remove skipped modules\n+        _skipped_modules = set()\n+        for _module in _valid_modules:\n+            for _skip_module in model.config.quantization_config[\"skip_modules\"]:\n+                if _skip_module in _module:\n+                    _skipped_modules.add(_module)\n+        _valid_modules -= _skipped_modules\n+\n+        # Append new expected layers based on _ref_keys\n+        _ref_keys = HQQLinear(\n+            linear_layer=None,\n+            quant_config=None,\n+            compute_dtype=torch.float16,\n+            device=\"cpu\",\n+            del_orig=False,\n+        ).state_dict_keys() - {\"bias\"}\n+\n+        # Clean-up\n+        _rm_keys = set()\n+        for key in new_keys:\n+            if any(_module in key for _module in _valid_modules):\n+                _rm_keys.add(key)\n+        new_keys -= _rm_keys\n+        # At this point, new_keys contains all the keys of the layers that are NOT HQQLinear or torch.nn.Linear\n+\n+        # Re-populate Linear/HQQLinear\n+        for _module in _valid_modules:\n+            if _module + \".weight\" in loaded_keys:\n+                new_keys.add(_module + \".weight\")\n+            else:\n+                new_keys.update({_module + \".\" + _ref_key for _ref_key in _ref_keys})\n+            if _module + \".bias\" in loaded_keys:\n+                new_keys.add(_module + \".bias\")\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n-        if is_hqq_available():\n-            from hqq.core.quantize import HQQLinear\n-        module, tensor_name = get_module_from_name(model, param_name)\n+        return list(new_keys)\n \n-        if self.pre_quantized:\n-            return (isinstance(module, (torch.nn.Linear, HQQLinear))) and tensor_name != \"weight\"\n-        else:\n-            return (\n-                isinstance(module, torch.nn.Linear)\n-                and tensor_name == \"weight\"\n-                # bias doesn't need to be quantized, we use this as a workaround to avoid loading bias into HQQLinear assuming it was loaded\n-                # in the state_dict directly with the weight because hqq overwrote load_state_dict for this layer\n-                or (isinstance(module, HQQLinear) and tensor_name == \"bias\")\n-            )\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n+        module, _ = get_module_from_name(model, param_name)\n+        # Since we do not prepare the modules in advance, we need every param of the Linear layer to go through\n+        # `create_quantized_param`, even when `self.is_quantized == True`\n+        return isinstance(module, torch.nn.Linear)\n \n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n-        \"\"\"\n-        Each nn.Linear layer is processed here.\n-        We first check if the corresponding module state_dict contains already HQQ quantized parameters.\n-        If not, we create a temp linear layer with the module state_dict params and use it for quantization\n-        \"\"\"\n-\n-        if is_hqq_available():\n-            from hqq.core.quantize import HQQLinear\n-\n-            # TODO: This is a compatibility hack. HQQ-quantized linear layers do not have a `weight` attribute,\n-            # but some models attempt to access `weight.dtype` during the forward pass. To prevent runtime errors,\n-            # we patch HQQLinear with a dummy `weight` property that returns an empty tensor with the correct dtype and device.\n-            @property\n-            def weight(_self: HQQLinear):\n-                return torch.empty(0, dtype=_self.compute_dtype, device=_self.device)\n-\n-            HQQLinear.weight = weight\n-\n         module, tensor_name = get_module_from_name(model, param_name)\n-        layer_name = \".\".join(param_name.split(\".\")[:-1])\n-        parent_module = find_parent(model, layer_name)\n-        node = layer_name.split(\".\")[-1]\n+        module_name = param_name.rsplit(\".\", 1)[0]\n+        parent_module, node = get_module_from_name(model, module_name)\n \n-        if tensor_name == \"bias\":\n-            # this should already be set\n-            return\n+        quant_config = model.config.quantization_config[\"quant_config\"]\n+        skip_modules = model.config.quantization_config[\"skip_modules\"]\n \n-        # set module state_dict\n-        module_state_dict = {}\n-        for k, v in state_dict.items():\n-            if layer_name + \".\" in k:\n-                module_state_dict[k.split(\".\")[-1]] = v\n+        # In this case we do not quantize this layer (it's explicitly skipped) -> simply load param\n+        if any(skip_module in module.name for skip_module in skip_modules):\n+            module.load_state_dict(\n+                {tensor_name: param_value.to(device=target_device, dtype=self.dtype)}, strict=False, assign=True\n+            )\n+            return\n \n+        # We need this hack as the model is not pre-prepared as an empty skeleton on meta device\n         if self.pre_quantized:\n-            if isinstance(module, HQQLinear):\n-                return\n-            else:\n+            # Save them for later\n+            if not hasattr(self, \"hqq_params\"):\n+                self.hqq_params = defaultdict(dict)\n+            self.hqq_params[module_name].update({tensor_name: param_value})\n+            hqq_params = self.hqq_params[module_name]\n+\n+            # If they are all present and saved, make it a HQQLinear layer! (we cannot do it param after param because\n+            # hqq does not support it...)\n+            if all(k in hqq_params for k in self.hqq_keys) and (\"bias\" in hqq_params or module.bias is None):\n                 hqq_layer = HQQLinear(\n                     linear_layer=None,\n                     quant_config=None,\n                     compute_dtype=self.dtype,\n                     device=target_device,\n                     del_orig=False,\n                 )\n+                hqq_layer.load_state_dict(hqq_params)\n \n-            hqq_layer.load_state_dict(module_state_dict)\n-\n-            if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n-                hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n+                if hqq_layer.bias is not None and isinstance(hqq_layer.bias, torch.Tensor):\n+                    hqq_layer.bias = torch.nn.Parameter(hqq_layer.bias)\n+                if self.using_multi_gpu:\n+                    hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n \n-            if self.using_multi_gpu:\n-                hqq_layer = self._patch_layer_for_multigpu(hqq_layer)\n+                setattr(parent_module, node, hqq_layer)\n+                del self.hqq_params[module_name], module\n+            return\n \n-            setattr(parent_module, node, hqq_layer)\n+        # Load param in the module (without caring about device or dtype, it will be changed later)\n+        module.load_state_dict({tensor_name: param_value}, strict=False, assign=True)\n \n-            # cleanup\n-            del module.__dict__, module\n-            torch.cuda.empty_cache()\n-            return\n+        # If both the weight and bias have already been loaded, time to quantize!\n+        module_is_ready = module.weight.device.type != \"meta\" and (\n+            module.bias is None or module.bias.device.type != \"meta\"\n+        )\n \n-        # Step 1: populate module with weight/bias from module state dict\n-        for key, tensor in module_state_dict.items():\n-            setattr(module, key, torch.nn.Parameter(tensor))\n+        if module_is_ready:\n+            module_tag = \".\".join(module.name.split(\".\")[-2:])\n+            if \"weight_quant_params\" in quant_config:\n+                module_quant_config = quant_config\n+            elif module_tag in quant_config:\n+                module_quant_config = quant_config[module_tag]\n \n-        # Step 2: Replace module with either HQQLinear or move it to device. We do this via setattr on the parent as doing on it on the module\n-        # directly doesn't work.\n-        quant_config = model.config.quantization_config[\"quant_config\"]\n-        skip_modules = model.config.quantization_config[\"skip_modules\"]\n-        module_tag = \".\".join(module.name.split(\".\")[-2:])\n-        module_quant_config = None\n-        if \"weight_quant_params\" in quant_config:\n-            module_quant_config = quant_config\n-        elif module_tag in quant_config:\n-            module_quant_config = quant_config[module_tag]\n-\n-        for skip_module in skip_modules:\n-            if skip_module in module.name:\n-                module_quant_config = None\n-                break\n-\n-        if module_quant_config is not None:\n             hqq_layer = HQQLinear(\n                 module,\n                 quant_config=module_quant_config,\n@@ -279,16 +239,7 @@ def weight(_self: HQQLinear):\n \n             setattr(parent_module, node, hqq_layer)\n \n-        else:\n-            module = module.to(dtype=self.dtype, device=target_device)\n-            setattr(parent_module, node, module)\n-\n-        torch.cuda.empty_cache()\n-\n-    # Remove accelerate hook and uses a simpler forward pass. Otherwise, this breaks with multi-gpu\n     def _patch_layer_for_multigpu(self, hqq_layer):\n-        hqq_layer = remove_hook_from_module(hqq_layer)\n-\n         def forward_with_device(self, x):\n             out = torch.matmul(x.to(self.device), self.dequantize().t())\n             if self.bias is not None:"
        },
        {
            "sha": "04cf8ec56c96f1f3818bb6c4f9e757d951d49dfe",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -11,7 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Optional\n \n from .base import HfQuantizer\n \n@@ -153,14 +153,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         return dtype\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ):\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         from ..integrations import Mxfp4GptOssExperts\n         from ..models.gpt_oss.modeling_gpt_oss import GptOssExperts\n \n@@ -183,7 +176,6 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n         **kwargs,\n     ):\n         from ..integrations import ("
        },
        {
            "sha": "451179aaf723d8167a2cf1244758d8928ed3de83",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 24,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import importlib\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n from packaging import version\n \n@@ -103,26 +103,10 @@ def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> li\n                         not_missing_keys.append(missing)\n         return [k for k in missing_keys if k not in not_missing_keys]\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         if is_optimum_quanto_available():\n             from optimum.quanto import QModuleMixin\n \n-        device_map = kwargs.get(\"device_map\")\n-        param_device = kwargs.get(\"param_device\")\n-        # we don't quantize the model if the module is going to be offloaded to the cpu\n-        if device_map is not None and param_device is not None:\n-            device_map_values = set(device_map.values())\n-            if param_device == \"cpu\" and len(device_map_values) > 1:\n-                if not (device_map_values == {\"cpu\"} or device_map_values == {\"cpu\", \"disk\"}):\n-                    return False\n-\n         module, tensor_name = get_module_from_name(model, param_name)\n         # We only quantize the weights and the bias is not quantized.\n         if isinstance(module, QModuleMixin) and \"weight\" in tensor_name:\n@@ -141,15 +125,11 @@ def create_quantized_param(\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        *args,\n         **kwargs,\n     ):\n-        \"\"\"\n-        Create the quantized parameter by calling .freeze() after setting it to the module.\n-        \"\"\"\n-        from accelerate.utils import set_module_tensor_to_device\n+        from ..modeling_utils import _load_parameter_into_model\n \n-        set_module_tensor_to_device(model, param_name, target_device, param_value)\n+        _load_parameter_into_model(model, param_name, param_value.to(target_device))\n         module, _ = get_module_from_name(model, param_name)\n         module.freeze()\n         module.weight.requires_grad = False"
        },
        {
            "sha": "8ed6249bf5b9c9385ae6721290f030ef82c25498",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 7,
            "deletions": 19,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -13,23 +13,16 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import TYPE_CHECKING, Any\n+from typing import TYPE_CHECKING\n \n-from ..file_utils import is_torch_available\n from .base import HfQuantizer\n \n \n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-    if is_torch_available():\n-        import torch\n+from ..utils import is_quark_available, logging\n \n-from ..utils import is_accelerate_available, is_quark_available, logging\n-\n-\n-if is_accelerate_available():\n-    from accelerate.utils import set_module_tensor_to_device\n \n logger = logging.get_logger(__name__)\n \n@@ -82,23 +75,18 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwarg\n \n         return model\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         return True\n \n-    def create_quantized_param(self, model, param, param_name, param_device, state_dict) -> \"torch.nn.Parameter\":\n+    def create_quantized_param(self, model, param, param_name, param_device, **kwargs):\n+        from ..modeling_utils import _load_parameter_into_model\n+\n         postfix = param_name.split(\".\")[-1]\n \n         if postfix in CHECKPOINT_KEYS:\n             param_name = param_name.replace(postfix, CHECKPOINT_KEYS[postfix])\n \n-        set_module_tensor_to_device(model, param_name, param_device, value=param)\n+        _load_parameter_into_model(model, param_name, param.to(param_device))\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         return model"
        },
        {
            "sha": "d1610214acb11a697d15dde4373f0a3e454464d5",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 74,
            "deletions": 45,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5426edecab645af05ba95410b71c56887a11f97c/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=5426edecab645af05ba95410b71c56887a11f97c",
            "patch": "@@ -14,6 +14,7 @@\n import importlib\n import re\n import types\n+from collections import defaultdict\n from typing import TYPE_CHECKING, Optional, Union\n \n from packaging import version\n@@ -25,10 +26,12 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from typing import Any\n \n-from ..utils import is_torch_available, is_torchao_available, logging\n-from ..utils.quantization_config import TorchAoConfig\n+from ..utils import is_safetensors_available, is_torch_available, is_torchao_available, logging\n+\n+\n+if is_safetensors_available():\n+    from safetensors import safe_open\n \n \n if is_torch_available():\n@@ -64,15 +67,6 @@ def fuzzy_match_size(config_name: str) -> Optional[str]:\n     return None\n \n \n-# Finds the parent of a node module named \"name\"\n-def find_parent(model, name):\n-    module_tree = name.split(\".\")[:-1]\n-    parent = model\n-    for m in module_tree:\n-        parent = parent._modules[m]\n-    return parent\n-\n-\n def _quantization_type(weight):\n     from torchao.dtypes import AffineQuantizedTensor\n     from torchao.quantization.linear_activation_quantized_tensor import LinearActivationQuantizedTensor\n@@ -113,6 +107,20 @@ class TorchAoHfQuantizer(HfQuantizer):\n     def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n+        if isinstance(self.quantization_config.quant_type, str):\n+            is_int_4 = \"int4\" in self.quantization_config.quant_type\n+        else:\n+            config_name = self.quantization_config.quant_type.__class__.__name__\n+            is_int_4 = fuzzy_match_size(config_name) == \"4\"\n+\n+        # TODO: better way to get the serialized key names? Hard to read from torchao codebase\n+        if is_int_4:\n+            self.weight_ao_keys = [\"qdata\", \"scale\", \"zero_point\"]\n+        else:\n+            self.weight_ao_keys = [\"qdata\", \"scale\"]\n+        # Instead of serializing the simple torch.Tensor like usual, torchao adds a `:_data` suffix so we need this\n+        self.full_ao_keys = self.weight_ao_keys + [\"_data\"]\n+\n     def validate_environment(self, *args, **kwargs):\n         if not is_torchao_available():\n             raise ImportError(\"Loading an torchao quantized model requires torchao library (`pip install torchao`)\")\n@@ -229,61 +237,82 @@ def _process_model_before_weight_loading(\n             ]\n         return\n \n-    def param_needs_quantization(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n+    def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n+        return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.full_ao_keys)]\n+\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         if self.quantization_config.quant_type == \"autoquant\":\n             return False\n \n-        param_device = kwargs.pop(\"param_device\", None)\n         # check if the param_name is not in self.modules_to_not_convert\n-        if any((key + \".\" in param_name) or (key == param_name) for key in self.modules_to_not_convert):\n-            return False\n-        elif param_device == \"cpu\" and self.offload:\n-            # We don't quantize weights that we offload\n+        if any(key + \".\" in param_name or key == param_name for key in self.modules_to_not_convert):\n             return False\n+        elif any(param_name.endswith(f\":{x}\") for x in self.full_ao_keys):\n+            return True\n         else:\n             # we only quantize the weight of nn.Linear and nn.Embedding\n             module, tensor_name = get_module_from_name(model, param_name)\n             _QUANTIZABLE = [torch.nn.Linear]\n             if self.quantization_config.include_input_output_embeddings:\n                 _QUANTIZABLE.append(torch.nn.Embedding)\n-            return isinstance(module, tuple(_QUANTIZABLE)) and (tensor_name == \"weight\")\n+            return isinstance(module, tuple(_QUANTIZABLE)) and tensor_name == \"weight\"\n \n     def create_quantized_param(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         target_device: \"torch.device\",\n-        state_dict: dict[str, Any],\n+        **kwargs,\n     ):\n         \"\"\"\n         Each nn.Linear layer that needs to be quantized is processed here.\n         First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n         \"\"\"\n-        if self.quantization_config.quant_type == \"autoquant\":\n-            return\n-\n         from torchao.quantization import quantize_\n \n+        full_name = param_name\n+        # Those are the pre quantized weights\n+        if \":\" in param_name:\n+            param_name = param_name.rsplit(\":\", 1)[0]\n         module, tensor_name = get_module_from_name(model, param_name)\n+\n         if self.pre_quantized:\n-            module._parameters[tensor_name] = torch.nn.Parameter(\n-                param_value.to(device=target_device), requires_grad=param_value.requires_grad\n-            )\n+            # If it's a bias, no need to do anything special (except removing the \":_data\" part of the key, but was\n+            # already done) - if it's unsafe-serialized (i.e. not safetensors), not need for anything either\n+            is_unsafe_serialization = \":\" not in full_name\n+            if tensor_name == \"bias\" or is_unsafe_serialization:\n+                module._parameters[tensor_name] = torch.nn.Parameter(\n+                    param_value.to(target_device), requires_grad=param_value.requires_grad\n+                )\n+                return\n+            # Sanity check for the new serialization format\n+            elif not (TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(self.metadata)):\n+                raise ValueError(\"To use `safetensors` serialization, you should have `torchao>=0.14.0` installed\")\n+\n+            # Save the states for later quantization when they are all gathered\n+            if not hasattr(self, \"ao_params\"):\n+                self.ao_params = defaultdict(dict)\n+            self.ao_params[param_name].update({full_name: param_value})\n+\n+            # We are ready for quantization in this case (we retrieved all the needed keys)\n+            if len(self.ao_params[param_name]) == len(self.weight_ao_keys):\n+                new_param = unflatten_tensor_state_dict(self.ao_params[param_name], self.metadata)[param_name]\n+                # Set it\n+                module._parameters[tensor_name] = torch.nn.Parameter(\n+                    new_param.to(target_device), requires_grad=new_param.requires_grad\n+                )\n+\n+                # Free memory\n+                del self.ao_params[param_name]\n+\n+            # Add repr to the module\n             if isinstance(module, nn.Linear):\n                 module.extra_repr = types.MethodType(_linear_extra_repr, module)\n         else:\n-            assert isinstance(self.quantization_config, TorchAoConfig)\n             module._parameters[tensor_name] = torch.nn.Parameter(\n                 param_value, requires_grad=param_value.requires_grad\n-            ).to(device=target_device)\n+            ).to(target_device)\n             # if we are quantizing tied parameters, to avoid tying the quantized weights\n             # the correct order to do it is\n             # 1. load the weight to model\n@@ -313,16 +342,6 @@ def create_quantized_param(\n \n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n-    def update_state_dict_with_metadata(self, state_dict, metadata):\n-        \"\"\"\n-        If the metadata contains torchao tensor subclass information, we reconstruct the tensor subclass state dict\n-        from the provided state_dict and metadata.\n-        \"\"\"\n-        if TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(metadata):\n-            return unflatten_tensor_state_dict(state_dict, metadata)\n-        else:\n-            return state_dict\n-\n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n         if self.quantization_config.quant_type == \"autoquant\":\n@@ -415,3 +434,13 @@ def is_trainable(self) -> bool:\n     @property\n     def is_compileable(self) -> bool:\n         return True\n+\n+    def set_metadata(self, checkpoint_files: list[str]):\n+        if checkpoint_files[0].endswith(\".safetensors\") and is_safetensors_available():\n+            metadata = {}\n+            for checkpoint in checkpoint_files:\n+                with safe_open(checkpoint, framework=\"pt\") as f:\n+                    metadata_ = f.metadata() or {}\n+                    metadata.update(metadata_)\n+            # Save it\n+            self.metadata = metadata"
        }
    ],
    "stats": {
        "total": 894,
        "additions": 356,
        "deletions": 538
    }
}