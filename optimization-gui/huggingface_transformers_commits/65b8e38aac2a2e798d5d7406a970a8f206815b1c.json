{
    "author": "MekkCyber",
    "message": "Upgrading torch version and cuda version in quantization docker (#36264)\n\n* update\n\n* small update\n\n* no spqr quant\n\n* testing\n\n* testing\n\n* test nightly\n\n* gptqmodel\n\n* flute\n\n* fix hadamard\n\n* running tests\n\n* new docker\n\n* fix docker\n\n* run tests\n\n* testing new docker\n\n* new docker\n\n* run tests\n\n* new docker\n\n* run tests\n\n* final test\n\n* update\n\n* update\n\n* run tests\n\n* new docker\n\n* launch tests\n\n* test_docker\n\n* running tests\n\n* add comments\n\n* fixing yml\n\n* revert",
    "sha": "65b8e38aac2a2e798d5d7406a970a8f206815b1c",
    "files": [
        {
            "sha": "ff0655c7e3a71d8881691a809fc4e8276ee0f40f",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 20,
            "deletions": 17,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/65b8e38aac2a2e798d5d7406a970a8f206815b1c/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/65b8e38aac2a2e798d5d7406a970a8f206815b1c/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=65b8e38aac2a2e798d5d7406a970a8f206815b1c",
            "patch": "@@ -1,4 +1,4 @@\n-FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04\n+FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04\n LABEL maintainer=\"Hugging Face\"\n \n ARG DEBIAN_FRONTEND=noninteractive\n@@ -9,9 +9,9 @@ SHELL [\"sh\", \"-lc\"]\n # The following `ARG` are mainly used to specify the versions explicitly & directly in this docker file, and not meant\n # to be used as arguments for docker build (so far).\n \n-ARG PYTORCH='2.5.1'\n+ARG PYTORCH='2.6.0'\n # Example: `cu102`, `cu113`, etc.\n-ARG CUDA='cu118'\n+ARG CUDA='cu121'\n \n RUN apt update\n RUN apt install -y git libsndfile1-dev tesseract-ocr espeak-ng python3 python3-pip ffmpeg\n@@ -26,8 +26,6 @@ RUN echo torch=$VERSION\n # Currently, let's just use their latest releases (when `torch` is installed with a release version)\n RUN python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA\n \n-RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n-\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n \n # needed in bnb and awq\n@@ -36,10 +34,9 @@ RUN python3 -m pip install --no-cache-dir einops\n # Add bitsandbytes for mixed int8 testing\n RUN python3 -m pip install --no-cache-dir bitsandbytes\n \n-# Add auto-gptq for gtpq quantization testing, installed from source for pytorch==2.5.1 compatibility\n-# TORCH_CUDA_ARCH_LIST=\"7.5+PTX\" is added to make the package compile for Tesla T4 gpus available for the CI.\n-RUN pip install gekko\n-RUN git clone https://github.com/PanQiWei/AutoGPTQ.git && cd AutoGPTQ && TORCH_CUDA_ARCH_LIST=\"7.5+PTX\" python3 setup.py install\n+# Add gptqmodel for gtpq quantization testing, installed from source for pytorch==2.6.0 compatibility\n+RUN python3 -m pip install lm_eval\n+RUN git clone https://github.com/ModelCloud/GPTQModel.git && cd GPTQModel && pip install -v . --no-build-isolation\n \n # Add optimum for gptq quantization testing\n RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/optimum@main#egg=optimum\n@@ -51,10 +48,11 @@ RUN python3 -m pip install --no-cache-dir git+https://github.com/huggingface/pef\n RUN python3 -m pip install --no-cache-dir aqlm[gpu]==1.0.2\n \n # Add vptq for quantization testing\n-RUN python3 -m pip install --no-cache-dir vptq\n+RUN pip install vptq\n \n # Add spqr for quantization testing\n-RUN python3 -m pip install --no-cache-dir spqr_quant[gpu]\n+# Commented for now as No matching distribution found we need to reach out to the authors\n+# RUN python3 -m pip install --no-cache-dir spqr_quant[gpu]\n \n # Add hqq for quantization testing\n RUN python3 -m pip install --no-cache-dir hqq\n@@ -63,22 +61,27 @@ RUN python3 -m pip install --no-cache-dir hqq\n RUN python3 -m pip install --no-cache-dir gguf\n \n # Add autoawq for quantization testing\n-# >=v0.2.7 needed for compatibility with transformers > 4.46\n-RUN python3 -m pip install --no-cache-dir https://github.com/casper-hansen/AutoAWQ/releases/download/v0.2.7.post2/autoawq-0.2.7.post2-py3-none-any.whl\n+# New release v0.2.8\n+RUN python3 -m pip install --no-cache-dir autoawq[kernels]\n \n # Add quanto for quantization testing\n RUN python3 -m pip install --no-cache-dir optimum-quanto\n \n # Add eetq for quantization testing\n-RUN python3 -m pip install git+https://github.com/NetEase-FuXi/EETQ.git\n+RUN git clone https://github.com/NetEase-FuXi/EETQ.git && cd EETQ/ && git submodule update --init --recursive && pip install .\n \n-# Add flute-kernel and fast_hadamard_transform for quantization testing\n-RUN python3 -m pip install --no-cache-dir flute-kernel==0.3.0 -i https://flute-ai.github.io/whl/cu118\n-RUN python3 -m pip install --no-cache-dir fast_hadamard_transform==1.0.4.post1\n+# # Add flute-kernel and fast_hadamard_transform for quantization testing\n+# # Commented for now as they cause issues with the build\n+# # TODO: create a new workflow to test them\n+# RUN python3 -m pip install --no-cache-dir flute-kernel==0.4.1\n+# RUN python3 -m pip install --no-cache-dir git+https://github.com/Dao-AILab/fast-hadamard-transform.git\n \n # Add compressed-tensors for quantization testing\n RUN python3 -m pip install --no-cache-dir compressed-tensors\n \n+# Add transformers in editable mode\n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n+\n # When installing in editable mode, `transformers` is not recognized as a package.\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 20,
        "deletions": 17
    }
}