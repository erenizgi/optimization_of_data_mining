{
    "author": "SunMarc",
    "message": "[v5] Bump accelerate to 1.1.0  (#41234)\n\n* bump to 1.1.0 !\n\n* bump accelerate\n\n* fix\n\n* None\n\n* fixed !\n\n* style",
    "sha": "c562c5d801f94d2a3ef94a2cc09d16e274095397",
    "files": [
        {
            "sha": "41743d8e2ef1d4587a50ab45b5334280cf20a4d5",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -94,7 +94,7 @@\n # 2. once modified, run: `make deps_table_update` to update src/transformers/dependency_versions_table.py\n _deps = [\n     \"Pillow>=10.0.1,<=15.0\",\n-    \"accelerate>=0.26.0\",\n+    \"accelerate>=1.1.0\",\n     \"av\",\n     \"beautifulsoup4\",\n     \"blobfile\","
        },
        {
            "sha": "a6b6a9c445e65ebaa4ba04e1e194ed69d7ad0e84",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -3,7 +3,7 @@\n # 2. run `make deps_table_update``\n deps = {\n     \"Pillow\": \"Pillow>=10.0.1,<=15.0\",\n-    \"accelerate\": \"accelerate>=0.26.0\",\n+    \"accelerate\": \"accelerate>=1.1.0\",\n     \"av\": \"av\",\n     \"beautifulsoup4\": \"beautifulsoup4\",\n     \"blobfile\": \"blobfile\","
        },
        {
            "sha": "ba9659013fe52e801e69ab1a780417dd87438584",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -131,10 +131,7 @@\n         offload_weight,\n         save_offload_index,\n     )\n-\n-    accelerate_version = version.parse(importlib.metadata.version(\"accelerate\"))\n-    if accelerate_version >= version.parse(\"0.31\"):\n-        from accelerate.utils.modeling import get_state_dict_from_offload\n+    from accelerate.utils.modeling import get_state_dict_from_offload\n \n if is_peft_available():\n     from .utils import find_adapter_config_file\n@@ -4024,11 +4021,6 @@ def save_pretrained(\n \n             # remake shard with onloaded parameters if necessary\n             if module_map:\n-                if accelerate_version < version.parse(\"0.31\"):\n-                    raise ImportError(\n-                        f\"You need accelerate version to be greater or equal than 0.31 to save models with offloaded parameters. Detected version {accelerate_version}. \"\n-                        f\"Please upgrade accelerate with `pip install -U accelerate`\"\n-                    )\n                 # init state_dict for this shard\n                 shard_state_dict = dict.fromkeys(shard, \"\")\n                 for module_name in shard:\n@@ -5741,12 +5733,7 @@ def unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n     if is_accelerate_available():\n         kwargs = {}\n         if recursive:\n-            if not is_accelerate_available(\"0.29.0\"):\n-                raise RuntimeError(\n-                    \"Setting `recursive=True` to `unwrap_model` requires `accelerate` v0.29.0. Please upgrade your version of accelerate\"\n-                )\n-            else:\n-                kwargs[\"recursive\"] = recursive\n+            kwargs[\"recursive\"] = recursive\n         return extract_model_from_parallel(model, **kwargs)\n     else:\n         # since there could be multiple levels of wrapping, unwrap recursively"
        },
        {
            "sha": "33136ae0d1b487072f1758bab55005894f193754",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -128,19 +128,11 @@ def validate_environment(self, *args, **kwargs):\n                 )\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n-            from accelerate.utils import CustomDtype\n+        from accelerate.utils import CustomDtype\n \n-            if target_dtype != torch.int8:\n-                logger.info(\"target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\")\n-            return CustomDtype.INT4\n-        else:\n-            raise ValueError(\n-                \"You are using `device_map='auto'` on a 4bit loaded version of the model. To automatically compute\"\n-                \" the appropriate device map, you should upgrade your `accelerate` library,\"\n-                \"`pip install --upgrade accelerate` or install it from source to support fp4 auto device map\"\n-                \"calculation. You may encounter unexpected behavior, or pass your own device map\"\n-            )\n+        if target_dtype != torch.int8:\n+            logger.info(\"target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\")\n+        return CustomDtype.INT4\n \n     def update_unexpected_keys(self, model, unexpected_keys: list[str]) -> list[str]:\n         return [k for k in unexpected_keys if not any(k.endswith(x) for x in self.bnb_keys)]"
        },
        {
            "sha": "e0d17f5ec28de954da8c6e3bf516dd72de289150",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -56,9 +56,9 @@ def validate_environment(self, *args, **kwargs):\n                 \"Please install the latest version of fbgemm-gpu library by following : https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries\"\n             )\n \n-        if not is_accelerate_available(\"0.32.2\"):\n+        if not is_accelerate_available():\n             raise ImportError(\n-                \"Loading an FP8 quantized model requires accelerate > 0.32.1 (`pip install --upgrade accelerate`)\"\n+                \"Loading an FP8 quantized model requires accelerate (`pip install --upgrade accelerate`)\"\n             )\n \n         if not torch.cuda.is_available():"
        },
        {
            "sha": "0fa738feee0a133c6e05130e2cc8c5e80cbc4a35",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 10,
            "deletions": 20,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -11,11 +11,8 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import importlib\n from typing import TYPE_CHECKING, Optional, Union\n \n-from packaging import version\n-\n from .base import HfQuantizer\n from .quantizers_utils import get_module_from_name\n \n@@ -135,23 +132,16 @@ def create_quantized_param(\n         module.weight.requires_grad = False\n \n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.27.0\"):\n-            from accelerate.utils import CustomDtype\n-\n-            mapping = {\n-                \"int8\": torch.int8,\n-                \"float8\": CustomDtype.FP8,\n-                \"int4\": CustomDtype.INT4,\n-                \"int2\": CustomDtype.INT2,\n-            }\n-            target_dtype = mapping[self.quantization_config.weights]\n-            return target_dtype\n-        else:\n-            raise ValueError(\n-                \"You are using `device_map='auto'` on an optimum-quanto quantized model. To automatically compute\"\n-                \" the appropriate device map, you should upgrade your `accelerate` library,\"\n-                \"`pip install --upgrade accelerate` or install it from source.\"\n-            )\n+        from accelerate.utils import CustomDtype\n+\n+        mapping = {\n+            \"int8\": torch.int8,\n+            \"float8\": CustomDtype.FP8,\n+            \"int4\": CustomDtype.INT4,\n+            \"int2\": CustomDtype.INT2,\n+        }\n+        target_dtype = mapping[self.quantization_config.weights]\n+        return target_dtype\n \n     def _process_model_before_weight_loading(\n         self, model: \"PreTrainedModel\", keep_in_fp32_modules: Optional[list[str]] = None, **kwargs"
        },
        {
            "sha": "0b9924f667b351d21f76aacb71766447a6d65d37",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 18,
            "deletions": 19,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -178,25 +178,24 @@ def get_state_dict_and_metadata(self, model, safe_serialization: Optional[bool]\n             return None, {}\n \n     def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n-        if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n-            from accelerate.utils import CustomDtype\n-\n-            # Import AOBaseConfig directly since we know we have the right version\n-            if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n-                from torchao.core.config import AOBaseConfig\n-\n-                quant_type = self.quantization_config.quant_type\n-                if isinstance(quant_type, AOBaseConfig):\n-                    # Extract size digit using fuzzy match on the class name\n-                    config_name = quant_type.__class__.__name__\n-                    size_digit = fuzzy_match_size(config_name)\n-\n-                    # Map the extracted digit to appropriate dtype\n-                    if size_digit == \"4\":\n-                        return CustomDtype.INT4\n-                    else:\n-                        # Default to int8\n-                        return torch.int8\n+        from accelerate.utils import CustomDtype\n+\n+        # Import AOBaseConfig directly since we know we have the right version\n+        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                # Map the extracted digit to appropriate dtype\n+                if size_digit == \"4\":\n+                    return CustomDtype.INT4\n+                else:\n+                    # Default to int8\n+                    return torch.int8\n \n             # Original mapping for non-AOBaseConfig types\n             map_to_target_dtype = {"
        },
        {
            "sha": "fdd7925f5b1fdfcb7d315fc7489deffe6d420d80",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 28,
            "deletions": 57,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -215,6 +215,7 @@\n     from accelerate.state import AcceleratorState\n     from accelerate.utils import (\n         AutocastKwargs,\n+        DataLoaderConfiguration,\n         DistributedDataParallelKwargs,\n         DistributedType,\n         load_fsdp_model,\n@@ -223,19 +224,9 @@\n         save_fsdp_optimizer,\n     )\n \n-    DATA_SAMPLERS = [RandomSampler]\n-    if version.parse(accelerate_version) > version.parse(\"1.3.0\"):\n-        from accelerate.utils import TorchTensorParallelPlugin\n-    from accelerate.data_loader import SeedableRandomSampler\n-\n-    DATA_SAMPLERS += [SeedableRandomSampler]\n-\n     if is_deepspeed_available():\n         from accelerate.utils import DeepSpeedSchedulerWrapper\n \n-if is_accelerate_available(\"0.28.0\"):\n-    from accelerate.utils import DataLoaderConfiguration\n-\n \n def _is_peft_model(model):\n     if is_peft_available():\n@@ -250,8 +241,7 @@ def _is_peft_model(model):\n \n \n def _get_fsdp_ckpt_kwargs():\n-    # TODO: @AjayP13, @younesbelkada replace this check with version check at the next `accelerate` release\n-    if is_accelerate_available() and \"adapter_only\" in list(inspect.signature(save_fsdp_model).parameters):\n+    if \"adapter_only\" in list(inspect.signature(save_fsdp_model).parameters):\n         return {\"adapter_only\": True}\n     else:\n         return {}\n@@ -1625,8 +1615,6 @@ def optimizer_hook(param):\n                     \"You need to install `lomo_optim` in order to use LOMO optimizers\"\n                     \" install it with `pip install lomo-optim`\"\n                 )\n-            if not is_accelerate_available(\"0.30.0\"):\n-                raise ImportError(\"You need to have `accelerate>=0.30.0` to be able to use LOMO optimizers\")\n \n             if model is None:\n                 raise ValueError(\"You need to pass a `model` in order to correctly initialize a LOMO optimizer.\")\n@@ -1693,8 +1681,6 @@ def optimizer_hook(param):\n                     \"You need to install `schedulefree` in order to use schedulefree optimizers. \"\n                     \"Install it with `pip install schedulefree.`\"\n                 )\n-            if not is_accelerate_available(\"0.30.0\"):\n-                raise ImportError(\"You need to have `accelerate>=0.30.0` to be able to use schedulefree optimizers\")\n             from schedulefree import AdamWScheduleFree, SGDScheduleFree\n \n             additional_optim_kwargs = {}\n@@ -2596,10 +2582,7 @@ def _inner_training_loop(\n                                         args.max_grad_norm,\n                                     )\n \n-                            if (\n-                                is_accelerate_available()\n-                                and self.accelerator.distributed_type == DistributedType.DEEPSPEED\n-                            ):\n+                            if self.accelerator.distributed_type == DistributedType.DEEPSPEED:\n                                 grad_norm = model.get_global_grad_norm()\n                                 # In some cases the grad norm may not return a float\n                                 if hasattr(grad_norm, \"item\"):\n@@ -5058,7 +5041,7 @@ def _add_sm_patterns_to_gitignore(self) -> None:\n     def create_accelerator_and_postprocess(self):\n         # We explicitly don't rely on the `Accelerator` to do gradient accumulation\n         grad_acc_kwargs = {}\n-        if is_accelerate_available(\"0.28.0\") and self.args.accelerator_config.gradient_accumulation_kwargs is not None:\n+        if self.args.accelerator_config.gradient_accumulation_kwargs is not None:\n             grad_acc_kwargs = self.args.accelerator_config.gradient_accumulation_kwargs\n \n         # check if num_steps is attempted to be passed in gradient_accumulation_kwargs\n@@ -5074,55 +5057,44 @@ def create_accelerator_and_postprocess(self):\n \n         accelerator_config = self.args.accelerator_config.to_dict()\n \n-        if is_accelerate_available(\"0.28.0\"):\n-            # Extract dataloader config params from accelerator config\n-            dataloader_params = [\"split_batches\", \"dispatch_batches\", \"even_batches\", \"use_seedable_sampler\"]\n-            dataloader_config = DataLoaderConfiguration(\n-                **{param: accelerator_config.pop(param) for param in dataloader_params}\n-            )\n-            if is_accelerate_available(\"1.1.0\"):\n-                dataloader_config.data_seed = self.args.data_seed\n+        # Extract dataloader config params from accelerator config\n+        dataloader_params = [\"split_batches\", \"dispatch_batches\", \"even_batches\", \"use_seedable_sampler\"]\n+        dataloader_config = DataLoaderConfiguration(\n+            **{param: accelerator_config.pop(param) for param in dataloader_params}\n+        )\n+        dataloader_config.data_seed = self.args.data_seed\n \n         non_blocking = accelerator_config.pop(\"non_blocking\")\n-        if not is_accelerate_available(\"0.30.0\"):\n-            if non_blocking:\n-                raise ImportError(\n-                    \"`non_blocking` is only supported in accelerate v0.30.0 and above. Please upgrade accelerate to use this feature.\"\n-                )\n-        else:\n-            if non_blocking and not self.args.dataloader_pin_memory:\n-                logger.warning(\n-                    \"`non_blocking` is enabled but `dataloader_pin_memory` is not. For the best performance, it's recommended to enable both.\"\n-                )\n-            dataloader_config.non_blocking = non_blocking\n+\n+        if non_blocking and not self.args.dataloader_pin_memory:\n+            logger.warning(\n+                \"`non_blocking` is enabled but `dataloader_pin_memory` is not. For the best performance, it's recommended to enable both.\"\n+            )\n+        dataloader_config.non_blocking = non_blocking\n         # this would have been updated above, no need for it anymore\n         accelerator_config.pop(\"gradient_accumulation_kwargs\")\n \n-        args = {\n-            \"deepspeed_plugin\": self.args.deepspeed_plugin,\n-        }\n+        args = {\"deepspeed_plugin\": self.args.deepspeed_plugin, \"dataloader_config\": dataloader_config}\n \n         # We defer compatibility checks to accelerator\n         if self.args.parallelism_config is not None:\n             if not is_accelerate_available(\"1.10.1\"):\n                 raise ImportError(\n                     \"ParallelismConfig requires accelerate v1.10.1 and above. Please upgrade accelerate to use this feature.\"\n                 )\n-\n             args[\"parallelism_config\"] = self.args.parallelism_config\n \n-        if is_accelerate_available(\"0.28.0\"):\n-            args[\"dataloader_config\"] = dataloader_config\n-        else:\n-            args.update(accelerator_config)\n-        # tp is initialized at Accelerator init phase so\n-        # args should be prepared here\n-        if hasattr(self.model, \"tp_size\") and self.model.tp_size is not None and self.model.tp_size > 1:\n+        self.is_tp_enabled = False\n+        if getattr(self.model, \"tp_size\", None) is not None and self.model.tp_size > 1:\n             self.is_tp_enabled = True\n-            if version.parse(accelerate_version) > version.parse(\"1.3.0\"):\n-                args[\"torch_tp_plugin\"] = TorchTensorParallelPlugin(tp_size=self.model.tp_size)\n-            else:\n-                raise ValueError(\"Requires accelerate>1.3.0 to use Tensor Parallelism.\")\n+            if self.args.parallelism_config is not None:\n+                if version.parse(accelerate_version) > version.parse(\"1.10.1\"):\n+                    if self.args.parallelism_config is not None:\n+                        from accelerate import ParallelismConfig\n+\n+                        args[\"parallelism_config\"] = ParallelismConfig(tp_size=self.model.tp_size)\n+                else:\n+                    raise ValueError(\"Requires accelerate>1.10.1 to use Tensor Parallelism.\")\n \n         # create accelerator object\n         self.accelerator = Accelerator(**args)\n@@ -5137,7 +5109,7 @@ def create_accelerator_and_postprocess(self):\n         # deepspeed and accelerate flags covering both trainer args and accelerate launcher\n         self.is_deepspeed_enabled = getattr(self.accelerator.state, \"deepspeed_plugin\", None) is not None\n         self.is_fsdp_enabled = getattr(self.accelerator.state, \"fsdp_plugin\", None) is not None\n-        self.is_tp_enabled = getattr(self.accelerator.state, \"torch_tp_plugin\", None) is not None\n+\n         # post accelerator creation setup\n         if self.is_fsdp_enabled:\n             fsdp_plugin = self.accelerator.state.fsdp_plugin\n@@ -5200,7 +5172,6 @@ def _fsdp_qlora_plugin_updates(self):\n             if (\n                 getattr(self.model, \"quantization_method\", None) == QuantizationMethod.BITS_AND_BYTES\n                 and self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage.is_floating_point\n-                and version.parse(accelerate_version) > version.parse(\"0.27.0\")\n             ):\n                 self.accelerator.state.fsdp_plugin.set_mixed_precision(\n                     self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True"
        },
        {
            "sha": "8a6cab2bfb5289329047aae4c59c3389b9198f5f",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -35,7 +35,6 @@\n     ACCELERATE_MIN_VERSION,\n     ExplicitEnum,\n     is_accelerate_available,\n-    is_ipex_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n     is_torch_available,\n@@ -1894,19 +1893,6 @@ def __post_init__(self):\n                 \" when --dataloader_num_workers > 1.\"\n             )\n \n-        if self.eval_use_gather_object and not is_accelerate_available(\"0.30.0\"):\n-            raise ValueError(\n-                \"--eval_use_gather_object requires Accelerate to be version of `accelerate` > 0.30.0.\"\n-                \"This is not supported and we recommend you to update your version.\"\n-            )\n-\n-        if self.data_seed is not None:\n-            if not is_accelerate_available(\"1.1.0\"):\n-                raise NotImplementedError(\n-                    \"data_seed requires Accelerate version `accelerate` >= 1.1.0. \"\n-                    \"This is not supported and we recommend you to update your version.\"\n-                )\n-\n         if self.include_tokens_per_second is not None:\n             logger.warning(\n                 \"include_tokens_per_second is deprecated and will be removed in v5. Use `include_num_input_tokens_seen` instead. \"\n@@ -2035,8 +2021,6 @@ def _setup_devices(self) -> \"torch.device\":\n             elif is_torch_mps_available():\n                 device = torch.device(\"mps\")\n             elif is_torch_xpu_available():\n-                if not is_ipex_available() and not is_accelerate_available(\"0.32.0.dev\"):\n-                    raise ImportError(\"Using the XPU PyTorch backend requires `accelerate>=0.32.0.dev`\")\n                 device = torch.device(\"xpu:0\")\n                 torch.xpu.set_device(device)\n             elif is_torch_mlu_available():"
        },
        {
            "sha": "ae8cc12f1a079f7bac37aaf39236c6a4321822fe",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -71,7 +71,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n # Try to run a native pytorch job in an environment with TorchXLA installed by setting this value to 0.\n USE_TORCH_XLA = os.environ.get(\"USE_TORCH_XLA\", \"1\").upper()\n \n-ACCELERATE_MIN_VERSION = \"0.26.0\"\n+ACCELERATE_MIN_VERSION = \"1.1.0\"\n SCHEDULEFREE_MIN_VERSION = \"1.2.6\"\n FSDP_MIN_VERSION = \"1.12.0\"\n GGUF_MIN_VERSION = \"0.10.0\""
        },
        {
            "sha": "4711561e314785bae5ef50905f137a64569092d6",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c562c5d801f94d2a3ef94a2cc09d16e274095397/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c562c5d801f94d2a3ef94a2cc09d16e274095397/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=c562c5d801f94d2a3ef94a2cc09d16e274095397",
            "patch": "@@ -163,9 +163,6 @@\n     import datasets\n \n # for version specific tests in TrainerIntegrationTest\n-require_accelerate_version_min_0_28 = partial(require_accelerate, min_version=\"0.28\")\n-require_accelerate_version_min_0_30 = partial(require_accelerate, min_version=\"0.30\")\n-GRAD_ACCUM_KWARGS_VERSION_AVAILABLE = is_accelerate_available(\"0.28\")\n if is_accelerate_available():\n     from accelerate import Accelerator\n     from accelerate.state import AcceleratorState\n@@ -4499,10 +4496,8 @@ def test_accelerator_config_empty(self):\n             self.assertEqual(trainer.accelerator.dispatch_batches, None)\n             self.assertEqual(trainer.accelerator.even_batches, True)\n             self.assertEqual(trainer.accelerator.use_seedable_sampler, True)\n-\n-            if GRAD_ACCUM_KWARGS_VERSION_AVAILABLE:\n-                # gradient accumulation kwargs configures gradient_state\n-                self.assertNotIn(\"sync_each_batch\", trainer.accelerator.gradient_state.plugin_kwargs)\n+            # gradient accumulation kwargs configures gradient_state\n+            self.assertNotIn(\"sync_each_batch\", trainer.accelerator.gradient_state.plugin_kwargs)\n \n     def test_accelerator_config_from_dict(self):\n         # Checks that accelerator kwargs can be passed through\n@@ -4518,8 +4513,7 @@ def test_accelerator_config_from_dict(self):\n                 \"even_batches\": False,\n                 \"use_seedable_sampler\": True,\n             }\n-            if GRAD_ACCUM_KWARGS_VERSION_AVAILABLE:\n-                accelerator_config[\"gradient_accumulation_kwargs\"] = {\"sync_each_batch\": True}\n+            accelerator_config[\"gradient_accumulation_kwargs\"] = {\"sync_each_batch\": True}\n \n             # Leaves all options as something *not* basic\n             args = RegressionTrainingArguments(output_dir=tmp_dir, accelerator_config=accelerator_config)\n@@ -4575,7 +4569,6 @@ def test_accelerator_config_from_dataclass(self):\n             self.assertEqual(trainer.accelerator.even_batches, False)\n             self.assertEqual(trainer.accelerator.use_seedable_sampler, False)\n \n-    @require_accelerate_version_min_0_28\n     def test_accelerate_config_from_dataclass_grad_accum(self):\n         # Checks that accelerator kwargs can be passed through\n         # and the accelerator is initialized respectively\n@@ -4632,7 +4625,6 @@ def test_accelerator_custom_state(self):\n             _ = RegressionTrainingArguments(output_dir=tmp_dir, accelerator_config={\"use_configured_state\": True})\n         AcceleratorState._reset_state(reset_partial_state=True)\n \n-    @require_accelerate_version_min_0_28\n     def test_accelerator_config_from_dict_grad_accum_num_steps(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             config = RegressionModelConfig(a=1.5, b=2.5)\n@@ -4728,7 +4720,6 @@ class TorchDtypeTrainingArguments(TrainingArguments):\n                 self.assertIn(\"dtype\", args_dict)\n                 self.assertEqual(args_dict[\"dtype\"], dtype)\n \n-    @require_accelerate_version_min_0_30\n     def test_eval_use_gather_object(self):\n         train_dataset = RegressionDataset()\n         eval_dataset = RegressionDataset()"
        }
    ],
    "stats": {
        "total": 226,
        "additions": 70,
        "deletions": 156
    }
}