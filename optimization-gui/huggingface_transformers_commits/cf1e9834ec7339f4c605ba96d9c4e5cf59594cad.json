{
    "author": "remi-or",
    "message": "Restore cuda graphs to continuous batching (#41421)\n\n* Type hints and small fixes\n\n* Remove unusued params\n\n* Made slice inputs the default\n\n* ruffed\n\n* Updated some var name and moved index slicing\n\n* Logging arg in example\n\n* Added some padding debug var and reformat out cg\n\n* First working CG, fixe size\n\n* Working flexible CG\n\n* CG are compatible with all implementations\n\n* Fixed CG API\n\n* Update example\n\n* Documentation\n\n* Fix padding tokens in FA\n\n* Review compliance\n\n* Better doc around weird bug\n\n* Style\n\n* Fix for sliding with CG",
    "sha": "cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
    "files": [
        {
            "sha": "72a371d471e0c4f57c55a13fc9b64a35b39188fd",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 32,
            "deletions": 24,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -26,22 +26,25 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer\n from transformers.generation import GenerationConfig\n+from transformers.generation.continuous_batching.requests import logger\n \n \n # MODEL_ID = \"Qwen/Qwen3-4B-Instruct-2507\"\n SLIDING_WINDOW = 0\n-MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"Qwen/Qwen3-4B-Instruct-2507\"\n+MODEL_ID = \"google/gemma-2-2b-it\" if SLIDING_WINDOW > 0 else \"meta-llama/Meta-Llama-3-8B\"\n FORCE_MAX_LENGTH = False  # should be False unless you are debugging sliding window features\n+SKIP_SPECIAL_TOKENS = False\n \n \n def generate_simple(\n     attn_impl: str, simple_batch_inputs: list[int], generation_config: GenerationConfig\n ) -> dict[str, str]:\n     attn_impl = {\n-        \"sdpa_paged\": \"sdpa\",\n-        \"eager_paged\": \"eager\",\n+        \"sdpa\": \"sdpa\",\n+        \"eager\": \"eager\",\n         \"paged_attention\": \"eager\",  # TODO: this does not work on AMD docker\n         \"flash_paged\": \"flash_attention_2\",  # TODO: this does not work on AMD docker\n+        \"kernels-community/flash-attn\": \"eager\",\n     }[attn_impl]\n \n     model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=torch.bfloat16, attn_implementation=attn_impl)\n@@ -56,7 +59,7 @@ def generate_simple(\n         # attention_mask = torch.ones_like(input_ids)\n         outputs = model.generate(input_ids, generation_config=generation_config, use_model_defaults=False)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n-        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+        decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         decoded_outputs[key] = decoded_output\n     return decoded_outputs\n \n@@ -99,7 +102,6 @@ def batch_generate(\n     displayed_samples: int = 0,  # -1: no display, 0: display stats, >0: display inputs and some outputs\n     output_file: Optional[str] = None,\n     expected_outputs: Optional[list[str]] = None,\n-    slice_inputs: bool = True,\n ) -> tuple[float, float]:\n     # Actual batch generation\n     if displayed_samples >= 0:\n@@ -108,7 +110,6 @@ def batch_generate(\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=slice_inputs,  # TODO: move this to the generation config\n     )\n     end_time_simple = time.time()\n     if displayed_samples >= 0:\n@@ -118,19 +119,21 @@ def batch_generate(\n     token_count = 0\n     data = []\n     for i, request in enumerate(batch_outputs):\n-        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=True)\n+        input_text = tokenizer.decode(batch_outputs[request].prompt_ids, skip_special_tokens=SKIP_SPECIAL_TOKENS)\n         # The key is used to tie back to the output of unbatched generation\n         key = \" \".join(map(str, batch_outputs[request].prompt_ids))\n         data.append({\"input\": input_text, \"key\": key})\n \n         # Try to decode the output\n         try:\n-            output_text = tokenizer.decode(batch_outputs[request].generated_tokens, skip_special_tokens=True)\n+            output_text = tokenizer.decode(\n+                batch_outputs[request].generated_tokens, skip_special_tokens=SKIP_SPECIAL_TOKENS\n+            )\n             token_count += len(batch_outputs[request].generated_tokens[1:])\n-            data[-1][\"output\"] = output_text\n+            data[-1][\"cb_outputs\"] = output_text\n         except Exception as e:\n             print(f\"Decoding failed for request {request}: {e}\")\n-            data[-1][\"output\"] = \"__ERROR__\"\n+            data[-1][\"cb_outputs\"] = \"__ERROR__\"\n             continue\n \n         # Display sample if asked\n@@ -148,7 +151,7 @@ def batch_generate(\n         if expected_outputs is not None:\n             expected_output = expected_outputs.pop(key)\n             matches = output_text == expected_output  # TODO: rework this for a better distance metric\n-            data[-1][\"ref\"] = expected_output\n+            data[-1][\"without_cb\"] = expected_output\n             data[-1][\"matches\"] = matches\n             data[-1].pop(\"key\")\n             print(f\"Request {i} matches\" if matches else f\"Request {i} does NOT match!\")\n@@ -186,19 +189,20 @@ def batch_generate(\n \n     parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n-    parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n-    parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")\n-    parser.add_argument(\"--compile\", action=\"store_true\")\n+    parser.add_argument(\"--cuda-graph\", \"-cg\", help=\"Use cuda graphs\", type=str, default=None)\n+    parser.add_argument(\"--compile\", action=\"store_true\", help=\"Compile the model using torch.compile\")\n \n-    parser.add_argument(\"--samples\", type=int, default=500)\n+    parser.add_argument(\"--samples\", type=int, default=500, help=\"Number of samples to generate\")\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n+    parser.add_argument(\"--log-level\", type=str, default=\"INFO\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n     parser.add_argument(\"--compare\", action=\"store_true\")\n     parser.add_argument(\"--metrics\", action=\"store_true\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n-    args.slice_inputs = not args.no_slice_inputs\n+    # Set log level\n+    logger.setLevel(args.log_level.upper())\n \n     # If turned on, we setup metrics\n     if args.metrics:\n@@ -207,6 +211,15 @@ def batch_generate(\n     # Set matmul precision if not none\n     if args.matmul_precision != \"none\":\n         torch.set_float32_matmul_precision(args.matmul_precision)\n+    # Parse cuda graph argument\n+    if args.cuda_graph is not None:\n+        use_cuda_graph = {\n+            \"none\": None,\n+            \"yes\": True, \"y\": True, \"true\": True, \"t\": True, \"1\": True,\n+            \"no\": False, \"n\": False, \"false\": False, \"f\": False, \"0\": False,\n+        }[args.cuda_graph.lower()]  # fmt: skip\n+    else:\n+        use_cuda_graph = None\n \n     # Prepare model\n     model = AutoModelForCausalLM.from_pretrained(\n@@ -222,9 +235,6 @@ def batch_generate(\n     # If turned on, we compile the model\n     if args.compile:\n         model.forward = torch.compile(model.forward, mode=\"max-autotune-no-cudagraphs\")\n-    if args.slice_inputs:\n-        assert not args.compile, \"Slicing inputs requires is not the model to be compiled\"\n-        assert not args.use_cuda_graph, \"Slicing inputs is not compatible with cuda graphs\"\n \n     # Prepare tokenizer and dataset\n     tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, padding_side=\"left\")\n@@ -237,10 +247,10 @@ def batch_generate(\n     # Prepare generation config\n     generation_config = GenerationConfig(\n         max_new_tokens=512,\n-        use_cuda_graph=args.use_cuda_graph,\n+        use_cuda_graph=use_cuda_graph,\n         eos_token_id=tokenizer.pad_token_id if FORCE_MAX_LENGTH else tokenizer.eos_token_id,\n         pad_token_id=tokenizer.pad_token_id,\n-        do_sample=True,\n+        do_sample=not args.compare,\n         temperature=0.8,\n         top_p=0.9,\n         num_blocks=args.num_blocks,\n@@ -265,7 +275,6 @@ def batch_generate(\n         generation_config,\n         tokenizer,\n         displayed_samples=-1,\n-        slice_inputs=args.slice_inputs,\n     )\n \n     if args.profile is not None:\n@@ -282,12 +291,11 @@ def batch_generate(\n             displayed_samples=args.displayed,\n             output_file=args.output_file,\n             expected_outputs=expected_outputs,\n-            slice_inputs=args.slice_inputs,\n         )\n     if args.profile is not None:\n         filename = args.profile if args.profile.endswith(\".json\") else args.profile + \".json\"\n         prof.export_chrome_trace(filename)\n \n # Example usage:\n-# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --slice-inputs --samples 3 --compare\n+# python examples/pytorch/continuous_batching.py --attn sdpa_paged -mp none --samples 3 --compare\n # python examples/pytorch/continuous_batching.py --num-blocks 369 --max-batch-tokens 23 --attn sdpa_paged -mp none --samples 1 --displayed 0 --output-file sliced.json"
        },
        {
            "sha": "2caed741c3cfb67c2c1f02de07995a7b0526d7d3",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -68,7 +68,6 @@\n     _ = model.generate_batch(\n         inputs=simple_batch_inputs[: min(5, args.samples)],\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n \n     # Actual batch generation\n@@ -77,7 +76,6 @@\n     batch_outputs = model.generate_batch(\n         inputs=simple_batch_inputs,\n         generation_config=generation_config,\n-        slice_inputs=True,\n     )\n     end_time = time.time()\n     print(\"Done with batch generation.\")"
        },
        {
            "sha": "45841ee4e19712de8622573355d7f726d8b69b3e",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -204,8 +204,8 @@ def __init__(\n         # Initialize the cache\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n-        # We add one extra token to the cache to handle padding and generally discard unwanted tokens\n-        self.cache_shape = (num_blocks * self.block_size + 1, self.num_key_value_heads, self.head_dim)\n+        # We add two extra tokens to the cache to handle padding and generally discard unwanted tokens\n+        self.cache_shape = (num_blocks * self.block_size + 2, self.num_key_value_heads, self.head_dim)\n         for _ in range(group_size):\n             new_layer_key_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n             new_layer_value_cache = torch.empty(self.cache_shape, dtype=self.dtype, device=self.device)\n@@ -290,7 +290,6 @@ def update(\n         layer_idx: int,\n         read_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_kv + past_length]\n         write_index: list[torch.Tensor],  # shape [num_layer_groups, seqlen_q]\n-        **kwargs,\n     ) -> tuple[torch.Tensor, torch.Tensor]:  # shape [seqlen_kv + past_length, num_kv_heads, head_dim]\n         \"\"\"Update the cache with new key-value states for a specific layer. This method writes new KV states to the\n         appropriate cache locations. The behavior differs based on the layer's attention type:\n@@ -324,11 +323,11 @@ def update(\n         # the only case where you may write over cache you need to use\n         else:\n             # Add the cache to the key and value states\n-            mask = layer_read_index == -1  # TODO: can this can be efficiently precomputed?\n+            mask = (layer_read_index == -1).unsqueeze(-1).unsqueeze(-1)  # TODO: should this be precomputed?\n             key_states_with_cache = k_cache[layer_read_index, :, :]\n-            key_states_with_cache[mask] = key_states\n+            key_states_with_cache.masked_scatter_(mask, key_states)\n             value_states_with_cache = v_cache[layer_read_index, :, :]\n-            value_states_with_cache[mask] = value_states\n+            value_states_with_cache.masked_scatter_(mask, value_states)\n             # Write new KV values to the cache\n             k_cache[layer_write_index, :, :] = key_states\n             v_cache[layer_write_index, :, :] = value_states"
        },
        {
            "sha": "55911f775a9cb65665bf3102f8ec331dba3aff3b",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 316,
            "deletions": 195,
            "changes": 511,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -15,18 +15,21 @@\n # limitations under the License.\n import queue\n import threading\n+from collections.abc import Generator\n from dataclasses import dataclass\n from functools import partial\n from itertools import count\n+from math import ceil\n from time import perf_counter\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from tqdm import tqdm\n \n-from ...configuration_utils import PreTrainedConfig\n+from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...generation.logits_process import LogitsProcessor\n from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n@@ -35,10 +38,44 @@\n from .scheduler import SCHEDULER_MAPPING, FIFOScheduler, Scheduler\n \n \n+\"\"\"\n+To enable cuda graphs, we need the dimensions of all tensors to be static, which is counter-intuitive for CB. In CB, as\n+generation goes on, there are two dimensions that change:\n+- the number of queries tokens (Q), which can vary from batch to batch\n+- the number of keys/values tokens (KV), which grows as the cache does\n+\n+To solve this, we slice along those dimensions to fixed lengths. The size of the slices is controlled by the variables\n+below: NUM_X_CUDA_GRAPHS means that we create at most NUM_X_CUDA_GRAPHS graphs for the X dimension. So if the maximum\n+number of queries tokens is 1000, and NUM_Q_CUDA_GRAPHS is 4, we will slice the number of queries token by intervals of\n+1000 / 4 = 250 tokens, ie. to 250, 500, 750 or 1000 queries tokens.\n+\n+Smaller slices means more granularity and thus less padding. But since each graph takes up space on the GPU and time to\n+create, we don't want to many graphs. And since the size of the KV dimension is the number of queries tokens plus the\n+number of tokens cached, dimension of KV is usually much larger than the the dimension of Q. So we have more granularity\n+for the KV dimension than the query dimension.\n+\"\"\"\n+NUM_Q_CUDA_GRAPHS = 4\n+NUM_KV_CUDA_GRAPHS = 8\n+\n+\n+def pad_by_intervals(size: int, max_value: int, nb_intervals: int) -> int:\n+    \"\"\"Return the smallest multiple of (max_value) // (nb_intervals) greater than (size).\"\"\"\n+    interval_size = max_value // nb_intervals\n+    if interval_size == 0:\n+        return max_value\n+    padded = ceil(size / interval_size) * interval_size\n+    return min(padded, max_value)\n+\n+\n+def attn_mask_is_needed(config: PretrainedConfig) -> bool:\n+    \"\"\"Checks if attention mask is needed for the given (config).\"\"\"\n+    return config._attn_implementation in [\"paged|eager\", \"paged|sdpa\"]\n+\n+\n def build_attention_mask(\n     attention_mask: torch.Tensor,\n-    cumulative_seqlens_q: torch.Tensor,\n-    cumulative_seqlens_k: torch.Tensor,\n+    cumulative_seqlens_q: list[int],\n+    cumulative_seqlens_k: list[int],\n     sliding_window: int = 1,\n ) -> None:\n     \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n@@ -57,7 +94,7 @@ def build_attention_mask(\n            █ █ █ █ █ █ █ █\n \n     SLIDING WINDOW MASK:\n-         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the right\n+         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the left\n        <─┴─>\n      ░ █ | █ █ █ █ █ █ █ █\n      ░ ░ | █ █ █ █ █ █ █ █\n@@ -80,7 +117,7 @@ def build_attention_mask(\n            █ █ █ █ █\n \n     SLIDING WINDOW MASK:\n-         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the right\n+         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the left\n         <┴>\n          | ░ █ █ █ █\n          | ░ ░ █ █ █\n@@ -141,16 +178,16 @@ class ContinuousBatchProcessor:\n     def __init__(\n         self,\n         cache: PagedAttentionCache,\n-        config: PreTrainedConfig,\n+        config: PretrainedConfig,\n         generation_config: GenerationConfig,\n         input_queue: queue.Queue,\n         output_queue: queue.Queue,\n         stop_event: threading.Event,\n         model_device: torch.device,\n         model_dtype: torch.dtype,\n         scheduler: Scheduler,\n-        manual_eviction: bool = False,\n-        slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n+        manual_eviction: bool,\n+        use_cuda_graph: bool,\n     ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n@@ -165,7 +202,8 @@ def __init__(\n             model_dtype: Data type for model inputs/outputs\n             scheduler: The [`Scheduler`] to use\n             manual_eviction: Whether to manually evict blocks from the cache\n-            slice_inputs: Whether to slice the inputs to the model\n+            use_cuda_graph: Whether to use cuda graphs or not during CB. Check the docstring at the top of the file for\n+                more details.\n         \"\"\"\n         self.cache = cache\n         self.config = config\n@@ -177,36 +215,39 @@ def __init__(\n         self.model_dtype = model_dtype\n         self.scheduler = scheduler\n         self.manual_eviction = manual_eviction\n-        self.slice_inputs = slice_inputs\n \n         # Retrieve the size of the sliding window if there is one\n         self.sliding_window = 1 if getattr(config, \"sliding_window\", None) is None else config.sliding_window\n-\n+        # Accumulator for batch scheduling\n         self.requests_in_batch: list[RequestState] = []\n+        # Cuda graphs for the generation step\n+        self._graphs: Optional[dict[tuple[int, int], torch.cuda.CUDAGraph]] = {} if use_cuda_graph else None\n \n         # Set up metrics collector\n         self.max_batch_tokens = cache.max_batch_tokens\n         self.metrics = ContinuousBatchProcessorMetrics(cache.max_batch_tokens)\n \n         # Setup static tensors\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0  # This is the actual number of queries tokens in the batch\n+        self.actual_key_length = 0  # This is the actual number of keys/values tokens in the batch\n+        self.actual_batch_size = 0  # This is the actual number of requests in the batch\n+        self.actual_index_sizes = [(0, 0) for _ in range(cache.num_groups)]\n         self.setup_static_tensors(cache.num_groups)\n \n     @traced(standalone=True)\n     def setup_static_tensors(self, num_groups: int) -> None:\n-        T = self.max_batch_tokens\n+        \"\"\"Setup the static tensors that are used for storage during the generation step. No other tensor will be\n+        allowed for the inputs or the outputs of the generation step.\"\"\"\n         num_pages = self.cache.num_blocks * self.cache.block_size\n         self.tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n \n         # Some tensors always have the same shape regardless of the model\n-        self.input_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.position_ids = torch.empty((1, T), **self.tensor_metadata)\n-        self.cumulative_seqlens_q = torch.empty((T + 1,), **self.tensor_metadata)\n+        self.input_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.position_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((self.max_batch_tokens + 1,), **self.tensor_metadata)\n         self.max_seqlen_q = 0\n-        self.logits_indices = torch.empty((T,), **self.tensor_metadata)\n-        self.output_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.logits_indices = torch.empty((self.max_batch_tokens,), **self.tensor_metadata)\n+        self.output_ids = torch.empty((1, self.max_batch_tokens), **self.tensor_metadata)\n \n         # For some kwargs, we have a dict of tensors with as many items as there are attention types\n         layer_types = getattr(self.config, \"layer_types\", None)\n@@ -216,13 +257,13 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         layer_types = list(set(layer_types))\n \n         self.cumulative_seqlens_k = {\n-            layer_type: torch.empty((T + 1), **self.tensor_metadata) for layer_type in layer_types\n+            l_type: torch.empty((self.max_batch_tokens + 1), **self.tensor_metadata) for l_type in layer_types\n         }\n         self.max_seqlen_k = dict.fromkeys(layer_types, 0)\n \n-        if self.return_attention_mask():\n+        if attn_mask_is_needed(self.config):\n             attn_mask_kwargs = {\n-                \"size\": (1, 1, T, num_pages + T),\n+                \"size\": (1, 1, self.max_batch_tokens, num_pages + self.max_batch_tokens),\n                 \"dtype\": self.model_dtype,\n                 \"device\": self.model_device,\n             }\n@@ -231,33 +272,26 @@ def setup_static_tensors(self, num_groups: int) -> None:\n             self.attention_mask = None\n \n         # For other kwargs, we need a list of tensors with as many tensors as there are groups\n-        self.write_index_storage = [torch.empty((T,), **self.tensor_metadata) for _ in range(num_groups)]\n-        self.read_index_storage = [torch.empty((num_pages + T), **self.tensor_metadata) for _ in range(num_groups)]\n+        self.write_index_storage = [\n+            torch.empty((self.max_batch_tokens,), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n+        self.read_index_storage = [\n+            torch.empty((num_pages + self.max_batch_tokens), **self.tensor_metadata) for _ in range(num_groups)\n+        ]\n         # For read index, the +T is because there are -1 for seqlen_q when model uses a sliding window\n \n         # After allocating empty tensors, we reset them to the right value\n         self.reset_static_tensors(full_reset=True)\n \n-    def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation in [\n-            \"paged|eager\",\n-            \"paged|sdpa\",\n-        ]  # we set `is_causal` to True in paged call\n-\n     @traced\n     @torch.no_grad()\n-    def reset_static_tensors(self, full_reset: bool = False):\n+    def reset_static_tensors(self, full_reset: bool = False) -> None:\n         \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n         batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        if full_reset or not self.slice_inputs:\n-            q_len = self.write_index_storage[0].size(-1)\n-            k_len = self.read_index_storage[0].size(-1)\n-            b_size = self.write_index_storage[0].size(0)\n-        else:\n-            q_len = self.total_query_length\n-            k_len = self.total_key_length\n-            b_size = self.total_batch_size\n+        q_len = self.write_index_storage[0].size(-1) if full_reset else self.actual_query_length\n+        k_len = self.read_index_storage[0].size(-1) if full_reset else self.actual_key_length\n+        b_size = self.write_index_storage[0].size(0) if full_reset else self.actual_batch_size\n \n         # Reset the attributes that always have the same shape\n         self.input_ids[:, :q_len].zero_()\n@@ -276,14 +310,19 @@ def reset_static_tensors(self, full_reset: bool = False):\n \n         # Reset the attributes that are lists of tensors\n         for i in range(self.cache.num_groups):\n-            self.write_index_storage[i][:q_len].fill_(-1)\n-            self.read_index_storage[i][: q_len + k_len].fill_(-1)\n-\n-    def get_model_kwargs(self) -> PagedAttentionArgs:\n-        \"\"\"Get model keyword arguments for the current batch.\"\"\"\n-        # Compute the slice to return\n-        q_len = self.total_query_length if self.slice_inputs else self.write_index_storage[0].size(-1)\n-        b_size = self.total_batch_size if self.slice_inputs else self.cumulative_seqlens_q.size(-1) - 1\n+            self.write_index_storage[i][:q_len].fill_(-2)  # -1 is used to let the cache where new states go\n+            self.read_index_storage[i][: q_len + k_len].fill_(-2)  # same\n+\n+    def get_model_kwargs(self, padded_q_size: int = 0, padded_kv_cache_size: int = 0) -> PagedAttentionArgs:\n+        \"\"\"Get model keyword arguments for the current batch, eventually padding the query dimension to (padded_q_size)\n+        and the keys/values dimension to (padded_kv_cache_size). The padding is only useful if we want static shapes,\n+        like when using cuda graphs AND only activated if both Q and KV are padded.\"\"\"\n+        # Compute the slice to return, with the given padding if we are using cuda graphs\n+        use_padding = padded_q_size > 0 and padded_kv_cache_size > 0\n+        q_len = padded_q_size if use_padding else self.actual_query_length\n+        b_size = padded_q_size if use_padding else self.actual_batch_size\n+        # If there is padding, the size of the KV is the nb of padded Q tokens + the size padded of the padded KV cache\n+        padded_kv_size = padded_q_size + padded_kv_cache_size\n \n         # Prepare the kwargs, the attributes that are either tensors or dict of tensors are initialized to empty dicts\n         kwargs = {\n@@ -295,43 +334,57 @@ def get_model_kwargs(self) -> PagedAttentionArgs:\n             \"cu_seq_lens_k\": {},\n             \"max_seqlen_k\": {},\n             \"attention_mask\": {},\n-            \"read_index\": self.read_index,  # slicing is done during building\n-            \"write_index\": self.write_index,  # slicing is done during building\n+            \"read_index\": [],\n+            \"write_index\": [],\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n \n+        # If we use constant-sized slicing, there are some \"padding\" queries tokens which FA has some issues with. In\n+        # some models like Qwen3-4B-Instruct-2507, if we don't include these tokens in cumulative_seqlens_q, there are\n+        # some NaNs in the output logits even for non-padded tokens.\n+        if use_padding:\n+            self.max_seqlen_q = max(self.max_seqlen_q, q_len - self.total_seqlen_q)\n+            self.cumulative_seqlens_q[self.actual_batch_size + 1 :] = q_len\n+            # FIXME: is there another way to avoid this? It has a very slight impact on performance (~5 tok/s)\n+\n+        # For the attributes that are lists of tensors, we construct list of tensor references\n+        for i, (read_index_size, write_index_size) in enumerate(self.actual_index_sizes):\n+            read_index_size = padded_kv_size if use_padding else read_index_size\n+            write_index_size = padded_q_size if use_padding else write_index_size\n+            kwargs[\"read_index\"].append(self.read_index_storage[i][:read_index_size])\n+            kwargs[\"write_index\"].append(self.write_index_storage[i][:write_index_size])\n+\n         # For the attributes that are dict of tensors, we replace the dict with a tensor if there is only one entry\n         layer_types = list(self.cumulative_seqlens_k.keys())\n         if len(layer_types) > 1:\n             for layer_type, seqlens_k in self.cumulative_seqlens_k.items():\n                 kwargs[\"cu_seq_lens_k\"][layer_type] = seqlens_k[: b_size + 1]\n                 kwargs[\"max_seqlen_k\"][layer_type] = self.max_seqlen_k[layer_type]\n                 if self.attention_mask is not None:\n-                    k_len = seqlens_k[b_size] if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                    k_len = padded_kv_size if use_padding else seqlens_k[b_size]\n                     kwargs[\"attention_mask\"][layer_type] = self.attention_mask[layer_type][..., :q_len, :k_len]\n         else:\n             layer_type = layer_types[0]\n             kwargs[\"cu_seq_lens_k\"] = self.cumulative_seqlens_k[layer_type][: b_size + 1]\n             kwargs[\"max_seqlen_k\"] = self.max_seqlen_k[layer_type]\n             if self.attention_mask is not None:\n-                k_len = self.cumulative_seqlens_k[layer_type][b_size]\n-                k_len = k_len if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                k_len = padded_kv_size if use_padding else self.cumulative_seqlens_k[layer_type][b_size]\n                 kwargs[\"attention_mask\"] = self.attention_mask[layer_type][..., :q_len, :k_len]\n \n         if self.attention_mask is None:\n             kwargs[\"attention_mask\"] = None\n         return kwargs\n \n-    def __repr__(self):\n+    def __repr__(self) -> str:\n         return (\n             f\"ContinuousBatchProcessor(input_queue={self.input_queue}, output_queue={self.output_queue}, \"\n             f\"active_requests={self.scheduler.active_requests}, waiting_requests={self.scheduler.waiting_requests})\"\n             + self.get_model_kwargs().__repr__()\n         )\n \n     @traced\n-    def _get_new_requests(self):\n+    def _get_new_requests(self) -> None:\n         \"\"\"Pull new requests from the input queue and add to waiting list.\"\"\"\n         while not self.input_queue.empty():\n             try:\n@@ -349,7 +402,7 @@ def _get_new_requests(self):\n                     self._handle_request_error(e, state)\n \n     @traced\n-    def _handle_request_error(self, error, state: RequestState):\n+    def _handle_request_error(self, error: Exception, state: RequestState) -> None:\n         \"\"\"Handle general request processing error.\"\"\"\n         state.status = RequestStatus.FAILED\n         state.error = str(error)\n@@ -382,12 +435,12 @@ def prepare_next_batch(self) -> bool:\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n         # Reset the static tensors used for storage\n-        self.reset_static_tensors()  # TODO: with slice_inputs, this might be unnecessary\n+        self.reset_static_tensors()  # TODO: this might be unnecessary\n \n         # Prepare accumulators\n-        self.total_query_length = 0\n-        self.total_key_length = 0\n-        self.total_batch_size = 0\n+        self.actual_query_length = 0\n+        self.actual_key_length = 0\n+        self.actual_batch_size = 0\n \n         input_ids = []\n         position_ids = []\n@@ -410,10 +463,10 @@ def prepare_next_batch(self) -> bool:\n             seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n             # Then we update the total lengths that are used for slicing\n-            self.total_query_length += query_length\n+            self.actual_query_length += query_length\n             # total_key_length is used to slice the keys so we need to take the max of all the key lengths\n-            self.total_key_length += max(seqlens_k.values())\n-            self.total_batch_size += 1\n+            self.actual_key_length += max(seqlens_k.values())\n+            self.actual_batch_size += 1\n             # And the attribute tracking the position in the request object\n             state.position_offset += query_length\n \n@@ -476,6 +529,7 @@ def _build_tensors(\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n         self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n         self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n+        self.total_seqlen_q = cumulative_seqlens_q[-1]\n \n         # Those kwargs are either dict of tensors or tensors, so we need to handle both cases\n         for layer_type, layer_type_seqlens_k in cumulative_seqlens_k.items():\n@@ -492,42 +546,32 @@ def _build_tensors(\n         self.read_index = []\n         self.write_index = []\n         for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n-            # Write in the actual tensors\n             self.read_index_storage[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n             self.write_index_storage[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n-            # Slice to the right size\n-            r = len(group_read_indices) if self.slice_inputs else self.read_index_storage[i].size(-1)\n-            w = len(group_write_indices) if self.slice_inputs else self.write_index_storage[i].size(-1)\n-            # Add to the index\n-            self.read_index.append(self.read_index_storage[i][:r])\n-            self.write_index.append(self.write_index_storage[i][:w])\n+            self.actual_index_sizes[i] = (len(group_read_indices), len(group_write_indices))\n \n     @traced\n-    def _sync(self):\n+    def _sync(self) -> list[int]:\n         if self.output_ids is not None:\n             try:\n-                out = self.output_ids.tolist()[0]  # should be the only sync we do\n+                return self.output_ids.tolist()[0]\n             except Exception:\n-                out = [0, 1]\n-        else:\n-            out = [0, 0]\n-        return out\n+                return [0, 1]\n+        return [0, 0]\n \n     @traced\n-    def _maybe_send_output(self, state: RequestState, token: int):\n+    def _maybe_send_output(self, state: RequestState) -> None:\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n         if state.streaming:\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n \n     @traced\n-    def update_batch(self):\n+    def update_batch(self) -> None:\n         \"\"\"Update request states based on generated tokens.\"\"\"\n         out_tokens = self._sync()\n-        finished_request_ids = []\n         for i, state in enumerate(self.requests_in_batch):\n-            req_id = state.request_id\n             if len(state.remaining_prompt_ids) == 0:\n                 self.metrics.record_ttft_metric(state.created_time, state.request_id)\n                 state.status = RequestStatus.DECODING\n@@ -536,8 +580,7 @@ def update_batch(self):\n                 if state.update_with_token(token):\n                     self.metrics.record_request_completion(state.created_time, state.request_id)\n                     self.scheduler.finish_request(state.request_id, evict_from_cache=(not self.manual_eviction))\n-                    finished_request_ids.append(req_id)\n-                self._maybe_send_output(state, token)\n+                self._maybe_send_output(state)\n             elif state.status == RequestStatus.PREFILLING_SPLIT:\n                 state.status = RequestStatus.SPLIT_PENDING_REMAINDER\n         if self.cache.get_num_free_blocks() == 0:\n@@ -557,7 +600,7 @@ def handle_batch_error(self, error):\n             self.scheduler.finish_request(req.request_id)\n \n     @traced\n-    def fail_all_requests(self, error):\n+    def fail_all_requests(self, error: Exception) -> None:\n         \"\"\"Fail all active requests with the given error.\n \n         Args:\n@@ -577,6 +620,95 @@ def fail_all_requests(self, error):\n         # Clear the ordering queue\n         self.scheduler.waiting_requests_order.clear()\n \n+    @traced\n+    @torch.no_grad\n+    def _generation_step(self, model: nn.Module, logit_processor: LogitsProcessor, do_sample: bool) -> None:\n+        \"\"\"Perform a single generation step.\"\"\"\n+\n+        # If cuda graphs are disabled, we just use the actual size\n+        if self._graphs is None:\n+            batch_data = self.get_model_kwargs()\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+            return None\n+\n+        # Determine the padded size of the queries and keys/values\n+        padded_q = pad_by_intervals(self.actual_query_length, self.max_batch_tokens, NUM_Q_CUDA_GRAPHS)\n+\n+        max_read_index_size = max(self.actual_index_sizes[i][0] for i in range(self.cache.num_groups))\n+        padded_read_index_size = pad_by_intervals(\n+            max_read_index_size - self.max_batch_tokens,\n+            self.cache.num_blocks * self.cache.block_size,\n+            NUM_KV_CUDA_GRAPHS,\n+        )\n+\n+        # Get the batch data and the associated graph\n+        batch_data = self.get_model_kwargs(padded_q, padded_read_index_size)\n+\n+        graph = self._graphs.get((padded_q, padded_read_index_size))\n+\n+        # If we have a graph that fits, we replay it\n+        if graph is not None:\n+            graph.replay()\n+            return None\n+\n+        # Otherwise, we need to create it\n+        logger.info(f\"Creating graph for {(padded_q, padded_read_index_size) = }\")\n+        stream = torch.cuda.Stream(device=model.device)\n+        stream.wait_stream(torch.cuda.current_stream())\n+        # Warmup\n+        with torch.cuda.stream(stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        torch.cuda.current_stream().wait_stream(stream)\n+        # Catpure\n+        graph = torch.cuda.CUDAGraph()\n+        with torch.cuda.graph(graph, stream=stream):\n+            self._forward_process_and_sample(model, batch_data, logit_processor, do_sample)\n+        self._graphs[(padded_q, padded_read_index_size)] = graph\n+\n+    @traced\n+    def _forward_process_and_sample(\n+        self, model: nn.Module, batch_data: dict, logit_processor: LogitsProcessor, do_sample: bool\n+    ) -> None:\n+        \"\"\"This function performs the forward pass, logits processing, and sampling; which are broken down into smaller\n+        function to be easier to trace with OpenTelemetry.\"\"\"\n+        # with torch.no_grad():\n+        logits = self._model_forward(model, batch_data)\n+        # if self.log_prob_generation:    batch_processor.output_probs.copy_(logits)  # TODO\n+        probs = self._process_logit(batch_data, logits, logit_processor)\n+        self._sample(probs, do_sample)\n+\n+    @traced(span_name=\"model_forward\")\n+    def _model_forward(self, model: nn.Module, batch_data: dict) -> torch.Tensor:\n+        return model(**batch_data).logits\n+\n+    @traced(span_name=\"logit_processing\")\n+    def _process_logit(self, batch_data: dict, logits: torch.Tensor, logit_processor: LogitsProcessor) -> torch.Tensor:\n+        # Pass continuous batching context to logits processor if it supports it.\n+        if hasattr(logit_processor, \"set_continuous_batching_context\"):\n+            logit_processor.set_continuous_batching_context(batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"])\n+        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n+        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n+        batch_size, seq_len, vocab_size = logits.shape\n+        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n+        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n+        # Process with 2D tensors\n+        processed_logits_2d = logit_processor(input_ids_2d, logits_2d)\n+        # Reshape back to 3D\n+        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n+\n+    @traced(span_name=\"sampling\")\n+    def _sample(self, probs: torch.Tensor, do_sample: bool) -> None:\n+        if do_sample:\n+            probs = nn.functional.softmax(probs, dim=-1)\n+            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n+            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n+            # Add batch dimension back to match argmax output\n+            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n+        else:\n+            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n+        tokens = next_tokens.size(1)  # Get seq_len dimension\n+        self.output_ids[:, :tokens].copy_(next_tokens)\n+\n \n # Manager Class (User Interface)\n @attach_tracer()\n@@ -589,19 +721,21 @@ class ContinuousBatchingManager:\n \n     def __init__(\n         self,\n-        model,\n+        model: nn.Module,\n         generation_config: GenerationConfig,\n         manual_eviction: bool = False,\n-        max_queue_size=0,\n-        slice_inputs: bool = True,\n-    ):\n-        \"\"\"\n-        Initialize the continuous batching manager.\n+        max_queue_size: int = 0,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n+    ) -> None:\n+        \"\"\"Initialize the continuous batching manager.\n \n         Args:\n             model: The language model for generation\n             generation_config: Configuration for generation parameters\n             max_queue_size: Maximum size of the request queue (0 = unlimited)\n+            num_q_cuda_graphs: (optional) Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: (optional) Number of CUDA graphs to use for the keys/values dimension\n         \"\"\"\n         if \"paged|\" not in model.config._attn_implementation:\n             attn_implementation = f\"paged|{model.config._attn_implementation}\"\n@@ -627,17 +761,38 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", False)  # TODO: same as do_sample\n-        self.profile = getattr(generation_config, \"profile\", False)\n+        use_cuda_graph: Optional[bool] = getattr(generation_config, \"use_cuda_graph\", None)\n+        self.profile = getattr(generation_config, \"profile\", False)  # TODO: not supported yet\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.slice_inputs = slice_inputs\n \n+        # If a number of cuda graphs was specified for either Q or KV, we activate cuda graphs\n+        if num_q_cuda_graphs > 0 or num_kv_cuda_graphs > 0:\n+            self.use_cuda_graph = True\n+        # If use_cuda_graph is specified, we follow the user's choice\n+        elif use_cuda_graph is not None:\n+            self.use_cuda_graph = use_cuda_graph\n+        # If the use of cuda graphs is not specified, we follow the user's choice, otherwise we have a default heuristic\n+        else:\n+            # Attention implementations where an attention mask is needed suffer a lot more from the padding associated\n+            # with cuda graphs, so default is to turn cuda graphs off for those implementations\n+            self.use_cuda_graph = not attn_mask_is_needed(self.model.config)\n+            logger.warning(\n+                f\"No behavior specified for use_cuda_graph, defaulting to {self.use_cuda_graph = } because \"\n+                f\"{self.model.config._attn_implementation = }. If you want to save memory, turn off cuda graphs, but \"\n+                \"they can improve performances.\"\n+            )\n+\n+        # If cuda graphs are activated, we set the number of cuda graphs for Q and KV if not specified\n         if self.use_cuda_graph:\n-            raise NotImplementedError(\"Cuda graphs are not supported yet\")\n+            self.num_q_cuda_graphs = num_q_cuda_graphs if num_q_cuda_graphs > 0 else NUM_Q_CUDA_GRAPHS\n+            self.num_kv_cuda_graphs = num_kv_cuda_graphs if num_kv_cuda_graphs > 0 else NUM_KV_CUDA_GRAPHS\n+\n+        if self.log_prob_generation:\n+            raise NotImplementedError(\"log_prob_generation is not supported yet\")\n \n     @traced\n-    def start(self):\n+    def start(self) -> None:\n         \"\"\"Start the background generation thread.\"\"\"\n         if self._generation_thread is not None and self._generation_thread.is_alive():\n             logger.warning(\"Manager thread is already running.\")\n@@ -647,11 +802,11 @@ def start(self):\n         self._generation_thread = threading.Thread(target=self._run_generation_loop)\n         self._generation_thread.start()\n \n-    def is_running(self):\n+    def is_running(self) -> bool:\n         \"\"\"Check if the background generation thread is running.\"\"\"\n         return self._generation_thread is not None and self._generation_thread.is_alive()\n \n-    def stop(self, block: bool = False, timeout: Optional[float] = None):\n+    def stop(self, block: bool = False, timeout: Optional[float] = None) -> None:\n         \"\"\"Signal the background thread to stop.\n \n         Args:\n@@ -669,7 +824,7 @@ def stop(self, block: bool = False, timeout: Optional[float] = None):\n         if block:\n             self.join(timeout)\n \n-    def join(self, timeout: Optional[float] = None):\n+    def join(self, timeout: Optional[float] = None) -> None:\n         \"\"\"Wait for the background thread to finish.\n \n         Args:\n@@ -719,14 +874,13 @@ def add_request(\n \n         # Use block=True with timeout to handle backpressure if queue is full\n         self.input_queue.put(state, block=True, timeout=10)  # XXX: pass timeout as fn arg?\n-        logger.debug(f\"Added request {request_id} to queue.\")\n         return request_id\n \n-    def add_requests(self, inputs: list[list[int]], **kwargs):\n+    def add_requests(self, inputs: list[list[int]], max_new_tokens: Optional[int] = None) -> None:\n         for input_ids in inputs:\n-            self.add_request(input_ids, **kwargs)\n+            self.add_request(input_ids, max_new_tokens=max_new_tokens)\n \n-    def cancel_request(self, request_id: str):\n+    def cancel_request(self, request_id: str) -> None:\n         \"\"\"Cancel a request by its ID.\n \n         Args:\n@@ -735,7 +889,9 @@ def cancel_request(self, request_id: str):\n         if self.batch_processor is not None:\n             self.batch_processor.scheduler.set_request_cancellation(request_id)\n \n-    def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n+    def get_result(\n+        self, request_id: Optional[str] = None, timeout: Optional[float] = None\n+    ) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n@@ -763,7 +919,7 @@ def __iter__(self):\n             if result is not None:\n                 yield result\n \n-    def request_id_iter(self, request_id):\n+    def request_id_iter(self, request_id: str) -> Generator[GenerationOutput]:\n         \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n         request_cancelled = False\n         while self._generation_thread is not None and self._generation_thread.is_alive() and not request_cancelled:\n@@ -773,8 +929,16 @@ def request_id_iter(self, request_id):\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n+    @staticmethod\n+    def supported_attention_implementations() -> set[str]:\n+        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n+\n+    @staticmethod\n+    def default_attention_implementation() -> str:\n+        return \"sdpa_paged\"\n+\n     @traced\n-    def warmup(self, batch_processor):\n+    def warmup(self, batch_processor: ContinuousBatchProcessor) -> None:\n         stream = torch.cuda.Stream(device=self.model.device)\n         stream.wait_stream(torch.cuda.current_stream())\n         with torch.cuda.stream(stream):\n@@ -788,67 +952,23 @@ def warmup(self, batch_processor):\n \n     @traced\n     # @torch.compile\n-    def _generation_step(self, batch_processor: ContinuousBatchProcessor):\n+    def _generation_step(self) -> None:\n         \"\"\"Perform a single generation step. This is cuda graphed\"\"\"\n-        batch_data = batch_processor.get_model_kwargs()\n-        with torch.no_grad():\n-            logits = self._model_forward(batch_data)\n-            if self.log_prob_generation:\n-                batch_processor.output_probs.copy_(logits)  # TODO\n-            probs = self._process_logit(batch_data, logits)\n-            self._sample(batch_processor, probs)\n-\n-    @traced(span_name=\"model_forward\")\n-    def _model_forward(self, batch_data):\n-        return self.model(**batch_data).logits\n-\n-    @traced(span_name=\"logit_processing\")\n-    def _process_logit(self, batch_data, logits):\n-        # Pass continuous batching context to logits processor if it supports it. TODO we should find a way to make this a little bit cleaner!\n-        if hasattr(self.logit_processor, \"set_continuous_batching_context\"):\n-            self.logit_processor.set_continuous_batching_context(\n-                batch_data[\"logits_indices\"], batch_data[\"cu_seq_lens_q\"]\n-            )\n-\n-        # Handle shape compatibility: logit processors expect 2D tensors [batch_size, vocab_size]\n-        # but continuous batching always produces 3D tensors [batch_size, seq_len, vocab_size]\n-        batch_size, seq_len, vocab_size = logits.shape\n-        logits_2d = logits.view(batch_size * seq_len, vocab_size)\n-        input_ids_2d = batch_data[\"input_ids\"].view(batch_size * seq_len)\n-\n-        # Process with 2D tensors\n-        processed_logits_2d = self.logit_processor(input_ids_2d, logits_2d)\n+        self.batch_processor._generation_step(self.model, self.logit_processor, self.do_sample)\n \n-        # Reshape back to 3D\n-        return processed_logits_2d.view(batch_size, seq_len, vocab_size)\n-\n-    @traced(span_name=\"sampling\")\n-    def _sample(self, batch_processor: ContinuousBatchProcessor, probs):\n-        if self.do_sample:  # sample\n-            probs = nn.functional.softmax(probs, dim=-1)\n-            # probs[0] has shape [seq_len, vocab_size], multinomial returns [seq_len, 1]\n-            next_tokens = torch.multinomial(probs[0], num_samples=1).squeeze(-1)  # Now [seq_len]\n-            # Add batch dimension back to match argmax output\n-            next_tokens = next_tokens.unsqueeze(0)  # Now [1, seq_len]\n-        else:\n-            next_tokens = torch.argmax(probs, dim=-1)  # Already [1, seq_len]\n-\n-        tokens = next_tokens.size(1)  # Get seq_len dimension\n-        batch_processor.output_ids[:, :tokens].copy_(next_tokens)\n-\n-    def _run_generation_loop(self):\n+    def _run_generation_loop(self) -> None:\n         \"\"\"Main processing loop running in the background thread.\"\"\"\n-        batch_processor = None\n+        batch_processor: Optional[ContinuousBatchProcessor] = None\n         try:\n-            ref_time = perf_counter()\n+            t0 = perf_counter()\n             paged_attention_cache = PagedAttentionCache(\n                 self.model.config,\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n-            logger.debug(f\"PagedAttentionCache created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"PagedAttentionCache created in {perf_counter() - t0} seconds\")\n \n             scheduler = None\n             if hasattr(self.generation_config, \"scheduler\"):\n@@ -860,23 +980,23 @@ def _run_generation_loop(self):\n                 # Default to fifo\n                 scheduler = FIFOScheduler\n \n-            ref_time = perf_counter()\n+            t1 = perf_counter()\n             batch_processor = ContinuousBatchProcessor(\n-                paged_attention_cache,\n-                self.model.config,\n-                self.generation_config,\n-                self.input_queue,\n-                self.output_queue,\n-                self.stop_event,\n-                self.model.device,\n-                self.model.dtype,\n-                scheduler(paged_attention_cache, self.manual_eviction),\n-                self.manual_eviction,\n-                slice_inputs=self.slice_inputs,\n+                cache=paged_attention_cache,\n+                config=self.model.config,\n+                generation_config=self.generation_config,\n+                input_queue=self.input_queue,\n+                output_queue=self.output_queue,\n+                stop_event=self.stop_event,\n+                model_device=self.model.device,\n+                model_dtype=self.model.dtype,\n+                scheduler=scheduler(paged_attention_cache, self.manual_eviction),\n+                manual_eviction=self.manual_eviction,\n+                use_cuda_graph=self.use_cuda_graph,\n             )\n             self.batch_processor = batch_processor\n             self.current_batch = 0\n-            logger.debug(f\"batch_processor created in {perf_counter() - ref_time} seconds\")\n+            logger.debug(f\"batch_processor created in {perf_counter() - t1} seconds\")\n             while (not self.stop_event.is_set()) or batch_processor.has_pending_requests():\n                 self._inner_generation_loop(batch_processor)\n                 self.current_batch += 1\n@@ -888,38 +1008,27 @@ def _run_generation_loop(self):\n             logger.info(\"Generation loop finished.\")\n \n     @traced(span_name=\"generation_loop\")\n-    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n+    def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor) -> None:\n+        # Pre-loop synchronization\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Loop body ends if there is no requests in the batch\n         if not batch_processor.prepare_next_batch():\n             return\n+        # Debug logging of the current memory usage\n         if logger.level <= logging.DEBUG:\n             device, total, reserved, allocated = get_device_and_memory_breakdown()\n             logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n-        if torch.cuda.is_available() and self.use_cuda_graph:\n-            if self.current_batch == 0:\n-                self.warmup(batch_processor)\n-            elif hasattr(self, \"graph\"):\n-                try:\n-                    self._graph_replay()\n-                except Exception as e:\n-                    logger.error(f\"Model forward pass failed: {e}\", exc_info=True)\n-                    batch_processor.handle_batch_error(e)\n-                    return\n-            else:\n-                self._generation_step(batch_processor)\n-        else:\n-            self._generation_step(batch_processor)\n+\n+        self._generation_step()\n+\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n+        # Processor updates the batch after generation step is truly over\n         batch_processor.update_batch()\n \n-    @traced(span_name=\"graph_replay\")\n-    def _graph_replay(self):\n-        self.graph.replay()\n-\n     @traced\n-    def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatchProcessor]):\n+    def _handle_critical_error(self, error: Exception, batch_processor: Optional[ContinuousBatchProcessor]) -> None:\n         \"\"\"Handle critical errors that terminate the generation loop.\"\"\"\n         # Signal stop\n         self.stop_event.set()\n@@ -938,7 +1047,7 @@ def _handle_critical_error(self, error, batch_processor: Optional[ContinuousBatc\n             batch_processor.fail_all_requests(error)\n \n     @traced\n-    def evict_request_from_cache(self, request_id: str):\n+    def evict_request_from_cache(self, request_id: str) -> None:\n         \"\"\"Evict a request from the cache. It is assumed that the request is already finished.\"\"\"\n         if not self.manual_eviction:\n             raise RuntimeError(\"Manual eviction is not enabled for this manager.\")\n@@ -954,13 +1063,17 @@ def init_continuous_batching(\n         generation_config: Optional[GenerationConfig] = None,\n         manual_eviction: bool = False,\n         max_queue_size: int = 0,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n     ) -> ContinuousBatchingManager:\n         \"\"\"Initialize a manager for continuous batching inference.\n \n         Args:\n             generation_config: Custom generation configuration\n+            manual_eviction: Whether to manually evict requests from the cache\n             max_queue_size: Maximum size of the input request queue\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n \n         Returns:\n             `ContinuousBatchingManager`: The manager instance to add requests and retrieve results.\n@@ -982,7 +1095,8 @@ def init_continuous_batching(\n             generation_config=gen_config,\n             manual_eviction=manual_eviction,\n             max_queue_size=max_queue_size,\n-            slice_inputs=slice_inputs,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n         )\n \n     @traced\n@@ -992,14 +1106,17 @@ def generate_batch(\n         inputs: list[list[int]],\n         generation_config: Optional[GenerationConfig] = None,\n         progress_bar: bool = True,\n-        slice_inputs: bool = True,\n+        num_q_cuda_graphs: int = 0,\n+        num_kv_cuda_graphs: int = 0,\n         **kwargs,\n-    ) -> list[list[int]]:\n+    ) -> dict[str, GenerationOutput]:\n         \"\"\"Generate sequences for a batch of prompts using continuous batching.\n \n         Args:\n             inputs: List of input token sequences (prompts)\n             generation_config: Optional generation configuration\n+            num_q_cuda_graphs: Number of CUDA graphs to use for the query dimension\n+            num_kv_cuda_graphs: Number of CUDA graphs to use for the keys/values dimension\n             **kwargs: Additional generation parameters\n \n         Returns:\n@@ -1008,13 +1125,17 @@ def generate_batch(\n                                 Returns an empty list `[]` for requests that failed.\n         \"\"\"\n         if not inputs:\n-            return []\n+            return {}\n         if logger.getEffectiveLevel() <= logging.DEBUG:\n             logger.warning(\"Progress bar is disabled when logger level is less than DEBUG\")\n             progress_bar = False\n \n         # Initialize manager with the batch inputs\n-        manager = self.init_continuous_batching(generation_config=generation_config, slice_inputs=slice_inputs)\n+        manager = self.init_continuous_batching(\n+            generation_config=generation_config,\n+            num_q_cuda_graphs=num_q_cuda_graphs,\n+            num_kv_cuda_graphs=num_kv_cuda_graphs,\n+        )\n         manager.start()\n         results = {}\n         num_requests = len(inputs)\n@@ -1028,7 +1149,7 @@ def generate_batch(\n                     desc=f\"Solving {num_requests} requests\",\n                     unit=\"request\",\n                 ) as pbar:\n-                    manager.add_requests(inputs, **kwargs)\n+                    manager.add_requests(inputs=inputs, max_new_tokens=kwargs.get(\"max_new_tokens\"))\n                     finished_count = 0\n                     while finished_count < num_requests:\n                         result = manager.get_result(timeout=1)"
        },
        {
            "sha": "5f2d6f97c7cfddde5e884367b2f503f964336dba",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -25,7 +25,6 @@\n \n # We centralize the logger here to coordinate between logging and progress bar\n logger = logging.getLogger(\"ContinuousBatchingLogger\")\n-# logger.setLevel(logging.INFO)\n \n \n @staticmethod"
        },
        {
            "sha": "eb9df21dd0fa7dcec5b684d443e0d2d4151dea1a",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -3,6 +3,8 @@\n import torch\n from torch import nn\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def eager_paged_attention_forward(\n     **kwargs,\n ):\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
        },
        {
            "sha": "99dcc24d5b2affd5a3143e9f9abbea84e28b312c",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -64,7 +64,13 @@ def paged_attention_forward(\n \n     # .update changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n     if cache is not None:\n-        k, v = cache.update(k, v, module.layer_idx, **kwargs)\n+        k, v = cache.update(\n+            key_states=k,\n+            value_states=v,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n \n     # Retrieve the cumulative sequence lengths for the current layer\n     if isinstance(cu_seq_lens_k, dict):"
        },
        {
            "sha": "57c3f74a399e27157c24647fdd773f237ad7e834",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf1e9834ec7339f4c605ba96d9c4e5cf59594cad/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=cf1e9834ec7339f4c605ba96d9c4e5cf59594cad",
            "patch": "@@ -2,6 +2,8 @@\n \n import torch\n \n+from ..generation.continuous_batching.cache import PagedAttentionCache\n+\n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n@@ -26,10 +28,16 @@ def sdpa_attention_paged_forward(\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     # Add KV cache to the key and value tensors\n-    cache = kwargs.pop(\"cache\", None)\n+    cache: Optional[PagedAttentionCache] = kwargs.pop(\"cache\", None)\n     if cache is not None:\n         # This changes the shape of k and v from [1, num_kv_heads, seqlen_kv, head_dim] to [-1, num_kv_heads, head_dim]\n-        key, value = cache.update(key, value, module.layer_idx, **kwargs)\n+        key, value = cache.update(\n+            key_states=key,\n+            value_states=value,\n+            layer_idx=module.layer_idx,\n+            read_index=kwargs[\"read_index\"],\n+            write_index=kwargs[\"write_index\"],\n+        )\n         key = key.transpose(0, 1).unsqueeze(0)\n         value = value.transpose(0, 1).unsqueeze(0)\n "
        }
    ],
    "stats": {
        "total": 613,
        "additions": 380,
        "deletions": 233
    }
}