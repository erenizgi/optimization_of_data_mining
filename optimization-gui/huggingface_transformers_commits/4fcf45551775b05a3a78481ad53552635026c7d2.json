{
    "author": "oToToT",
    "message": "Fix broken links (#39809)\n\nReplace links in the form of `[text]((url))` to `[text](url)`. This is\nthe correct format of a url in the markdown.",
    "sha": "4fcf45551775b05a3a78481ad53552635026c7d2",
    "files": [
        {
            "sha": "d6611b26d14a5c1b7347228f7b92a317392f458f",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -242,7 +242,7 @@ pipeline(\n \n - This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n - The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n-- The [example scripts]((https://github.com/huggingface/transformers/tree/main/examples)) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n+- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n \n ## 100 projects using Transformers\n "
        },
        {
            "sha": "6697da612ccda308b0c4e1e264d5fa03d7e894f6",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -17,7 +17,7 @@\n \n 2.  **Ø§Flash Attention:** Ø¥Ù† Flash Attention ÙˆÙ‡ÙŠ Ù†Ø³Ø®Ø© Ù…ÙØ¹Ø¯ÙÙ‘Ù„Ø© Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙˆÙØ± ÙÙ‚Ø· Ù†Ù‡Ø¬Ù‹Ø§ Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© ÙÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆÙ„ÙƒÙ†Ù‡Ø§ ØªØ­Ù‚Ù‚ Ø£ÙŠØ¶Ù‹Ø§ ÙƒÙØ§Ø¡Ø© Ù…ØªØ²Ø§ÙŠØ¯Ø© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ø°Ø§ÙƒØ±Ø© GPU.\n \n-3.  **Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©:** Ø­ÙŠØ« ØªÙ… Ø§Ù‚ØªØ±Ø§Ø­ Ù‡ÙŠØ§ÙƒÙ„ Ù…ØªØ®ØµØµØ© ØªØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙŠØªÙ… Ù†Ø´Ø±Ù‡Ø§ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø£Ø«Ù†Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø£ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„ØŒ ÙÙ‚Ø¯ ØªÙ… Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ù†ÙŠØ§Øª Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ®ØµØµØ© ØªØ³Ù…Ø­ Ø¨Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©. Ø£Ù‡Ù… ØªÙ‚Ø¯Ù… ÙÙŠ Ø¨Ù†ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù‡Ù†Ø§ Ù‡Ùˆ [Ø¹Ø°Ø±](https://huggingface.co/papers/2108.12409)ØŒ [Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¯ÙˆØ§Ø±](https://huggingface.co/papers/2104.09864)ØŒ [Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (MQA)](https://huggingface.co/papers/1911.02150) Ùˆ [Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (GQA)]((https://huggingface.co/papers/2305.13245)).\n+3.  **Ø§Ù„Ø§Ø¨ØªÙƒØ§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠØ©:** Ø­ÙŠØ« ØªÙ… Ø§Ù‚ØªØ±Ø§Ø­ Ù‡ÙŠØ§ÙƒÙ„ Ù…ØªØ®ØµØµØ© ØªØ³Ù…Ø­ Ø¨Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø£ÙƒØ«Ø± ÙØ¹Ø§Ù„ÙŠØ© Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙŠØªÙ… Ù†Ø´Ø±Ù‡Ø§ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø£Ø«Ù†Ø§Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ØŒ Ø£ÙŠ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ø¹ Ø³ÙŠØ§Ù‚ Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ Ø§Ù„Ø·ÙˆÙŠÙ„ØŒ ÙÙ‚Ø¯ ØªÙ… Ø§Ù‚ØªØ±Ø§Ø­ Ø¨Ù†ÙŠØ§Øª Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ®ØµØµØ© ØªØ³Ù…Ø­ Ø¨Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©. Ø£Ù‡Ù… ØªÙ‚Ø¯Ù… ÙÙŠ Ø¨Ù†ÙŠØ§Øª Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù‡Ù†Ø§ Ù‡Ùˆ [Ø¹Ø°Ø±](https://huggingface.co/papers/2108.12409)ØŒ [Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¯ÙˆØ§Ø±](https://huggingface.co/papers/2104.09864)ØŒ [Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù…Ø§Øª (MQA)](https://huggingface.co/papers/1911.02150) Ùˆ [Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¨Ø§Ù„Ø§Ø³ØªØ¹Ù„Ø§Ù… (GQA)](https://huggingface.co/papers/2305.13245).\n \n Ø¹Ù„Ù‰ Ù…Ø¯Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ØŒ Ø³Ù†Ù‚Ø¯Ù… ØªØ­Ù„ÙŠÙ„Ù‹Ø§ Ù„Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ†Ø¨Ø¤ÙŠ Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ù…Ù†Ø¸ÙˆØ± Ø§Ù„Ù…ÙÙˆØªÙÙ‘Ø±Ø§Øª. Ù†ØªØ¹Ù…Ù‚ ÙÙŠ Ù…Ø²Ø§ÙŠØ§ ÙˆØ¹ÙŠÙˆØ¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ù‚Ø© Ø£Ù‚Ù„ØŒ ÙˆÙ†Ù‚Ø¯Ù… Ø§Ø³ØªÙƒØ´Ø§ÙÙ‹Ø§ Ø´Ø§Ù…Ù„Ø§Ù‹ Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø§Ù„Ø£Ø­Ø¯Ø«ØŒ ÙˆÙ†Ù†Ø§Ù‚Ø´ Ø¨Ù†ÙŠØ§Øª Ù†Ù…Ø§Ø°Ø¬ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©. Ø³Ù†Ø¯Ø¹Ù… Ø§Ù„Ø´Ø±Ø­ Ø¨Ø£Ù…Ø«Ù„Ø© Ø¹Ù…Ù„ÙŠØ© ØªÙØ¨Ø±ÙØ² ÙƒÙ„ ØªØ­Ø³ÙŠÙ† Ø¹Ù„Ù‰ Ø­Ø¯Ø©.\n "
        },
        {
            "sha": "b6df3aa137b3f07d4a4eb8285d7771b6a37ec19f",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -27,7 +27,7 @@ In this guide, we will go over the effective techniques for efficient LLM deploy\n \n 2.  **Flash Attention:** Flash Attention is a variation of the attention algorithm that not only provides a more memory-efficient approach but also realizes increased efficiency due to optimized GPU memory utilization.\n \n-3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://huggingface.co/papers/2108.12409), [Rotary embeddings](https://huggingface.co/papers/2104.09864), [Multi-Query Attention (MQA)](https://huggingface.co/papers/1911.02150) and [Grouped-Query-Attention (GQA)]((https://huggingface.co/papers/2305.13245)).\n+3.  **Architectural Innovations:** Considering that LLMs are always deployed in the same way during inference, namely autoregressive text generation with a long input context, specialized model architectures have been proposed that allow for more efficient inference. The most important advancement in model architectures hereby are [Alibi](https://huggingface.co/papers/2108.12409), [Rotary embeddings](https://huggingface.co/papers/2104.09864), [Multi-Query Attention (MQA)](https://huggingface.co/papers/1911.02150) and [Grouped-Query-Attention (GQA)](https://huggingface.co/papers/2305.13245).\n \n Throughout this guide, we will offer an analysis of auto-regressive generation from a tensor's perspective. We delve into the pros and cons of adopting lower precision, provide a comprehensive exploration of the latest attention algorithms, and discuss improved LLM architectures. While doing so, we run practical examples showcasing each of the feature improvements.\n "
        },
        {
            "sha": "5379a60d521dcd6a357dede100487c6cf05a4dff",
            "filename": "docs/source/en/model_doc/mgp-str.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmgp-str.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -33,7 +33,7 @@ alt=\"drawing\" width=\"600\"/>\n \n <small> MGP-STR architecture. Taken from the <a href=\"https://huggingface.co/papers/2209.03592\">original paper</a>. </small>\n \n-MGP-STR is trained on two synthetic datasets [MJSynth]((http://www.robots.ox.ac.uk/~vgg/data/text/)) (MJ) and [SynthText](http://www.robots.ox.ac.uk/~vgg/data/scenetext/) (ST) without fine-tuning on other datasets. It achieves state-of-the-art results on six standard Latin scene text benchmarks, including 3 regular text datasets (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE).\n+MGP-STR is trained on two synthetic datasets [MJSynth](http://www.robots.ox.ac.uk/~vgg/data/text/) (MJ) and [SynthText](http://www.robots.ox.ac.uk/~vgg/data/scenetext/) (ST) without fine-tuning on other datasets. It achieves state-of-the-art results on six standard Latin scene text benchmarks, including 3 regular text datasets (IC13, SVT, IIIT) and 3 irregular ones (IC15, SVTP, CUTE).\n This model was contributed by [yuekun](https://huggingface.co/yuekun). The original code can be found [here](https://github.com/AlibabaResearch/AdvancedLiterateMachinery/tree/main/OCR/MGP-STR).\n \n ## Inference example"
        },
        {
            "sha": "e765affc36d471d52d8f60e3b7515ddeaa225679",
            "filename": "docs/source/en/model_doc/qwen2_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_moe.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -24,7 +24,7 @@ rendered properly in your Markdown viewer.\n # Qwen2MoE\n \n \n-[Qwen2MoE]((https://huggingface.co/papers/2407.10671) ) is a Mixture-of-Experts (MoE) variant of [Qwen2](./qwen2), available as a base model and an aligned chat model. It uses SwiGLU activation, group query attention and a mixture of sliding window attention and full attention. The tokenizer can also be adapted to multiple languages and codes.\n+[Qwen2MoE](https://huggingface.co/papers/2407.10671) is a Mixture-of-Experts (MoE) variant of [Qwen2](./qwen2), available as a base model and an aligned chat model. It uses SwiGLU activation, group query attention and a mixture of sliding window attention and full attention. The tokenizer can also be adapted to multiple languages and codes.\n \n The MoE architecture uses upcyled models from the dense language models. For example, Qwen1.5-MoE-A2.7B is upcycled from Qwen-1.8B. It has 14.3B parameters but only 2.7B parameters are activated during runtime.\n "
        },
        {
            "sha": "087a2ff489d5e90e3cb9d7e896c26a6504296b72",
            "filename": "docs/source/en/quantization/spqr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fspqr.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # SpQR\n \n-The [SpQR]((https://hf.co/papers/2306.03078)) quantization algorithm involves a 16x16 tiled bi-level group 3-bit quantization structure with sparse outliers.\n+The [SpQR](https://hf.co/papers/2306.03078) quantization algorithm involves a 16x16 tiled bi-level group 3-bit quantization structure with sparse outliers.\n \n <div class=\"flex justify-center\">\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/spqr-diagram.png\">"
        },
        {
            "sha": "b62b76d1f0632a86f92c52ce6c1dd4b58adc674c",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n [ONNX](http://onnx.ai) is an open standard that defines a common set of operators and a file format to represent deep learning models in different frameworks, including PyTorch and TensorFlow. When a model is exported to ONNX, the operators construct a computational graph (or *intermediate representation*) which represents the flow of data through the model. Standardized operators and data types makes it easy to switch between frameworks.\n \n-The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to ONNX with configuration objects which are supported for [many architectures]((https://huggingface.co/docs/optimum/exporters/onnx/overview)) and can be easily extended. If a model isn't supported, feel free to make a [contribution](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) to Optimum.\n+The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to ONNX with configuration objects which are supported for [many architectures](https://huggingface.co/docs/optimum/exporters/onnx/overview) and can be easily extended. If a model isn't supported, feel free to make a [contribution](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute) to Optimum.\n \n The benefits of exporting to ONNX include the following.\n "
        },
        {
            "sha": "8dfdbeed464d0f7578daf21898f9f0949b8775fd",
            "filename": "docs/source/en/tflite.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Ftflite.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fen%2Ftflite.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftflite.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -18,7 +18,7 @@ rendered properly in your Markdown viewer.\n \n [LiteRT](https://ai.google.dev/edge/litert) (previously known as TensorFlow Lite) is a high-performance runtime designed for on-device machine learning.\n \n-The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to LiteRT for [many architectures]((https://huggingface.co/docs/optimum/exporters/onnx/overview)).\n+The [Optimum](https://huggingface.co/docs/optimum/index) library exports a model to LiteRT for [many architectures](https://huggingface.co/docs/optimum/exporters/onnx/overview).\n \n The benefits of exporting to LiteRT include the following.\n "
        },
        {
            "sha": "34083f9f6cb4c065588b4f608d439327806b94d8",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -25,7 +25,7 @@ GPT3/4, [Falcon](https://huggingface.co/tiiuae/falcon-40b), [Llama](https://hugg\n \n 2.  **í”Œë˜ì‹œ ì–´í…ì…˜:** í”Œë˜ì‹œ ì–´í…ì…˜ì€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±ì„ ë†’ì¼ ë¿ë§Œ ì•„ë‹ˆë¼ ìµœì í™”ëœ GPU ë©”ëª¨ë¦¬ í™œìš©ì„ í†µí•´ íš¨ìœ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ì–´í…ì…˜ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€í˜•ì…ë‹ˆë‹¤.\n \n-3.  **ì•„í‚¤í…ì²˜ í˜ì‹ :** ì¶”ë¡  ì‹œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ì£¼ë¡œ ë™ì¼í•œ ë°©ì‹(ê¸´ ì…ë ¥ ë§¥ë½ì„ ê°€ì§„ ìê¸°íšŒê·€ í…ìŠ¤íŠ¸ ìƒì„± ë°©ì‹)ìœ¼ë¡œ ë°°í¬ë˜ëŠ”ë°, ë” íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íŠ¹í™”ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë°œì „ìœ¼ë¡œëŠ” [Alibi](https://huggingface.co/papers/2108.12409), [Rotary embeddings](https://huggingface.co/papers/2104.09864), [Multi-Query Attention (MQA)](https://huggingface.co/papers/1911.02150), [Grouped-Query-Attention (GQA)]((https://huggingface.co/papers/2305.13245))ì´ ìˆìŠµë‹ˆë‹¤. \n+3.  **ì•„í‚¤í…ì²˜ í˜ì‹ :** ì¶”ë¡  ì‹œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì€ ì£¼ë¡œ ë™ì¼í•œ ë°©ì‹(ê¸´ ì…ë ¥ ë§¥ë½ì„ ê°€ì§„ ìê¸°íšŒê·€ í…ìŠ¤íŠ¸ ìƒì„± ë°©ì‹)ìœ¼ë¡œ ë°°í¬ë˜ëŠ”ë°, ë” íš¨ìœ¨ì ì¸ ì¶”ë¡ ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ” íŠ¹í™”ëœ ëª¨ë¸ ì•„í‚¤í…ì²˜ê°€ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ëª¨ë¸ ì•„í‚¤í…ì²˜ì˜ ê°€ì¥ ì¤‘ìš”í•œ ë°œì „ìœ¼ë¡œëŠ” [Alibi](https://huggingface.co/papers/2108.12409), [Rotary embeddings](https://huggingface.co/papers/2104.09864), [Multi-Query Attention (MQA)](https://huggingface.co/papers/1911.02150), [Grouped-Query-Attention (GQA)](https://huggingface.co/papers/2305.13245)ì´ ìˆìŠµë‹ˆë‹¤. \n \n ì´ ê°€ì´ë“œì—ì„œëŠ” í…ì„œì˜ ê´€ì ì—ì„œ ìê¸°íšŒê·€ ìƒì„±ì— ëŒ€í•œ ë¶„ì„ì„ ì œê³µí•©ë‹ˆë‹¤. ë‚®ì€ ì •ë°€ë„ë¥¼ ì±„íƒí•˜ëŠ” ê²ƒì˜ ì¥ë‹¨ì ì„ ë…¼ì˜í•˜ê³ , ìµœì‹  ì–´í…ì…˜ ì•Œê³ ë¦¬ì¦˜ì„ í¬ê´„ì ìœ¼ë¡œ íƒêµ¬í•˜ë©°, í–¥ìƒëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ë…¼í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ê° ê¸°ëŠ¥ì˜ ê°œì„  ì‚¬í•­ì„ ë³´ì—¬ì£¼ëŠ” ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\n \n@@ -756,4 +756,4 @@ GQAì˜ ê°€ì¥ ì£¼ëª©í•  ë§Œí•œ ì ìš© ì‚¬ë¡€ëŠ” [Llama-v2](https://huggingface.c\n \n ì—°êµ¬ ì»¤ë®¤ë‹ˆí‹°ëŠ” ì ì  ë” í° ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì˜ ì¶”ë¡  ì‹œê°„ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ê¸°ë°œí•œ ë°©ë²•ë“¤ì„ ëŠì„ì—†ì´ ì°¾ì•„ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, [ì¶”ì¸¡ ë””ì½”ë”©](https://huggingface.co/papers/2211.17192)ì´ë¼ëŠ” ìœ ë§í•œ ì—°êµ¬ ë°©í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ \"ì‰¬ìš´ í† í°\"ì€ ë” ì‘ê³  ë¹ ë¥¸ ì–¸ì–´ ëª¨ë¸ì— ì˜í•´ ìƒì„±ë˜ê³ , \"ì–´ë ¤ìš´ í† í°\"ë§Œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ ìì²´ì— ì˜í•´ ìƒì„±ë©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ ì´ ë…¸íŠ¸ë¶ì˜ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ë§Œ, [ë©‹ì§„ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://huggingface.co/blog/assisted-generation)ì—ì„œ ì½ì–´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n \n-GPT3/4, Llama-2-70b, Claude, PaLMê³¼ ê°™ì€ ê±°ëŒ€í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ [Hugging Face Chat](https://huggingface.co/chat/) ë˜ëŠ” ChatGPTì™€ ê°™ì€ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì •ë°€ë„, ì•Œê³ ë¦¬ì¦˜, ì•„í‚¤í…ì²˜ì˜ ê°œì„  ë•ë¶„ì…ë‹ˆë‹¤. ì•ìœ¼ë¡œ GPU, TPU ë“±ê³¼ ê°™ì€ ê°€ì†ê¸°ëŠ” ì ì  ë” ë¹¨ë¼ì§€ê³  ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ì¥ ì¢‹ì€ ì•Œê³ ë¦¬ì¦˜ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê³ ì˜ íš¨ìœ¨ì„ ì–»ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤ ğŸ¤—\n\\ No newline at end of file\n+GPT3/4, Llama-2-70b, Claude, PaLMê³¼ ê°™ì€ ê±°ëŒ€í•œ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ [Hugging Face Chat](https://huggingface.co/chat/) ë˜ëŠ” ChatGPTì™€ ê°™ì€ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì—ì„œ ë¹ ë¥´ê²Œ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” ì´ìœ ëŠ” ìœ„ì—ì„œ ì–¸ê¸‰í•œ ì •ë°€ë„, ì•Œê³ ë¦¬ì¦˜, ì•„í‚¤í…ì²˜ì˜ ê°œì„  ë•ë¶„ì…ë‹ˆë‹¤. ì•ìœ¼ë¡œ GPU, TPU ë“±ê³¼ ê°™ì€ ê°€ì†ê¸°ëŠ” ì ì  ë” ë¹¨ë¼ì§€ê³  ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•  ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°€ì¥ ì¢‹ì€ ì•Œê³ ë¦¬ì¦˜ê³¼ ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœê³ ì˜ íš¨ìœ¨ì„ ì–»ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤ ğŸ¤—"
        },
        {
            "sha": "58e5c636eb90fcc56e79a31c3f17bbee4d5be166",
            "filename": "examples/pytorch/semantic-segmentation/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fcf45551775b05a3a78481ad53552635026c7d2/examples%2Fpytorch%2Fsemantic-segmentation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fcf45551775b05a3a78481ad53552635026c7d2/examples%2Fpytorch%2Fsemantic-segmentation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2FREADME.md?ref=4fcf45551775b05a3a78481ad53552635026c7d2",
            "patch": "@@ -155,7 +155,7 @@ accelerate launch run_semantic_segmentation_no_trainer.py --output_dir segformer\n \n and boom, you're training, possibly on multiple GPUs, logging everything to all trackers found in your environment (like Weights and Biases, Tensorboard) and regularly pushing your model to the hub (with the repo name being equal to `args.output_dir` at your HF username) ğŸ¤—\n \n-With the default settings, the script fine-tunes a [SegFormer]((https://huggingface.co/docs/transformers/main/en/model_doc/segformer)) model on the [segments/sidewalk-semantic](https://huggingface.co/datasets/segments/sidewalk-semantic) dataset.\n+With the default settings, the script fine-tunes a [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer) model on the [segments/sidewalk-semantic](https://huggingface.co/datasets/segments/sidewalk-semantic) dataset.\n \n The resulting model can be seen here: https://huggingface.co/nielsr/segformer-finetuned-sidewalk. Note that the script usually requires quite a few epochs to achieve great results, e.g. the SegFormer authors fine-tuned their model for 160k steps (batches) on [`scene_parse_150`](https://huggingface.co/datasets/scene_parse_150).\n "
        }
    ],
    "stats": {
        "total": 22,
        "additions": 11,
        "deletions": 11
    }
}