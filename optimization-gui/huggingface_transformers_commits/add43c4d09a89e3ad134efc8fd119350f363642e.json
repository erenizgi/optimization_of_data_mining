{
    "author": "zucchini-nlp",
    "message": "[autodocstring] add video and audio inputs (#39420)\n\n* add  video and audio inputs in auto docstring\n\n* fix copies",
    "sha": "add43c4d09a89e3ad134efc8fd119350f363642e",
    "files": [
        {
            "sha": "76bf6ef097f3181ce5e2883577c5ab1ea5f1f82b",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1450,9 +1450,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n             the features.\n@@ -1707,9 +1704,6 @@ def get_audio_features(\n         return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n             the features.\n@@ -1764,9 +1758,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, ClapOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n             the features.\n@@ -1941,9 +1932,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, ClapAudioModelOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n             the features."
        },
        {
            "sha": "cd4af11d873d8121de63b0fc3ce5ed6444fd3535",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1623,9 +1623,6 @@ def get_speech_features(\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Input text Tokens. Processed from the [`ClvpTokenizer`]. If speech_ids is not provided, then input_ids\n                 and input_features will be used.\n-            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`, *optional*):\n-                Indicates log-melspectrogram representations for audio returned by [`ClvpFeatureExtractor`]. If\n-                speech_ids is not provided, then input_ids and input_features will be used.\n             conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n                 inputs_embeds for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.\n             attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -1713,8 +1710,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, ClvpOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`):\n-            Indicates log mel-spectrogram representations for audio returned by [`ClvpFeatureExtractor`].\n         conditioning_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n             inputs_embeds for `ClvpConditioningEncoder`. Can be used in place of `input_ids`.\n         text_encoder_inputs_embeds (`torch.FloatTensor`, *optional*):\n@@ -1857,8 +1852,6 @@ def generate(\n         Args:\n             input_ids (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Input text Tokens. Processed from the [`ClvpTokenizer`].\n-            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, time_dim)`, *optional*):\n-                Indicates log-melspectrogram representations for audio returned by [`ClvpFeatureExtractor`].\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Mask to avoid performing attention on padding text token indices. Mask values selected in `[0, 1]`:\n "
        },
        {
            "sha": "9cb17a6a5b84a112d3d192272b19fc940c6820df",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -2180,7 +2180,7 @@ def get_audio_features(\n         Args:\n             input_features (`torch.FloatTensor]` of shape `(num_images, seq_length, num_features)`):\n                The tensors corresponding to the input audio.\n-            input_features (`torch.FloatTensor]` of shape `(num_images, seq_length)`):\n+            input_features_mask (`torch.FloatTensor]` of shape `(num_images, seq_length)`):\n                The attention mask for the input audio.\n \n         Returns:\n@@ -2263,8 +2263,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n-        input_features (torch.Tensor, *optional*, defaults to None):\n-            The audio inputs to be encoded.\n         input_features_mask (torch.Tensor, *optional*, defaults to None):\n             The attention mask for the input audio.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "84c5fcfb430054037cdcdd5ac80740a7d96a60c3",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -2450,7 +2450,7 @@ def get_audio_features(\n         Args:\n             input_features (`torch.FloatTensor]` of shape `(num_images, seq_length, num_features)`):\n                The tensors corresponding to the input audio.\n-            input_features (`torch.FloatTensor]` of shape `(num_images, seq_length)`):\n+            input_features_mask (`torch.FloatTensor]` of shape `(num_images, seq_length)`):\n                The attention mask for the input audio.\n \n         Returns:\n@@ -2503,8 +2503,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Gemma3nCausalLMOutputWithPast:\n         r\"\"\"\n-        input_features (torch.Tensor, *optional*, defaults to None):\n-            The audio inputs to be encoded.\n         input_features_mask (torch.Tensor, *optional*, defaults to None):\n             The attention mask for the input audio.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "b607c48cc30b4258b3acf43b6b9b81975af77f35",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1225,10 +1225,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Glm4vImageProcessor.__call__`] for details. [`Glm4vProcessor`] uses\n-            [`Glm4vImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n@@ -1466,10 +1462,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Glm4vImageProcessor.__call__`] for details. [`Glm4vProcessor`] uses\n-            [`Glm4vImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "945c9a7e7f645b52468fa256518df9477ee317af",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1222,10 +1222,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Glm4vModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Glm4vImageProcessor.__call__`] for details. [`Glm4vProcessor`] uses\n-            [`Glm4vImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n@@ -1392,10 +1388,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Glm4vImageProcessor.__call__`] for details. [`Glm4vProcessor`] uses\n-            [`Glm4vImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "d4d8b3767ef5187c992aed5d603efc85bf6adc45",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -373,10 +373,6 @@ def forward(\n         **lm_kwargs,\n     ) -> Union[tuple[torch.Tensor], GraniteSpeechCausalLMOutputWithPast]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, audio seq len, mel feat dim)):\n-            The tensors corresponding to the input audios. input features can be obtained using\n-            [`AutoFeatureExtractor`]. See [`GraniteSpeechFeatureExtractor.__call__`] for details.\n-            [`GraniteSpeechProcessor`] uses [`GraniteSpeechFeatureExtractor`] for processing audio.\n         input_features_mask (`torch.Tensor`, *optional*):\n             Mask to be applied to audio features prior to scattering into the language embeddings.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):"
        },
        {
            "sha": "e2b2effba949ab9864ce35261846eeabc7f1aed5",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -498,10 +498,10 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, LlavaNextVideoModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoVideoProcessor`] for processing videos.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n+            If `\"full\"`, the full vision features are used.\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -757,10 +757,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoVideoProcessor`] for processing videos.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "cfbb7aa36e2733ecf057d11204419a903a6ea5a4",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -418,12 +418,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, LlavaNextVideoModelOutputWithPast]:\n-        r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoVideoProcessor`] for processing videos.\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -560,10 +554,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaNextVideoCausalLMOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`LlavaNextVideoVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoVideoProcessor`] for processing videos.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "e82f18b0ed0c8f91fd1f71e86b557b9cb0756ce1",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -516,16 +516,8 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, LlavaOnevisionModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoProcessor`] for processing videos.\n         image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n             The sizes of the videos in the batch, being (height, width) for each frame in the video.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n-            If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         batch_num_images (`torch.LongTensor`, *optional*):\n@@ -801,16 +793,8 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoProcessor`] for processing videos.\n         image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n             The sizes of the videos in the batch, being (height, width) for each frame in the video.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n-            If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         batch_num_images (`torch.LongTensor`, *optional*):"
        },
        {
            "sha": "233a96110b7af17b848790c4809968facb1b80af",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -500,16 +500,8 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, LlavaOnevisionModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoProcessor`] for processing videos.\n         image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n             The sizes of the videos in the batch, being (height, width) for each frame in the video.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n-            If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         batch_num_images (`torch.LongTensor`, *optional*):\n@@ -652,16 +644,8 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaOnevisionCausalLMOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoProcessor`] for processing videos.\n         image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n             The sizes of the videos in the batch, being (height, width) for each frame in the video.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n-            If `\"full\"`, the full vision features are used.\n         vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n             Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         batch_num_images (`torch.LongTensor`, *optional*):"
        },
        {
            "sha": "0111ba51d6a8f1f8a3d595be59ab411466cf483b",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1609,10 +1609,6 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple, MusicgenMelodyOutputWithPast]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, audio_sequence_length, num_chroma)`):\n-            Input audio features.\n-            This should be returned by the [`MusicgenMelodyFeatureExtractor`] class that you can also\n-            retrieve from [`AutoFeatureExtractor`]. See [`MusicgenMelodyFeatureExtractor.__call__`] for details.\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary, corresponding to the sequence of audio codes.\n "
        },
        {
            "sha": "4a0e01a2415b34a96dbbdcc6e9a0d5c4e9b3f908",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 25,
            "deletions": 70,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -231,41 +231,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[tuple, PerceptionLMModelOutputWithPast]:\n-        \"\"\"\n-        Forward pass of the PerceptionLM model.\n-\n-        Args:\n-            input_ids (`torch.LongTensor`, *optional*):\n-                Indices of input sequence tokens in the vocabulary.\n-            pixel_values (`torch.FloatTensor`, *optional*):\n-                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n-            pixel_values_videos (`torch.FloatTensor`, *optional*):\n-                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n-            attention_mask (`torch.Tensor`, *optional*):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor`, *optional*):\n-                Indices of positions of each input sequence token in the position embeddings.\n-            past_key_values (`list[torch.FloatTensor]`, *optional*):\n-                Precomputed key and value hidden states for fast autoregressive generation.\n-            inputs_embeds (`torch.FloatTensor`, *optional*):\n-                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n-            use_cache (`bool`, *optional*):\n-                Whether or not to use past key values to speed up decoding.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n-            cache_position (`torch.LongTensor`, *optional*):\n-                Position indices for caching.\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-                Number of logits to keep.\n-            **lm_kwargs:\n-                Additional keyword arguments for the language model.\n-\n-        Returns:\n-            [`PerceptionLMModelOutputWithPast`] or `tuple`:\n-                Model outputs as a `PerceptionLMModelOutputWithPast` if `return_dict=True`, otherwise a tuple.\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -381,43 +346,33 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[tuple, PerceptionLMCausalLMOutputWithPast]:\n-        \"\"\"\n-        Forward pass for the PerceptionLMForConditionalGeneration model.\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n-        Args:\n-            input_ids (`torch.LongTensor`, *optional*):\n-                Indices of input sequence tokens in the vocabulary.\n-            pixel_values (`torch.FloatTensor`, *optional*):\n-                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n-            pixel_values_videos (`torch.FloatTensor`, *optional*):\n-                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n-            attention_mask (`torch.Tensor`, *optional*):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor`, *optional*):\n-                Indices of positions of each input sequence token in the position embeddings.\n-            past_key_values (`list[torch.FloatTensor]`, *optional*):\n-                Precomputed key and value hidden states for fast autoregressive generation.\n-            inputs_embeds (`torch.FloatTensor`, *optional*):\n-                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n-            labels (`torch.LongTensor`, *optional*):\n-                Labels for computing the language modeling loss.\n-            use_cache (`bool`, *optional*):\n-                Whether or not to use past key values to speed up decoding.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n-            cache_position (`torch.LongTensor`, *optional*):\n-                Position indices for caching.\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-                Number of logits to keep.\n-            **lm_kwargs:\n-                Additional keyword arguments for the language model.\n+        Example:\n \n-        Returns:\n-            [`PerceptionLMCausalLMOutputWithPast`] or `tuple`:\n-                Model outputs as a `PerceptionLMCausalLMOutputWithPast` if `return_dict=True`, otherwise a tuple.\n-        \"\"\"\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, PerceptionLMForConditionalGeneration\n+\n+        >>> model = PerceptionLMForConditionalGeneration.from_pretrained(\"perception_lm-hf/perception_lm-1.5-7b-hf\")\n+        >>> processor = AutoProcessor.from_pretrained(\"perception_lm-hf/perception_lm-1.5-7b-hf\")\n+\n+        >>> prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a busy city street with a stop sign prominently displayed\"\n+        ```\"\"\"\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
        },
        {
            "sha": "05c9e8e9d04355dffb79705e8489dfb34054c28c",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 72,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -160,41 +160,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[tuple, PerceptionLMModelOutputWithPast]:\n-        \"\"\"\n-        Forward pass of the PerceptionLM model.\n-\n-        Args:\n-            input_ids (`torch.LongTensor`, *optional*):\n-                Indices of input sequence tokens in the vocabulary.\n-            pixel_values (`torch.FloatTensor`, *optional*):\n-                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n-            pixel_values_videos (`torch.FloatTensor`, *optional*):\n-                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n-            attention_mask (`torch.Tensor`, *optional*):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor`, *optional*):\n-                Indices of positions of each input sequence token in the position embeddings.\n-            past_key_values (`list[torch.FloatTensor]`, *optional*):\n-                Precomputed key and value hidden states for fast autoregressive generation.\n-            inputs_embeds (`torch.FloatTensor`, *optional*):\n-                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n-            use_cache (`bool`, *optional*):\n-                Whether or not to use past key values to speed up decoding.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n-            cache_position (`torch.LongTensor`, *optional*):\n-                Position indices for caching.\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-                Number of logits to keep.\n-            **lm_kwargs:\n-                Additional keyword arguments for the language model.\n-\n-        Returns:\n-            [`PerceptionLMModelOutputWithPast`] or `tuple`:\n-                Model outputs as a `PerceptionLMModelOutputWithPast` if `return_dict=True`, otherwise a tuple.\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -308,43 +273,6 @@ def forward(\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[tuple, PerceptionLMCausalLMOutputWithPast]:\n-        \"\"\"\n-        Forward pass for the PerceptionLMForConditionalGeneration model.\n-\n-        Args:\n-            input_ids (`torch.LongTensor`, *optional*):\n-                Indices of input sequence tokens in the vocabulary.\n-            pixel_values (`torch.FloatTensor`, *optional*):\n-                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n-            pixel_values_videos (`torch.FloatTensor`, *optional*):\n-                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n-            attention_mask (`torch.Tensor`, *optional*):\n-                Mask to avoid performing attention on padding token indices.\n-            position_ids (`torch.LongTensor`, *optional*):\n-                Indices of positions of each input sequence token in the position embeddings.\n-            past_key_values (`list[torch.FloatTensor]`, *optional*):\n-                Precomputed key and value hidden states for fast autoregressive generation.\n-            inputs_embeds (`torch.FloatTensor`, *optional*):\n-                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n-            labels (`torch.LongTensor`, *optional*):\n-                Labels for computing the language modeling loss.\n-            use_cache (`bool`, *optional*):\n-                Whether or not to use past key values to speed up decoding.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n-            cache_position (`torch.LongTensor`, *optional*):\n-                Position indices for caching.\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n-                Number of logits to keep.\n-            **lm_kwargs:\n-                Additional keyword arguments for the language model.\n-\n-        Returns:\n-            [`PerceptionLMCausalLMOutputWithPast`] or `tuple`:\n-                Model outputs as a `PerceptionLMCausalLMOutputWithPast` if `return_dict=True`, otherwise a tuple.\n-        \"\"\"\n         outputs = self.model(\n             input_ids=input_ids,\n             pixel_values=pixel_values,"
        },
        {
            "sha": "0cebfaef8c3ad6c2a29f1150ae4ae61317b32887",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1154,9 +1154,6 @@ def forward(\n             `[0, 1]`:\n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Does the same task as `inputs_embeds`. If `inputs_embeds` is not present but `input_features` is present\n-            then `input_features` will be considered as `inputs_embeds`.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[-100, 0, ...,\n             config.vocab_size - 1]`. All labels set to `-100` are ignored (masked), the loss is only computed for"
        },
        {
            "sha": "b890b81297d150c8b9bda8ce9b8ce26edefdadba",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -808,13 +808,6 @@ def forward(\n         **kwargs,\n     ):\n         r\"\"\"\n-        input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-            Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n-            the soundfile library (`pip install soundfile`). To prepare the array into\n-            `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n-            and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n         feature_lens (`torch.LongTensor` of shape `(batch_size,)`):\n             mel length\n         aftercnn_lens (`torch.LongTensor` of shape `(batch_size,)`):\n@@ -1822,17 +1815,6 @@ def forward(\n         video_second_per_grid: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, Qwen2_5OmniThinkerCausalLMOutputWithPast]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses\n-            [`SiglipImageProcessor`] for processing videos).\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "62aadd28048e91cbc8b96cb540e73998e4d53250",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1794,13 +1794,6 @@ def forward(\n         **kwargs,\n     ):\n         r\"\"\"\n-        input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-            Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-            obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-            `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n-            the soundfile library (`pip install soundfile`). To prepare the array into\n-            `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n-            and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n         feature_lens (`torch.LongTensor` of shape `(batch_size,)`):\n             mel length\n         aftercnn_lens (`torch.LongTensor` of shape `(batch_size,)`):\n@@ -2276,17 +2269,6 @@ def forward(\n         video_second_per_grid: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple, Qwen2_5OmniThinkerCausalLMOutputWithPast]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size), *optional*):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`SiglipImageProcessor.__call__`] for details ([]`NewTaskModelProcessor`] uses\n-            [`SiglipImageProcessor`] for processing videos).\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "eaa612dc83d678af5f1c159eb1d0dde94093a53a",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1214,10 +1214,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5_VLModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n@@ -1459,10 +1455,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "bd9d51612024986d49b268bf6a530b7a22830a70",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -565,10 +565,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2_5_VLModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n@@ -734,10 +730,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2_5_VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "569fd4bdc25eb27f554362b160bfca67c941dacb",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -360,13 +360,6 @@ def forward(\n     ):\n         r\"\"\"\n         Args:\n-            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n-                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a\n-                `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec libary (`pip install torchcodec`) or\n-                the soundfile library (`pip install soundfile`). To prepare the array into\n-                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n-                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n             attention_mask (`torch.Tensor`)`, *optional*):\n                 Qwen2Audio does not support masking of the `input_features`, this argument is preserved for compatibility,\n                 but it is not used. By default the silence in the input log mel spectrogram are ignored.\n@@ -741,13 +734,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, Qwen2AudioCausalLMOutputWithPast]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, feature_sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n         feature_attention_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n             Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n "
        },
        {
            "sha": "d6435667a5de41b6f8b5502f85a1d3bf8412fba4",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1153,10 +1153,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Qwen2VLModelOutputWithPast]:\n         r\"\"\"\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n@@ -1359,10 +1355,6 @@ def forward(\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(seq_length, num_channels * temporal_size * image_size * image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Qwen2VLImageProcessor.__call__`] for details. [`Qwen2VLProcessor`] uses\n-            [`Qwen2VLImageProcessor`] for processing videos.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):"
        },
        {
            "sha": "4c6e91ffeeac5c4904fb4e403642e57ac21a0564",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -374,13 +374,6 @@ def forward(\n             (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n             To prepare the array into *input_values*, the [`Wav2Vec2Processor`] should be used for padding and conversion\n             into a tensor of type *torch.FloatTensor*. See [`Wav2Vec2Processor.__call__`] for details.\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`, *optional*):\n-            Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*\n-            via the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`Speech2TextFeatureExtractor`] should be used for extracting\n-            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~Speech2TextFeatureExtractor.__call__`]\n \n         Examples:\n "
        },
        {
            "sha": "b304f457beb391f425c279bda094f1c45d4e4ada",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1095,14 +1095,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`):\n-            Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n-            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n-            (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting\n-            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~Speech2TextFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1259,14 +1251,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, feature_size)`):\n-            Float values of fbank features extracted from the raw speech waveform. Raw speech waveform can be obtained\n-            by loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a\n-            `torch.Tensor`, *e.g.* via the torchcodec library (`pip install torchcodec`) or the soundfile library\n-            (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting\n-            the fbank features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~Speech2TextFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n "
        },
        {
            "sha": "5afbd669aed2878c553d85bc451ffdf63dab5acc",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -478,9 +478,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.FloatTensor], UnivNetModelOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor`):\n-            Tensor containing the log-mel spectrograms. Can be batched and of shape `(batch_size, sequence_length,\n-            config.num_mel_channels)`, or un-batched and of shape `(sequence_length, config.num_mel_channels)`.\n         noise_sequence (`torch.FloatTensor`, *optional*):\n             Tensor containing a noise sequence of standard Gaussian noise. Can be batched and of shape `(batch_size,\n             sequence_length, config.model_in_channels)`, or un-batched and of shape (sequence_length,"
        },
        {
            "sha": "8ed85c94e53c346ff50f817ed36848b0ecc407fa",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -307,10 +307,6 @@ def forward(\n             The tensors corresponding to the input images. Pixel values can be obtained using\n             [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n             [`VideoLlavaImageProcessor`] for processing images).\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input video. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n-            [`VideoLlavaImageProcessor`] for processing videos).\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -499,10 +495,6 @@ def forward(\n             The tensors corresponding to the input images. Pixel values can be obtained using\n             [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n             [`VideoLlavaImageProcessor`] for processing images).\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input video. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`VideoLlavaImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n-            [`VideoLlavaImageProcessor`] for processing videos).\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored"
        },
        {
            "sha": "52cb2c7baa17b905f0b8dcfbceec4204ea254999",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1070,8 +1070,6 @@ def forward(\n         **kwargs,\n     ) -> VJEPA2WithMaskedInputModelOutput:\n         r\"\"\"\n-        pixel_values_videos (`torch.Tensor` with shape `[batch size x num_frames x num_channels x height x width]`, required):\n-            The input video pixels which is processed by VJEPA2VideoProcessor.\n         context_head_mask (`torch.Tensor` with shape `[num_heads]` or `[num_hidden_layers x num_heads]`, *optional*):\n             The mask indicating if we should keep the heads or not (1.0 for keep, 0.0 for discard) for the context.\n         context_mask (`torch.Tensor` with shape `[batch_size, patch_size, 1]`, *optional*):\n@@ -1175,8 +1173,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n     ) -> Union[tuple, ImageClassifierOutput]:\n         r\"\"\"\n-        pixel_values_videos (`torch.Tensor` with shape `[batch size x num_frames x num_channels x height x width]`):\n-            The input video pixels which is processed by VJEPA2VideoProcessor.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "67755e38c8a4cb0c41a30d9997bcbfbd4d1586ce",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -995,12 +995,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, Wav2Vec2BertBaseModelOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*  via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n             masked extracted features in *config.proj_codevector_dim* space.\n@@ -1093,12 +1087,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n             Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n             the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n@@ -1205,12 +1193,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1301,12 +1283,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1465,12 +1441,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "6664506a49322f89d614155863a6489f5c0d681a",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -706,12 +706,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, Wav2Vec2BertBaseModelOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.*  via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         mask_time_indices (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Indices to mask extracted features for contrastive loss. When in training mode, model learns to predict\n             masked extracted features in *config.proj_codevector_dim* space.\n@@ -778,12 +772,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, CausalLMOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size, target_length)`, *optional*):\n             Labels for connectionist temporal classification. Note that `target_length` has to be smaller or equal to\n             the sequence length of the output logits. Indices are selected in `[-100, 0, ..., config.vocab_size - 1]`.\n@@ -875,12 +863,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, SequenceClassifierOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2BertProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -951,12 +933,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, TokenClassifierOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n@@ -1018,12 +994,6 @@ def forward(\n         labels: Optional[torch.Tensor] = None,\n     ) -> Union[tuple, XVectorOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Float values of input raw speech waveform. Values can be obtained by loading a `.flac` or `.wav` audio file\n-            into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via the torchcodec library\n-            (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoProcessor`] should be used for padding and conversion\n-            into a tensor of type `torch.FloatTensor`. See [`Wav2Vec2ConformerProcessor.__call__`] for details.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "1d5c808517cc9ad3fce0ba0567e622b0019bae82",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -1083,13 +1083,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqModelOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1254,13 +1247,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple[torch.Tensor], Seq2SeqLMOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary.\n \n@@ -1589,13 +1575,6 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], SequenceClassifierOutput]:\n         r\"\"\"\n-        input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n-            Float values mel features extracted from the raw speech waveform. Raw speech waveform can be obtained by\n-            loading a `.flac` or `.wav` audio file into an array of type `list[float]`, a `numpy.ndarray` or a `torch.Tensor`, *e.g.* via\n-            the torchcodec library (`pip install torchcodec`) or the soundfile library (`pip install soundfile`).\n-            To prepare the array into `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the\n-            mel features, padding and conversion into a tensor of type `torch.FloatTensor`.\n-            See [`~WhisperFeatureExtractor.__call__`]\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If"
        },
        {
            "sha": "a08dd7fff363ea188b56723a36ac2fc6aee28276",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/add43c4d09a89e3ad134efc8fd119350f363642e/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=add43c4d09a89e3ad134efc8fd119350f363642e",
            "patch": "@@ -45,6 +45,7 @@\n \n PLACEHOLDER_TO_AUTO_MODULE = {\n     \"image_processor_class\": (\"image_processing_auto\", \"IMAGE_PROCESSOR_MAPPING_NAMES\"),\n+    \"video_processor_class\": (\"video_processing_auto\", \"VIDEO_PROCESSOR_MAPPING_NAMES\"),\n     \"feature_extractor_class\": (\"feature_extraction_auto\", \"FEATURE_EXTRACTOR_MAPPING_NAMES\"),\n     \"processor_class\": (\"processing_auto\", \"PROCESSOR_MAPPING_NAMES\"),\n     \"config_class\": (\"configuration_auto\", \"CONFIG_MAPPING_NAMES\"),\n@@ -533,6 +534,15 @@ class ModelArgs:\n         \"shape\": \"of shape `(batch_size, num_channels, image_size, image_size)`\",\n     }\n \n+    pixel_values_videos = {\n+        \"description\": \"\"\"\n+    The tensors corresponding to the input video. Pixel values for videos can be obtained using\n+    [`{video_processor_class}`]. See [`{video_processor_class}.__call__`] for details ([`{processor_class}`] uses\n+    [`{video_processor_class}`] for processing videos).\n+    \"\"\",\n+        \"shape\": \"of shape `(batch_size, num_frames, num_channels, frame_size, frame_size)`\",\n+    }\n+\n     vision_feature_layer = {\n         \"description\": \"\"\"\n     The index of the layer to select the vision feature. If multiple indices are provided,\n@@ -569,6 +579,15 @@ class ModelArgs:\n         \"shape\": \"of shape `(batch_size, height, width)`\",\n     }\n \n+    input_features = {\n+        \"description\": \"\"\"\n+    The tensors corresponding to the input audio features. Audio features can be obtained using\n+    [`{feature_extractor_class}`]. See [`{feature_extractor_class}.__call__`] for details ([`{processor_class}`] uses\n+    [`{feature_extractor_class}`] for processing audios).\n+    \"\"\",\n+        \"shape\": \"of shape `(batch_size, sequence_length, feature_dim)`\",\n+    }\n+\n \n class ModelOutputArgs:\n     last_hidden_state = {\n@@ -822,6 +841,20 @@ class ModelOutputArgs:\n         \"shape\": \"of shape `(batch_size,config.project_dim)`\",\n     }\n \n+    image_hidden_states = {\n+        \"description\": \"\"\"\n+    Image hidden states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\",\n+        \"shape\": \"of shape `(batch_size, num_images, sequence_length, hidden_size)`\",\n+    }\n+\n+    video_hidden_states = {\n+        \"description\": \"\"\"\n+    Video hidden states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\",\n+        \"shape\": \"of shape `(batch_size * num_frames, num_images, sequence_length, hidden_size)`\",\n+    }\n+\n \n class ClassDocstring:\n     PreTrainedModel = r\"\"\""
        }
    ],
    "stats": {
        "total": 501,
        "additions": 64,
        "deletions": 437
    }
}