{
    "author": "albertvillanova",
    "message": "Fix CUDA index out of bounds for q_idx in VLM token type masking for Gemma3, PaliGemma, and example modular (#41757)\n\n* Fix CUDA index out of bounds for q_idx in Gemma3 token type masking\n\n* Fix CUDA index out of bounds for q_idx in modular modeling_new_task_model\n\n* Revert \"Fix CUDA index out of bounds for q_idx in Gemma3 token type masking\"\n\nThis reverts commit f8e5c2a42c305aebd00c46161bf22f520009c8fc.\n\n* Fix CUDA index out of bounds for q_idx in PaliGemma token type masking\n\n* Fix CUDA index out of bounds for q_idx in Gemma3 token type masking",
    "sha": "9a273028038bea9601be52985dfb969f0877ab59",
    "files": [
        {
            "sha": "c74ce212d834f536df008276489a3ca54b1ac33b",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a273028038bea9601be52985dfb969f0877ab59/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a273028038bea9601be52985dfb969f0877ab59/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=9a273028038bea9601be52985dfb969f0877ab59",
            "patch": "@@ -125,15 +125,23 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         # If it's 1 for both query and key/value, we are in an image block\n         # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n         # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n-        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n-        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        safe_q_idx = torch.where(q_idx < token_type_ids.shape[1], q_idx, 0)\n+        safe_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+\n+        token_type_ids_at_q_idx = token_type_ids[batch_idx, safe_q_idx]\n+        token_type_ids_at_q_idx = torch.where(q_idx < token_type_ids.shape[1], token_type_ids_at_q_idx, 0)\n+\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_kv_idx]\n         token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n \n-        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_q_idx = image_group_ids[batch_idx, safe_q_idx]\n+        image_group_ids_at_q_idx = torch.where(q_idx < image_group_ids.shape[1], image_group_ids_at_q_idx, -1)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_kv_idx]\n         image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n \n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n-        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n+        is_image_block = (token_type_ids_at_q_idx == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids_at_q_idx == image_group_ids_at_kv_idx\n \n         # This is bidirectional attention whenever we are dealing with image tokens\n         return is_image_block & same_image_block"
        },
        {
            "sha": "8dff40771914b6a84bcdf1e9601fd0d0aba0df0e",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a273028038bea9601be52985dfb969f0877ab59/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a273028038bea9601be52985dfb969f0877ab59/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=9a273028038bea9601be52985dfb969f0877ab59",
            "patch": "@@ -768,15 +768,23 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         # If it's 1 for both query and key/value, we are in an image block\n         # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n         # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n-        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n-        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        safe_q_idx = torch.where(q_idx < token_type_ids.shape[1], q_idx, 0)\n+        safe_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+\n+        token_type_ids_at_q_idx = token_type_ids[batch_idx, safe_q_idx]\n+        token_type_ids_at_q_idx = torch.where(q_idx < token_type_ids.shape[1], token_type_ids_at_q_idx, 0)\n+\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_kv_idx]\n         token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n \n-        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_q_idx = image_group_ids[batch_idx, safe_q_idx]\n+        image_group_ids_at_q_idx = torch.where(q_idx < image_group_ids.shape[1], image_group_ids_at_q_idx, -1)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_kv_idx]\n         image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n \n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n-        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n+        is_image_block = (token_type_ids_at_q_idx == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids_at_q_idx == image_group_ids_at_kv_idx\n \n         # This is bidirectional attention whenever we are dealing with image tokens\n         return is_image_block & same_image_block"
        },
        {
            "sha": "2779022e33299cd194880d5a4f717b145b93acb3",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a273028038bea9601be52985dfb969f0877ab59/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a273028038bea9601be52985dfb969f0877ab59/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=9a273028038bea9601be52985dfb969f0877ab59",
            "patch": "@@ -116,15 +116,23 @@ def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         # If it's 1 for both query and key/value, we are in an image block\n         # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n         # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n-        safe_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n-        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_idx]\n+        safe_q_idx = torch.where(q_idx < token_type_ids.shape[1], q_idx, 0)\n+        safe_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+\n+        token_type_ids_at_q_idx = token_type_ids[batch_idx, safe_q_idx]\n+        token_type_ids_at_q_idx = torch.where(q_idx < token_type_ids.shape[1], token_type_ids_at_q_idx, 0)\n+\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_kv_idx]\n         token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n \n-        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_idx]\n+        image_group_ids_at_q_idx = image_group_ids[batch_idx, safe_q_idx]\n+        image_group_ids_at_q_idx = torch.where(q_idx < image_group_ids.shape[1], image_group_ids_at_q_idx, -1)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_kv_idx]\n         image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n \n-        is_image_block = (token_type_ids[batch_idx, q_idx] == 1) & (token_type_ids_at_kv_idx == 1)\n-        same_image_block = image_group_ids[batch_idx, q_idx] == image_group_ids_at_kv_idx\n+        is_image_block = (token_type_ids_at_q_idx == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids_at_q_idx == image_group_ids_at_kv_idx\n \n         # This is bidirectional attention whenever we are dealing with image tokens\n         return is_image_block & same_image_block"
        }
    ],
    "stats": {
        "total": 54,
        "additions": 39,
        "deletions": 15
    }
}