{
    "author": "stevhliu",
    "message": "[docs] Manual tp-plan (#41674)\n\n* manual tp-plan\n\n* feedback",
    "sha": "e7592f25089bae648d3cebf7e0445a510e719eea",
    "files": [
        {
            "sha": "cb426b81916c7cff60526e3fe691b9164e92d257",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 33,
            "deletions": 4,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/e7592f25089bae648d3cebf7e0445a510e719eea/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e7592f25089bae648d3cebf7e0445a510e719eea/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=e7592f25089bae648d3cebf7e0445a510e719eea",
            "patch": "@@ -45,17 +45,21 @@ This guide shows how to enable tensor parallelism with Transformers and differen\n \n ## Partitioning a model\n \n-Transformers supports tensor parallelism if a model has a `tp_plan`. Set `tp_plan=\"auto\"` to automatically use a tensor parallelism plan based on a model's predefined configuration.\n+Transformers supports tensor parallelism if a model has a `tp_plan`. There are two ways to partition a model.\n+\n+- Set `tp_plan=\"auto\"` to automatically use a tensor parallelism plan based on a model's predefined configuration.\n+- Define and pass a manual `tp_plan`.\n+\n+<hfoptions id=\"tp_plan\">\n+<hfoption id=\"auto plan\">\n \n ```py\n import os\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n # model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # better to visualize all the possible strategies\n-model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # better for smaller number of GPUs\n-\n-model = AutoModelForCausalLM.from_pretrained(model_id, dtype=torch.bfloat16, tp_plan=\"auto\")\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\" , dtype=torch.bfloat16, tp_plan=\"auto\")\n print(model._tp_plan)\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n@@ -72,6 +76,31 @@ Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/\n torchrun --nproc-per-node 4 demo.py\n ```\n \n+</hfoption>\n+<hfoption id=\"manual plan\">\n+\n+Define a tensor parallel plan for each layer in `tp_plan` and pass it to [`~PreTrainedModel.from_pretrained`]. The example below uses column and row partitioning. See the [Partitioning strategies](#partitioning-strategies) section for other supported strategies.\n+\n+Manual partitioning requires deep understanding of model architecture and strategy interactions. Poor partitioning choices create slow models that fail or produce incorrect results. The [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism) explains partitioning strategies in detail.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+tp_plan = {\n+    \"model.layers.*.self_attn.q_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.k_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.v_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n+    ...\n+}\n+\n+model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=\"auto\", tp_plan=tp_plan)\n+print(model.tp_plan)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n ## Partitioning strategies\n \n All partitioning strategies are defined in the [`ParallelInterface`] class which maps a string to the strategy implementation. You don't need to interact with this class directly since all the strategies are set with `tp_plan` in [`~PreTrainedModel.from_pretrained`], but it is useful for checking what strategies are available."
        }
    ],
    "stats": {
        "total": 37,
        "additions": 33,
        "deletions": 4
    }
}