{
    "author": "akshay-babbar",
    "message": "Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test) (#40218)\n\n* Fix #40067 : add UMT5 support in GGUF loader (config, tokenizer, test)\n\n* chore: fix code formatting and linting issues\n\n* refactor: move UMT5 GGUF test to quantization directory and clean up comments\n\n* chore: trigger CI pipeline\n\n* refactor(tests): Move UMT5 Encoder GGUF test to GgufModelTests. This consolidates the new test into the main class for consistency.\n\n* Add regression check to UMT5 encoder GGUF test\n\nVerify encoder output against reference tensor values with appropriate tolerances for stability.\n\n* Update tests/quantization/ggml/test_ggml.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/ggml/test_ggml.py\r\n\r\nremove comments\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7",
    "files": [
        {
            "sha": "703fd015636576ba00f332bb874eac6d248b72e9",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7",
            "patch": "@@ -250,6 +250,19 @@\n         \"attention.sliding_window\": \"sliding_window\",\n         \"vocab_size\": \"vocab_size\",\n     },\n+    \"umt5\": {\n+        \"context_length\": \"n_positions\",\n+        \"block_count\": \"num_layers\",\n+        \"feed_forward_length\": \"d_ff\",\n+        \"embedding_length\": \"d_model\",\n+        \"attention.key_length\": \"d_kv\",\n+        \"attention.head_count\": \"num_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n+        \"attention.relative_buckets_count\": \"relative_attention_num_buckets\",\n+        \"decoder_start_token_id\": \"decoder_start_token_id\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n     \"deci\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n@@ -728,6 +741,7 @@ def converted(self) -> Tokenizer:\n     \"nemotron\": GGUFGPTConverter,\n     \"gemma2\": GGUFGemmaConverter,\n     \"gemma3_text\": GGUFGemmaConverter,\n+    \"umt5\": GGUFT5Converter,\n     \"deci\": GGUFLlamaConverter,\n     \"decilm\": GGUFLlamaConverter,\n }"
        },
        {
            "sha": "9b90fb82afa2580c7a7e091efbfabe8e58713665",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7",
            "patch": "@@ -300,6 +300,8 @@ def get_gguf_hf_weights_map(\n         model_type = \"qwen3moe\"\n     elif model_type == \"gemma3_text\":\n         model_type = \"gemma3\"\n+    elif model_type == \"umt5\":\n+        model_type = \"t5\"\n     arch = None\n     for key, value in MODEL_ARCH_NAMES.items():\n         if value == model_type:\n@@ -386,9 +388,14 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     # It needs to be developed for supporting legacy t5.\n     elif \"t5\" in architecture or \"t5encoder\" in architecture:\n         parsed_parameters[\"config\"][\"is_gated_act\"] = True\n-        if \"t5encoder\" in architecture:\n-            parsed_parameters[\"config\"][\"architectures\"] = [\"T5EncoderModel\"]\n-        updated_architecture = \"t5\"\n+        if model_name and \"umt5\" in model_name[0].lower():\n+            updated_architecture = \"umt5\"\n+            if \"t5encoder\" in architecture:\n+                parsed_parameters[\"config\"][\"architectures\"] = [\"UMT5EncoderModel\"]\n+        else:\n+            if \"t5encoder\" in architecture:\n+                parsed_parameters[\"config\"][\"architectures\"] = [\"T5EncoderModel\"]\n+            updated_architecture = \"t5\"\n     else:\n         updated_architecture = architecture\n "
        },
        {
            "sha": "ac6fb30fe606287575075c087bbb4dad48304b7e",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 45,
            "deletions": 1,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=8428c7b9c8afa85caf2dbf27b5fff14f68d8f6a7",
            "patch": "@@ -16,7 +16,14 @@\n \n from parameterized import parameterized\n \n-from transformers import AddedToken, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n+from transformers import (\n+    AddedToken,\n+    AutoModelForCausalLM,\n+    AutoModelForSeq2SeqLM,\n+    AutoTokenizer,\n+    UMT5Config,\n+    UMT5EncoderModel,\n+)\n from transformers.testing_utils import (\n     require_gguf,\n     require_read_token,\n@@ -303,6 +310,7 @@ class GgufModelTests(unittest.TestCase):\n     gemma3_vision_model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n     qwen3_model_id = \"Qwen/Qwen3-0.6B-GGUF\"\n     qwen3moe_model_id = \"Qwen/Qwen3-30B-A3B-GGUF\"\n+    umt5_encoder_model_id = \"city96/umt5-xxl-encoder-gguf\"\n \n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n@@ -341,6 +349,7 @@ class GgufModelTests(unittest.TestCase):\n     fp16_deci_model_id = \"decilm-7b-uniform-gqa-f16.gguf\"\n     q8_0_qwen3_model_id = \"Qwen3-0.6B-Q8_0.gguf\"\n     q4_k_m_qwen3moe_model_id = \"Qwen3-30B-A3B-Q4_K_M.gguf\"\n+    q8_0_umt5_encoder_model_id = \"umt5-xxl-encoder-Q8_0.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -1072,3 +1081,38 @@ def test_qwen3moe_q4_k_m(self):\n \n         EXPECTED_TEXT = \"Hello, I am a 20 year old male\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_umt5_encoder_q8_0(self):\n+        \"\"\"\n+        Verifies that a UMT5 encoder loads directly from a GGUF file using\n+        UMT5EncoderModel.from_pretrained(...), and the config is correctly UMT5.\n+        \"\"\"\n+        model = UMT5EncoderModel.from_pretrained(\n+            self.umt5_encoder_model_id,\n+            gguf_file=self.q8_0_umt5_encoder_model_id,\n+            dtype=torch.float16,\n+            device_map=\"auto\",\n+        )\n+        model.eval()\n+\n+        self.assertIsInstance(model, UMT5EncoderModel)\n+        self.assertIsInstance(model.config, UMT5Config)\n+        self.assertEqual(model.config.model_type, \"umt5\")\n+        self.assertIn(\"UMT5EncoderModel\", getattr(model.config, \"architectures\", []))\n+\n+        input_ids = torch.tensor([[1, 2, 3, 4]], dtype=torch.long).to(torch_device)\n+        with torch.no_grad():\n+            outputs = model(input_ids=input_ids)\n+\n+        self.assertTrue(hasattr(outputs, \"last_hidden_state\"))\n+        self.assertEqual(outputs.last_hidden_state.dim(), 3)  # (batch, seq_len, hidden)\n+\n+        EXPECTED_OUTPUT = torch.tensor(\n+            [\n+                [-0.0010, -0.0145, 0.0133],\n+                [-0.0006, 0.1814, 0.1132],\n+                [0.0005, 0.0083, -0.0285],\n+            ]\n+        ).to(torch_device)\n+\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], EXPECTED_OUTPUT, rtol=6e-3, atol=4e-4)"
        }
    ],
    "stats": {
        "total": 73,
        "additions": 69,
        "deletions": 4
    }
}