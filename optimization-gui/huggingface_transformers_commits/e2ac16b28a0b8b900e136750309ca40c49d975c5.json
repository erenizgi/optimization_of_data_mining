{
    "author": "Cyrilvallez",
    "message": "Large modular logic refactoring (#34487)\n\n* rework converter\r\n\r\n* Update modular_model_converter.py\r\n\r\n* Update modular_model_converter.py\r\n\r\n* Update modular_model_converter.py\r\n\r\n* Update modular_model_converter.py\r\n\r\n* cleaning\r\n\r\n* cleaning\r\n\r\n* finalize imports\r\n\r\n* imports\r\n\r\n* Update modular_model_converter.py\r\n\r\n* Better renaming to avoid visiting same file multiple times\r\n\r\n* start converting files\r\n\r\n* style\r\n\r\n* address most comments\r\n\r\n* style\r\n\r\n* remove unused stuff in get_needed_imports\r\n\r\n* style\r\n\r\n* move class dependency functions outside class\r\n\r\n* Move main functions outside class\r\n\r\n* style\r\n\r\n* Update modular_model_converter.py\r\n\r\n* rename func\r\n\r\n* add augmented dependencies\r\n\r\n* Update modular_model_converter.py\r\n\r\n* Add types_to_file_type + tweak annotation handling\r\n\r\n* Allow assignment dependency mapping + fix regex\r\n\r\n* style + update modular examples\r\n\r\n* fix modular_roberta example (wrong redefinition of __init__)\r\n\r\n* slightly correct order in which dependencies will appear\r\n\r\n* style\r\n\r\n* review comments\r\n\r\n* Performance + better handling of dependencies when they are imported\r\n\r\n* style\r\n\r\n* Add advanced new classes capabilities\r\n\r\n* style\r\n\r\n* add forgotten check\r\n\r\n* Update modeling_llava_next_video.py\r\n\r\n* Add prority list ordering in check_conversion as well\r\n\r\n* Update check_modular_conversion.py\r\n\r\n* Update configuration_gemma.py",
    "sha": "e2ac16b28a0b8b900e136750309ca40c49d975c5",
    "files": [
        {
            "sha": "aa0aac55ba91918ec5cde307aaa21bbc12688703",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_my_new_model.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_my_new_model.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n@@ -158,6 +158,13 @@ def __init__(\n         new_param=0,\n         **kwargs,\n     ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -187,11 +194,3 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n         self.new_param = new_param\n-\n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )"
        },
        {
            "sha": "f05ace94b62241d59728968cfd724cecff29e9a9",
            "filename": "examples/modular-transformers/configuration_my_new_model2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 107,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model2.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,116 +1,16 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_my_new_model2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_my_new_model2.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n \n from ...configuration_utils import PretrainedConfig\n from ...modeling_rope_utils import rope_config_validation\n \n \n class MyNewModel2Config(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`MyNewModel2Model`]. It is used to instantiate an MyNewModel2\n-    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the MyNewModel2-7B.\n-\n-    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n-    documentation from [`PretrainedConfig`] for more information.\n-\n-\n-    Args:\n-        vocab_size (`int`, *optional*, defaults to 32000):\n-            Vocabulary size of the MyNewModel2 model. Defines the number of different tokens that can be represented by the\n-            `inputs_ids` passed when calling [`MyNewModel2Model`]\n-        hidden_size (`int`, *optional*, defaults to 4096):\n-            Dimension of the hidden representations.\n-        intermediate_size (`int`, *optional*, defaults to 11008):\n-            Dimension of the MLP representations.\n-        num_hidden_layers (`int`, *optional*, defaults to 32):\n-            Number of hidden layers in the Transformer decoder.\n-        num_attention_heads (`int`, *optional*, defaults to 32):\n-            Number of attention heads for each attention layer in the Transformer decoder.\n-        num_key_value_heads (`int`, *optional*):\n-            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n-            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n-            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n-            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n-            by meanpooling all the original heads within that group. For more details checkout [this\n-            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n-            `num_attention_heads`.\n-        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n-            The non-linear activation function (function or string) in the decoder.\n-        max_position_embeddings (`int`, *optional*, defaults to 2048):\n-            The maximum sequence length that this model might ever be used with. MyNewModel2 1 supports up to 2048 tokens,\n-            MyNewModel2 2 up to 4096, CodeMyNewModel2 up to 16384.\n-        initializer_range (`float`, *optional*, defaults to 0.02):\n-            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n-        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n-            The epsilon used by the rms normalization layers.\n-        use_cache (`bool`, *optional*, defaults to `True`):\n-            Whether or not the model should return the last key/values attentions (not used by all models). Only\n-            relevant if `config.is_decoder=True`.\n-        pad_token_id (`int`, *optional*):\n-            Padding token id.\n-        bos_token_id (`int`, *optional*, defaults to 1):\n-            Beginning of stream token id.\n-        eos_token_id (`int`, *optional*, defaults to 2):\n-            End of stream token id.\n-        pretraining_tp (`int`, *optional*, defaults to 1):\n-            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n-            document](https://huggingface.co/docs/transformers/main/perf_train_gpu_many#tensor-parallelism) to\n-            understand more about it. This value is necessary to ensure exact reproducibility of the pretraining\n-            results. Please refer to [this issue](https://github.com/pytorch/pytorch/issues/76232).\n-        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-            Whether to tie weight embeddings\n-        rope_theta (`float`, *optional*, defaults to 10000.0):\n-            The base period of the RoPE embeddings.\n-        rope_scaling (`Dict`, *optional*):\n-            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n-            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n-            accordingly.\n-            Expected contents:\n-                `rope_type` (`str`):\n-                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n-                    'my_new_model23'], with 'default' being the original RoPE implementation.\n-                `factor` (`float`, *optional*):\n-                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n-                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n-                    original maximum pre-trained length.\n-                `original_max_position_embeddings` (`int`, *optional*):\n-                    Used with 'dynamic', 'longrope' and 'my_new_model23'. The original max position embeddings used during\n-                    pretraining.\n-                `attention_factor` (`float`, *optional*):\n-                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n-                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n-                    `factor` field to infer the suggested value.\n-                `beta_fast` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 32.\n-                `beta_slow` (`float`, *optional*):\n-                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n-                    ramp function. If unspecified, it defaults to 1.\n-                `short_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `long_factor` (`List[float]`, *optional*):\n-                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n-                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n-                    size divided by the number of attention heads divided by 2\n-                `low_freq_factor` (`float`, *optional*):\n-                    Only used with 'my_new_model23'. Scaling factor applied to low frequency components of the RoPE\n-                `high_freq_factor` (`float`, *optional*):\n-                    Only used with 'my_new_model23'. Scaling factor applied to high frequency components of the RoPE\n-        attention_bias (`bool`, *optional*, defaults to `False`):\n-            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n-        attention_dropout (`float`, *optional*, defaults to 0.0):\n-            The dropout ratio for the attention probabilities.\n-        mlp_bias (`bool`, *optional*, defaults to `False`):\n-            Whether to use a bias in up_proj, down_proj and gate_proj layers in the MLP layers.\n-        head_dim (`int`, *optional*):\n-            The attention head dimension. If None, it will default to hidden_size // num_heads\n     This is the configuration class to store the configuration of a [`GemmaModel`]. It is used to instantiate an Gemma\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the Gemma-7B.\n@@ -121,7 +21,6 @@ class MyNewModel2Config(PretrainedConfig):\n         vocab_size (`int`, *optional*, defaults to 256000):\n             Vocabulary size of the Gemma model. Defines the number of different tokens that can be represented by the\n             `inputs_ids` passed when calling [`GemmaModel`]\n-\n     ```python\n     >>> from transformers import GemmaModel, GemmaConfig\n     >>> # Initializing a Gemma gemma-7b style configuration"
        },
        {
            "sha": "4d164fe3e75f1847cc65cfe3dbf69d7a1701e861",
            "filename": "examples/modular-transformers/configuration_new_model.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fconfiguration_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_new_model.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,9 +1,9 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_new_model.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_new_model.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n # Example where we only want to overwrite the defaults of an init\n \n from ...configuration_utils import PretrainedConfig\n@@ -104,6 +104,13 @@ def __init__(\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size\n@@ -121,14 +128,6 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n \n-        super().__init__(\n-            pad_token_id=pad_token_id,\n-            bos_token_id=bos_token_id,\n-            eos_token_id=eos_token_id,\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n-\n     @property\n     def num_heads(self):\n         return self.num_attention_heads"
        },
        {
            "sha": "ed7e3c64d7a895aa03f44ed46582d08fef612c84",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 106,
            "deletions": 102,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,26 +1,24 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_dummy.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dummy.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -33,59 +31,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n class DummyRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -193,40 +138,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 4]\n-    x2 = x[..., x.shape[-1] // 4 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n class DummyMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -261,6 +172,40 @@ def forward(self, x):\n         return down_proj\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 4]\n+    x2 = x[..., x.shape[-1] // 4 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -423,6 +368,7 @@ def forward(\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n@@ -507,6 +453,7 @@ def forward(\n             sliding_window=getattr(self, \"sliding_window\", None),\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -871,6 +818,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -952,6 +900,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -1011,10 +960,9 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -1023,13 +971,12 @@ def _update_causal_mask(\n             )\n \n         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n             device=device,\n-            min_dtype=min_dtype,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -1043,6 +990,63 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask"
        },
        {
            "sha": "e18e6a19e8a3fabb818a169a132524214bc5b1a1",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 81,
            "deletions": 109,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,27 +1,20 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_dummy_bert.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_dummy_bert.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n import os\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from packaging import version\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_attn_mask_utils import (\n-    _prepare_4d_attention_mask_for_sdpa,\n-    _prepare_4d_causal_attention_mask_for_sdpa,\n-)\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPastAndCrossAttentions,\n-    BaseModelOutputWithPoolingAndCrossAttentions,\n-)\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n@@ -40,79 +33,6 @@\n _CONFIG_FOR_DOC = \"DummyBertConfig\"\n \n \n-def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n-    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n-    try:\n-        import re\n-\n-        import numpy as np\n-        import tensorflow as tf\n-    except ImportError:\n-        logger.error(\n-            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n-            \"https://www.tensorflow.org/install/ for installation instructions.\"\n-        )\n-        raise\n-    tf_path = os.path.abspath(tf_checkpoint_path)\n-    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n-    # Load weights from TF model\n-    init_vars = tf.train.list_variables(tf_path)\n-    names = []\n-    arrays = []\n-    for name, shape in init_vars:\n-        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n-        array = tf.train.load_variable(tf_path, name)\n-        names.append(name)\n-        arrays.append(array)\n-\n-    for name, array in zip(names, arrays):\n-        name = name.split(\"/\")\n-        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n-        # which are not required for using pretrained model\n-        if any(\n-            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n-            for n in name\n-        ):\n-            logger.info(f\"Skipping {'/'.join(name)}\")\n-            continue\n-        pointer = model\n-        for m_name in name:\n-            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n-                scope_names = re.split(r\"_(\\d+)\", m_name)\n-            else:\n-                scope_names = [m_name]\n-            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n-                pointer = getattr(pointer, \"bias\")\n-            elif scope_names[0] == \"output_weights\":\n-                pointer = getattr(pointer, \"weight\")\n-            elif scope_names[0] == \"squad\":\n-                pointer = getattr(pointer, \"classifier\")\n-            else:\n-                try:\n-                    pointer = getattr(pointer, scope_names[0])\n-                except AttributeError:\n-                    logger.info(f\"Skipping {'/'.join(name)}\")\n-                    continue\n-            if len(scope_names) >= 2:\n-                num = int(scope_names[1])\n-                pointer = pointer[num]\n-        if m_name[-11:] == \"_embeddings\":\n-            pointer = getattr(pointer, \"weight\")\n-        elif m_name == \"kernel\":\n-            array = np.transpose(array)\n-        try:\n-            if pointer.shape != array.shape:\n-                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n-        except ValueError as e:\n-            e.args += (pointer.shape, array.shape)\n-            raise\n-        logger.info(f\"Initialize PyTorch weight {name}\")\n-        pointer.data = torch.from_numpy(array)\n-    return model\n-\n-\n class DummyBertEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n \n@@ -706,6 +626,79 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return pooled_output\n \n \n+def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n+    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n+    try:\n+        import re\n+\n+        import numpy as np\n+        import tensorflow as tf\n+    except ImportError:\n+        logger.error(\n+            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n+            \"https://www.tensorflow.org/install/ for installation instructions.\"\n+        )\n+        raise\n+    tf_path = os.path.abspath(tf_checkpoint_path)\n+    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n+    # Load weights from TF model\n+    init_vars = tf.train.list_variables(tf_path)\n+    names = []\n+    arrays = []\n+    for name, shape in init_vars:\n+        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n+        array = tf.train.load_variable(tf_path, name)\n+        names.append(name)\n+        arrays.append(array)\n+\n+    for name, array in zip(names, arrays):\n+        name = name.split(\"/\")\n+        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n+        # which are not required for using pretrained model\n+        if any(\n+            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n+            for n in name\n+        ):\n+            logger.info(f\"Skipping {'/'.join(name)}\")\n+            continue\n+        pointer = model\n+        for m_name in name:\n+            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n+                scope_names = re.split(r\"_(\\d+)\", m_name)\n+            else:\n+                scope_names = [m_name]\n+            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n+                pointer = getattr(pointer, \"bias\")\n+            elif scope_names[0] == \"output_weights\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"squad\":\n+                pointer = getattr(pointer, \"classifier\")\n+            else:\n+                try:\n+                    pointer = getattr(pointer, scope_names[0])\n+                except AttributeError:\n+                    logger.info(f\"Skipping {'/'.join(name)}\")\n+                    continue\n+            if len(scope_names) >= 2:\n+                num = int(scope_names[1])\n+                pointer = pointer[num]\n+        if m_name[-11:] == \"_embeddings\":\n+            pointer = getattr(pointer, \"weight\")\n+        elif m_name == \"kernel\":\n+            array = np.transpose(array)\n+        try:\n+            if pointer.shape != array.shape:\n+                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n+        except ValueError as e:\n+            e.args += (pointer.shape, array.shape)\n+            raise\n+        logger.info(f\"Initialize PyTorch weight {name}\")\n+        pointer.data = torch.from_numpy(array)\n+    return model\n+\n+\n class DummyBertPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n@@ -871,26 +864,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         r\"\"\"\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n@@ -1027,7 +1000,6 @@ def forward(\n \n         if not return_dict:\n             return (sequence_output, pooled_output) + encoder_outputs[1:]\n-        return super().forward(input_ids)\n \n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,"
        },
        {
            "sha": "16f9e525a05e906cae8777e323383f2acb1a300f",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 71,
            "deletions": 93,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,25 +1,20 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_modular_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the modular. If any change should be done, please apply the change to the\n-#                           modular_xxx.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_my_new_model2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_my_new_model2.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n@@ -30,6 +25,9 @@\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n class MyNewModel2RMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -50,9 +48,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MyNewModel2RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -448,59 +443,6 @@ def forward(\n         return attn_output, attn_weights, past_key_value\n \n \n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n MY_NEW_MODEL2_ATTENTION_CLASSES = {\n     \"eager\": MyNewModel2Attention,\n     \"flash_attention_2\": MyNewModel2FlashAttention2,\n@@ -893,10 +835,9 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -905,13 +846,12 @@ def _update_causal_mask(\n             )\n \n         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n             device=device,\n-            min_dtype=min_dtype,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -925,10 +865,67 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @add_start_docstrings(\n     \"\"\"\n@@ -1019,27 +1016,8 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            labels = labels.to(logits.device)\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(pooled_logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(pooled_logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n         if not return_dict:\n             output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output"
        },
        {
            "sha": "4556308f1ea077e65989dad7fc243e11d41edf16",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 29,
            "deletions": 119,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -8,7 +8,6 @@\n from typing import ClassVar, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...cache_utils import Cache, StaticCache\n@@ -18,92 +17,15 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    logging,\n     replace_return_docstrings,\n )\n-from .configuration_new_task_model import NewTaskModelConfig\n-\n-\n-if is_flash_attn_2_available():\n-    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n-\n from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_new_task_model import NewTaskModelConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"NewTaskModelConfig\"\n \n \n-# Adapted from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-# But NewTaskModel has no causal mask on prefix\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-    is_training: bool = False,\n-    token_type_ids: torch.Tensor = None,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-        is_training (`bool`):\n-            Whether the model is in training mode or in inference. The condition is checked by presence/absence of `token_type_ids/labels`\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            if is_training:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            else:\n-                causal_mask[:, :sequence_length] = 0.0\n-\n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n-            if is_training:\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n-                )\n-    return causal_mask\n-\n-\n @dataclass\n class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -182,12 +104,12 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NewTaskModelMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = False\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_sdpa = True\n     _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n \n     def _init_weights(self, module):\n         # important: this ported version of NewTaskModelisn't meant for training from scratch - only\n@@ -210,14 +132,6 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n \n-    @property\n-    def _supports_sdpa(self):\n-        \"\"\"\n-        Retrieve language_model's attribute to check whether the model supports\n-        SDPA or not.\n-        \"\"\"\n-        return self.language_model._supports_sdpa\n-\n \n NEW_TASK_MODEL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n@@ -301,11 +215,8 @@ def __init__(self, config):\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)\n         self.multi_modal_projector = NewTaskModelMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self._attn_implementation = config._attn_implementation\n \n-        language_model = AutoModelForCausalLM.from_config(\n-            config=config.text_config, attn_implementation=self._attn_implementation\n-        )\n+        language_model = AutoModelForCausalLM.from_config(config=config.text_config)\n \n         if language_model._tied_weights_keys is not None:\n             self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n@@ -344,6 +255,11 @@ def tie_weights(self):\n     def _update_causal_mask(\n         self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n     ):\n+        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         dtype = inputs_embeds.dtype\n         min_dtype = torch.finfo(dtype).min\n@@ -388,6 +304,22 @@ def _update_causal_mask(\n                 )\n         return causal_mask\n \n+    def get_image_features(self, pixel_values: torch.FloatTensor):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values)\n+        selected_image_feature = image_outputs.last_hidden_state\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = image_features / (self.config.hidden_size**0.5)\n+        return image_features\n+\n     @add_start_docstrings_to_model_forward(NEW_TASK_MODEL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=NewTaskModelCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -426,9 +358,9 @@ def forward(\n         ```python\n         >>> from PIL import Image\n         >>> import requests\n-        >>> from transformers import AutoProcessor, NewTaskModelForNewTask\n+        >>> from transformers import AutoProcessor, NewTaskModelForConditionalGeneration\n \n-        >>> model = NewTaskModelForNewTask.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n+        >>> model = NewTaskModelForConditionalGeneration.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n         >>> processor = AutoProcessor.from_pretrained(\"google/NewTaskModel-test-224px-hf\")\n \n         >>> prompt = \"answer en Where is the cow standing?\"\n@@ -484,6 +416,7 @@ def prepare_inputs_for_generation(\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n+        # Overwritten -- custom `position_ids` and `pixel_values` handling\n         model_inputs = self.language_model.prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n@@ -493,33 +426,10 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             use_cache=use_cache,\n             num_logits_to_keep=num_logits_to_keep,\n+            token_type_ids=token_type_ids,\n             **kwargs,\n         )\n \n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            dtype = self.get_output_embeddings().weight.dtype\n-            min_dtype = torch.finfo(dtype).min\n-\n-            model_inputs[\"attention_mask\"] = _prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_length(),\n-                dtype=dtype,\n-                device=device,\n-                min_dtype=min_dtype,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-\n-        model_inputs[\"token_type_ids\"] = token_type_ids\n-\n         # position_ids in NewTaskModel are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1"
        },
        {
            "sha": "e50cf60c3a4ed4b450cd7093a42822d232dcb774",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "added",
            "additions": 1014,
            "deletions": 0,
            "changes": 1014,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -0,0 +1,1014 @@\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_roberta.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_roberta.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+import math\n+import os\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+from packaging import version\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n+from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    get_torch_version,\n+    logging,\n+)\n+from .configuration_roberta import RobertaConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"google-roberta/roberta-base-uncased\"\n+_CONFIG_FOR_DOC = \"RobertaConfig\"\n+\n+\n+class RobertaEmbeddings(nn.Module):\n+    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n+        self.position_embeddings = nn.Embedding(\n+            config.max_position_embeddings, config.hidden_size, config.pad_token_id\n+        )\n+        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n+\n+        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n+        # any TensorFlow checkpoint file\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+        self.register_buffer(\n+            \"token_type_ids\", torch.zeros(self.position_ids.size(), dtype=torch.long), persistent=False\n+        )\n+        self.pad_token_id = config.pad_token_id\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        past_key_values_length: int = 0,\n+    ) -> torch.Tensor:\n+        if input_ids is not None:\n+            input_shape = input_ids.size()\n+        else:\n+            input_shape = inputs_embeds.size()[:-1]\n+\n+        seq_length = input_shape[1]\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n+\n+        # Setting the token_type_ids to the registered buffer in constructor where it is all zeros, which usually occurs\n+        # when its auto-generated, registered buffer helps users when tracing the model without passing token_type_ids, solves\n+        # issue #5664\n+        if token_type_ids is None:\n+            if hasattr(self, \"token_type_ids\"):\n+                buffered_token_type_ids = self.token_type_ids[:, :seq_length]\n+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(input_shape[0], seq_length)\n+                token_type_ids = buffered_token_type_ids_expanded\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=self.position_ids.device)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.word_embeddings(input_ids)\n+        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n+\n+        embeddings = inputs_embeds + token_type_embeddings\n+        if self.position_embedding_type == \"absolute\":\n+            position_embeddings = self.position_embeddings(position_ids)\n+            embeddings += position_embeddings\n+        embeddings = self.LayerNorm(embeddings)\n+        embeddings = self.dropout(embeddings)\n+        return embeddings\n+\n+\n+class RobertaSelfAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({config.num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n+        self.position_embedding_type = position_embedding_type or getattr(\n+            config, \"position_embedding_type\", \"absolute\"\n+        )\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            self.max_position_embeddings = config.max_position_embeddings\n+            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n+\n+        self.is_decoder = config.is_decoder\n+\n+    def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n+        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n+        x = x.view(new_x_shape)\n+        return x.permute(0, 2, 1, 3)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        mixed_query_layer = self.query(hidden_states)\n+\n+        # If this is instantiated as a cross-attention module, the keys\n+        # and values come from an encoder; the attention mask needs to be\n+        # such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        if is_cross_attention and past_key_value is not None:\n+            # reuse k,v, cross_attentions\n+            key_layer = past_key_value[0]\n+            value_layer = past_key_value[1]\n+            attention_mask = encoder_attention_mask\n+        elif is_cross_attention:\n+            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n+            attention_mask = encoder_attention_mask\n+        elif past_key_value is not None:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(hidden_states))\n+            value_layer = self.transpose_for_scores(self.value(hidden_states))\n+\n+        query_layer = self.transpose_for_scores(mixed_query_layer)\n+\n+        use_cache = past_key_value is not None\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n+            query_length, key_length = query_layer.shape[2], key_layer.shape[2]\n+            if use_cache:\n+                position_ids_l = torch.tensor(key_length - 1, dtype=torch.long, device=hidden_states.device).view(\n+                    -1, 1\n+                )\n+            else:\n+                position_ids_l = torch.arange(query_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n+            position_ids_r = torch.arange(key_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n+            distance = position_ids_l - position_ids_r\n+\n+            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n+            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n+\n+            if self.position_embedding_type == \"relative_key\":\n+                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores\n+            elif self.position_embedding_type == \"relative_key_query\":\n+                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n+                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n+                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+        if attention_mask is not None:\n+            # Apply the attention mask is (precomputed for all layers in RobertaModel forward() function)\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        # Mask heads if we want to\n+        if head_mask is not None:\n+            attention_probs = attention_probs * head_mask\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n+class RobertaSdpaSelfAttention(RobertaSelfAttention):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__(config, position_embedding_type=position_embedding_type)\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.require_contiguous_qkv = version.parse(get_torch_version()) < version.parse(\"2.2.0\")\n+\n+    # Adapted from RobertaSelfAttention\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        if self.position_embedding_type != \"absolute\" or output_attentions or head_mask is not None:\n+            # TODO: Improve this warning with e.g. `model.config._attn_implementation = \"manual\"` once implemented.\n+            logger.warning_once(\n+                \"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to \"\n+                \"the manual attention implementation, but specifying the manual implementation will be required from \"\n+                \"Transformers version v5.0.0 onwards. This warning can be removed using the argument \"\n+                '`attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                past_key_value,\n+                output_attentions,\n+            )\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        # If this is instantiated as a cross-attention module, the keys and values come from an encoder; the attention\n+        # mask needs to be such that the encoder's padding tokens are not attended to.\n+        is_cross_attention = encoder_hidden_states is not None\n+\n+        current_states = encoder_hidden_states if is_cross_attention else hidden_states\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n+\n+        # Check `seq_length` of `past_key_value` == `len(current_states)` to support prefix tuning\n+        if is_cross_attention and past_key_value and past_key_value[0].shape[2] == current_states.shape[1]:\n+            key_layer, value_layer = past_key_value\n+        else:\n+            key_layer = self.transpose_for_scores(self.key(current_states))\n+            value_layer = self.transpose_for_scores(self.value(current_states))\n+            if past_key_value is not None and not is_cross_attention:\n+                key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n+                value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n+\n+        if self.is_decoder:\n+            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n+            # Further calls to cross_attention layer can then reuse all cross-attention\n+            # key/value_states (first \"if\" case)\n+            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n+            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n+            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n+            # if encoder bi-directional self-attention `past_key_value` is always `None`\n+            past_key_value = (key_layer, value_layer)\n+\n+        # SDPA with memory-efficient backend is broken in torch==2.1.2 when using non-contiguous inputs and a custom\n+        # attn_mask, so we need to call `.contiguous()` here. This was fixed in torch==2.2.0.\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577\n+        if self.require_contiguous_qkv and query_layer.device.type == \"cuda\" and attention_mask is not None:\n+            query_layer = query_layer.contiguous()\n+            key_layer = key_layer.contiguous()\n+            value_layer = value_layer.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        # The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\n+        # a causal mask in case tgt_len == 1.\n+        is_causal = (\n+            True if self.is_decoder and not is_cross_attention and attention_mask is None and tgt_len > 1 else False\n+        )\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout_prob if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2)\n+        attn_output = attn_output.reshape(bsz, tgt_len, self.all_head_size)\n+\n+        outputs = (attn_output,)\n+        if self.is_decoder:\n+            outputs = outputs + (past_key_value,)\n+        return outputs\n+\n+\n+class RobertaSelfOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n+        return hidden_states\n+\n+\n+ROBERTA_SELF_ATTENTION_CLASSES = {\n+    \"eager\": RobertaSelfAttention,\n+    \"sdpa\": RobertaSdpaSelfAttention,\n+}\n+\n+\n+class RobertaAttention(nn.Module):\n+    def __init__(self, config, position_embedding_type=None):\n+        super().__init__()\n+        self.self = ROBERTA_SELF_ATTENTION_CLASSES[config._attn_implementation](\n+            config, position_embedding_type=position_embedding_type\n+        )\n+        self.output = RobertaSelfOutput(config)\n+        self.pruned_heads = set()\n+\n+    def prune_heads(self, heads):\n+        if len(heads) == 0:\n+            return\n+        heads, index = find_pruneable_heads_and_indices(\n+            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n+        )\n+\n+        # Prune linear layers\n+        self.self.query = prune_linear_layer(self.self.query, index)\n+        self.self.key = prune_linear_layer(self.self.key, index)\n+        self.self.value = prune_linear_layer(self.self.value, index)\n+        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n+\n+        # Update hyper params and store pruned heads\n+        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n+        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n+        self.pruned_heads = self.pruned_heads.union(heads)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        self_outputs = self.self(\n+            hidden_states,\n+            attention_mask,\n+            head_mask,\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+            past_key_value,\n+            output_attentions,\n+        )\n+        attention_output = self.output(self_outputs[0], hidden_states)\n+        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n+        return outputs\n+\n+\n+class RobertaIntermediate(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n+        if isinstance(config.hidden_act, str):\n+            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n+        else:\n+            self.intermediate_act_fn = config.hidden_act\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.intermediate_act_fn(hidden_states)\n+        return hidden_states\n+\n+\n+class RobertaOutput(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n+        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+\n+    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.dense(hidden_states)\n+        hidden_states = self.dropout(hidden_states)\n+        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n+        return hidden_states\n+\n+\n+class RobertaLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n+        self.seq_len_dim = 1\n+        self.attention = RobertaAttention(config)\n+        self.is_decoder = config.is_decoder\n+        self.add_cross_attention = config.add_cross_attention\n+        if self.add_cross_attention:\n+            if not self.is_decoder:\n+                raise ValueError(f\"{self} should be used as a decoder model if cross attention is added\")\n+            self.crossattention = RobertaAttention(config, position_embedding_type=\"absolute\")\n+        self.intermediate = RobertaIntermediate(config)\n+        self.output = RobertaOutput(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor]:\n+        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n+        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n+        self_attention_outputs = self.attention(\n+            hidden_states,\n+            attention_mask,\n+            head_mask,\n+            output_attentions=output_attentions,\n+            past_key_value=self_attn_past_key_value,\n+        )\n+        attention_output = self_attention_outputs[0]\n+\n+        # if decoder, the last output is tuple of self-attn cache\n+        if self.is_decoder:\n+            outputs = self_attention_outputs[1:-1]\n+            present_key_value = self_attention_outputs[-1]\n+        else:\n+            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        cross_attn_present_key_value = None\n+        if self.is_decoder and encoder_hidden_states is not None:\n+            if not hasattr(self, \"crossattention\"):\n+                raise ValueError(\n+                    f\"If `encoder_hidden_states` are passed, {self} has to be instantiated with cross-attention layers\"\n+                    \" by setting `config.add_cross_attention=True`\"\n+                )\n+\n+            # cross_attn cached key/values tuple is at positions 3,4 of past_key_value tuple\n+            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n+            cross_attention_outputs = self.crossattention(\n+                attention_output,\n+                attention_mask,\n+                head_mask,\n+                encoder_hidden_states,\n+                encoder_attention_mask,\n+                cross_attn_past_key_value,\n+                output_attentions,\n+            )\n+            attention_output = cross_attention_outputs[0]\n+            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights\n+\n+            # add cross-attn cache to positions 3,4 of present_key_value tuple\n+            cross_attn_present_key_value = cross_attention_outputs[-1]\n+            present_key_value = present_key_value + cross_attn_present_key_value\n+\n+        layer_output = apply_chunking_to_forward(\n+            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n+        )\n+        outputs = (layer_output,) + outputs\n+\n+        # if decoder, return the attn key/values as the last output\n+        if self.is_decoder:\n+            outputs = outputs + (present_key_value,)\n+\n+        return outputs\n+\n+    def feed_forward_chunk(self, attention_output):\n+        intermediate_output = self.intermediate(attention_output)\n+        layer_output = self.output(intermediate_output, attention_output)\n+        return layer_output\n+\n+\n+class RobertaEncoder(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.layer = nn.ModuleList([RobertaLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n+        encoder_attention_mask: Optional[torch.FloatTensor] = None,\n+        past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = False,\n+        output_hidden_states: Optional[bool] = False,\n+        return_dict: Optional[bool] = True,\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPastAndCrossAttentions]:\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n+\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        next_decoder_cache = () if use_cache else None\n+        for i, layer_module in enumerate(self.layer):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            layer_head_mask = head_mask[i] if head_mask is not None else None\n+            past_key_value = past_key_values[i] if past_key_values is not None else None\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    past_key_value,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = layer_module(\n+                    hidden_states,\n+                    attention_mask,\n+                    layer_head_mask,\n+                    encoder_hidden_states,\n+                    encoder_attention_mask,\n+                    past_key_value,\n+                    output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+            if use_cache:\n+                next_decoder_cache += (layer_outputs[-1],)\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+                if self.config.add_cross_attention:\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    next_decoder_cache,\n+                    all_hidden_states,\n+                    all_self_attentions,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+        return BaseModelOutputWithPastAndCrossAttentions(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_decoder_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+class RobertaPooler(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n+        self.activation = nn.Tanh()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        # We \"pool\" the model by simply taking the hidden state corresponding\n+        # to the first token.\n+        first_token_tensor = hidden_states[:, 0]\n+        pooled_output = self.dense(first_token_tensor)\n+        pooled_output = self.activation(pooled_output)\n+        return pooled_output\n+\n+\n+def load_tf_weights_in_roberta(model, config, tf_checkpoint_path):\n+    \"\"\"Load tf checkpoints in a pytorch model.\"\"\"\n+    try:\n+        import re\n+\n+        import numpy as np\n+        import tensorflow as tf\n+    except ImportError:\n+        logger.error(\n+            \"Loading a TensorFlow model in PyTorch, requires TensorFlow to be installed. Please see \"\n+            \"https://www.tensorflow.org/install/ for installation instructions.\"\n+        )\n+        raise\n+    tf_path = os.path.abspath(tf_checkpoint_path)\n+    logger.info(f\"Converting TensorFlow checkpoint from {tf_path}\")\n+    # Load weights from TF model\n+    init_vars = tf.train.list_variables(tf_path)\n+    names = []\n+    arrays = []\n+    for name, shape in init_vars:\n+        logger.info(f\"Loading TF weight {name} with shape {shape}\")\n+        array = tf.train.load_variable(tf_path, name)\n+        names.append(name)\n+        arrays.append(array)\n+\n+    for name, array in zip(names, arrays):\n+        name = name.split(\"/\")\n+        # adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v\n+        # which are not required for using pretrained model\n+        if any(\n+            n in [\"adam_v\", \"adam_m\", \"AdamWeightDecayOptimizer\", \"AdamWeightDecayOptimizer_1\", \"global_step\"]\n+            for n in name\n+        ):\n+            logger.info(f\"Skipping {'/'.join(name)}\")\n+            continue\n+        pointer = model\n+        for m_name in name:\n+            if re.fullmatch(r\"[A-Za-z]+_\\d+\", m_name):\n+                scope_names = re.split(r\"_(\\d+)\", m_name)\n+            else:\n+                scope_names = [m_name]\n+            if scope_names[0] == \"kernel\" or scope_names[0] == \"gamma\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"output_bias\" or scope_names[0] == \"beta\":\n+                pointer = getattr(pointer, \"bias\")\n+            elif scope_names[0] == \"output_weights\":\n+                pointer = getattr(pointer, \"weight\")\n+            elif scope_names[0] == \"squad\":\n+                pointer = getattr(pointer, \"classifier\")\n+            else:\n+                try:\n+                    pointer = getattr(pointer, scope_names[0])\n+                except AttributeError:\n+                    logger.info(f\"Skipping {'/'.join(name)}\")\n+                    continue\n+            if len(scope_names) >= 2:\n+                num = int(scope_names[1])\n+                pointer = pointer[num]\n+        if m_name[-11:] == \"_embeddings\":\n+            pointer = getattr(pointer, \"weight\")\n+        elif m_name == \"kernel\":\n+            array = np.transpose(array)\n+        try:\n+            if pointer.shape != array.shape:\n+                raise ValueError(f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\")\n+        except ValueError as e:\n+            e.args += (pointer.shape, array.shape)\n+            raise\n+        logger.info(f\"Initialize PyTorch weight {name}\")\n+        pointer.data = torch.from_numpy(array)\n+    return model\n+\n+\n+class RobertaPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = RobertaConfig\n+    load_tf_weights = load_tf_weights_in_roberta\n+    base_model_prefix = \"roberta\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+ROBERTA_START_DOCSTRING = r\"\"\"\n+\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`RobertaConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `({0})`):\n+            Indices of input sequence tokens in the vocabulary.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n+            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n+            1]`:\n+\n+            - 0 corresponds to a *sentence A* token,\n+            - 1 corresponds to a *sentence B* token.\n+\n+            [What are token type IDs?](../glossary#token-type-ids)\n+        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare Roberta Model transformer outputting raw hidden-states without any specific head on top.\",\n+    ROBERTA_START_DOCSTRING,\n+)\n+class RobertaModel(RobertaPreTrainedModel):\n+    \"\"\"\n+\n+    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n+    cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n+    all you need](https://arxiv.org/abs/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n+    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n+\n+    To behave as an decoder the model needs to be initialized with the `is_decoder` argument of the configuration set\n+    to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n+    `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n+    \"\"\"\n+\n+    _no_split_modules = [\"RobertaEmbeddings\", \"RobertaLayer\"]\n+\n+    def __init__(self, config, add_pooling_layer=True):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.embeddings = RobertaEmbeddings(config)\n+        self.encoder = RobertaEncoder(config)\n+\n+        self.pooler = RobertaPooler(config) if add_pooling_layer else None\n+\n+        self.attn_implementation = config._attn_implementation\n+        self.position_embedding_type = config.position_embedding_type\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n+    def _prune_heads(self, heads_to_prune):\n+        \"\"\"\n+        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n+        class PreTrainedModel\n+        \"\"\"\n+        for layer, heads in heads_to_prune.items():\n+            self.encoder.layer[layer].attention.prune_heads(heads)\n+\n+    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n+        config_class=_CONFIG_FOR_DOC,\n+    )\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        head_mask: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        encoder_attention_mask: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n+        r\"\"\"\n+        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n+            the model is configured as a decoder.\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n+            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n+            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n+            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if self.config.is_decoder:\n+            use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        else:\n+            use_cache = False\n+\n+        if input_ids is not None and inputs_embeds is not None:\n+            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n+        elif input_ids is not None:\n+            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n+            input_shape = input_ids.size()\n+        elif inputs_embeds is not None:\n+            input_shape = inputs_embeds.size()[:-1]\n+        else:\n+            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n+\n+        batch_size, seq_length = input_shape\n+        device = input_ids.device if input_ids is not None else inputs_embeds.device\n+\n+        # past_key_values_length\n+        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n+\n+        if token_type_ids is None:\n+            if hasattr(self.embeddings, \"token_type_ids\"):\n+                buffered_token_type_ids = self.embeddings.token_type_ids[:, :seq_length]\n+                buffered_token_type_ids_expanded = buffered_token_type_ids.expand(batch_size, seq_length)\n+                token_type_ids = buffered_token_type_ids_expanded\n+            else:\n+                token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n+\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            token_type_ids=token_type_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if attention_mask is None:\n+            attention_mask = torch.ones((batch_size, seq_length + past_key_values_length), device=device)\n+\n+        use_sdpa_attention_masks = (\n+            self.attn_implementation == \"sdpa\"\n+            and self.position_embedding_type == \"absolute\"\n+            and head_mask is None\n+            and not output_attentions\n+        )\n+\n+        # Expand the attention mask\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n+            # Expand the attention mask for SDPA.\n+            # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+            if self.config.is_decoder:\n+                extended_attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n+                    attention_mask,\n+                    input_shape,\n+                    embedding_output,\n+                    past_key_values_length,\n+                )\n+            else:\n+                extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+        else:\n+            # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n+            # ourselves in which case we just need to make it broadcastable to all heads.\n+            extended_attention_mask = self.get_extended_attention_mask(attention_mask, input_shape)\n+\n+        # If a 2D or 3D attention mask is provided for the cross-attention\n+        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+        if self.config.is_decoder and encoder_hidden_states is not None:\n+            encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n+            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n+            if encoder_attention_mask is None:\n+                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n+\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n+                # Expand the attention mask for SDPA.\n+                # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n+                encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa(\n+                    encoder_attention_mask, embedding_output.dtype, tgt_len=seq_length\n+                )\n+            else:\n+                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n+        else:\n+            encoder_extended_attention_mask = None\n+\n+        # Prepare head mask if needed\n+        # 1.0 in head_mask indicate we keep the head\n+        # attention_probs has shape bsz x n_heads x N x N\n+        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n+        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n+        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n+\n+        encoder_outputs = self.encoder(\n+            embedding_output,\n+            attention_mask=extended_attention_mask,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            encoder_attention_mask=encoder_extended_attention_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        sequence_output = encoder_outputs[0]\n+        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n+\n+        if not return_dict:\n+            return (sequence_output, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPoolingAndCrossAttentions(\n+            last_hidden_state=sequence_output,\n+            pooler_output=pooled_output,\n+            past_key_values=encoder_outputs.past_key_values,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+            cross_attentions=encoder_outputs.cross_attentions,\n+        )"
        },
        {
            "sha": "7df04bcc2a99e53c586ccae14f1732634a330997",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 115,
            "deletions": 112,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -1,26 +1,24 @@\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n-#               This file was automatically generated from <path_to_diff_file.py>.\n-#         Do NOT edit this file manually as any edits will be overwritten by the generation of\n-#         the file from the diff. If any change should be done, please apply the change to the\n-#                           diff.py file directly. One of our CI enforces this\n-#           ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_super.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_super.py file directly. One of our CI enforces this.\n+#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨\n import math\n from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-)\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n@@ -33,59 +31,6 @@\n logger = logging.get_logger(__name__)\n \n \n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    device: torch.device,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-):\n-    \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-    Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-    \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full((sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device)\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-        causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-    return causal_mask\n-\n-\n class SuperRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         \"\"\"\n@@ -123,7 +68,7 @@ def __init__(\n         if config is None:\n             logger.warning_once(\n                 \"`SuperRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.45\"\n+                \"`config` argument. All other arguments will be removed in v4.46\"\n             )\n             self.rope_kwargs = {\n                 \"rope_type\": rope_type,\n@@ -193,40 +138,6 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n-def rotate_half(x):\n-    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n-    x1 = x[..., : x.shape[-1] // 2]\n-    x2 = x[..., x.shape[-1] // 2 :]\n-    return torch.cat((-x2, x1), dim=-1)\n-\n-\n-def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n-    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n-\n-    Args:\n-        q (`torch.Tensor`): The query tensor.\n-        k (`torch.Tensor`): The key tensor.\n-        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n-        sin (`torch.Tensor`): The sine part of the rotary embedding.\n-        position_ids (`torch.Tensor`, *optional*):\n-            Deprecated and unused.\n-        unsqueeze_dim (`int`, *optional*, defaults to 1):\n-            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n-            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n-            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n-            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n-            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n-            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n-    Returns:\n-        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n-    \"\"\"\n-    cos = cos.unsqueeze(unsqueeze_dim)\n-    sin = sin.unsqueeze(unsqueeze_dim)\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n-    return q_embed, k_embed\n-\n-\n class SuperMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -261,6 +172,40 @@ def forward(self, x):\n         return down_proj\n \n \n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -302,7 +247,7 @@ def __init__(self, config: SuperConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n \n-        # TODO (joao): remove in v4.45 (RoPE is computed in the model, not in the decoder layers)\n+        # TODO (joao): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n         self.rotary_emb = SuperRotaryEmbedding(config=self.config)\n \n     def forward(\n@@ -314,7 +259,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         bsz, q_len, _ = hidden_states.size()\n@@ -349,7 +294,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -422,7 +367,8 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if isinstance(past_key_value, StaticCache):\n             raise ValueError(\n@@ -449,7 +395,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -507,6 +453,7 @@ def forward(\n             sliding_window=getattr(self, \"sliding_window\", None),\n             use_top_left_mask=self._flash_attn_uses_top_left_mask,\n             is_causal=self.is_causal,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n@@ -535,7 +482,7 @@ def forward(\n         output_attentions: bool = False,\n         use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         if output_attentions:\n@@ -569,7 +516,7 @@ def forward(\n             logger.warning_once(\n                 \"The attention layers in this model are transitioning from computing the RoPE embeddings internally \"\n                 \"through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed \"\n-                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.45 `position_ids` will be \"\n+                \"`position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be \"\n                 \"removed and `position_embeddings` will be mandatory.\"\n             )\n             cos, sin = self.rotary_emb(value_states, position_ids)\n@@ -644,7 +591,7 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.45\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n@@ -790,7 +737,8 @@ def _init_weights(self, module):\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n             Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance;\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n             - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n             shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n             cache format.\n@@ -916,10 +864,9 @@ def _update_causal_mask(\n                 return None\n \n         dtype, device = input_tensor.dtype, input_tensor.device\n-        min_dtype = torch.finfo(dtype).min\n         sequence_length = input_tensor.shape[1]\n         if using_static_cache:\n-            target_length = past_key_values.get_max_length()\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -928,13 +875,12 @@ def _update_causal_mask(\n             )\n \n         # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = _prepare_4d_causal_attention_mask_with_cache_position(\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n             attention_mask,\n             sequence_length=sequence_length,\n             target_length=target_length,\n             dtype=dtype,\n             device=device,\n-            min_dtype=min_dtype,\n             cache_position=cache_position,\n             batch_size=input_tensor.shape[0],\n         )\n@@ -948,6 +894,63 @@ def _update_causal_mask(\n             # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n             # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n             # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n             causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n \n         return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask"
        },
        {
            "sha": "13dca4845c132b1e735bcc34d2df2b81111cc97d",
            "filename": "examples/modular-transformers/modular_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodular_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/examples%2Fmodular-transformers%2Fmodular_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_roberta.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -13,8 +13,5 @@ def __init__(self, config):\n \n \n class RobertaModel(BertModel):\n-    def __init__(self, config):\n+    def __init__(self, config, add_pooling_layer=True):\n         super().__init__(self, config)\n-        # Error out here. Why? Because `RobertaEmbeddings` is defined but not used.\n-        # no, because it's defined, and RobertaModel should use RobertaEmbedding\n-        # here if initialized that way it won't use the new embedding."
        },
        {
            "sha": "fa3fadc4349a3c4fcdca9a0e15db5a80ca806427",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -23,7 +23,6 @@\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n@@ -49,7 +48,10 @@\n from .configuration_gemma import GemmaConfig\n \n \n+logger = logging.get_logger(__name__)\n+\n _CHECKPOINT_FOR_DOC = \"google/gemma-7b\"\n+_CONFIG_FOR_DOC = \"GemmaConfig\"\n \n \n class GemmaRMSNorm(nn.Module):\n@@ -72,9 +74,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class GemmaRotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -624,9 +623,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-_CONFIG_FOR_DOC = \"GemmaConfig\"\n-\n-\n GEMMA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "eb562b3a6893bd6f6f14004f72d38c14e159246e",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -19,8 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n-\n from ...configuration_utils import PretrainedConfig\n \n "
        },
        {
            "sha": "626e5537fc06bc744b7ae2a950f7a89cd6831a1b",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 9,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -23,7 +23,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n@@ -40,6 +39,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal,\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n@@ -48,7 +48,15 @@\n from .configuration_gemma2 import Gemma2Config\n \n \n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n _CHECKPOINT_FOR_DOC = \"google/gemma2-7b\"\n+_CONFIG_FOR_DOC = \"Gemma2Config\"\n \n \n class Gemma2RMSNorm(nn.Module):\n@@ -86,9 +94,6 @@ def forward(self, x):\n         return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Gemma2RotaryEmbedding(nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         super().__init__()\n@@ -198,12 +203,12 @@ def __init__(self, config: Gemma2Config, layer_idx: Optional[int] = None):\n         self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=config.attention_bias)\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n         self.rotary_emb = Gemma2RotaryEmbedding(\n             self.head_dim,\n             max_position_embeddings=self.max_position_embeddings,\n             base=self.rope_theta,\n         )\n+        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n \n     def forward(\n         self,\n@@ -495,12 +500,12 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.self_attn = GEMMA2_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n         self.mlp = Gemma2MLP(config)\n         self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.config = config\n         self.is_sliding = not bool(layer_idx % 2)\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.sliding_window = config.sliding_window\n-        self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n@@ -638,9 +643,6 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n         return config\n \n \n-_CONFIG_FOR_DOC = \"Gemma2Config\"\n-\n-\n GEMMA2_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -865,6 +867,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n+    @torch.no_grad()\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,"
        },
        {
            "sha": "248ec4021791b1dd907c6d63e4b20f527a6917ef",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 17,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -24,7 +24,6 @@\n \n import torch\n import torch.nn as nn\n-import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n@@ -50,7 +49,10 @@\n from .configuration_glm import GlmConfig\n \n \n+logger = logging.get_logger(__name__)\n+\n _CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n+_CONFIG_FOR_DOC = \"GlmConfig\"\n \n \n class GlmRMSNorm(nn.Module):\n@@ -121,7 +123,16 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n         return self.down_proj(up_states)\n \n \n-logger = logging.get_logger(__name__)\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n def rotate_half(x):\n@@ -172,18 +183,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n class GlmAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n@@ -608,9 +607,6 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n-_CONFIG_FOR_DOC = \"GlmConfig\"\n-\n-\n GLM_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "b0a494dcfe6cec439c65eef5c446eb3c5d5e21a0",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 99,
            "deletions": 99,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -24,7 +24,6 @@\n from typing import Any, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n@@ -347,104 +346,6 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-INSTRUCTBLIPVIDEO_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n-            [`InstructBlipVideoProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n-            [`InstructBlipVideoProcessor.__call__`] for details.\n-\n-        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n-            to serve as text prompt, which the Q-Former model will encode.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n-            encoder-decoder language model (like T5) is used.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n-\n-        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            Only relevant in case an encoder-decoder language model (like T5) is used.\n-\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-\"\"\"\n-\n-\n class InstructBlipVideoEncoder(nn.Module):\n     \"\"\"\n     Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n@@ -531,6 +432,24 @@ def forward(\n         )\n \n \n+INSTRUCTBLIPVIDEO_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n+            [`InstructBlipVideoProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+\"\"\"\n+\n+\n class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config_class = InstructBlipVideoVisionConfig\n@@ -1268,6 +1187,87 @@ def forward(\n         )\n \n \n+INSTRUCTBLIPVIDEO_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`InstructBlipVideoConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`InstructBlipVideoProcessor`]. See\n+            [`InstructBlipVideoProcessor.__call__`] for details.\n+\n+        qformer_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the Q-Former. Input tokens can optionally be provided\n+            to serve as text prompt, which the Q-Former model will encode.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        qformer_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n+            provided to serve as text prompt, which the language model can continue.\n+\n+            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n+            details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Indices of decoder input sequence tokens in the vocabulary of the language model. Only relevant in case an\n+            encoder-decoder language model (like T5) is used.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details. [What are decoder input IDs?](../glossary#decoder-input-ids)\n+\n+        decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n+            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n+            be used by default.\n+\n+            Only relevant in case an encoder-decoder language model (like T5) is used.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+\"\"\"\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     InstructBlipVideo Model for generating text given an image and an optional text prompt. The model consists of a vision"
        },
        {
            "sha": "73118f4bfcd300bbe4abf7cf6a803f43cc3172bc",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 108,
            "deletions": 114,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -25,20 +25,14 @@\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from ..auto import AutoModel, AutoModelForCausalLM\n from .configuration_llava_next_video import LlavaNextVideoConfig\n \n@@ -48,113 +42,6 @@\n _CONFIG_FOR_DOC = \"LlavaNextVideoConfig\"\n \n \n-def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n-    \"\"\"\n-    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n-\n-    Args:\n-        image_size (`tuple`):\n-            The size of the input image in the format (width, height).\n-        grid_pinpoints (`List`):\n-            A list containing possible resolutions. Each item in the list should be a tuple or list\n-            of the form `(height, width)`.\n-        patch_size (`int`):\n-            The size of each image patch.\n-\n-    Returns:\n-        tuple: The shape of the image patch grid in the format (width, height).\n-    \"\"\"\n-    if not isinstance(grid_pinpoints, list):\n-        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n-\n-    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n-    if not isinstance(image_size, (list, tuple)):\n-        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n-            raise TypeError(\n-                f\"image_size invalid type: {type(image_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n-            )\n-        image_size = image_size.tolist()\n-\n-    height, width = select_best_resolution(image_size, grid_pinpoints)\n-    return height // patch_size, width // patch_size\n-\n-\n-def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n-    \"\"\"\n-    Calculate the number of patches after the preprocessing for images of any resolution.\n-\n-    Args:\n-        image_size (`torch.LongTensor` or `np.ndarray` or `Tuple[int, int]`):\n-            The size of the input image in the format (height, width). ?\n-        grid_pinpoints (`List`):\n-            A list containing possible resolutions. Each item in the list should be a tuple or list\n-            of the form `(height, width)`.\n-        patch_size (`int`):\n-            The size of each image patch.\n-\n-    Returns:\n-        int: the number of patches\n-    \"\"\"\n-    if not isinstance(grid_pinpoints, list):\n-        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n-\n-    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n-    if not isinstance(image_size, (list, tuple)):\n-        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n-            raise TypeError(f\"image_size invalid type {type(image_size)} with value {image_size}\")\n-        image_size = image_size.tolist()\n-\n-    best_resolution = select_best_resolution(image_size, grid_pinpoints)\n-    height, width = best_resolution\n-    num_patches = 0\n-    # consider change to ceil(height/patch_size)*ceil(width/patch_size) + 1\n-    for i in range(0, height, patch_size):\n-        for j in range(0, width, patch_size):\n-            num_patches += 1\n-    # add the base patch\n-    num_patches += 1\n-    return num_patches\n-\n-\n-def unpad_image(tensor, original_size):\n-    \"\"\"\n-    Unpads a PyTorch tensor of a padded and resized image.\n-\n-    Args:\n-        tensor (`torch.Tensor`):\n-            The image tensor, assumed to be of shape (num_channels, height, width).\n-        original_size (`tuple`):\n-            The original size of the image (height, width).\n-\n-    Returns:\n-        `torch.Tensor`: The unpadded image tensor.\n-    \"\"\"\n-    if not isinstance(original_size, (list, tuple)):\n-        if not isinstance(original_size, (torch.Tensor, np.ndarray)):\n-            raise TypeError(\n-                f\"image_size invalid type: {type(original_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n-            )\n-        original_size = original_size.tolist()\n-    original_height, original_width = original_size\n-    current_height, current_width = tensor.shape[1:]\n-\n-    original_aspect_ratio = original_width / original_height\n-    current_aspect_ratio = current_width / current_height\n-\n-    if original_aspect_ratio > current_aspect_ratio:\n-        scale_factor = current_width / original_width\n-        new_height = int(round(original_height * scale_factor, 7))\n-        padding = (current_height - new_height) // 2\n-        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n-    else:\n-        scale_factor = current_height / original_height\n-        new_width = int(round(original_width * scale_factor, 7))\n-        padding = (current_width - new_width) // 2\n-        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n-\n-    return unpadded_tensor\n-\n-\n @dataclass\n class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -304,6 +191,113 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n \n+def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n+    \"\"\"\n+    Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n+\n+    Args:\n+        image_size (`tuple`):\n+            The size of the input image in the format (width, height).\n+        grid_pinpoints (`List`):\n+            A list containing possible resolutions. Each item in the list should be a tuple or list\n+            of the form `(height, width)`.\n+        patch_size (`int`):\n+            The size of each image patch.\n+\n+    Returns:\n+        tuple: The shape of the image patch grid in the format (width, height).\n+    \"\"\"\n+    if not isinstance(grid_pinpoints, list):\n+        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n+\n+    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n+    if not isinstance(image_size, (list, tuple)):\n+        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(\n+                f\"image_size invalid type: {type(image_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n+            )\n+        image_size = image_size.tolist()\n+\n+    height, width = select_best_resolution(image_size, grid_pinpoints)\n+    return height // patch_size, width // patch_size\n+\n+\n+def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n+    \"\"\"\n+    Calculate the number of patches after the preprocessing for images of any resolution.\n+\n+    Args:\n+        image_size (`torch.LongTensor` or `np.ndarray` or `Tuple[int, int]`):\n+            The size of the input image in the format (height, width). ?\n+        grid_pinpoints (`List`):\n+            A list containing possible resolutions. Each item in the list should be a tuple or list\n+            of the form `(height, width)`.\n+        patch_size (`int`):\n+            The size of each image patch.\n+\n+    Returns:\n+        int: the number of patches\n+    \"\"\"\n+    if not isinstance(grid_pinpoints, list):\n+        raise TypeError(\"grid_pinpoints should be a list of tuples or lists\")\n+\n+    # ! VERY IMPORTANT if image_size is tensor, must convert to into tuple, otherwise it will cause wrong calculate\n+    if not isinstance(image_size, (list, tuple)):\n+        if not isinstance(image_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(f\"image_size invalid type {type(image_size)} with value {image_size}\")\n+        image_size = image_size.tolist()\n+\n+    best_resolution = select_best_resolution(image_size, grid_pinpoints)\n+    height, width = best_resolution\n+    num_patches = 0\n+    # consider change to ceil(height/patch_size)*ceil(width/patch_size) + 1\n+    for i in range(0, height, patch_size):\n+        for j in range(0, width, patch_size):\n+            num_patches += 1\n+    # add the base patch\n+    num_patches += 1\n+    return num_patches\n+\n+\n+def unpad_image(tensor, original_size):\n+    \"\"\"\n+    Unpads a PyTorch tensor of a padded and resized image.\n+\n+    Args:\n+        tensor (`torch.Tensor`):\n+            The image tensor, assumed to be of shape (num_channels, height, width).\n+        original_size (`tuple`):\n+            The original size of the image (height, width).\n+\n+    Returns:\n+        `torch.Tensor`: The unpadded image tensor.\n+    \"\"\"\n+    if not isinstance(original_size, (list, tuple)):\n+        if not isinstance(original_size, (torch.Tensor, np.ndarray)):\n+            raise TypeError(\n+                f\"image_size invalid type: {type(original_size)} not valid, should be either list, tuple, np.ndarray or tensor\"\n+            )\n+        original_size = original_size.tolist()\n+    original_height, original_width = original_size\n+    current_height, current_width = tensor.shape[1:]\n+\n+    original_aspect_ratio = original_width / original_height\n+    current_aspect_ratio = current_width / current_height\n+\n+    if original_aspect_ratio > current_aspect_ratio:\n+        scale_factor = current_width / original_width\n+        new_height = int(round(original_height * scale_factor, 7))\n+        padding = (current_height - new_height) // 2\n+        unpadded_tensor = tensor[:, padding : current_height - padding, :]\n+    else:\n+        scale_factor = current_height / original_height\n+        new_width = int(round(original_width * scale_factor, 7))\n+        padding = (current_width - new_width) // 2\n+        unpadded_tensor = tensor[:, :, padding : current_width - padding]\n+\n+    return unpadded_tensor\n+\n+\n LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):"
        },
        {
            "sha": "8018afa7244dd27fea7ddbd7698a1c59d7bd1d77",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -30,7 +30,6 @@\n from ...configuration_utils import PretrainedConfig\n from ...utils import (\n     logging,\n-    replace_return_docstrings,\n )\n from ..auto import CONFIG_MAPPING\n \n@@ -309,7 +308,6 @@ def get_video_features(\n         video_features = torch.split(video_features, frames, dim=0)\n         return video_features\n \n-    @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=\"LlavaNextVideoConfig\")\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,"
        },
        {
            "sha": "86af396e03a07cf26a7dc5c92bd8f65e73264cac",
            "filename": "utils/check_modular_conversion.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/utils%2Fcheck_modular_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/utils%2Fcheck_modular_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_modular_conversion.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5",
            "patch": "@@ -4,6 +4,8 @@\n import logging\n from io import StringIO\n \n+from create_dependency_mapping import find_priority_list\n+\n # Console for rich printing\n from modular_model_converter import convert_modular_file\n from rich.console import Console\n@@ -69,7 +71,7 @@ def compare_files(modular_file_path, fix_and_overwrite=False):\n     if args.files == [\"all\"]:\n         args.files = glob.glob(\"src/transformers/models/**/modular_*.py\", recursive=True)\n     non_matching_files = 0\n-    for modular_file_path in args.files:\n+    for modular_file_path in find_priority_list(args.files):\n         non_matching_files += compare_files(modular_file_path, args.fix_and_overwrite)\n \n     if non_matching_files and not args.fix_and_overwrite:"
        },
        {
            "sha": "b1dfa18a7a9c848b7020d39d7c443aadd9459e90",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 981,
            "deletions": 675,
            "changes": 1656,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2ac16b28a0b8b900e136750309ca40c49d975c5/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2ac16b28a0b8b900e136750309ca40c49d975c5/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=e2ac16b28a0b8b900e136750309ca40c49d975c5"
        }
    ],
    "stats": {
        "total": 4268,
        "additions": 2668,
        "deletions": 1600
    }
}