{
    "author": "Cyrilvallez",
    "message": "Fix Llama4 offset (#37414)\n\n* add +1\n\n* Update modeling_llama4.py",
    "sha": "6d8b0b33786f6cf41791077624bf731f8c78e36b",
    "files": [
        {
            "sha": "6764c3691f22ec4f800eab8a08171837054ab51f",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d8b0b33786f6cf41791077624bf731f8c78e36b/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d8b0b33786f6cf41791077624bf731f8c78e36b/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=6d8b0b33786f6cf41791077624bf731f8c78e36b",
            "patch": "@@ -731,7 +731,6 @@ def _update_causal_mask(\n         attention_chunk_size = self.config.attention_chunk_size\n \n         first_cache_position = cache_position[0]\n-        last_cache_position = cache_position[-1]\n \n         if past_key_values is not None:\n             full_cache_length = past_key_values.get_max_cache_shape() or sequence_length\n@@ -754,7 +753,7 @@ def _update_causal_mask(\n \n         if self.config._attn_implementation == \"flex_attention\":\n             if isinstance(attention_mask, torch.Tensor):\n-                offsets = (first_cache_position, max(last_cache_position - key_length, 0))\n+                offsets = (first_cache_position, max(first_cache_position - attention_chunk_size + 1, 0))\n                 chunked_attention_mask = make_flex_block_causal_mask(\n                     attention_mask, self.config.attention_chunk_size, sequence_length, key_length, offsets=offsets\n                 )\n@@ -780,10 +779,8 @@ def _update_causal_mask(\n             batch_size=input_tensor.shape[0],\n         )\n         if full_cache_length > self.config.attention_chunk_size:\n-            start_idx = max(last_cache_position - key_length, 0)\n-            end_idx = last_cache_position + 1 if sequence_length > 1 else last_cache_position\n-            # We always need a mask of at least attention_chunk_size, so we use the max here\n-            end_idx = max(end_idx, start_idx + attention_chunk_size)\n+            start_idx = max(first_cache_position - attention_chunk_size + 1, 0)\n+            end_idx = start_idx + key_length\n             chunked_attention_mask = self.create_chunked_attention_mask(\n                 self.config.attention_chunk_size,\n                 start=start_idx,  # same offset as with flex"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 3,
        "deletions": 6
    }
}