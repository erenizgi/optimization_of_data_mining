{
    "author": "simonreise",
    "message": "Updated `backbone_config` docstrings and type annotations (#42927)\n\n* Updated backbone_config docstrings\n\n* Fix typos",
    "sha": "3e4baf8e62c49796ab1a5b391dc81446d18152cd",
    "files": [
        {
            "sha": "0143a3bf980a1088556b80bca84969c39c99aaf0",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class ConditionalDetrConfig(PreTrainedConfig):\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "33ab14f44c2302b6fb3c9dbe711de299f0d08388",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -47,7 +47,7 @@ class DFineConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        backbone_config (`Dict`, *optional*, defaults to `RTDetrResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `HGNetV2Config()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n@@ -288,8 +288,7 @@ def __init__(\n             )\n             backbone_model_type = \"hgnet_v2\"\n             config_class = CONFIG_MAPPING[backbone_model_type]\n-            # this will map it to RTDetrResNetConfig\n-            # note: we can instead create HGNetV2Config\n+            # this will map it to HGNetV2Config\n             # and we would need to create HGNetV2Backbone\n             backbone_config = config_class(\n                 num_channels=3,"
        },
        {
            "sha": "3fc71727d49b0a29ac976ed7ed0ff6e5ab1b89dd",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -67,7 +67,7 @@ class DFineConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        backbone_config (`Dict`, *optional*, defaults to `RTDetrResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `HGNetV2Config()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n@@ -308,8 +308,7 @@ def __init__(\n             )\n             backbone_model_type = \"hgnet_v2\"\n             config_class = CONFIG_MAPPING[backbone_model_type]\n-            # this will map it to RTDetrResNetConfig\n-            # note: we can instead create HGNetV2Config\n+            # this will map it to HGNetV2Config\n             # and we would need to create HGNetV2Backbone\n             backbone_config = config_class(\n                 num_channels=3,"
        },
        {
            "sha": "61bae62752138f6a2e1bc164739930de9f2c9c3e",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class DabDetrConfig(PreTrainedConfig):\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         backbone (`str`, *optional*, defaults to `\"resnet50\"`):"
        },
        {
            "sha": "ba3c04c62e01bffc17306ce23c69c1c738fcf14d",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class DeformableDetrConfig(PreTrainedConfig):\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "dce3be952f3d0a38b7b37f1562dd58e641150f11",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -34,9 +34,8 @@ class DepthAnythingConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n-            The configuration of the backbone model. Only used in case `is_hybrid` is `True` or in case you want to\n-            leverage the [`AutoBackbone`] API.\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `Dinov2Config()`):\n+            The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n             will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`"
        },
        {
            "sha": "4c3675f590535c32f803007f90eb221f324ad436",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class DetrConfig(PreTrainedConfig):\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "740aab8b4dadf349694b3aa9b7b962ccfc3e0348",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -102,7 +102,7 @@ class DPTConfig(PreTrainedConfig):\n             Used only for the `hybrid` embedding type. The shape of the feature maps of the backbone.\n         neck_ignore_stages (`list[int]`, *optional*, defaults to `[0, 1]`):\n             Used only for the `hybrid` embedding type. The stages of the readout layers to ignore.\n-        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `BitConfig()`):\n             The configuration of the backbone model. Only used in case `is_hybrid` is `True` or in case you want to\n             leverage the [`AutoBackbone`] API.\n         backbone (`str`, *optional*):"
        },
        {
            "sha": "ef5e4008269b7608e12bc16a8a42fe34bf4da90d",
            "filename": "src/transformers/models/edgetam/configuration_edgetam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fconfiguration_edgetam.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -33,7 +33,7 @@ class EdgeTamVisionConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `timm/repvit_m1.dist_in1k`):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         backbone_channel_list (`List[int]`, *optional*, defaults to `[384, 192, 96, 48]`):"
        },
        {
            "sha": "a885a978af4b20e42fb1efa04b01052916943e13",
            "filename": "src/transformers/models/edgetam/modular_edgetam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fedgetam%2Fmodular_edgetam.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -57,7 +57,7 @@ class EdgeTamVisionConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `timm/repvit_m1.dist_in1k`):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         backbone_channel_list (`List[int]`, *optional*, defaults to `[384, 192, 96, 48]`):"
        },
        {
            "sha": "3f0253b03973cabda779d4ecfc88e1f18b2c083a",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -34,7 +34,7 @@ class GroundingDinoConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "bf3ec5c74e480a44d914929bf6dd878d18ed41bc",
            "filename": "src/transformers/models/mask2former/configuration_mask2former.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Mask2Former model configuration\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n@@ -39,7 +39,7 @@ class Mask2FormerConfig(PreTrainedConfig):\n     Currently, Mask2Former only supports the [Swin Transformer](swin) as backbone.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `SwinConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the backbone model. If unset, the configuration corresponding to\n             `swin-base-patch4-window12-384` will be used.\n         backbone (`str`, *optional*):\n@@ -134,7 +134,7 @@ class Mask2FormerConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        backbone_config: Optional[dict] = None,\n+        backbone_config: Optional[Union[dict, PreTrainedConfig]] = None,\n         feature_size: int = 256,\n         mask_feature_size: int = 256,\n         hidden_dim: int = 256,"
        },
        {
            "sha": "678753155f88ccb9869557f18fa07598aa401578",
            "filename": "src/transformers/models/maskformer/configuration_maskformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"MaskFormer model configuration\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n@@ -49,7 +49,7 @@ class MaskFormerConfig(PreTrainedConfig):\n         use_auxiliary_loss(`bool`, *optional*, defaults to `False`):\n             If `True` [`MaskFormerForInstanceSegmentationOutput`] will contain the auxiliary losses computed using the\n             logits from each decoder's stage.\n-        backbone_config (`Dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration passed to the backbone, if unset, the configuration corresponding to\n             `swin-base-patch4-window12-384` will be used.\n         backbone (`str`, *optional*):\n@@ -114,7 +114,7 @@ def __init__(\n         mask_feature_size: int = 256,\n         no_object_weight: float = 0.1,\n         use_auxiliary_loss: bool = False,\n-        backbone_config: Optional[dict] = None,\n+        backbone_config: Optional[Union[dict, PreTrainedConfig]] = None,\n         decoder_config: Optional[dict] = None,\n         init_std: float = 0.02,\n         init_xavier_std: float = 1.0,"
        },
        {
            "sha": "1b47c4d74aa8ef4ee3a6a0d970a21b7373253f7f",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -38,7 +38,7 @@ class MMGroundingDinoConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "a937d9a151920428f3be30e692b81a0d21079185",
            "filename": "src/transformers/models/mm_grounding_dino/modular_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodular_mm_grounding_dino.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -51,7 +51,7 @@ class MMGroundingDinoConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `ResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "019ef821a621120e3a3e3a8a1ad42db524b46eaa",
            "filename": "src/transformers/models/omdet_turbo/configuration_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -36,7 +36,7 @@ class OmDetTurboConfig(PreTrainedConfig):\n     Args:\n         text_config (`PreTrainedConfig`, *optional*):\n             The configuration of the text backbone.\n-        backbone_config (`PreTrainedConfig`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the vision backbone.\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether to use the timm for the vision backbone."
        },
        {
            "sha": "b186065a3d18fc026e4e47ca111c4759869c23dd",
            "filename": "src/transformers/models/oneformer/configuration_oneformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"OneFormer model configuration\"\"\"\n \n-from typing import Optional\n+from typing import Optional, Union\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n@@ -37,7 +37,7 @@ class OneFormerConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig`, *optional*, defaults to `SwinConfig`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `SwinConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n@@ -151,7 +151,7 @@ class OneFormerConfig(PreTrainedConfig):\n \n     def __init__(\n         self,\n-        backbone_config: Optional[dict] = None,\n+        backbone_config: Optional[Union[dict, PreTrainedConfig]] = None,\n         backbone: Optional[str] = None,\n         use_pretrained_backbone: bool = False,\n         use_timm_backbone: bool = False,"
        },
        {
            "sha": "e2ce4c54a75872fc153428ed503679856b4bdec6",
            "filename": "src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,9 +37,8 @@ class PromptDepthAnythingConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*):\n-            The configuration of the backbone model. Only used in case `is_hybrid` is `True` or in case you want to\n-            leverage the [`AutoBackbone`] API.\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `Dinov2Config()`):\n+            The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n             will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`"
        },
        {
            "sha": "0385c0332ed5f52eb1cd29dc6898c0a0a62480d8",
            "filename": "src/transformers/models/rt_detr/configuration_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -44,7 +44,7 @@ class RTDetrConfig(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        backbone_config (`Dict`, *optional*, defaults to `RTDetrResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `RTDetrResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "83c8151a93b05d44e3a2e7a313d47c990e395512",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -48,7 +48,7 @@ class RTDetrV2Config(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        backbone_config (`Dict`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "97728cca20c4ab3ab2b1a8620f64290f537187cc",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -60,7 +60,7 @@ class RTDetrV2Config(PreTrainedConfig):\n             The epsilon used by the layer normalization layers.\n         batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n             The epsilon used by the batch normalization layers.\n-        backbone_config (`Dict`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "236a6e2bc4e54c7a0981a2fc9ad8fd8277805294",
            "filename": "src/transformers/models/sam2/configuration_sam2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fconfiguration_sam2.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -152,7 +152,7 @@ class Sam2VisionConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `Sam2HieraDetConfig()`):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         backbone_channel_list (`List[int]`, *optional*, defaults to `[768, 384, 192, 96]`):"
        },
        {
            "sha": "fa622e9e8e0a616aa56316af906c172babce23c7",
            "filename": "src/transformers/models/sam3/configuration_sam3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconfiguration_sam3.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -122,7 +122,7 @@ class Sam3VisionConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `Sam3ViTConfig()`):\n             Configuration for the vision backbone. This is used to instantiate the backbone using\n             `AutoModel.from_config`.\n         fpn_hidden_size (`int`, *optional*, defaults to 256):"
        },
        {
            "sha": "6be8d14b0818d8537d8cfc00831701f9cb7274da",
            "filename": "src/transformers/models/table_transformer/configuration_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class TableTransformerConfig(PreTrainedConfig):\n         use_timm_backbone (`bool`, *optional*, defaults to `True`):\n             Whether or not to use the `timm` library for the backbone. If set to `False`, will use the [`AutoBackbone`]\n             API.\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model. Only used in case `use_timm_backbone` is set to `False` in which\n             case it will default to `ResNetConfig()`.\n         num_channels (`int`, *optional*, defaults to 3):"
        },
        {
            "sha": "f8a12b4a42a0c805b6cce625f546e54da9dce575",
            "filename": "src/transformers/models/tvp/configuration_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -35,7 +35,7 @@ class TvpConfig(PreTrainedConfig):\n \n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `ResNetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "6cbba814bdc8e63ab895dbc3f8611a409aa49d93",
            "filename": "src/transformers/models/vitmatte/configuration_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -36,7 +36,7 @@ class VitMatteConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `VitDetConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `VitDetConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "606328bfbd9d0f424a99c64cbcc43a234aaab64c",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -36,7 +36,7 @@ class VitPoseConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`PreTrainedConfig` or `dict`, *optional*, defaults to `VitPoseBackboneConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `VitPoseBackboneConfig()`):\n             The configuration of the backbone model. Currently, only `backbone_config` with `vitpose_backbone` as `model_type` is supported.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        },
        {
            "sha": "49fb94331861785012ea7dac184669f0571affb8",
            "filename": "src/transformers/models/zoedepth/configuration_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3e4baf8e62c49796ab1a5b391dc81446d18152cd/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py?ref=3e4baf8e62c49796ab1a5b391dc81446d18152cd",
            "patch": "@@ -37,7 +37,7 @@ class ZoeDepthConfig(PreTrainedConfig):\n     documentation from [`PreTrainedConfig`] for more information.\n \n     Args:\n-        backbone_config (`Union[dict[str, Any], PreTrainedConfig]`, *optional*, defaults to `BeitConfig()`):\n+        backbone_config (`Union[dict, \"PreTrainedConfig\"]`, *optional*, defaults to `BeitConfig()`):\n             The configuration of the backbone model.\n         backbone (`str`, *optional*):\n             Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this"
        }
    ],
    "stats": {
        "total": 80,
        "additions": 38,
        "deletions": 42
    }
}