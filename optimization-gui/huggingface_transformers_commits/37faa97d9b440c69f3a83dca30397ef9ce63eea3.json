{
    "author": "ydshieh",
    "message": "Fix repo consistency (#36063)\n\n* fix 1\r\n\r\n* fix 2\r\n\r\n* fix modular\r\n\r\n* simplify at the same time\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "37faa97d9b440c69f3a83dca30397ef9ce63eea3",
    "files": [
        {
            "sha": "d83ee58af5eec5ab0a301fdb5df6a95ad3cd78b6",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/37faa97d9b440c69f3a83dca30397ef9ce63eea3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37faa97d9b440c69f3a83dca30397ef9ce63eea3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=37faa97d9b440c69f3a83dca30397ef9ce63eea3",
            "patch": "@@ -927,11 +927,7 @@ def forward(\n         hidden_states = outputs[0]\n         logits = self.score(hidden_states)\n \n-        if input_ids is not None:\n-            batch_size, sequence_length = input_ids.shape[:2]\n-        else:\n-            batch_size, sequence_length = inputs_embeds.shape[:2]\n-\n+        batch_size = logits.shape[0]\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:"
        },
        {
            "sha": "295882a9eedb3350867de8494bdbce3d4e994fec",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 13,
            "deletions": 18,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/37faa97d9b440c69f3a83dca30397ef9ce63eea3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37faa97d9b440c69f3a83dca30397ef9ce63eea3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=37faa97d9b440c69f3a83dca30397ef9ce63eea3",
            "patch": "@@ -625,29 +625,24 @@ def forward(\n         hidden_states = outputs[0]\n         logits = self.score(hidden_states)\n \n-        if input_ids is not None:\n-            batch_size, sequence_length = input_ids.shape[:2]\n-        else:\n-            batch_size, sequence_length = inputs_embeds.shape[:2]\n-\n+        batch_size = logits.shape[0]\n         if self.config.pad_token_id is None and batch_size != 1:\n             raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n         if self.config.pad_token_id is None:\n-            sequence_lengths = -1\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n         else:\n-            if input_ids is not None:\n-                # if no pad token found, use modulo instead of reverse indexing for ONNX compatibility\n-                sequence_lengths = torch.eq(input_ids, self.config.pad_token_id).int().argmax(-1) - 1\n-                sequence_lengths = sequence_lengths % input_ids.shape[-1]\n-                sequence_lengths = sequence_lengths.to(logits.device)\n-            else:\n-                sequence_lengths = -1\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                    \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-                )\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n \n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n \n         loss = None\n         if labels is not None:"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 14,
        "deletions": 23
    }
}