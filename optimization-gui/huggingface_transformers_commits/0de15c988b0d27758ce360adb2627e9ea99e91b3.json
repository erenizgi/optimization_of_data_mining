{
    "author": "sambhavnoobcoder",
    "message": "Fix Audio Classification Pipeline top_k Documentation Mismatch and Bug #35736 (#35771)\n\n* added condition for top_k Doc mismatch fix\r\n\r\n* initilation of test file for top_k changes\r\n\r\n* added test for returning all labels\r\n\r\n* added test for few labels\r\n\r\n* tests/test_audio_classification_top_k.py\r\n\r\n* final fix\r\n\r\n* ruff fix\r\n\r\n---------\r\n\r\nCo-authored-by: sambhavnoobcoder <indosambahv@gmail.com>",
    "sha": "0de15c988b0d27758ce360adb2627e9ea99e91b3",
    "files": [
        {
            "sha": "86dfe72d3c5dc1aefe9f56da720dfe7ca6353d4a",
            "filename": "src/transformers/pipelines/audio_classification.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0de15c988b0d27758ce360adb2627e9ea99e91b3/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0de15c988b0d27758ce360adb2627e9ea99e91b3/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_classification.py?ref=0de15c988b0d27758ce360adb2627e9ea99e91b3",
            "patch": "@@ -91,8 +91,11 @@ class AudioClassificationPipeline(Pipeline):\n     \"\"\"\n \n     def __init__(self, *args, **kwargs):\n-        # Default, might be overriden by the model.config.\n-        kwargs[\"top_k\"] = kwargs.get(\"top_k\", 5)\n+        # Only set default top_k if explicitly provided\n+        if \"top_k\" in kwargs and kwargs[\"top_k\"] is None:\n+            kwargs[\"top_k\"] = None\n+        elif \"top_k\" not in kwargs:\n+            kwargs[\"top_k\"] = 5\n         super().__init__(*args, **kwargs)\n \n         if self.framework != \"pt\":\n@@ -141,12 +144,16 @@ def __call__(\n         return super().__call__(inputs, **kwargs)\n \n     def _sanitize_parameters(self, top_k=None, function_to_apply=None, **kwargs):\n-        # No parameters on this pipeline right now\n         postprocess_params = {}\n-        if top_k is not None:\n+\n+        # If top_k is None, use all labels\n+        if top_k is None:\n+            postprocess_params[\"top_k\"] = self.model.config.num_labels\n+        else:\n             if top_k > self.model.config.num_labels:\n                 top_k = self.model.config.num_labels\n             postprocess_params[\"top_k\"] = top_k\n+\n         if function_to_apply is not None:\n             if function_to_apply not in [\"softmax\", \"sigmoid\", \"none\"]:\n                 raise ValueError("
        },
        {
            "sha": "9911bd732343a72e4110f4a7828673b502201378",
            "filename": "tests/test_audio_classification_top_k.py",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/0de15c988b0d27758ce360adb2627e9ea99e91b3/tests%2Ftest_audio_classification_top_k.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0de15c988b0d27758ce360adb2627e9ea99e91b3/tests%2Ftest_audio_classification_top_k.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_audio_classification_top_k.py?ref=0de15c988b0d27758ce360adb2627e9ea99e91b3",
            "patch": "@@ -0,0 +1,60 @@\n+import unittest\n+\n+import numpy as np\n+\n+from transformers import pipeline\n+from transformers.testing_utils import require_torch\n+\n+\n+@require_torch\n+class AudioClassificationTopKTest(unittest.TestCase):\n+    def test_top_k_none_returns_all_labels(self):\n+        model_name = \"superb/wav2vec2-base-superb-ks\"  # model with more than 5 labels\n+        classification_pipeline = pipeline(\n+            \"audio-classification\",\n+            model=model_name,\n+            top_k=None,\n+        )\n+\n+        # Create dummy input\n+        sampling_rate = 16000\n+        signal = np.zeros((sampling_rate,), dtype=np.float32)\n+\n+        result = classification_pipeline(signal)\n+        num_labels = classification_pipeline.model.config.num_labels\n+\n+        self.assertEqual(len(result), num_labels, \"Should return all labels when top_k is None\")\n+\n+    def test_top_k_none_with_few_labels(self):\n+        model_name = \"superb/hubert-base-superb-er\"  # model with fewer labels\n+        classification_pipeline = pipeline(\n+            \"audio-classification\",\n+            model=model_name,\n+            top_k=None,\n+        )\n+\n+        # Create dummy input\n+        sampling_rate = 16000\n+        signal = np.zeros((sampling_rate,), dtype=np.float32)\n+\n+        result = classification_pipeline(signal)\n+        num_labels = classification_pipeline.model.config.num_labels\n+\n+        self.assertEqual(len(result), num_labels, \"Should handle models with fewer labels correctly\")\n+\n+    def test_top_k_greater_than_labels(self):\n+        model_name = \"superb/hubert-base-superb-er\"\n+        classification_pipeline = pipeline(\n+            \"audio-classification\",\n+            model=model_name,\n+            top_k=100,  # intentionally large number\n+        )\n+\n+        # Create dummy input\n+        sampling_rate = 16000\n+        signal = np.zeros((sampling_rate,), dtype=np.float32)\n+\n+        result = classification_pipeline(signal)\n+        num_labels = classification_pipeline.model.config.num_labels\n+\n+        self.assertEqual(len(result), num_labels, \"Should cap top_k to number of labels\")"
        }
    ],
    "stats": {
        "total": 75,
        "additions": 71,
        "deletions": 4
    }
}