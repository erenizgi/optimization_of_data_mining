{
    "author": "unknown",
    "message": "Run model as compressed/uncompressed mode (#34719)\n\n* draft, run model as compreszed/uncompressed mode\n\n* draft\n\n* run run_compressed=False\n\n* run_compressed as attr\n\n* set run_compressed=False using quantization_config\n\n* remove redundant line\n\n* make is_qat_trainable dependent on run_compressed status\n\n* add tests\n\n* lint\n\n* full in docstring\n\n* add decompress\n\n* comments\n\n* decompress if model is compresssed and not run_compressed\n\n* apply_quant_config logic fix -- populate statedict properly\n\n* comments\n\n* remove non  compressed model\n\n* make is_compressed as property\n\n* cosmetic\n\n* run apply_quant_config for non-compressed models -- popualte scales and zeropoints\n\n* add pahtway for decompressing sparse models\n\n* typo on is_quantization_compressed\n\n* lint\n\n* fix typo",
    "sha": "e4e404fdd0074a163d1b8b85e54ae8b05d949375",
    "files": [
        {
            "sha": "22dd1b7ccea56c38b4b39659c34c030e26e4e55b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -3597,7 +3597,12 @@ def from_pretrained(\n                 )\n             else:\n                 config.quantization_config = quantization_config\n-            hf_quantizer = AutoHfQuantizer.from_config(config.quantization_config, pre_quantized=pre_quantized)\n+\n+            hf_quantizer = AutoHfQuantizer.from_config(\n+                config.quantization_config,\n+                pre_quantized=pre_quantized,\n+            )\n+\n         else:\n             hf_quantizer = None\n \n@@ -4281,7 +4286,7 @@ def from_pretrained(\n                 dispatch_model(model, **device_map_kwargs)\n \n         if hf_quantizer is not None:\n-            hf_quantizer.postprocess_model(model)\n+            hf_quantizer.postprocess_model(model, config=config)\n             model.hf_quantizer = hf_quantizer\n \n         if _adapter_model_path is not None:"
        },
        {
            "sha": "818072a0d916472ae8ad6c52876888acf2f0f3bf",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -173,13 +173,14 @@ def merge_quantization_configs(\n             quantization_config = AutoQuantizationConfig.from_dict(quantization_config)\n \n         if (\n-            isinstance(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config))\n+            isinstance(quantization_config, (GPTQConfig, AwqConfig, FbgemmFp8Config, CompressedTensorsConfig))\n             and quantization_config_from_args is not None\n         ):\n             # special case for GPTQ / AWQ / FbgemmFp8 config collision\n             loading_attr_dict = quantization_config_from_args.get_loading_attributes()\n             for attr, val in loading_attr_dict.items():\n                 setattr(quantization_config, attr, val)\n+\n             warning_msg += f\"However, loading attributes (e.g. {list(loading_attr_dict.keys())}) will be overwritten with the one you passed to `from_pretrained`. The rest will be ignored.\"\n \n         if warning_msg != \"\":"
        },
        {
            "sha": "7b81c93edf1fac272f656f80e07a773c2fe8fab7",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -111,7 +111,7 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwarg\n                 \" Please double check your model architecture, or submit an issue on github if you think this is a bug.\"\n             )\n \n-    def _process_model_after_weight_loading(self, model):\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n         if self.quantization_config.do_fuse:\n             from ..integrations import fuse_awq_modules\n "
        },
        {
            "sha": "5064f2c019d74ecdd389821b04fe78861981a3cc",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 51,
            "deletions": 10,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -12,8 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+\n+import os\n+\n from ..utils import is_compressed_tensors_available, is_torch_available, logging\n-from ..utils.quantization_config import QuantizationConfigMixin\n+from ..utils.quantization_config import CompressedTensorsConfig\n from .base import HfQuantizer\n \n \n@@ -32,12 +35,13 @@ class CompressedTensorsHfQuantizer(HfQuantizer):\n     requires_calibration = True\n     required_packages = [\"compressed_tensors\"]\n \n-    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+    def __init__(self, quantization_config: CompressedTensorsConfig, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n-\n         from compressed_tensors.compressors import ModelCompressor\n \n         self.compressor = ModelCompressor.from_compression_config(quantization_config)\n+        self.run_compressed = quantization_config.run_compressed\n+        self.quantization_config = quantization_config\n \n     def validate_environment(self, *args, **kwargs):\n         if not is_compressed_tensors_available():\n@@ -63,20 +67,57 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n         from compressed_tensors.quantization import apply_quantization_config\n \n         ct_quantization_config = self.compressor.quantization_config\n-        apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n \n-    def _process_model_after_weight_loading(self, model, **kwargs) -> None:\n-        pass\n+        if self.run_compressed and self.is_quantization_compressed:\n+            apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n+        elif not self.is_quantization_compressed:\n+            apply_quantization_config(model, ct_quantization_config)\n+\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n+        \"\"\"Decompress loaded model if necessary - need for qat\"\"\"\n+\n+        if (self.is_quantization_compressed and not self.run_compressed) or self.is_sparsification_compressed:\n+            config = kwargs.get(\"config\", None)\n+            cache_path = config._name_or_path\n+\n+            if not os.path.exists(cache_path):\n+                from transformers.utils import cached_file\n+\n+                config_file_path = cached_file(cache_path, \"config.json\")\n+                cache_path = os.path.sep.join(config_file_path.split(os.path.sep)[:-1])\n+\n+            if self.is_quantization_compressed and not self.run_compressed:\n+                from compressed_tensors.quantization import QuantizationStatus\n+\n+                self.compressor.quantization_config.quantization_status = QuantizationStatus.FROZEN\n+            self.compressor.decompress(model_path=cache_path, model=model)\n \n     @property\n-    def is_trainable(self) -> bool:\n-        \"\"\"Models quantized using compressed tensors can be finetuned\"\"\"\n-        return True\n+    def is_quantization_compressed(self):\n+        from compressed_tensors.quantization import QuantizationStatus\n+\n+        return (\n+            self.quantization_config.quantization_config is not None\n+            and self.quantization_config.quantization_config.quantization_status == QuantizationStatus.COMPRESSED\n+        )\n+\n+    @property\n+    def is_sparsification_compressed(self):\n+        from compressed_tensors.config.base import CompressionFormat\n+\n+        return (\n+            self.quantization_config.sparsity_config is not None\n+            and self.quantization_config.sparsity_config.format != CompressionFormat.dense.value\n+        )\n \n     @property\n+    def is_trainable(self):\n+        return True\n+\n     def is_qat_trainable(self) -> bool:\n         \"\"\"Loaded Models can carry out quantization aware training\"\"\"\n-        return True\n+        # models need to be decompressed carry out qat\n+        return not self.run_compressed or not self.is_quantization_compressed\n \n     def is_serializable(self, safe_serialization=None) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\""
        },
        {
            "sha": "230e8efe15067290be354e3da5a10f3f0d79248c",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -197,7 +197,7 @@ def _process_model_before_weight_loading(\n         )\n         model.config.quantization_config = self.quantization_config\n \n-    def _process_model_after_weight_loading(self, model):\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n         return model\n \n     @property"
        },
        {
            "sha": "10d2b184ef146b63ad5928f2620e7002f0eba19c",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -195,7 +195,7 @@ def create_quantized_param(\n             module._parameters[tensor_name] = torch.nn.Parameter(param_value).to(device=target_device)\n             quantize_(module, self.quantization_config.get_apply_tensor_subclass())\n \n-    def _process_model_after_weight_loading(self, model):\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\"\n         return\n "
        },
        {
            "sha": "253cc4a0621080597e28baa3abb1e6ec1a631e0c",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -1077,7 +1077,8 @@ class CompressedTensorsConfig(QuantizationConfigMixin):\n         config_groups (`typing.Dict[str, typing.Union[ForwardRef('QuantizationScheme'), typing.List[str]]]`, *optional*):\n             dictionary mapping group name to a quantization scheme definition\n         format (`str`, *optional*, defaults to `\"dense\"`):\n-            format the model is represented as\n+            format the model is represented as. Set `run_compressed` True to execute model as the\n+            compressed format if not `dense`\n         quantization_status (`QuantizationStatus`, *optional*, defaults to `\"initialized\"`):\n             status of model in the quantization lifecycle, ie 'initialized', 'calibration', 'frozen'\n         kv_cache_scheme (`typing.Union[QuantizationArgs, NoneType]`, *optional*):\n@@ -1090,6 +1091,8 @@ class CompressedTensorsConfig(QuantizationConfigMixin):\n             configuration for sparsity compression\n         quant_method (`str`, *optional*, defaults to `\"compressed-tensors\"`):\n             do not override, should be compressed-tensors\n+        run_compressed (`bool`, *optional*, defaults to `True`): alter submodules (usually linear) in order to\n+            emulate compressed model execution if True, otherwise use default submodule\n     \"\"\"\n \n     def __init__(\n@@ -1102,14 +1105,17 @@ def __init__(\n         ignore: Optional[List[str]] = None,\n         sparsity_config: Dict[str, Any] = None,\n         quant_method: str = \"compressed-tensors\",\n+        run_compressed: bool = True,\n         **kwargs,\n     ):\n-        from compressed_tensors import QuantizationConfig\n         from compressed_tensors.config import SparsityCompressionConfig\n+        from compressed_tensors.quantization import QuantizationConfig\n \n         self.quantization_config = None\n         self.sparsity_config = None\n \n+        self.run_compressed = run_compressed\n+\n         # parse from dict to load nested QuantizationScheme objects\n         if config_groups or kv_cache_scheme:\n             self.quantization_config = QuantizationConfig.parse_obj(\n@@ -1121,6 +1127,7 @@ def __init__(\n                     \"kv_cache_scheme\": kv_cache_scheme,\n                     \"global_compression_ratio\": global_compression_ratio,\n                     \"ignore\": ignore,\n+                    \"run_compressed\": run_compressed,\n                     **kwargs,\n                 }\n             )\n@@ -1149,6 +1156,7 @@ def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n \n         Returns:\n             [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\n+\n         \"\"\"\n \n         if \"quantization_config\" in config_dict:\n@@ -1200,6 +1208,9 @@ def to_diff_dict(self) -> Dict[str, Any]:\n \n         return serializable_config_dict\n \n+    def get_loading_attributes(self):\n+        return {\"run_compressed\": self.run_compressed}\n+\n \n @dataclass\n class FbgemmFp8Config(QuantizationConfigMixin):"
        },
        {
            "sha": "8992cd3d9bd470b3b1bd9209ceaa440d5155e65b",
            "filename": "tests/quantization/compressed_tensor/test_load_sparse_model.py",
            "status": "added",
            "additions": 80,
            "deletions": 0,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2Ftest_load_sparse_model.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -0,0 +1,80 @@\n+import gc\n+import unittest\n+\n+from transformers import AutoModelForCausalLM\n+from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_compressed_tensors\n+@require_torch\n+class CompressedTensorsTest(unittest.TestCase):\n+    model_sparse_uncompressed = \"horheynm/llama2.c_stories15M_pruned_50.2of4_uncompressed\"\n+    model_sparse_compressed = \"horheynm/llama2.c_stories15M_pruned_50.2of4_compressed\"\n+\n+    prompt = \"Paris is the capital of which country?\"\n+\n+    stubs = [model_sparse_uncompressed, model_sparse_compressed]\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_compressed_uncompressed_model_shapes(self):\n+        \"\"\"\n+        Check that the weights are the same between\n+         uncompressed and compressed-decompressed model\n+        Sparse compressed modules' weights are \"packed\" and shape/value will\n+         differ\n+        \"\"\"\n+\n+        def _has_nested_attr(obj, attr_path):\n+            attrs = attr_path.split(\".\")\n+            for attr in attrs:\n+                if not hasattr(obj, attr):\n+                    return None\n+                obj = getattr(obj, attr)\n+            return obj\n+\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n+        uncompressed_model = AutoModelForCausalLM.from_pretrained(\n+            self.model_sparse_uncompressed,\n+        )\n+\n+        compressed_model_decompressed = AutoModelForCausalLM.from_pretrained(\n+            self.model_sparse_compressed,\n+        )\n+\n+        for name, submodule in iter_named_leaf_modules(\n+            uncompressed_model,\n+        ):\n+            if comp_decomp_obj := _has_nested_attr(compressed_model_decompressed, name):\n+                if hasattr(submodule, \"weight\"):\n+                    assert torch.equal(submodule.weight, comp_decomp_obj.weight)\n+\n+    def test_run_compressed_outputs_match(self):\n+        \"\"\"Check that uncompressed and compressed-decompressed model outputs are the same\"\"\"\n+\n+        from transformers import AutoTokenizer\n+\n+        for stub in self.stubs:\n+            tokenizer = AutoTokenizer.from_pretrained(stub)\n+            input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n+\n+            uncompressed_model = AutoModelForCausalLM.from_pretrained(\n+                self.model_sparse_uncompressed,\n+            )\n+            output_rc_true = uncompressed_model.generate(input_ids, max_new_tokens=100)\n+\n+            compressed_model_decompressed = AutoModelForCausalLM.from_pretrained(\n+                self.model_sparse_compressed,\n+            )\n+            output_rc_false = compressed_model_decompressed.generate(input_ids, max_new_tokens=100)\n+\n+            assert tokenizer.decode(output_rc_true[0]) == tokenizer.decode(output_rc_false[0])"
        },
        {
            "sha": "b168ca382ccefa194a1f264390649d1b8f3e87b2",
            "filename": "tests/quantization/compressed_tensor/test_run_compressed_model.py",
            "status": "added",
            "additions": 94,
            "deletions": 0,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4e404fdd0074a163d1b8b85e54ae8b05d949375/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4e404fdd0074a163d1b8b85e54ae8b05d949375/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2Ftest_run_compressed_model.py?ref=e4e404fdd0074a163d1b8b85e54ae8b05d949375",
            "patch": "@@ -0,0 +1,94 @@\n+import gc\n+import unittest\n+\n+from transformers import AutoModelForCausalLM\n+from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_compressed_tensors\n+@require_torch\n+class CompressedTensorsTest(unittest.TestCase):\n+    tinyllama_w4a16 = \"nm-testing/tinyllama-w4a16-compressed-hf-quantizer\"\n+    tinyllama_w8a8 = \"nm-testing/tinyllama-w8a8-compressed-hf-quantizer\"\n+\n+    prompt = \"Paris is the capital of which country?\"\n+\n+    stubs = [tinyllama_w4a16, tinyllama_w8a8]\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_default_run_compressed__True(self):\n+        from compressed_tensors.linear.compressed_linear import CompressedLinear\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n+        for stub in self.stubs:\n+            model = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+            )\n+            compressed_linear_counts = 0\n+\n+            for _, submodule in iter_named_leaf_modules(\n+                model,\n+            ):\n+                if isinstance(submodule, CompressedLinear):\n+                    compressed_linear_counts += 1\n+\n+            # some linear models are not compressed - ex. lm_head\n+            assert compressed_linear_counts > 0\n+\n+    def test_default_run_compressed__False(self):\n+        from compressed_tensors.linear.compressed_linear import CompressedLinear\n+        from compressed_tensors.quantization.utils import iter_named_leaf_modules\n+\n+        from transformers.utils.quantization_config import CompressedTensorsConfig\n+\n+        quantization_config = CompressedTensorsConfig(run_compressed=False)\n+\n+        for stub in self.stubs:\n+            model = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+                quantization_config=quantization_config,\n+            )\n+            compressed_linear_counts = 0\n+\n+            for _, submodule in iter_named_leaf_modules(\n+                model,\n+            ):\n+                if isinstance(submodule, CompressedLinear):\n+                    compressed_linear_counts += 1\n+\n+            # No modules should be CompressedLinear\n+            assert compressed_linear_counts == 0\n+\n+    def test_run_compressed_outputs_match(self):\n+        \"\"\"Check that run_compressed=True/False output are the same\"\"\"\n+\n+        from transformers import AutoTokenizer\n+        from transformers.utils.quantization_config import CompressedTensorsConfig\n+\n+        quantization_config = CompressedTensorsConfig(run_compressed=False)\n+\n+        for stub in self.stubs:\n+            tokenizer = AutoTokenizer.from_pretrained(stub)\n+            input_ids = tokenizer(self.prompt, return_tensors=\"pt\").input_ids\n+\n+            model_run_compressed__True = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+            )\n+            output_rc_true = model_run_compressed__True.generate(input_ids, max_new_tokens=100)\n+\n+            model_run_compressed__False = AutoModelForCausalLM.from_pretrained(\n+                stub,\n+                quantization_config=quantization_config,\n+            )\n+            output_rc_false = model_run_compressed__False.generate(input_ids, max_new_tokens=100)\n+\n+            assert tokenizer.decode(output_rc_true[0]) == tokenizer.decode(output_rc_false[0])"
        }
    ],
    "stats": {
        "total": 268,
        "additions": 250,
        "deletions": 18
    }
}