{
    "author": "rahzaazhar",
    "message": "added Textnet fast image processor (#39884)\n\n* feat: add fast image processor implementation for TextNet model\n\n* chore: override to_dict method to TextNetImageProcessorFast for slow processor compatibility tests\n\n* chore: update init method\n\n* chore: coding and style checks\n\n* chore: fixed code quality issue\n\n* chore: override resize to handle size_divisor, move all preprocessing logic to child class\n\n* fix: autoImageProcessor issue for textnet\n\n* chore: cleanup\n\n* simplify resize\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "5521c62b89b2ffbd061804c241b22618d0f0d4d5",
    "files": [
        {
            "sha": "36382664b852bf962fc38f8a81ec6de32041cbda",
            "filename": "docs/source/en/model_doc/textnet.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5521c62b89b2ffbd061804c241b22618d0f0d4d5/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5521c62b89b2ffbd061804c241b22618d0f0d4d5/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md?ref=5521c62b89b2ffbd061804c241b22618d0f0d4d5",
            "patch": "@@ -47,6 +47,11 @@ TextNet is the backbone for Fast, but can also be used as an efficient text/imag\n [[autodoc]] TextNetImageProcessor\n     - preprocess\n \n+## TextNetImageProcessorFast\n+\n+[[autodoc]] TextNetImageProcessorFast\n+    - preprocess\n+\n ## TextNetModel\n \n [[autodoc]] TextNetModel"
        },
        {
            "sha": "b6c4143b2dbead3cb8f72a7a993cea7ba7dc6876",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=5521c62b89b2ffbd061804c241b22618d0f0d4d5",
            "patch": "@@ -169,7 +169,8 @@\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin2sr\", (\"Swin2SRImageProcessor\", \"Swin2SRImageProcessorFast\")),\n             (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"table-transformer\", (\"DetrImageProcessor\", None)),\n+            (\"table-transformer\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n+            (\"textnet\", (\"TextNetImageProcessor\", \"TextNetImageProcessorFast\")),\n             (\"timesformer\", (\"VideoMAEImageProcessor\", None)),\n             (\"timm_wrapper\", (\"TimmWrapperImageProcessor\", None)),\n             (\"tvlt\", (\"TvltImageProcessor\", None)),"
        },
        {
            "sha": "0785102f9e209fa4d9f11ef82dd825daff5f7f25",
            "filename": "src/transformers/models/textnet/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py?ref=5521c62b89b2ffbd061804c241b22618d0f0d4d5",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_textnet import *\n     from .image_processing_textnet import *\n+    from .image_processing_textnet_fast import *\n     from .modeling_textnet import *\n else:\n     import sys"
        },
        {
            "sha": "f6a163c4268738d694f75a4595a28e65b7f680f5",
            "filename": "src/transformers/models/textnet/image_processing_textnet_fast.py",
            "status": "added",
            "additions": 163,
            "deletions": 0,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5521c62b89b2ffbd061804c241b22618d0f0d4d5/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet_fast.py?ref=5521c62b89b2ffbd061804c241b22618d0f0d4d5",
            "patch": "@@ -0,0 +1,163 @@\n+# coding=utf-8\n+# Copyright 2025 the Fast authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for TextNet.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_transforms import (\n+    get_resize_output_image_size,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class TextNetFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    size_divisor (`int`, *optional*, defaults to 32):\n+        Ensures height and width are rounded to a multiple of this value after resizing.\n+    \"\"\"\n+\n+    size_divisor: Optional[int]\n+\n+\n+@auto_docstring\n+class TextNetImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 640}\n+    default_to_square = False\n+    crop_size = {\"height\": 224, \"width\": 224}\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    size_divisor = 32\n+    valid_kwargs = TextNetFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[TextNetFastImageProcessorKwargs]) -> None:\n+        super().__init__(**kwargs)\n+\n+    @auto_docstring\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[TextNetFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def resize(\n+        self,\n+        image: \"torch.Tensor\",\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n+        size_divisor: int = 32,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        if size.shortest_edge:\n+            new_size = get_resize_output_image_size(\n+                image,\n+                size=size.shortest_edge,\n+                default_to_square=False,\n+                input_data_format=ChannelDimension.FIRST,\n+            )\n+        else:\n+            raise ValueError(f\"Size must contain 'shortest_edge' key. Got {size}.\")\n+        # ensure height and width are divisible by size_divisor\n+        height, width = new_size\n+        if height % size_divisor != 0:\n+            height += size_divisor - (height % size_divisor)\n+        if width % size_divisor != 0:\n+            width += size_divisor - (width % size_divisor)\n+\n+        return super().resize(\n+            image, SizeDict(height=height, width=width), interpolation=interpolation, antialias=antialias\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        size_divisor: int,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images, size=size, interpolation=interpolation, size_divisor=size_divisor\n+                )\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_center_crop:\n+                stacked_images = self.center_crop(stacked_images, crop_size)\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"TextNetImageProcessorFast\"]"
        },
        {
            "sha": "7006b7a712ac94bc029fc39e96024af8c1a66408",
            "filename": "tests/models/textnet/test_image_processing_textnet.py",
            "status": "modified",
            "additions": 24,
            "deletions": 18,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/5521c62b89b2ffbd061804c241b22618d0f0d4d5/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5521c62b89b2ffbd061804c241b22618d0f0d4d5/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py?ref=5521c62b89b2ffbd061804c241b22618d0f0d4d5",
            "patch": "@@ -16,14 +16,17 @@\n import unittest\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_vision_available():\n     from transformers import TextNetImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import TextNetImageProcessorFast\n+\n \n class TextNetImageProcessingTester:\n     def __init__(\n@@ -94,6 +97,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class TextNetImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = TextNetImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = TextNetImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -104,22 +108,24 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n-        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n-\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+            self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})"
        }
    ],
    "stats": {
        "total": 214,
        "additions": 195,
        "deletions": 19
    }
}