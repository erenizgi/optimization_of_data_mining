{
    "author": "sushmanthreddy",
    "message": "Samhq model addition  (#35147)\n\n* added the configuartion for sam_hq\n\n* added the modeelling for sam_hq\n\n* added the sam hq mask decoder with hq features\n\n* added the code for the samhq\n\n* added the code for the samhq\n\n* added the code for the samhq\n\n* Delete src/transformers/models/sam_hq/modelling_sam_hq.py\n\n* added the code for the samhq\n\n* added the code for the samhq\n\n* added the chnages for the modeelling\n\n* added the code for sam hq for image processing\n\n* added code for the sam hq model\n\n* added the required changes\n\n* added the changes\n\n* added the key mappings for the sam hq\n\n* adding the working code of samhq\n\n* added the required files\n\n* adding the pt object\n\n* added the push to hub account\n\n* added the args for the sam maks  decoder\n\n* added the args for the sam hq vision config\n\n* aded the some more documentation\n\n* removed the unecessary spaces\n\n* all required chnages\n\n* removed the image processor\n\n* added the required file\n\n* added the changes for the checkcopies\n\n* added the code for modular file\n\n* added the changes for the __init file\n\n* added the code for the interm embeds\n\n* added the code for sam hq\n\n* added the changes for modular file\n\n* added the test file\n\n* added the changes required\n\n* added the changes required\n\n* added the code for the\n\n* added the cl errors\n\n* added the changes\n\n* added the required changes\n\n* added the some code\n\n* added the code for the removing image processor\n\n* added the test dimensins\n\n* added the code for the removing extra used variables\n\n* added the code for modeluar file hf_mlp for a better name\n\n* removed abbrevaation in core functionality\n\n* removed abbrevaation in core functionality\n\n* .contiguous() method is often used to ensure that the tensor is stored in a contiguous block of memory\n\n* added the code which is after make fixup\n\n* added some test for the intermediate embeddings test\n\n* added the code for the torch support in sam hq\n\n* added the code for the updated modular file\n\n* added the changes for documentations as mentioned\n\n* removed the heading\n\n* add the changes for the code\n\n* first mentioned issue resolved\n\n* added the changes code to processor\n\n* added the easy loading to init file\n\n* added the changes to code\n\n* added the code to changes\n\n* added the code to work\n\n* added the code for sam hq\n\n* added the code for sam hq\n\n* added the code for the point pad value\n\n* added the small test for the image embeddings and intermediate embedding\n\n* added the code\n\n* added the code\n\n* added the code for the tests\n\n* added the code\n\n* added ythe code for the processor file\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code for tests and some checks\n\n* added some code\n\n* added the code\n\n* added the code\n\n* added some code\n\n* added some code\n\n* added the changes for required\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added the code\n\n* added some changes\n\n* added some changes\n\n* removed spaces and quality checks\n\n* added some code\n\n* added some code\n\n* added some code\n\n* added code quality checks\n\n* added the checks for quality checks\n\n* addded some code which fixes test_inference_mask_generation_no_point\n\n* added code for the test_inference_mask_generation_one_point_one_bb\n\n* added code for the test_inference_mask_generation_one_point_one_bb_zero\n\n* added code for the test_inference_mask_generation_one_box\n\n* added some code in modelling for testing\n\n* added some code which sort maks with high score\n\n* added some code\n\n* added some code\n\n* added some code for the move KEYS_TO_MODIFY_MAPPING\n\n* added some code for the  unsqueeze removal\n\n* added some code for the  unsqueeze removal\n\n* added some code\n\n* added some code\n\n* add some code\n\n* added some code\n\n* added some code\n\n* added some testign values changed\n\n* added changes to code in sam hq for readbility purpose\n\n* added pre commit checks\n\n* added the fix samvisionmodel for compatibilty\n\n* added the changes made on sam by cyyever\n\n* fixed the tests for samhq\n\n* added some the code\n\n* added some code related to init file issue during merge conflicts\n\n* remobved the merge conflicts\n\n* added changes mentioned by aruther and mobap\n\n* added changes mentioned by aruther and mobap\n\n* solving quality checks\n\n* added the changes for input clearly\n\n* added the changes\n\n* added changes in mask generation file rgearding model inputs and  sam hq quargs  in processor file\n\n* added changes in processor file\n\n* added the  Setup -> setupclass conversion\n\n* added the code mentioned for processor\n\n* added changes for the code\n\n* added some code\n\n* added some code\n\n* added some code\n\n---------\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "65e940208c38ebf82b5d7a2441eec361d2c968b1",
    "files": [
        {
            "sha": "171ca01f6520d92a6f6583b1be4a12170477aaa8",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -1017,6 +1017,8 @@\n         title: Qwen2VL\n       - local: model_doc/sam\n         title: Segment Anything\n+      - local: model_doc/sam_hq\n+        title: Segment Anything High Quality\n       - local: model_doc/shieldgemma2\n         title: ShieldGemma2\n       - local: model_doc/siglip"
        },
        {
            "sha": "8c60b86117fdf488aa616413d9820b7938627b6d",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,127 @@\n+# SAM-HQ\n+\n+## Overview\n+\n+SAM-HQ (High-Quality Segment Anything Model) was proposed in [Segment Anything in High Quality](https://arxiv.org/pdf/2306.01567.pdf) by Lei Ke, Mingqiao Ye, Martin Danelljan, Yifan Liu, Yu-Wing Tai, Chi-Keung Tang, Fisher Yu.\n+\n+The model is an enhancement to the original SAM model that produces significantly higher quality segmentation masks while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability.\n+\n+![example image](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-output.png)\n+\n+\n+SAM-HQ introduces several key improvements over the original SAM model:\n+\n+1. High-Quality Output Token: A learnable token injected into SAM's mask decoder for higher quality mask prediction\n+2. Global-local Feature Fusion: Combines features from different stages of the model for improved mask details\n+3. Training Data: Uses a carefully curated dataset of 44K high-quality masks instead of SA-1B\n+4. Efficiency: Adds only 0.5% additional parameters while significantly improving mask quality\n+5. Zero-shot Capability: Maintains SAM's strong zero-shot performance while improving accuracy\n+\n+The abstract from the paper is the following:\n+\n+*The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures. We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced dataset of 44k masks, which takes only 4 hours on 8 GPUs.*\n+\n+Tips:\n+\n+- SAM-HQ produces higher quality masks than the original SAM model, particularly for objects with intricate structures and fine details\n+- The model predicts binary masks with more accurate boundaries and better handling of thin structures\n+- Like SAM, the model performs better with input 2D points and/or input bounding boxes\n+- You can prompt multiple points for the same image and predict a single high-quality mask\n+- The model maintains SAM's zero-shot generalization capabilities\n+- SAM-HQ only adds ~0.5% additional parameters compared to SAM\n+- Fine-tuning the model is not supported yet\n+\n+This model was contributed by [sushmanth](https://huggingface.co/sushmanth).\n+The original code can be found [here](https://github.com/SysCV/SAM-HQ).\n+\n+Below is an example on how to run mask generation given an image and a 2D point:\n+\n+```python\n+import torch\n+from PIL import Image\n+import requests\n+from transformers import SamHQModel, SamHQProcessor\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\").to(device)\n+processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+input_points = [[[450, 600]]]  # 2D location of a window in the image\n+\n+inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(device)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+masks = processor.image_processor.post_process_masks(\n+    outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n+)\n+scores = outputs.iou_scores\n+```\n+\n+You can also process your own masks alongside the input images in the processor to be passed to the model:\n+\n+```python\n+import torch\n+from PIL import Image\n+import requests\n+from transformers import SamHQModel, SamHQProcessor\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\").to(device)\n+processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+mask_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+segmentation_map = Image.open(requests.get(mask_url, stream=True).raw).convert(\"1\")\n+input_points = [[[450, 600]]]  # 2D location of a window in the image\n+\n+inputs = processor(raw_image, input_points=input_points, segmentation_maps=segmentation_map, return_tensors=\"pt\").to(device)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+\n+masks = processor.image_processor.post_process_masks(\n+    outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu()\n+)\n+scores = outputs.iou_scores\n+```\n+\n+\n+## Resources\n+\n+A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SAM-HQ:\n+\n+- Demo notebook for using the model (coming soon)\n+- Paper implementation and code: [SAM-HQ GitHub Repository](https://github.com/SysCV/SAM-HQ)\n+\n+## SamHQConfig\n+\n+[[autodoc]] SamHQConfig\n+\n+## SamHQVisionConfig\n+\n+[[autodoc]] SamHQVisionConfig\n+\n+## SamHQMaskDecoderConfig\n+\n+[[autodoc]] SamHQMaskDecoderConfig\n+\n+## SamHQPromptEncoderConfig\n+\n+[[autodoc]] SamHQPromptEncoderConfig\n+\n+## SamHQProcessor\n+\n+[[autodoc]] SamHQProcessor\n+\n+## SamHQVisionModel\n+\n+[[autodoc]] SamHQVisionModel\n+\n+\n+## SamHQModel\n+\n+[[autodoc]] SamHQModel\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "5feb76f1a13de2b899f078609d9b4cce1a65c612",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -254,6 +254,7 @@\n     from .rt_detr_v2 import *\n     from .rwkv import *\n     from .sam import *\n+    from .sam_hq import *\n     from .seamless_m4t import *\n     from .seamless_m4t_v2 import *\n     from .segformer import *"
        },
        {
            "sha": "bbef0bc920a62e52798aca3cfb1b1b82ec2e0f4c",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -286,6 +286,8 @@\n         (\"rt_detr_v2\", \"RTDetrV2Config\"),\n         (\"rwkv\", \"RwkvConfig\"),\n         (\"sam\", \"SamConfig\"),\n+        (\"sam_hq\", \"SamHQConfig\"),\n+        (\"sam_hq_vision_model\", \"SamHQVisionConfig\"),\n         (\"sam_vision_model\", \"SamVisionConfig\"),\n         (\"seamless_m4t\", \"SeamlessM4TConfig\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2Config\"),\n@@ -658,6 +660,8 @@\n         (\"rt_detr_v2\", \"RT-DETRv2\"),\n         (\"rwkv\", \"RWKV\"),\n         (\"sam\", \"SAM\"),\n+        (\"sam_hq\", \"SAM-HQ\"),\n+        (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n         (\"seamless_m4t\", \"SeamlessM4T\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2\"),\n@@ -807,6 +811,7 @@\n         (\"qwen2_5_vl_text\", \"qwen2_5_vl\"),\n         (\"qwen2_vl_text\", \"qwen2_vl\"),\n         (\"sam_vision_model\", \"sam\"),\n+        (\"sam_hq_vision_model\", \"sam_hq\"),\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),\n     ]"
        },
        {
            "sha": "ee941eed3594b95f8d559640bec3058bbcff17e3",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -141,6 +141,7 @@\n             (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n             (\"sam\", (\"SamImageProcessor\",)),\n+            (\"sam_hq\", (\"SamImageProcessor\",)),\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n             (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),"
        },
        {
            "sha": "fe83a8d1b97465114d8ef1a0b34220f278119cd8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -257,6 +257,8 @@\n         (\"rt_detr_v2\", \"RTDetrV2Model\"),\n         (\"rwkv\", \"RwkvModel\"),\n         (\"sam\", \"SamModel\"),\n+        (\"sam_hq\", \"SamHQModel\"),\n+        (\"sam_hq_vision_model\", \"SamHQVisionModel\"),\n         (\"sam_vision_model\", \"SamVisionModel\"),\n         (\"seamless_m4t\", \"SeamlessM4TModel\"),\n         (\"seamless_m4t_v2\", \"SeamlessM4Tv2Model\"),\n@@ -1495,6 +1497,12 @@\n     ]\n )\n \n+MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = OrderedDict(\n+    [\n+        (\"sam_hq\", \"SamHQModel\"),\n+    ]\n+)\n+\n \n MODEL_FOR_KEYPOINT_DETECTION_MAPPING_NAMES = OrderedDict(\n     ["
        },
        {
            "sha": "d49301cd5f41830fdb347dc37a86707d5396eaa7",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -104,6 +104,7 @@\n         (\"qwen2_audio\", \"Qwen2AudioProcessor\"),\n         (\"qwen2_vl\", \"Qwen2VLProcessor\"),\n         (\"sam\", \"SamProcessor\"),\n+        (\"sam_hq\", \"SamHQProcessor\"),\n         (\"seamless_m4t\", \"SeamlessM4TProcessor\"),\n         (\"sew\", \"Wav2Vec2Processor\"),\n         (\"sew-d\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "8074c56727d6360ffe11e960ed07be55f026c196",
            "filename": "src/transformers/models/sam_hq/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_sam_hq import *\n+    from .modeling_sam_hq import *\n+    from .processing_samhq import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "49062efc68d69342312f728fca9f4eef34bb6a10",
            "filename": "src/transformers/models/sam_hq/configuration_sam_hq.py",
            "status": "added",
            "additions": 315,
            "deletions": 0,
            "changes": 315,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconfiguration_sam_hq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,315 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam_hq/modular_sam_hq.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam_hq.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class SamHQPromptEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SamHQPromptEncoderModel`].The [`SamHQPromptEncoderModel`]\n+    module is used to encode the input 2D points and bounding boxes. Instantiating a configuration defaults will yield a\n+    similar configuration to that of the SAM_HQ model. The configuration is used to store the configuration of the model.\n+    [Uminosachi/sam-hq](https://huggingface.co/Uminosachi/sam-hq) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model's output.Read the documentation from\n+    [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+    \"\"\"\n+\n+    base_config_key = \"prompt_encoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        image_size=1024,\n+        patch_size=16,\n+        mask_input_channels=16,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-6,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.image_embedding_size = image_size // patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+\n+\n+class SamHQVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SamHQVisionModel`]. It is used to instantiate a SAM_HQ\n+    vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    defaults will yield a similar configuration to that of the SAM_HQ ViT-h\n+    [facebook/sam_hq-vit-huge](https://huggingface.co/facebook/sam_hq-vit-huge) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        output_channels (`int`, *optional*, defaults to 256):\n+            Dimensionality of the output channels in the Patch Encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input image.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            Expected resolution. Target size of the resized input image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Size of the patches to be extracted from the input image.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string)\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 1e-10):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        qkv_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to add a bias to query, key, value projections.\n+        mlp_ratio (`float`, *optional*, defaults to 4.0):\n+            Ratio of mlp hidden dim to embedding dim.\n+        use_abs_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use absolute position embedding.\n+        use_rel_pos (`bool`, *optional*, defaults to `True`):\n+            Whether to use relative position embedding.\n+        window_size (`int`, *optional*, defaults to 14):\n+            Window size for relative position.\n+        global_attn_indexes (`List[int]`, *optional*, defaults to `[2, 5, 8, 11]`):\n+            The indexes of the global attention layers.\n+        num_pos_feats (`int`, *optional*, defaults to 128):\n+            The dimensionality of the position embedding.\n+        mlp_dim (`int`, *optional*):\n+            The dimensionality of the MLP layer in the Transformer encoder. If `None`, defaults to `mlp_ratio *\n+            hidden_size`.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import (\n+    ...     SamHQVisionConfig,\n+    ...     SamHQVisionModel,\n+    ... )\n+\n+    >>> # Initializing a SamHQVisionConfig with `\"facebook/sam_hq-vit-huge\"` style configuration\n+    >>> configuration = SamHQVisionConfig()\n+\n+    >>> # Initializing a SamHQVisionModel (with random weights) from the `\"facebook/sam_hq-vit-huge\"` style configuration\n+    >>> model = SamHQVisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    base_config_key = \"vision_config\"\n+    model_type = \"sam_hq_vision_model\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        output_channels=256,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        image_size=1024,\n+        patch_size=16,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        attention_dropout=0.0,\n+        initializer_range=1e-10,\n+        qkv_bias=True,\n+        mlp_ratio=4.0,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        num_pos_feats=128,\n+        mlp_dim=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.qkv_bias = qkv_bias\n+        self.mlp_ratio = mlp_ratio\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.num_pos_feats = num_pos_feats\n+        self.mlp_dim = int(hidden_size * mlp_ratio) if mlp_dim is None else mlp_dim\n+\n+\n+class SamHQMaskDecoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SamHQMaskDecoder`]. It is used to instantiate a SAM_HQ\n+    mask decoder to the specified arguments, defining the model architecture. Instantiating a configuration defaults\n+    will yield a similar configuration to that of the SAM_HQ-vit-h\n+    [facebook/sam_hq-vit-huge](https://huggingface.co/facebook/sam_hq-vit-huge) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        hidden_act (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function used inside the `SamHQMaskDecoder` module.\n+        mlp_dim (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 2):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        attention_downsample_rate (`int`, *optional*, defaults to 2):\n+            The downsampling rate of the attention layer.\n+        num_multimask_outputs (`int`, *optional*, defaults to 3):\n+            The number of outputs from the `SamHQMaskDecoder` module. In the Segment Anything paper, this is set to 3.\n+        iou_head_depth (`int`, *optional*, defaults to 3):\n+            The number of layers in the IoU head module.\n+        iou_head_hidden_dim (`int`, *optional*, defaults to 256):\n+            The dimensionality of the hidden states in the IoU head module.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+\n+\n+        vit_dim (`int`, *optional*, defaults to 768):\n+            Dimensionality of the Vision Transformer (ViT) used in the `SamHQMaskDecoder` module.\n+    \"\"\"\n+\n+    base_config_key = \"mask_decoder_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=256,\n+        hidden_act=\"relu\",\n+        mlp_dim=2048,\n+        num_hidden_layers=2,\n+        num_attention_heads=8,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=256,\n+        layer_norm_eps=1e-6,\n+        vit_dim=768,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.mlp_dim = mlp_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_downsample_rate = attention_downsample_rate\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.layer_norm_eps = layer_norm_eps\n+        self.vit_dim = vit_dim\n+\n+\n+class SamHQConfig(PretrainedConfig):\n+    r\"\"\"\n+    [`SamHQConfig`] is the configuration class to store the configuration of a [`SamHQModel`]. It is used to instantiate a\n+    SAM-HQ model according to the specified arguments, defining the vision model, prompt-encoder model and mask decoder\n+    configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n+    SAM-HQ-ViT-H [sushmanth/sam_hq_vit_h](https://huggingface.co/sushmanth/sam_hq_vit_h) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `SamHQVisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQVisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `SamHQPromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQPromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `SamHQMaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQMaskDecoderConfig`].\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+    \"\"\"\n+\n+    model_type = \"sam_hq\"\n+    sub_configs = {\n+        \"prompt_encoder_config\": SamHQPromptEncoderConfig,\n+        \"mask_decoder_config\": SamHQMaskDecoderConfig,\n+        \"vision_config\": SamHQVisionConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        prompt_encoder_config=None,\n+        mask_decoder_config=None,\n+        initializer_range=0.02,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        vision_config = vision_config if vision_config is not None else {}\n+        prompt_encoder_config = prompt_encoder_config if prompt_encoder_config is not None else {}\n+        mask_decoder_config = mask_decoder_config if mask_decoder_config is not None else {}\n+\n+        if isinstance(vision_config, SamHQVisionConfig):\n+            vision_config = vision_config.to_dict()\n+        if isinstance(prompt_encoder_config, SamHQPromptEncoderConfig):\n+            prompt_encoder_config = prompt_encoder_config.to_dict()\n+        if isinstance(mask_decoder_config, SamHQMaskDecoderConfig):\n+            mask_decoder_config = mask_decoder_config.to_dict()\n+\n+        self.vision_config = SamHQVisionConfig(**vision_config)\n+        self.prompt_encoder_config = SamHQPromptEncoderConfig(**prompt_encoder_config)\n+        self.mask_decoder_config = SamHQMaskDecoderConfig(**mask_decoder_config)\n+        self.initializer_range = initializer_range\n+\n+\n+__all__ = [\"SamHQVisionConfig\", \"SamHQMaskDecoderConfig\", \"SamHQPromptEncoderConfig\", \"SamHQConfig\"]"
        },
        {
            "sha": "366b84abfccb8a41ec85905d969205c7abfca743",
            "filename": "src/transformers/models/sam_hq/convert_samhq_to_hf.py",
            "status": "added",
            "additions": 277,
            "deletions": 0,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconvert_samhq_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconvert_samhq_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fconvert_samhq_to_hf.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,277 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+Convert SAM-HQ checkpoints from the original repository.\n+\n+URL: https://github.com/SysCV/sam-hq\n+\n+\"\"\"\n+\n+import argparse\n+\n+import numpy as np\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+\n+from transformers import SamHQConfig, SamHQModel, SamHQProcessor, SamHQVisionConfig, SamImageProcessor\n+\n+\n+def get_config(model_name):\n+    if \"sam_hq_vit_b\" in model_name:\n+        vision_config = SamHQVisionConfig()\n+        vit_dim = 768  # Base model dimension\n+    elif \"sam_hq_vit_l\" in model_name:\n+        vision_config = SamHQVisionConfig(\n+            hidden_size=1024,\n+            num_hidden_layers=24,\n+            num_attention_heads=16,\n+            global_attn_indexes=[5, 11, 17, 23],\n+        )\n+        vit_dim = 1024  # Large model dimension\n+    elif \"sam_hq_vit_h\" in model_name:\n+        vision_config = SamHQVisionConfig(\n+            hidden_size=1280,\n+            num_hidden_layers=32,\n+            num_attention_heads=16,\n+            global_attn_indexes=[7, 15, 23, 31],\n+        )\n+        vit_dim = 1280  # Huge model dimension\n+\n+    # Create mask decoder config with appropriate vit_dim\n+    mask_decoder_config = {\"vit_dim\": vit_dim}\n+\n+    config = SamHQConfig(\n+        vision_config=vision_config,\n+        mask_decoder_config=mask_decoder_config,\n+    )\n+\n+    return config\n+\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"iou_prediction_head.layers.0\": \"iou_prediction_head.proj_in\",\n+    \"iou_prediction_head.layers.1\": \"iou_prediction_head.layers.0\",\n+    \"iou_prediction_head.layers.2\": \"iou_prediction_head.proj_out\",\n+    \"mask_decoder.output_upscaling.0\": \"mask_decoder.upscale_conv1\",\n+    \"mask_decoder.output_upscaling.1\": \"mask_decoder.upscale_layer_norm\",\n+    \"mask_decoder.output_upscaling.3\": \"mask_decoder.upscale_conv2\",\n+    \"mask_downscaling.0\": \"mask_embed.conv1\",\n+    \"mask_downscaling.1\": \"mask_embed.layer_norm1\",\n+    \"mask_downscaling.3\": \"mask_embed.conv2\",\n+    \"mask_downscaling.4\": \"mask_embed.layer_norm2\",\n+    \"mask_downscaling.6\": \"mask_embed.conv3\",\n+    \"point_embeddings\": \"point_embed\",\n+    \"pe_layer.positional_encoding_gaussian_matrix\": \"shared_embedding.positional_embedding\",\n+    \"image_encoder\": \"vision_encoder\",\n+    \"neck.0\": \"neck.conv1\",\n+    \"neck.1\": \"neck.layer_norm1\",\n+    \"neck.2\": \"neck.conv2\",\n+    \"neck.3\": \"neck.layer_norm2\",\n+    \"patch_embed.proj\": \"patch_embed.projection\",\n+    \".norm\": \".layer_norm\",\n+    \"blocks\": \"layers\",\n+    # HQ-specific mappings\n+    \"mask_decoder.hf_token\": \"mask_decoder.hq_token\",\n+    \"mask_decoder.compress_vit_feat.0\": \"mask_decoder.compress_vit_conv1\",\n+    \"mask_decoder.compress_vit_feat.1\": \"mask_decoder.compress_vit_norm\",\n+    \"mask_decoder.compress_vit_feat.3\": \"mask_decoder.compress_vit_conv2\",\n+    \"mask_decoder.embedding_encoder.0\": \"mask_decoder.encoder_conv1\",\n+    \"mask_decoder.embedding_encoder.1\": \"mask_decoder.encoder_norm\",\n+    \"mask_decoder.embedding_encoder.3\": \"mask_decoder.encoder_conv2\",\n+    \"mask_decoder.embedding_maskfeature.0\": \"mask_decoder.mask_conv1\",\n+    \"mask_decoder.embedding_maskfeature.1\": \"mask_decoder.mask_norm\",\n+    \"mask_decoder.embedding_maskfeature.3\": \"mask_decoder.mask_conv2\",\n+    \"mask_decoder.hf_mlp\": \"mask_decoder.hq_mask_mlp\",\n+    # Add patterns for the output_hypernetworks_mlps and hq_mask_mlp\n+    \"output_hypernetworks_mlps.0.layers.0\": \"output_hypernetworks_mlps.0.proj_in\",\n+    \"output_hypernetworks_mlps.0.layers.1\": \"output_hypernetworks_mlps.0.layers.0\",\n+    \"output_hypernetworks_mlps.0.layers.2\": \"output_hypernetworks_mlps.0.proj_out\",\n+    \"output_hypernetworks_mlps.1.layers.0\": \"output_hypernetworks_mlps.1.proj_in\",\n+    \"output_hypernetworks_mlps.1.layers.1\": \"output_hypernetworks_mlps.1.layers.0\",\n+    \"output_hypernetworks_mlps.1.layers.2\": \"output_hypernetworks_mlps.1.proj_out\",\n+    \"output_hypernetworks_mlps.2.layers.0\": \"output_hypernetworks_mlps.2.proj_in\",\n+    \"output_hypernetworks_mlps.2.layers.1\": \"output_hypernetworks_mlps.2.layers.0\",\n+    \"output_hypernetworks_mlps.2.layers.2\": \"output_hypernetworks_mlps.2.proj_out\",\n+    \"output_hypernetworks_mlps.3.layers.0\": \"output_hypernetworks_mlps.3.proj_in\",\n+    \"output_hypernetworks_mlps.3.layers.1\": \"output_hypernetworks_mlps.3.layers.0\",\n+    \"output_hypernetworks_mlps.3.layers.2\": \"output_hypernetworks_mlps.3.proj_out\",\n+    \"hq_mask_mlp.layers.0\": \"hq_mask_mlp.proj_in\",\n+    \"hq_mask_mlp.layers.1\": \"hq_mask_mlp.layers.0\",\n+    \"hq_mask_mlp.layers.2\": \"hq_mask_mlp.proj_out\",\n+}\n+\n+\n+def replace_keys(state_dict):\n+    model_state_dict = {}\n+    state_dict.pop(\"pixel_mean\", None)\n+    state_dict.pop(\"pixel_std\", None)\n+\n+    # Process each key in the state dict\n+    for key, value in state_dict.items():\n+        new_key = key\n+\n+        # Apply static mappings from KEYS_TO_MODIFY_MAPPING\n+        for key_to_modify, replacement in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in new_key:\n+                new_key = new_key.replace(key_to_modify, replacement)\n+\n+        model_state_dict[new_key] = value\n+\n+    # Add mapping for shared embedding for positional embedding\n+    if \"prompt_encoder.shared_embedding.positional_embedding\" in model_state_dict:\n+        model_state_dict[\"shared_image_embedding.positional_embedding\"] = model_state_dict[\n+            \"prompt_encoder.shared_embedding.positional_embedding\"\n+        ]\n+\n+    # Special handling for IOU prediction head keys\n+    # Check if we're missing the expected keys and have the converted ones instead\n+    if (\n+        \"mask_decoder.iou_prediction_head.layers.0.weight\" not in model_state_dict\n+        and \"mask_decoder.iou_prediction_head.proj_in.weight\" in model_state_dict\n+    ):\n+        # Copy the converted key back to the expected format\n+        model_state_dict[\"mask_decoder.iou_prediction_head.layers.0.weight\"] = model_state_dict[\n+            \"mask_decoder.iou_prediction_head.proj_in.weight\"\n+        ]\n+        model_state_dict[\"mask_decoder.iou_prediction_head.layers.0.bias\"] = model_state_dict[\n+            \"mask_decoder.iou_prediction_head.proj_in.bias\"\n+        ]\n+\n+    return model_state_dict\n+\n+\n+def convert_sam_hq_checkpoint(model_name, checkpoint_path, pytorch_dump_folder, push_to_hub, hub_path):\n+    config = get_config(model_name)\n+\n+    state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n+    state_dict = replace_keys(state_dict)\n+\n+    image_processor = SamImageProcessor()\n+    processor = SamHQProcessor(image_processor=image_processor)\n+    hf_model = SamHQModel(config)\n+    hf_model.eval()\n+\n+    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+    hf_model.load_state_dict(state_dict)\n+\n+    hf_model = hf_model.to(device)\n+\n+    # Test the model with a sample image\n+    img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+\n+    input_points = [[[500, 375]]]\n+    input_labels = [[1]]\n+\n+    # Basic test without prompts\n+    inputs = processor(images=np.array(raw_image), return_tensors=\"pt\").to(device)\n+\n+    with torch.no_grad():\n+        hf_model(**inputs)\n+\n+    if model_name == \"sam_hq_vit_b\":\n+        inputs = processor(\n+            images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(device)\n+\n+        with torch.no_grad():\n+            hf_model(**inputs)\n+\n+    elif model_name == \"sam_hq_vit_h\":\n+        inputs = processor(\n+            images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(device)\n+\n+        with torch.no_grad():\n+            hf_model(**inputs)\n+\n+        input_boxes = [[[75.0, 275.0, 1725.0, 850.0]]]\n+\n+        inputs = processor(images=np.array(raw_image), input_boxes=input_boxes, return_tensors=\"pt\").to(device)\n+\n+        with torch.no_grad():\n+            hf_model(**inputs)\n+\n+        input_points = [[[400, 650], [800, 650]]]\n+        input_labels = [[1, 1]]\n+\n+        inputs = processor(\n+            images=np.array(raw_image), input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(device)\n+\n+        with torch.no_grad():\n+            hf_model(**inputs)\n+\n+    if pytorch_dump_folder is not None:\n+        processor.save_pretrained(pytorch_dump_folder)\n+        hf_model.save_pretrained(pytorch_dump_folder)\n+\n+    if push_to_hub:\n+        repo_id = f\"{hub_path}/{model_name}\"\n+        processor.push_to_hub(repo_id)\n+        hf_model.push_to_hub(repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    choices = [\"sam_hq_vit_b\", \"sam_hq_vit_h\", \"sam_hq_vit_l\"]\n+    parser.add_argument(\n+        \"--model_name\",\n+        choices=choices,\n+        type=str,\n+        required=True,\n+        help=\"Name of the SAM-HQ model to convert\",\n+    )\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        type=str,\n+        required=False,\n+        help=\"Path to the SAM-HQ checkpoint (.pth file)\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        type=str,\n+        default=None,\n+        help=\"Path to save the converted model\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=\"store_true\",\n+        help=\"Whether to push the converted model to the hub\",\n+    )\n+    parser.add_argument(\n+        \"--hub_path\",\n+        type=str,\n+        default=\"sushmanth\",\n+        help=\"Hugging Face Hub path where the model will be uploaded\",\n+    )\n+\n+    args = parser.parse_args()\n+\n+    checkpoint_path = args.checkpoint_path\n+    if checkpoint_path is None:\n+        checkpoint_path = hf_hub_download(\"lkeab/hq-sam\", f\"{args.model_name}.pth\")\n+\n+    convert_sam_hq_checkpoint(\n+        args.model_name,\n+        checkpoint_path,\n+        args.pytorch_dump_folder_path,\n+        args.push_to_hub,\n+        args.hub_path,\n+    )"
        },
        {
            "sha": "21d9a60f2d9f46c0874ea4d7259561295e20cf68",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "added",
            "additions": 1793,
            "deletions": 0,
            "changes": 1793,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,1793 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/sam_hq/modular_sam_hq.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_sam_hq.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import collections\n+from dataclasses import dataclass\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_sam_hq import SamHQConfig, SamHQMaskDecoderConfig, SamHQPromptEncoderConfig, SamHQVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@dataclass\n+class SamHQVisionEncoderOutput(ModelOutput):\n+    \"\"\"\n+    Base class for sam_hq vision model's outputs that also contains image embeddings obtained by applying the projection\n+    layer to the pooler_output.\n+\n+    Args:\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The image embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+\n+        intermediate_embeddings (`list(torch.FloatTensor)`, *optional*):\n+            A list of intermediate embeddings collected from certain blocks within the model, typically those without\n+            windowed attention. Each element in the list is of shape `(batch_size, sequence_length, hidden_size)`.\n+            This is specific to SAM-HQ and not present in base SAM.\n+    \"\"\"\n+\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+    intermediate_embeddings: Optional[List[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+class SamHQImageSegmentationOutput(ModelOutput):\n+    \"\"\"\n+    Base class for Segment-Anything model's output\n+\n+    Args:\n+        iou_scores (`torch.FloatTensor` of shape `(batch_size, num_masks)`):\n+            The iou scores of the predicted masks.\n+        pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n+            The predicted low resolutions masks. Needs to be post-processed by the processor\n+        vision_hidden_states  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.\n+        vision_attentions  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n+    vision_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    vision_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    mask_decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+class SamHQPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values):\n+        batch_size, num_channels, height, width = pixel_values.shape\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+            )\n+        if height != self.image_size[0] or width != self.image_size[1]:\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]}).\"\n+            )\n+        embeddings = self.projection(pixel_values).permute(0, 2, 3, 1)\n+        return embeddings\n+\n+\n+class SamHQMLPBlock(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.lin1 = nn.Linear(config.hidden_size, config.mlp_dim)\n+        self.lin2 = nn.Linear(config.mlp_dim, config.hidden_size)\n+        self.act = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.lin1(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.lin2(hidden_states)\n+        return hidden_states\n+\n+\n+class SamHQVisionAttention(nn.Module):\n+    \"\"\"Multi-head Attention block with relative position embeddings.\"\"\"\n+\n+    def __init__(self, config, window_size):\n+        super().__init__()\n+        input_size = (\n+            (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+            if window_size == 0\n+            else (window_size, window_size)\n+        )\n+\n+        self.num_attention_heads = config.num_attention_heads\n+        head_dim = config.hidden_size // config.num_attention_heads\n+        self.scale = head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=config.qkv_bias)\n+        self.proj = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+        self.use_rel_pos = config.use_rel_pos\n+        if self.use_rel_pos:\n+            if input_size is None:\n+                raise ValueError(\"Input size must be provided if using relative positional encoding.\")\n+\n+            # initialize relative positional embeddings\n+            self.rel_pos_h = nn.Parameter(torch.zeros(2 * input_size[0] - 1, head_dim))\n+            self.rel_pos_w = nn.Parameter(torch.zeros(2 * input_size[1] - 1, head_dim))\n+\n+    def get_rel_pos(self, q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Get relative positional embeddings according to the relative positions of\n+            query and key sizes.\n+\n+        Args:\n+            q_size (int):\n+                size of the query.\n+            k_size (int):\n+                size of key k.\n+            rel_pos (`torch.Tensor`):\n+                relative position embeddings (L, channel).\n+\n+        Returns:\n+            Extracted positional embeddings according to relative positions.\n+        \"\"\"\n+        max_rel_dist = int(2 * max(q_size, k_size) - 1)\n+        # Interpolate rel pos.\n+        rel_pos_resized = F.interpolate(\n+            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),\n+            size=max_rel_dist,\n+            mode=\"linear\",\n+        )\n+        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)\n+\n+        # Scale the coords with short length if shapes for q and k are different.\n+        q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)\n+        k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)\n+        relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)\n+\n+        return rel_pos_resized[relative_coords.long()]\n+\n+    def get_decomposed_rel_pos(\n+        self,\n+        query: torch.Tensor,\n+        rel_pos_h: torch.Tensor,\n+        rel_pos_w: torch.Tensor,\n+        q_size: Tuple[int, int],\n+        k_size: Tuple[int, int],\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Calculate decomposed Relative Positional Embeddings from :paper:`mvitv2`.\n+        https://github.com/facebookresearch/mvit/blob/19786631e330df9f3622e5402b4a419a263a2c80/mvit/models/attention.py\n+\n+        Args:\n+            query (`torch.Tensor`):\n+                query q in the attention layer with shape (batch_size, query_height * query_width, channel).\n+            rel_pos_h (`torch.Tensor`):\n+                relative position embeddings (Lh, channel) for height axis.\n+            rel_pos_w (`torch.Tensor`):\n+                relative position embeddings (Lw, channel) for width axis.\n+            q_size (tuple):\n+                spatial sequence size of query q with (query_height, query_width).\n+            k_size (tuple):\n+                spatial sequence size of key k with (key_height, key_width).\n+\n+        Returns:\n+            decomposed_rel_pos (`torch.Tensor`):\n+                decomposed relative position embeddings.\n+        \"\"\"\n+        query_height, query_width = q_size\n+        key_height, key_width = k_size\n+        relative_position_height = self.get_rel_pos(query_height, key_height, rel_pos_h)\n+        relative_position_width = self.get_rel_pos(query_width, key_width, rel_pos_w)\n+\n+        batch_size, _, dim = query.shape\n+        reshaped_query = query.reshape(batch_size, query_height, query_width, dim)\n+        rel_h = torch.einsum(\"bhwc,hkc->bhwk\", reshaped_query, relative_position_height)\n+        rel_w = torch.einsum(\"bhwc,wkc->bhwk\", reshaped_query, relative_position_width)\n+\n+        decomposed_rel_pos = rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]\n+\n+        return decomposed_rel_pos\n+\n+    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (3, batch_size, nHead, height * width, channel)\n+        qkv = (\n+            self.qkv(hidden_states)\n+            .reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+            .permute(2, 0, 3, 1, 4)\n+        )\n+        # q, k, v with shape (batch_size * nHead, height * width, channel)\n+        query, key, value = qkv.reshape(3, batch_size * self.num_attention_heads, height * width, -1).unbind(0)\n+\n+        attn_weights = (query * self.scale) @ key.transpose(-2, -1)\n+\n+        if self.use_rel_pos:\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            )\n+            decomposed_rel_pos = decomposed_rel_pos.reshape_as(attn_weights)\n+            attn_weights = attn_weights + decomposed_rel_pos\n+\n+        attn_weights = torch.nn.functional.softmax(attn_weights, dtype=torch.float32, dim=-1).to(query.dtype)\n+\n+        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+\n+        attn_output = (attn_probs @ value).reshape(batch_size, self.num_attention_heads, height, width, -1)\n+        attn_output = attn_output.permute(0, 2, 3, 1, 4).reshape(batch_size, height, width, -1)\n+\n+        attn_output = self.proj(attn_output)\n+\n+        if output_attentions:\n+            outputs = (attn_output, attn_weights)\n+        else:\n+            outputs = (attn_output, None)\n+\n+        return outputs\n+\n+\n+class SamHQVisionSdpaAttention(SamHQVisionAttention):\n+    \"\"\"\n+    Multi-head Attention block with relative position embeddings.\n+    Using SDPA instead of the default attention.\n+    \"\"\"\n+\n+    def __init__(self, config, window_size):\n+        super().__init__(config, window_size)\n+\n+    def forward(self, hidden_states: torch.Tensor, output_attentions=False) -> torch.Tensor:\n+        if output_attentions:\n+            logger.warning_once(\n+                \"`SamHQVisionSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n+                \"`output_attentions=True`. Falling back to the manual attention implementation, but \"\n+                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n+                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                output_attentions=output_attentions,\n+            )\n+\n+        batch_size, height, width, _ = hidden_states.shape\n+        # qkv with shape (3, B, nHead, H * W, C)\n+        qkv = (\n+            self.qkv(hidden_states)\n+            .reshape(batch_size, height * width, 3, self.num_attention_heads, -1)\n+            .permute(2, 0, 3, 1, 4)\n+        )\n+        # q, k, v with shape (B * nHead, H * W, C)\n+        query, key, value = qkv.reshape(3, batch_size * self.num_attention_heads, height * width, -1).unbind(0)\n+\n+        attn_bias = None\n+        if self.use_rel_pos:\n+            decomposed_rel_pos = self.get_decomposed_rel_pos(\n+                query, self.rel_pos_h, self.rel_pos_w, (height, width), (height, width)\n+            )\n+            decomposed_rel_pos = decomposed_rel_pos.reshape(\n+                batch_size, self.num_attention_heads, height * width, height * width\n+            )\n+            attn_bias = decomposed_rel_pos\n+\n+        query = query.view(batch_size, self.num_attention_heads, height * width, -1)\n+        key = key.view(batch_size, self.num_attention_heads, height * width, -1)\n+        value = value.view(batch_size, self.num_attention_heads, height * width, -1)\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(query, key, value, attn_mask=attn_bias)\n+\n+        attn_output = (\n+            attn_output.view(batch_size, self.num_attention_heads, height, width, -1)\n+            .permute(0, 2, 3, 1, 4)\n+            .reshape(batch_size, height, width, -1)\n+        )\n+\n+        attn_output = self.proj(attn_output)\n+\n+        return attn_output, None\n+\n+\n+SAM_HQ_VISION_ATTENTION_CLASSES = {\n+    \"eager\": SamHQVisionAttention,\n+    \"sdpa\": SamHQVisionSdpaAttention,\n+}\n+\n+\n+class SamHQVisionLayer(nn.Module):\n+    def __init__(self, config, window_size):\n+        super().__init__()\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attn = SAM_HQ_VISION_ATTENTION_CLASSES[config._attn_implementation](config, window_size)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = SamHQMLPBlock(config)\n+        self.window_size = window_size\n+\n+    def window_partition(self, hidden_states: torch.Tensor, window_size: int) -> Tuple[torch.Tensor, Tuple[int, int]]:\n+        \"\"\"\n+        Args:\n+        Partition into non-overlapping windows with padding if needed.\n+            hidden_states (tensor): input tokens with [batch_size, height, width, channel]. window_size (int): window\n+            size.\n+\n+        Returns:\n+            windows: windows after partition with [batch_size * num_windows, window_size, window_size, channel].\n+            (pad_height, pad_width): padded height and width before partition\n+        \"\"\"\n+        batch_size, height, width, channel = hidden_states.shape\n+\n+        pad_h = (window_size - height % window_size) % window_size\n+        pad_w = (window_size - width % window_size) % window_size\n+        hidden_states = F.pad(hidden_states, (0, 0, 0, pad_w, 0, pad_h))\n+        pad_height, pad_width = height + pad_h, width + pad_w\n+\n+        hidden_states = hidden_states.reshape(\n+            batch_size, pad_height // window_size, window_size, pad_width // window_size, window_size, channel\n+        )\n+        windows = hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous().reshape(-1, window_size, window_size, channel)\n+        return windows, (pad_height, pad_width)\n+\n+    def window_unpartition(\n+        self, windows: torch.Tensor, window_size: int, padding_shape: Tuple[int, int], original_shape: Tuple[int, int]\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+        Window unpartition into original sequences and removing padding.\n+            hidden_states (tensor):\n+                input tokens with [batch_size * num_windows, window_size, window_size, channel].\n+            window_size (int):\n+                window size.\n+            padding_shape (Tuple):\n+                padded height and width (pad_height, pad_width).\n+            original_shape (Tuple): original height and width (height, width) before padding.\n+\n+        Returns:\n+            hidden_states: unpartitioned sequences with [batch_size, height, width, channel].\n+        \"\"\"\n+        pad_height, pad_width = padding_shape\n+        height, width = original_shape\n+        batch_size = windows.shape[0] // (pad_height * pad_width // window_size // window_size)\n+        hidden_states = windows.reshape(\n+            batch_size, pad_height // window_size, pad_width // window_size, window_size, window_size, -1\n+        )\n+        hidden_states = (\n+            hidden_states.permute(0, 1, 3, 2, 4, 5).contiguous().reshape(batch_size, pad_height, pad_width, -1)\n+        )\n+\n+        hidden_states = hidden_states[:, :height, :width, :].contiguous()\n+        return hidden_states\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        # Window partition\n+        if self.window_size > 0:\n+            height, width = hidden_states.shape[1], hidden_states.shape[2]\n+            hidden_states, padding_shape = self.window_partition(hidden_states, self.window_size)\n+\n+        hidden_states, attn_weights = self.attn(\n+            hidden_states=hidden_states,\n+            output_attentions=output_attentions,\n+        )\n+        # Reverse window partition\n+        if self.window_size > 0:\n+            hidden_states = self.window_unpartition(hidden_states, self.window_size, padding_shape, (height, width))\n+\n+        hidden_states = residual + hidden_states\n+        layernorm_output = self.layer_norm2(hidden_states)\n+        hidden_states = hidden_states + self.mlp(layernorm_output)\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class SamHQVisionNeck(nn.Module):\n+    def __init__(self, config: SamHQVisionConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.conv1 = nn.Conv2d(config.hidden_size, config.output_channels, kernel_size=1, bias=False)\n+        self.layer_norm1 = SamHQLayerNorm(config.output_channels, data_format=\"channels_first\")\n+        self.conv2 = nn.Conv2d(config.output_channels, config.output_channels, kernel_size=3, padding=1, bias=False)\n+        self.layer_norm2 = SamHQLayerNorm(config.output_channels, data_format=\"channels_first\")\n+\n+    def forward(self, hidden_states):\n+        hidden_states = hidden_states.permute(0, 3, 1, 2)\n+        hidden_states = self.conv1(hidden_states)\n+        hidden_states = self.layer_norm1(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        return hidden_states\n+\n+\n+class SamHQVisionEncoder(nn.Module):\n+    def __init__(self, config: SamHQVisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.image_size = config.image_size\n+\n+        self.patch_embed = SamHQPatchEmbeddings(config)\n+\n+        self.pos_embed = None\n+        if config.use_abs_pos:\n+            # Initialize absolute positional embedding with pretrain image size.\n+            self.pos_embed = nn.Parameter(\n+                torch.zeros(\n+                    1,\n+                    config.image_size // config.patch_size,\n+                    config.image_size // config.patch_size,\n+                    config.hidden_size,\n+                )\n+            )\n+\n+        self.layers = nn.ModuleList()\n+        for i in range(config.num_hidden_layers):\n+            layer = SamHQVisionLayer(\n+                config,\n+                window_size=config.window_size if i not in config.global_attn_indexes else 0,\n+            )\n+            self.layers.append(layer)\n+\n+        self.neck = SamHQVisionNeck(config)\n+\n+        self.gradient_checkpointing = False\n+\n+    def get_input_embeddings(self):\n+        return self.patch_embed\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, SamHQVisionEncoderOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.patch_embed(pixel_values)\n+        if self.pos_embed is not None:\n+            hidden_states = hidden_states + self.pos_embed\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+        intermediate_embeddings = []\n+\n+        for i, layer_module in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            # Collect embeddings from non-windowed blocks\n+            if hasattr(layer_module, \"window_size\") and layer_module.window_size == 0:\n+                intermediate_embeddings.append(hidden_states)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.neck(hidden_states)\n+\n+        if not return_dict:\n+            outputs = (hidden_states, intermediate_embeddings)\n+            if output_hidden_states:\n+                outputs = outputs + (all_hidden_states,)\n+            if output_attentions:\n+                outputs = outputs + (all_self_attentions,)\n+            return outputs\n+\n+        return SamHQVisionEncoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_embeddings=intermediate_embeddings,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class SamHQLayerNorm(nn.Module):\n+    r\"\"\"LayerNorm that supports two data formats: channels_last (default) or channels_first.\n+    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with shape (batch_size, height,\n+    width, channels) while channels_first corresponds to inputs with shape (batch_size, channels, height, width).\n+    \"\"\"\n+\n+    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(normalized_shape))\n+        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n+        self.eps = eps\n+        self.data_format = data_format\n+        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n+            raise NotImplementedError(f\"Unsupported data format: {self.data_format}\")\n+        self.normalized_shape = (normalized_shape,)\n+\n+    def forward(self, x: torch.Tensor) -> torch.Tensor:\n+        if self.data_format == \"channels_last\":\n+            x = torch.nn.functional.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n+        elif self.data_format == \"channels_first\":\n+            input_dtype = x.dtype\n+            x = x.float()\n+            u = x.mean(1, keepdim=True)\n+            s = (x - u).pow(2).mean(1, keepdim=True)\n+            x = (x - u) / torch.sqrt(s + self.eps)\n+            x = x.to(dtype=input_dtype)\n+            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n+        return x\n+\n+\n+class SamHQAttention(nn.Module):\n+    \"\"\"\n+    SAM_HQ's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n+\n+        self.internal_dim = config.hidden_size // downsample_rate\n+        self.num_attention_heads = config.num_attention_heads\n+        if self.internal_dim % config.num_attention_heads != 0:\n+            raise ValueError(\"num_attention_heads must divide hidden_size.\")\n+\n+        self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n+        self.out_proj = nn.Linear(self.internal_dim, self.hidden_size)\n+\n+    def _separate_heads(self, hidden_states: Tensor, num_attention_heads: int) -> Tensor:\n+        batch, point_batch_size, n_tokens, channel = hidden_states.shape\n+        c_per_head = channel // num_attention_heads\n+        hidden_states = hidden_states.reshape(batch * point_batch_size, n_tokens, num_attention_heads, c_per_head)\n+        return hidden_states.transpose(1, 2)\n+\n+    def _recombine_heads(self, hidden_states: Tensor, point_batch_size: int) -> Tensor:\n+        batch, n_heads, n_tokens, c_per_head = hidden_states.shape\n+        hidden_states = hidden_states.transpose(1, 2)\n+        return hidden_states.reshape(batch // point_batch_size, point_batch_size, n_tokens, n_heads * c_per_head)\n+\n+    def forward(\n+        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+    ) -> Tensor:\n+        # Input projections\n+        query = self.q_proj(query)\n+        key = self.k_proj(key)\n+        value = self.v_proj(value)\n+\n+        point_batch_size = query.shape[1]\n+        # Separate into heads\n+        query = self._separate_heads(query, self.num_attention_heads)\n+        key = self._separate_heads(key, self.num_attention_heads)\n+        value = self._separate_heads(value, self.num_attention_heads)\n+\n+        # SamHQAttention\n+        _, _, _, c_per_head = query.shape\n+        attn = query @ key.permute(0, 1, 3, 2)  # batch_size * point_batch_size  x N_heads x N_tokens x N_tokens\n+        attn = attn / (c_per_head**0.5)\n+        attn = torch.softmax(attn, dim=-1)\n+\n+        if attention_similarity is not None:\n+            attn = attn + attention_similarity\n+            attn = torch.softmax(attn, dim=-1)\n+\n+        # Get output\n+        out = attn @ value\n+        out = self._recombine_heads(out, point_batch_size)\n+        out = self.out_proj(out)\n+\n+        return out\n+\n+\n+class SamHQSdpaAttention(SamHQAttention):\n+    \"\"\"\n+    SAM_HQ's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n+    values. Using SDPA instead of the default attention.\n+    \"\"\"\n+\n+    def __init__(self, config, downsample_rate=None):\n+        super().__init__(config, downsample_rate)\n+\n+    def forward(\n+        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+    ) -> Tensor:\n+        # Input projections\n+        query = self.q_proj(query)\n+        key = self.k_proj(key)\n+        value = self.v_proj(value)\n+\n+        point_batch_size = query.shape[1]\n+        # Separate into heads\n+        query = self._separate_heads(query, self.num_attention_heads)\n+        key = self._separate_heads(key, self.num_attention_heads)\n+        value = self._separate_heads(value, self.num_attention_heads)\n+\n+        # Scaled dot product attention\n+        attn_mask = None\n+        if attention_similarity is not None:\n+            attn_mask = attention_similarity.unsqueeze(1).expand(-1, self.num_attention_heads, -1, -1)\n+\n+        out = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)\n+\n+        # Get output\n+        out = self._recombine_heads(out, point_batch_size)\n+        out = self.out_proj(out)\n+\n+        return out\n+\n+\n+SAM_HQ_ATTENTION_CLASSES = {\n+    \"eager\": SamHQAttention,\n+    \"sdpa\": SamHQSdpaAttention,\n+}\n+\n+\n+class SamHQTwoWayAttentionBlock(nn.Module):\n+    def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_pe: bool = False):\n+        \"\"\"\n+        A transformer block with four layers:\n+            (1) self-attention of sparse inputs (2) cross attention of sparse inputs -> dense inputs (3) mlp block on\n+            sparse inputs (4) cross attention of dense inputs -> sparse inputs\n+\n+        Arguments:\n+            config (`SamHQMaskDecoderConfig`):\n+                The configuration file used to instantiate the block\n+            attention_downsample_rate (*optionalk*, int, defaults to 2):\n+                The downsample ratio of the block used to reduce the inner dim of the attention.\n+            skip_first_layer_pe (*optional*, bool, defaults to `False`):\n+                Whether or not to skip the addition of the query_point_embedding on the first layer.\n+        \"\"\"\n+        super().__init__()\n+\n+        self.hidden_size = config.hidden_size\n+        self.layer_norm_eps = config.layer_norm_eps\n+\n+        self.self_attn = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](config, downsample_rate=1)\n+        self.layer_norm1 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n+\n+        self.cross_attn_token_to_image = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](\n+            config, downsample_rate=attention_downsample_rate\n+        )\n+        self.layer_norm2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n+\n+        self.mlp = SamHQMLPBlock(config)\n+        self.layer_norm3 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n+\n+        self.layer_norm4 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n+        self.cross_attn_image_to_token = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](\n+            config, downsample_rate=attention_downsample_rate\n+        )\n+        self.skip_first_layer_pe = skip_first_layer_pe\n+\n+    def forward(\n+        self,\n+        queries: Tensor,\n+        keys: Tensor,\n+        query_point_embedding: Tensor,\n+        key_point_embedding: Tensor,\n+        attention_similarity: Tensor,\n+        output_attentions: bool = False,\n+    ):\n+        # Self attention block\n+        if self.skip_first_layer_pe:\n+            queries = self.self_attn(query=queries, key=queries, value=queries)\n+        else:\n+            query = queries + query_point_embedding\n+            attn_out = self.self_attn(query=query, key=query, value=queries)\n+            queries = queries + attn_out\n+        queries = self.layer_norm1(queries)\n+\n+        # Cross attention block, tokens attending to image embedding\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out = self.cross_attn_token_to_image(\n+            query=query, key=key, value=keys, attention_similarity=attention_similarity\n+        )\n+        queries = queries + attn_out\n+\n+        queries = self.layer_norm2(queries)\n+\n+        # MLP block\n+        mlp_out = self.mlp(queries)\n+        queries = queries + mlp_out\n+        queries = self.layer_norm3(queries)\n+\n+        # Cross attention block, image embedding attending to tokens\n+        query = queries + query_point_embedding\n+        key = keys + key_point_embedding\n+\n+        attn_out = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n+        keys = keys + attn_out\n+\n+        keys = self.layer_norm4(keys)\n+\n+        outputs = (queries, keys)\n+\n+        if output_attentions:\n+            outputs = outputs + (attn_out,)\n+        else:\n+            outputs = outputs + (None,)\n+\n+        return outputs\n+\n+\n+class SamHQTwoWayTransformer(nn.Module):\n+    def __init__(self, config: SamHQMaskDecoderConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.layers = nn.ModuleList()\n+\n+        for i in range(self.num_hidden_layers):\n+            self.layers.append(SamHQTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n+\n+        self.final_attn_token_to_image = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n+\n+    def forward(\n+        self,\n+        point_embeddings: Tensor,\n+        image_embeddings: Tensor,\n+        image_positional_embeddings: Tensor,\n+        attention_similarity: Tensor,\n+        target_embedding=None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        all_attentions = ()\n+\n+        if image_embeddings is None:\n+            raise ValueError(\"You have to specify an image_embedding\")\n+\n+        image_embeddings = image_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+        image_positional_embeddings = image_positional_embeddings.flatten(2).permute(0, 2, 1).unsqueeze(1)\n+\n+        # Prepare queries\n+        queries = point_embeddings\n+        keys = image_embeddings\n+\n+        # Apply transformer blocks and final layernorm\n+        for layer in self.layers:\n+            if target_embedding is not None:\n+                queries += target_embedding\n+\n+            queries, keys, attention_outputs = layer(\n+                queries=queries,\n+                keys=keys,\n+                query_point_embedding=point_embeddings,\n+                key_point_embedding=image_positional_embeddings,\n+                attention_similarity=attention_similarity,\n+                output_attentions=output_attentions,\n+            )\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (attention_outputs,)\n+\n+        # Apply the final attenion layer from the points to the image\n+        query = queries + point_embeddings\n+        key = keys + image_positional_embeddings\n+\n+        attn_out = self.final_attn_token_to_image(query=query, key=key, value=keys)\n+\n+        queries = queries + attn_out\n+        queries = self.layer_norm_final_attn(queries)\n+        return queries, keys, all_attentions\n+\n+\n+class SamHQFeedForward(nn.Module):\n+    def __init__(\n+        self, input_dim: int, hidden_dim: int, output_dim: int, num_layers: int, sigmoid_output: bool = False\n+    ):\n+        super().__init__()\n+        self.num_layers = num_layers\n+        self.activation = nn.ReLU()\n+        self.proj_in = nn.Linear(input_dim, hidden_dim)\n+        self.proj_out = nn.Linear(hidden_dim, output_dim)\n+        self.layers = nn.ModuleList([nn.Linear(hidden_dim, hidden_dim) for _ in range(num_layers - 2)])\n+        self.sigmoid_output = sigmoid_output\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.proj_in(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        for layer in self.layers:\n+            hidden_states = self.activation(layer(hidden_states))\n+\n+        hidden_states = self.proj_out(hidden_states)\n+        if self.sigmoid_output:\n+            hidden_states = F.sigmoid(hidden_states)\n+        return hidden_states\n+\n+\n+class SamHQMaskDecoder(nn.Module):\n+    def __init__(self, config: SamHQMaskDecoderConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.num_multimask_outputs = config.num_multimask_outputs\n+        self.num_mask_tokens = config.num_multimask_outputs + 1\n+\n+        self.iou_token = nn.Embedding(1, self.hidden_size)\n+        self.mask_tokens = nn.Embedding(self.num_mask_tokens, self.hidden_size)\n+\n+        self.transformer = SamHQTwoWayTransformer(config)\n+\n+        self.upscale_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.upscale_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+        self.upscale_layer_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.activation = nn.GELU()\n+\n+        mlps_list = []\n+        for _ in range(self.num_mask_tokens):\n+            mlps_list += [SamHQFeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)]\n+        self.output_hypernetworks_mlps = nn.ModuleList(mlps_list)\n+\n+        self.iou_prediction_head = SamHQFeedForward(\n+            self.hidden_size, config.iou_head_hidden_dim, self.num_mask_tokens, config.iou_head_depth\n+        )\n+\n+        self.hq_token = nn.Embedding(1, self.hidden_size)\n+        self.hq_mask_mlp = SamHQFeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)\n+        self.num_mask_tokens = self.num_mask_tokens + 1\n+\n+        # Compress ViT features\n+        self.compress_vit_conv1 = nn.ConvTranspose2d(config.vit_dim, self.hidden_size, kernel_size=2, stride=2)\n+        self.compress_vit_norm = SamHQLayerNorm(self.hidden_size, data_format=\"channels_first\")\n+        self.compress_vit_conv2 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 8, kernel_size=2, stride=2)\n+\n+        # Embedding encoder\n+        self.encoder_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.encoder_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.encoder_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+\n+        # Embedding mask feature\n+        self.mask_conv1 = nn.Conv2d(self.hidden_size // 8, self.hidden_size // 4, kernel_size=3, stride=1, padding=1)\n+        self.mask_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.mask_conv2 = nn.Conv2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(\n+        self,\n+        image_embeddings: torch.Tensor,\n+        image_positional_embeddings: torch.Tensor,\n+        sparse_prompt_embeddings: torch.Tensor,\n+        dense_prompt_embeddings: torch.Tensor,\n+        multimask_output: bool,\n+        hq_token_only: bool,\n+        intermediate_embeddings: Optional[List[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = None,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Predict high-quality masks given image and prompt embeddings.\n+\n+        Args:\n+            image_embeddings (`torch.Tensor`):\n+                The embeddings from the image encoder.\n+            image_positional_embedding (`torch.Tensor`):\n+                Positional encoding with the shape of image_embeddings.\n+            sparse_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the points and boxes.\n+            dense_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the mask inputs.\n+            multimask_output (bool):\n+                Whether to return multiple masks or a single mask.\n+            hq_token_only (bool):\n+                Whether to use only the high-quality token output or combine with SAM output.\n+            intermediate_embeddings (`torch.Tensor`):\n+                Intermediate embeddings from the vision encoder for feature fusion.\n+            output_attentions (bool, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            attention_similarity (`torch.Tensor`, *optional*):\n+                Optional tensor for attention similarity computation.\n+            target_embedding (`torch.Tensor`, *optional*):\n+                Optional target embedding for transformer processing.\n+\n+        Returns:\n+            `Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]`: A tuple of tensors containing:\n+                - A tensor of shape `(batch_size, num_prompts, num_masks, height, width)` containing the output masks.\n+                - A tensor of shape `(batch_size, num_prompts, num_masks)` containing the iou predictions for each mask.\n+                - (Optional) A tuple containing attention tensors if output_attentions is True.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image_embeddings.shape\n+        point_batch_size = sparse_prompt_embeddings.shape[1]\n+\n+        has_intermediate = intermediate_embeddings is not None and len(intermediate_embeddings) > 0\n+\n+        if has_intermediate:\n+            vit_features = intermediate_embeddings[0].permute(0, 3, 1, 2).contiguous()\n+\n+        embed_encode = self.encoder_conv1(image_embeddings)\n+        embed_encode = self.activation(self.encoder_norm(embed_encode))\n+        embed_encode = self.encoder_conv2(embed_encode)\n+\n+        if has_intermediate:\n+            compressed_vit_features = self.compress_vit_conv1(vit_features)\n+            compressed_vit_features = self.activation(self.compress_vit_norm(compressed_vit_features))\n+            compressed_vit_features = self.compress_vit_conv2(compressed_vit_features)\n+\n+            hq_features = embed_encode + compressed_vit_features\n+        else:\n+            hq_features = embed_encode\n+\n+        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight, self.hq_token.weight], dim=0)\n+        output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n+\n+        if torch.any(sparse_prompt_embeddings != 0):\n+            tokens = torch.cat([output_tokens, sparse_prompt_embeddings], dim=2)\n+        else:\n+            tokens = output_tokens\n+        point_embeddings = tokens.to(self.iou_token.weight.dtype)\n+\n+        image_embeddings = image_embeddings + dense_prompt_embeddings\n+        image_embeddings = image_embeddings.repeat_interleave(point_batch_size, 0)\n+        image_positional_embeddings = image_positional_embeddings.repeat_interleave(point_batch_size, 0)\n+\n+        point_embedding, image_embeddings, attentions = self.transformer(\n+            point_embeddings=point_embeddings,\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            output_attentions=output_attentions,\n+        )\n+        iou_token_out = point_embedding[:, :, 0, :]\n+        mask_tokens_out = point_embedding[:, :, 1 : (1 + self.num_mask_tokens), :]\n+\n+        image_embeddings = image_embeddings.transpose(2, 3).reshape(\n+            batch_size * point_batch_size, num_channels, height, width\n+        )\n+\n+        upscaled_embedding = self.upscale_conv1(image_embeddings)\n+        upscaled_embedding = self.activation(self.upscale_layer_norm(upscaled_embedding))\n+        upscaled_embedding = self.activation(self.upscale_conv2(upscaled_embedding))\n+\n+        upscaled_embedding_hq = self.mask_conv1(upscaled_embedding)\n+        upscaled_embedding_hq = self.activation(self.mask_norm(upscaled_embedding_hq))\n+        upscaled_embedding_hq = self.mask_conv2(upscaled_embedding_hq)\n+\n+        if hq_features.shape[0] == 1:\n+            hq_features = hq_features.repeat(batch_size * point_batch_size, 1, 1, 1)\n+        elif hq_features.shape[0] == batch_size and batch_size * point_batch_size != batch_size:\n+            hq_features = hq_features.repeat_interleave(point_batch_size, 0)\n+        upscaled_embedding_hq = upscaled_embedding_hq + hq_features\n+\n+        hyper_in_list = []\n+        for mask_token_index in range(self.num_mask_tokens):\n+            if mask_token_index < self.num_mask_tokens - 1:\n+                current_mlp = self.output_hypernetworks_mlps[mask_token_index]\n+            else:\n+                current_mlp = self.hq_mask_mlp\n+            hyper_in_list += [current_mlp(mask_tokens_out[:, :, mask_token_index, :])]\n+\n+        hyper_in = torch.stack(hyper_in_list, dim=2)\n+        _, num_channels, height, width = upscaled_embedding.shape\n+        upscaled_embedding = upscaled_embedding.reshape(batch_size, point_batch_size, num_channels, height * width)\n+        upscaled_embedding_hq = upscaled_embedding_hq.reshape(\n+            batch_size, point_batch_size, num_channels, height * width\n+        )\n+\n+        masks_sam = (hyper_in[:, :, : self.num_mask_tokens - 1] @ upscaled_embedding).reshape(\n+            batch_size, point_batch_size, -1, height, width\n+        )\n+        masks_hq = (hyper_in[:, :, self.num_mask_tokens - 1 :] @ upscaled_embedding_hq).reshape(\n+            batch_size, point_batch_size, -1, height, width\n+        )\n+        masks = torch.cat([masks_sam, masks_hq], dim=2)\n+\n+        iou_pred = self.iou_prediction_head(iou_token_out)\n+\n+        if multimask_output:\n+            mask_slice = slice(1, self.num_mask_tokens - 1)\n+            iou_pred = iou_pred[:, :, mask_slice]\n+            # Sort the IoU scores in descending order and get indices\n+            iou_pred_sorted, sort_indices = torch.sort(iou_pred, dim=2, descending=True)\n+            # Reorder the masks according to sorted scores\n+            masks_sam = masks[:, :, mask_slice, :, :]\n+            masks_sam = torch.gather(\n+                masks_sam,\n+                2,\n+                sort_indices[..., None, None].expand(-1, -1, -1, masks_sam.shape[3], masks_sam.shape[4]),\n+            )\n+            # Update iou_pred with sorted scores\n+            iou_pred = iou_pred_sorted\n+        else:\n+            mask_slice = slice(0, 1)\n+            iou_pred = iou_pred[:, :, mask_slice]\n+            masks_sam = masks[:, :, mask_slice, :, :]\n+\n+        masks_hq = masks[:, :, slice(self.num_mask_tokens - 1, self.num_mask_tokens), :, :]\n+        if hq_token_only:\n+            masks = masks_hq\n+        else:\n+            masks = masks_sam + masks_hq\n+\n+        outputs = (masks, iou_pred)\n+        if output_attentions:\n+            outputs = outputs + (attentions,)\n+        else:\n+            outputs = outputs + (None,)\n+\n+        return outputs\n+\n+\n+class SamHQPreTrainedModel(PreTrainedModel):\n+    config_class = SamHQConfig\n+    base_model_prefix = \"sam_hq\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"SamHQVisionAttention\"]\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, (SamHQLayerNorm, nn.LayerNorm)):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, SamHQVisionAttention):\n+            if module.use_rel_pos:\n+                module.rel_pos_h.data.zero_()\n+                module.rel_pos_w.data.zero_()\n+        if isinstance(module, SamHQVisionEncoder):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n+\n+\n+SAM_HQ_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`SamHQProcessor`]. See [`SamHQProcessor.__call__`] for\n+            details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+SAM_HQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`SamHQConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The vision model from SAM-HQ without any head or projection on top.\"\"\",\n+    SAM_HQ_START_DOCSTRING,\n+)\n+class SamHQVisionModel(SamHQPreTrainedModel):\n+    config_class = SamHQVisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: SamHQVisionConfig):\n+        super().__init__(config)\n+        self.vision_encoder = SamHQVisionEncoder(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_encoder.patch_embed\n+\n+    @add_start_docstrings_to_model_forward(SAM_HQ_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=SamHQVisionEncoderOutput, config_class=SamHQVisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, SamHQVisionEncoderOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        return self.vision_encoder(\n+            pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+class SamHQPositionalEmbedding(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.scale = config.hidden_size // 2\n+        self.register_buffer(\"positional_embedding\", self.scale * torch.randn((2, config.num_pos_feats)))\n+\n+    def forward(self, input_coords, input_shape=None):\n+        \"\"\"Positionally encode points that are normalized to [0,1].\"\"\"\n+        coordinates = input_coords.clone()\n+\n+        if input_shape is not None:\n+            coordinates[:, :, :, 0] = coordinates[:, :, :, 0] / input_shape[1]\n+            coordinates[:, :, :, 1] = coordinates[:, :, :, 1] / input_shape[0]\n+\n+        # assuming coords are in [0, 1]^2 square and have d_1 x ... x d_n x 2 shape\n+        coordinates = 2 * coordinates - 1\n+        coordinates = coordinates.to(self.positional_embedding.dtype)\n+        coordinates = coordinates @ self.positional_embedding\n+        coordinates = 2 * np.pi * coordinates\n+        # outputs d_1 x ... x d_n x channel shape\n+        return torch.cat([torch.sin(coordinates), torch.cos(coordinates)], dim=-1)\n+\n+\n+class SamHQMaskEmbedding(nn.Module):\n+    def __init__(self, config: SamHQPromptEncoderConfig):\n+        super().__init__()\n+        self.mask_input_channels = config.mask_input_channels // 4\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.conv1 = nn.Conv2d(1, self.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv2 = nn.Conv2d(self.mask_input_channels, config.mask_input_channels, kernel_size=2, stride=2)\n+        self.conv3 = nn.Conv2d(config.mask_input_channels, config.hidden_size, kernel_size=1)\n+        self.layer_norm1 = SamHQLayerNorm(\n+            self.mask_input_channels, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+        self.layer_norm2 = SamHQLayerNorm(\n+            self.mask_input_channels * 4, eps=config.layer_norm_eps, data_format=\"channels_first\"\n+        )\n+\n+    def forward(self, masks):\n+        hidden_states = self.conv1(masks)\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        dense_embeddings = self.conv3(hidden_states)\n+        return dense_embeddings\n+\n+\n+class SamHQPromptEncoder(nn.Module):\n+    def __init__(self, config: SamHQPromptEncoderConfig):\n+        super().__init__()\n+        self.shared_embedding = SamHQPositionalEmbedding(config.vision_config)\n+        config = config.prompt_encoder_config\n+        self.mask_embed = SamHQMaskEmbedding(config)\n+        self.no_mask_embed = nn.Embedding(1, config.hidden_size)\n+\n+        self.image_embedding_size = (config.image_embedding_size, config.image_embedding_size)\n+        self.input_image_size = config.image_size\n+\n+        self.point_embed = nn.ModuleList(\n+            [nn.Embedding(1, config.hidden_size) for i in range(config.num_point_embeddings)]\n+        )\n+        self.hidden_size = config.hidden_size\n+        self.not_a_point_embed = nn.Embedding(1, config.hidden_size)\n+\n+    def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -> torch.Tensor:\n+        \"\"\"Embeds point prompts.\"\"\"\n+        points = points + 0.5  # Shift to center of pixel\n+        if pad:\n+            target_point_shape = (points.shape[0], points.shape[1], 1, points.shape[-1])\n+            target_labels_shape = (points.shape[0], points.shape[1], 1)\n+            padding_point = torch.zeros(target_point_shape, device=points.device)\n+            padding_label = -torch.ones(target_labels_shape, device=labels.device)\n+            points = torch.cat([points, padding_point], dim=2)\n+            labels = torch.cat([labels, padding_label], dim=2)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        point_embedding = self.shared_embedding(points, input_shape)\n+\n+        # torch.where and expanding the labels tensor is required by the ONNX export\n+        point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n+\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n+        point_embedding = torch.where(\n+            labels[..., None] != -10,\n+            point_embedding,\n+            torch.tensor(0.0, dtype=point_embedding.dtype, device=point_embedding.device),\n+        )\n+\n+        point_embedding = torch.where(\n+            (labels == 0)[:, :, :, None],\n+            point_embedding + self.point_embed[0].weight[None, None, :, :],\n+            point_embedding,\n+        )\n+\n+        point_embedding = torch.where(\n+            (labels == 1)[:, :, :, None],\n+            point_embedding + self.point_embed[1].weight[None, None, :, :],\n+            point_embedding,\n+        )\n+\n+        return point_embedding\n+\n+    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:\n+        \"\"\"Embeds box prompts.\"\"\"\n+        boxes = boxes + 0.5  # Shift to center of pixel\n+        batch_size, nb_boxes = boxes.shape[:2]\n+        coords = boxes.reshape(batch_size, nb_boxes, 2, 2)\n+        input_shape = (self.input_image_size, self.input_image_size)\n+        corner_embedding = self.shared_embedding(coords, input_shape)\n+        corner_embedding[:, :, 0, :] += self.point_embed[2].weight\n+        corner_embedding[:, :, 1, :] += self.point_embed[3].weight\n+        return corner_embedding\n+\n+    def forward(\n+        self,\n+        input_points: Optional[Tuple[torch.Tensor, torch.Tensor]],\n+        input_labels: Optional[torch.Tensor],\n+        input_boxes: Optional[torch.Tensor],\n+        input_masks: Optional[torch.Tensor],\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Embeds different types of prompts, returning both sparse and dense embeddings.\n+\n+        Args:\n+            points (`torch.Tensor`, *optional*):\n+                point coordinates and labels to embed.\n+            boxes (`torch.Tensor`, *optional*):\n+                boxes to embed\n+            masks (`torch.Tensor`, *optional*):\n+                masks to embed\n+        \"\"\"\n+        sparse_embeddings = None\n+        batch_size = 1\n+        target_device = self.shared_embedding.positional_embedding.device\n+        if input_points is not None:\n+            batch_size, point_batch_size = input_points.shape[:2]\n+            if input_labels is None:\n+                raise ValueError(\"If points are provided, labels must also be provided.\")\n+            point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))\n+            sparse_embeddings = point_embeddings\n+        if input_boxes is not None:\n+            batch_size = input_boxes.shape[0]\n+            box_embeddings = self._embed_boxes(input_boxes)\n+            if sparse_embeddings is None:\n+                sparse_embeddings = box_embeddings\n+            else:\n+                sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=2)\n+        if input_masks is not None:\n+            dense_embeddings = self.mask_embed(input_masks)\n+        else:\n+            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(\n+                batch_size, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n+            )\n+\n+        if sparse_embeddings is None:\n+            sparse_embeddings = torch.zeros((batch_size, 1, 1, self.hidden_size), device=target_device)\n+\n+        return sparse_embeddings, dense_embeddings\n+\n+\n+SAM_HQ_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`SamHQProcessor`]. See [`SamHQProcessor.__call__`] for\n+            details.\n+        input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+            Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+            better results. The points can be obtained by passing a list of list of list to the processor that will\n+            create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+            second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+            per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+            multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+            coordinates of the point. If a different number of points is passed either for each image, or for each\n+            mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+            computation of the embedding will be skipped for these points using the labels.\n+        input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+            Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+            official implementation, there are 3 types of labels\n+\n+            - `1`: the point is a point that contains the object of interest\n+            - `0`: the point is a point that does not contain the object of interest\n+            - `-1`: the point corresponds to the background\n+\n+            We added the label:\n+\n+            - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+            The padding labels should be automatically done by the processor.\n+        input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+            Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+            much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+            that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n+            In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+            - `x1`: the x coordinate of the top left point of the input box\n+            - `y1`: the y coordinate of the top left point of the input box\n+            - `x2`: the x coordinate of the bottom right point of the input box\n+            - `y2`: the y coordinate of the bottom right point of the input box\n+\n+        input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+            SAM_HQ model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+            generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+            manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+\n+        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+            Image embeddings, this is used by the mask decder to generate masks and iou scores. For more memory\n+            efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+            method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+        multimask_output (`bool`, *optional*):\n+            In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+            bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+            \"best\" mask, by specifying `multimask_output=False`.\n+        attention_similarity (`torch.FloatTensor`, *optional*):\n+            Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+            model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+        target_embedding (`torch.FloatTensor`, *optional*):\n+            Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+            the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"Segment Anything Model HQ (SAM-HQ) for generating masks,given an input image and\",\n+    \" optional 2D location and bounding boxes.\",\n+    SAM_HQ_START_DOCSTRING,\n+)\n+class SamHQModel(SamHQPreTrainedModel):\n+    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+\n+    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.shared_image_embedding = SamHQPositionalEmbedding(config.vision_config)\n+        self.vision_encoder = SamHQVisionEncoder(config.vision_config)\n+        self.prompt_encoder = SamHQPromptEncoder(config)\n+\n+        self.mask_decoder = SamHQMaskDecoder(config.mask_decoder_config)\n+\n+        self.post_init()\n+\n+    def _tie_weights(self):\n+        self.prompt_encoder.shared_embedding.positional_embedding.data = (\n+            self.shared_image_embedding.positional_embedding.data\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.vision_encoder.get_input_embeddings()\n+\n+    def get_image_wide_positional_embeddings(self):\n+        size = self.config.prompt_encoder_config.image_embedding_size\n+        target_device = self.shared_image_embedding.positional_embedding.device\n+        target_dtype = self.shared_image_embedding.positional_embedding.dtype\n+        grid = torch.ones((size, size), device=target_device, dtype=target_dtype)\n+        y_embed = grid.cumsum(dim=0) - 0.5\n+        x_embed = grid.cumsum(dim=1) - 0.5\n+        y_embed = y_embed / size\n+        x_embed = x_embed / size\n+\n+        positional_embedding = self.shared_image_embedding(torch.stack([x_embed, y_embed], dim=-1))\n+        return positional_embedding.permute(2, 0, 1).unsqueeze(0)  # channel x height x width\n+\n+    @torch.no_grad()\n+    def get_image_embeddings(\n+        self,\n+        pixel_values,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Returns the image embeddings by passing the pixel values through the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Input pixel values\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\n+        \"\"\"\n+        vision_output = self.vision_encoder(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        image_embeddings = vision_output[0]\n+        intermediate_embeddings = vision_output[1]\n+\n+        return image_embeddings, intermediate_embeddings\n+\n+    @torch.no_grad()\n+    def get_prompt_embeddings(\n+        self,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+    ):\n+        r\"\"\"\n+        Returns the prompt embeddings by passing the input points, labels, boxes and masks through the prompt encoder.\n+\n+        Args:\n+            input_points (`torch.FloatTensor` of shape `(batch_size, point_batch_size, num_points_per_image, 2)`):\n+                Optional input points for the prompt encoder. The padding of the point is automatically done by the\n+                processor. `point_batch_size` refers to the number of masks that we want the model to predict per\n+                point. The model will output `point_batch_size` times 3 masks in total.\n+            input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points_per_image)`):\n+                Optional input labels for the prompt encoder. The padding of the labels is automatically done by the\n+                processor, or can be fed by the user.\n+            input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes_per_image, 4)`):\n+                Optional input boxes for the prompt encoder. The padding of the boxes is automatically done by the\n+                processor. users can also pass manually the input boxes.\n+            input_masks (`torch.LongTensor` of shape `(batch_size, image_size, image_size)`):\n+                Optional input masks for the prompt encoder.\n+        \"\"\"\n+        prompt_output = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+        return prompt_output\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(SAM_HQ_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+        image_embeddings: Optional[torch.FloatTensor] = None,\n+        multimask_output: bool = True,\n+        hq_token_only: bool = False,\n+        attention_similarity: Optional[torch.FloatTensor] = None,\n+        target_embedding: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        intermediate_embeddings: Optional[List[torch.FloatTensor]] = None,\n+        **kwargs,\n+    ) -> List[Dict[str, torch.Tensor]]:\n+        r\"\"\"\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Pixel values. Pixel values can be obtained using [`SamHQProcessor`]. See [`SamHQProcessor.__call__`] for\n+                details.\n+            input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+                Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+                better results. The points can be obtained by passing a list of list of list to the processor that will\n+                create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+                second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+                per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+                multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+                coordinates of the point. If a different number of points is passed either for each image, or for each\n+                mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+                computation of the embedding will be skipped for these points using the labels.\n+            input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+                Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+                official implementation, there are 3 types of labels\n+\n+                - `1`: the point is a point that contains the object of interest\n+                - `0`: the point is a point that does not contain the object of interest\n+                - `-1`: the point corresponds to the background\n+\n+                We added the label:\n+\n+                - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+                The padding labels should be automatically done by the processor.\n+            input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+                Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+                much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+                that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+                size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+                In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+                - `x1`: the x coordinate of the top left point of the input box\n+                - `y1`: the y coordinate of the top left point of the input box\n+                - `x2`: the x coordinate of the bottom right point of the input box\n+                - `y2`: the y coordinate of the bottom right point of the input box\n+\n+            input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+                SAM_HQ model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+                generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+                manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+\n+            image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+                Image embeddings, this is used by the mask decder to generate masks and iou scores. For more memory\n+                efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+                method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+            multimask_output (`bool`, *optional*):\n+                In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+                bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+                \"best\" mask, by specifying `multimask_output=False`.\n+            hq_token_only (`bool`, *optional*, defaults to `False`):\n+                Whether to use only the HQ token path for mask generation. When False, combines both standard and HQ paths.\n+                This is specific to SAM-HQ's architecture.\n+            attention_similarity (`torch.FloatTensor`, *optional*):\n+                Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+                model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+            target_embedding (`torch.FloatTensor`, *optional*):\n+                Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+                the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+                tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+                more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            intermediate_embeddings (`List[torch.FloatTensor]`, *optional*):\n+                Intermediate embeddings from vision encoder's non-windowed blocks, used by SAM-HQ for enhanced mask quality.\n+                Required when providing pre-computed image_embeddings instead of pixel_values.\n+        Example:\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoModel, AutoProcessor\n+\n+        >>> model = AutoModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        >>> processor = AutoProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n+        >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        >>> input_points = [[[400, 650]]]  # 2D location of a window on the car\n+        >>> inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\")\n+\n+        >>> # Get high-quality segmentation mask\n+        >>> outputs = model(**inputs)\n+\n+        >>> # For high-quality mask only\n+        >>> outputs = model(**inputs, hq_token_only=True)\n+\n+        >>> # Postprocess masks\n+        >>> masks = processor.post_process_masks(\n+        ...     outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n+        ... )\n+        ```\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None and image_embeddings is None:\n+            raise ValueError(\"Either pixel_values or image_embeddings must be provided.\")\n+\n+        if pixel_values is not None and image_embeddings is not None:\n+            raise ValueError(\"Only one of pixel_values and image_embeddings can be provided.\")\n+\n+        if input_points is not None and len(input_points.shape) != 4:\n+            raise ValueError(\n+                \"The input_points must be a 4D tensor. Of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\"\n+                f\" got {input_points.shape}.\"\n+            )\n+\n+        if input_boxes is not None and len(input_boxes.shape) != 3:\n+            raise ValueError(\n+                \"The input_boxes must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.\"\n+                f\" got {input_boxes.shape}.\"\n+            )\n+\n+        # Add validation for point and box batch sizes\n+        if input_points is not None and input_boxes is not None:\n+            point_batch_size = input_points.shape[1]\n+            box_batch_size = input_boxes.shape[1]\n+            if point_batch_size != box_batch_size:\n+                raise ValueError(\n+                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n+                        point_batch_size, box_batch_size\n+                    )\n+                )\n+\n+        image_positional_embeddings = self.get_image_wide_positional_embeddings()\n+        # repeat with batch size\n+        batch_size = pixel_values.shape[0] if pixel_values is not None else image_embeddings.shape[0]\n+        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n+\n+        vision_attentions = None\n+        vision_hidden_states = None\n+\n+        if pixel_values is not None:\n+            vision_outputs = self.vision_encoder(\n+                pixel_values,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+\n+            if return_dict:\n+                image_embeddings = vision_outputs.last_hidden_state\n+                intermediate_embeddings = vision_outputs.intermediate_embeddings\n+                if output_hidden_states:\n+                    vision_hidden_states = vision_outputs.hidden_states\n+                if output_attentions:\n+                    vision_attentions = vision_outputs.attentions\n+            else:\n+                image_embeddings = vision_outputs[0]\n+                intermediate_embeddings = vision_outputs[1]\n+                if output_hidden_states:\n+                    vision_hidden_states = vision_outputs[2]\n+                if output_attentions:\n+                    vision_attentions = vision_outputs[-1]\n+\n+        if input_points is not None and input_labels is None:\n+            input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n+\n+        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+\n+        # Predict masks\n+        low_res_masks, iou_predictions, mask_decoder_attentions = self.mask_decoder(\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            sparse_prompt_embeddings=sparse_embeddings,\n+            dense_prompt_embeddings=dense_embeddings,\n+            multimask_output=multimask_output,\n+            hq_token_only=hq_token_only,\n+            intermediate_embeddings=intermediate_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            output_attentions=output_attentions,\n+        )\n+\n+        if not return_dict:\n+            output = (iou_predictions, low_res_masks)\n+            if output_hidden_states:\n+                output = output + (vision_hidden_states,)\n+\n+            if output_attentions:\n+                output = output + (vision_attentions, mask_decoder_attentions)\n+            return output\n+\n+        return SamHQImageSegmentationOutput(\n+            iou_scores=iou_predictions,\n+            pred_masks=low_res_masks,\n+            vision_hidden_states=vision_hidden_states,\n+            vision_attentions=vision_attentions,\n+            mask_decoder_attentions=mask_decoder_attentions,\n+        )\n+\n+\n+__all__ = [\"SamHQModel\", \"SamHQPreTrainedModel\", \"SamHQVisionModel\"]"
        },
        {
            "sha": "b86e300006f88400d7360798a765acc443109f51",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "added",
            "additions": 737,
            "deletions": 0,
            "changes": 737,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,737 @@\n+# coding=utf-8\n+# Copyright 2025 Google Inc. HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from dataclasses import dataclass\n+from typing import Dict, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...utils import add_start_docstrings, logging\n+from ..sam.configuration_sam import SamConfig, SamMaskDecoderConfig, SamPromptEncoderConfig, SamVisionConfig\n+from ..sam.modeling_sam import (\n+    SamFeedForward,\n+    SamImageSegmentationOutput,\n+    SamLayerNorm,\n+    SamModel,\n+    SamPreTrainedModel,\n+    SamTwoWayTransformer,\n+    SamVisionEncoder,\n+    SamVisionEncoderOutput,\n+    SamVisionModel,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class SamHQPromptEncoderConfig(SamPromptEncoderConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`SamHQPromptEncoderModel`].The [`SamHQPromptEncoderModel`]\n+    module is used to encode the input 2D points and bounding boxes. Instantiating a configuration defaults will yield a\n+    similar configuration to that of the SAM_HQ model. The configuration is used to store the configuration of the model.\n+    [Uminosachi/sam-hq](https://huggingface.co/Uminosachi/sam-hq) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model's output.Read the documentation from\n+    [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 256):\n+            Dimensionality of the hidden states.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            The expected output resolution of the image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        mask_input_channels (`int`, *optional*, defaults to 16):\n+            The number of channels to be fed to the `MaskDecoder` module.\n+        num_point_embeddings (`int`, *optional*, defaults to 4):\n+            The number of point embeddings to be used.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function in the encoder and pooler.\n+    \"\"\"\n+\n+    pass\n+\n+\n+class SamHQVisionConfig(SamVisionConfig):\n+    pass\n+\n+\n+class SamHQMaskDecoderConfig(SamMaskDecoderConfig):\n+    r\"\"\"\n+    vit_dim (`int`, *optional*, defaults to 768):\n+        Dimensionality of the Vision Transformer (ViT) used in the `SamHQMaskDecoder` module.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vit_dim=768,\n+        **super_kwargs,\n+    ):\n+        super().__init__(**super_kwargs)\n+        self.vit_dim = vit_dim\n+\n+\n+class SamHQConfig(SamConfig):\n+    r\"\"\"\n+    [`SamHQConfig`] is the configuration class to store the configuration of a [`SamHQModel`]. It is used to instantiate a\n+    SAM-HQ model according to the specified arguments, defining the vision model, prompt-encoder model and mask decoder\n+    configs. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n+    SAM-HQ-ViT-H [sushmanth/sam_hq_vit_h](https://huggingface.co/sushmanth/sam_hq_vit_h) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (Union[`dict`, `SamHQVisionConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQVisionConfig`].\n+        prompt_encoder_config (Union[`dict`, `SamHQPromptEncoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQPromptEncoderConfig`].\n+        mask_decoder_config (Union[`dict`, `SamHQMaskDecoderConfig`], *optional*):\n+            Dictionary of configuration options used to initialize [`SamHQMaskDecoderConfig`].\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+    \"\"\"\n+\n+    pass\n+\n+\n+@dataclass\n+class SamHQVisionEncoderOutput(SamVisionEncoderOutput):\n+    \"\"\"\n+    intermediate_embeddings (`list(torch.FloatTensor)`, *optional*):\n+        A list of intermediate embeddings collected from certain blocks within the model, typically those without\n+        windowed attention. Each element in the list is of shape `(batch_size, sequence_length, hidden_size)`.\n+        This is specific to SAM-HQ and not present in base SAM.\n+    \"\"\"\n+\n+    intermediate_embeddings: Optional[List[torch.FloatTensor]] = None\n+\n+\n+@dataclass\n+class SamHQImageSegmentationOutput(SamImageSegmentationOutput):\n+    pass\n+\n+\n+class SamHQVisionEncoder(SamVisionEncoder):\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, SamHQVisionEncoderOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.patch_embed(pixel_values)\n+        if self.pos_embed is not None:\n+            hidden_states = hidden_states + self.pos_embed\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attentions = () if output_attentions else None\n+        intermediate_embeddings = []\n+\n+        for i, layer_module in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    layer_module.__call__,\n+                    hidden_states,\n+                )\n+            else:\n+                layer_outputs = layer_module(hidden_states, output_attentions=output_attentions)\n+\n+            hidden_states = layer_outputs[0]\n+\n+            # Collect embeddings from non-windowed blocks\n+            if hasattr(layer_module, \"window_size\") and layer_module.window_size == 0:\n+                intermediate_embeddings.append(hidden_states)\n+\n+            if output_attentions:\n+                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            all_hidden_states = all_hidden_states + (hidden_states,)\n+\n+        hidden_states = self.neck(hidden_states)\n+\n+        if not return_dict:\n+            outputs = (hidden_states, intermediate_embeddings)\n+            if output_hidden_states:\n+                outputs = outputs + (all_hidden_states,)\n+            if output_attentions:\n+                outputs = outputs + (all_self_attentions,)\n+            return outputs\n+\n+        return SamHQVisionEncoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_embeddings=intermediate_embeddings,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attentions,\n+        )\n+\n+\n+class SamHQLayerNorm(SamLayerNorm):\n+    pass\n+\n+\n+class SamHQTwoWayTransformer(SamTwoWayTransformer):\n+    pass\n+\n+\n+class SamHQFeedForward(SamFeedForward):\n+    pass\n+\n+\n+class SamHQMaskDecoder(nn.Module):\n+    def __init__(self, config: SamHQMaskDecoderConfig):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.num_multimask_outputs = config.num_multimask_outputs\n+        self.num_mask_tokens = config.num_multimask_outputs + 1\n+\n+        self.iou_token = nn.Embedding(1, self.hidden_size)\n+        self.mask_tokens = nn.Embedding(self.num_mask_tokens, self.hidden_size)\n+\n+        self.transformer = SamHQTwoWayTransformer(config)\n+\n+        self.upscale_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.upscale_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+        self.upscale_layer_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.activation = nn.GELU()\n+\n+        mlps_list = []\n+        for _ in range(self.num_mask_tokens):\n+            mlps_list += [SamHQFeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)]\n+        self.output_hypernetworks_mlps = nn.ModuleList(mlps_list)\n+\n+        self.iou_prediction_head = SamHQFeedForward(\n+            self.hidden_size, config.iou_head_hidden_dim, self.num_mask_tokens, config.iou_head_depth\n+        )\n+\n+        self.hq_token = nn.Embedding(1, self.hidden_size)\n+        self.hq_mask_mlp = SamHQFeedForward(self.hidden_size, self.hidden_size, self.hidden_size // 8, 3)\n+        self.num_mask_tokens = self.num_mask_tokens + 1\n+\n+        # Compress ViT features\n+        self.compress_vit_conv1 = nn.ConvTranspose2d(config.vit_dim, self.hidden_size, kernel_size=2, stride=2)\n+        self.compress_vit_norm = SamHQLayerNorm(self.hidden_size, data_format=\"channels_first\")\n+        self.compress_vit_conv2 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 8, kernel_size=2, stride=2)\n+\n+        # Embedding encoder\n+        self.encoder_conv1 = nn.ConvTranspose2d(self.hidden_size, self.hidden_size // 4, kernel_size=2, stride=2)\n+        self.encoder_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.encoder_conv2 = nn.ConvTranspose2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=2, stride=2)\n+\n+        # Embedding mask feature\n+        self.mask_conv1 = nn.Conv2d(self.hidden_size // 8, self.hidden_size // 4, kernel_size=3, stride=1, padding=1)\n+        self.mask_norm = SamHQLayerNorm(self.hidden_size // 4, data_format=\"channels_first\")\n+        self.mask_conv2 = nn.Conv2d(self.hidden_size // 4, self.hidden_size // 8, kernel_size=3, stride=1, padding=1)\n+\n+    def forward(\n+        self,\n+        image_embeddings: torch.Tensor,\n+        image_positional_embeddings: torch.Tensor,\n+        sparse_prompt_embeddings: torch.Tensor,\n+        dense_prompt_embeddings: torch.Tensor,\n+        multimask_output: bool,\n+        hq_token_only: bool,\n+        intermediate_embeddings: Optional[List[torch.Tensor]] = None,\n+        output_attentions: Optional[bool] = None,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n+    ) -> Tuple[torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Predict high-quality masks given image and prompt embeddings.\n+\n+        Args:\n+            image_embeddings (`torch.Tensor`):\n+                The embeddings from the image encoder.\n+            image_positional_embedding (`torch.Tensor`):\n+                Positional encoding with the shape of image_embeddings.\n+            sparse_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the points and boxes.\n+            dense_prompt_embeddings (`torch.Tensor`):\n+                The embeddings of the mask inputs.\n+            multimask_output (bool):\n+                Whether to return multiple masks or a single mask.\n+            hq_token_only (bool):\n+                Whether to use only the high-quality token output or combine with SAM output.\n+            intermediate_embeddings (`torch.Tensor`):\n+                Intermediate embeddings from the vision encoder for feature fusion.\n+            output_attentions (bool, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            attention_similarity (`torch.Tensor`, *optional*):\n+                Optional tensor for attention similarity computation.\n+            target_embedding (`torch.Tensor`, *optional*):\n+                Optional target embedding for transformer processing.\n+\n+        Returns:\n+            `Tuple[torch.Tensor, torch.Tensor, Optional[torch.Tensor]]`: A tuple of tensors containing:\n+                - A tensor of shape `(batch_size, num_prompts, num_masks, height, width)` containing the output masks.\n+                - A tensor of shape `(batch_size, num_prompts, num_masks)` containing the iou predictions for each mask.\n+                - (Optional) A tuple containing attention tensors if output_attentions is True.\n+        \"\"\"\n+        batch_size, num_channels, height, width = image_embeddings.shape\n+        point_batch_size = sparse_prompt_embeddings.shape[1]\n+\n+        has_intermediate = intermediate_embeddings is not None and len(intermediate_embeddings) > 0\n+\n+        if has_intermediate:\n+            vit_features = intermediate_embeddings[0].permute(0, 3, 1, 2).contiguous()\n+\n+        embed_encode = self.encoder_conv1(image_embeddings)\n+        embed_encode = self.activation(self.encoder_norm(embed_encode))\n+        embed_encode = self.encoder_conv2(embed_encode)\n+\n+        if has_intermediate:\n+            compressed_vit_features = self.compress_vit_conv1(vit_features)\n+            compressed_vit_features = self.activation(self.compress_vit_norm(compressed_vit_features))\n+            compressed_vit_features = self.compress_vit_conv2(compressed_vit_features)\n+\n+            hq_features = embed_encode + compressed_vit_features\n+        else:\n+            hq_features = embed_encode\n+\n+        output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight, self.hq_token.weight], dim=0)\n+        output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n+\n+        if torch.any(sparse_prompt_embeddings != 0):\n+            tokens = torch.cat([output_tokens, sparse_prompt_embeddings], dim=2)\n+        else:\n+            tokens = output_tokens\n+        point_embeddings = tokens.to(self.iou_token.weight.dtype)\n+\n+        image_embeddings = image_embeddings + dense_prompt_embeddings\n+        image_embeddings = image_embeddings.repeat_interleave(point_batch_size, 0)\n+        image_positional_embeddings = image_positional_embeddings.repeat_interleave(point_batch_size, 0)\n+\n+        point_embedding, image_embeddings, attentions = self.transformer(\n+            point_embeddings=point_embeddings,\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            output_attentions=output_attentions,\n+        )\n+        iou_token_out = point_embedding[:, :, 0, :]\n+        mask_tokens_out = point_embedding[:, :, 1 : (1 + self.num_mask_tokens), :]\n+\n+        image_embeddings = image_embeddings.transpose(2, 3).reshape(\n+            batch_size * point_batch_size, num_channels, height, width\n+        )\n+\n+        upscaled_embedding = self.upscale_conv1(image_embeddings)\n+        upscaled_embedding = self.activation(self.upscale_layer_norm(upscaled_embedding))\n+        upscaled_embedding = self.activation(self.upscale_conv2(upscaled_embedding))\n+\n+        upscaled_embedding_hq = self.mask_conv1(upscaled_embedding)\n+        upscaled_embedding_hq = self.activation(self.mask_norm(upscaled_embedding_hq))\n+        upscaled_embedding_hq = self.mask_conv2(upscaled_embedding_hq)\n+\n+        if hq_features.shape[0] == 1:\n+            hq_features = hq_features.repeat(batch_size * point_batch_size, 1, 1, 1)\n+        elif hq_features.shape[0] == batch_size and batch_size * point_batch_size != batch_size:\n+            hq_features = hq_features.repeat_interleave(point_batch_size, 0)\n+        upscaled_embedding_hq = upscaled_embedding_hq + hq_features\n+\n+        hyper_in_list = []\n+        for mask_token_index in range(self.num_mask_tokens):\n+            if mask_token_index < self.num_mask_tokens - 1:\n+                current_mlp = self.output_hypernetworks_mlps[mask_token_index]\n+            else:\n+                current_mlp = self.hq_mask_mlp\n+            hyper_in_list += [current_mlp(mask_tokens_out[:, :, mask_token_index, :])]\n+\n+        hyper_in = torch.stack(hyper_in_list, dim=2)\n+        _, num_channels, height, width = upscaled_embedding.shape\n+        upscaled_embedding = upscaled_embedding.reshape(batch_size, point_batch_size, num_channels, height * width)\n+        upscaled_embedding_hq = upscaled_embedding_hq.reshape(\n+            batch_size, point_batch_size, num_channels, height * width\n+        )\n+\n+        masks_sam = (hyper_in[:, :, : self.num_mask_tokens - 1] @ upscaled_embedding).reshape(\n+            batch_size, point_batch_size, -1, height, width\n+        )\n+        masks_hq = (hyper_in[:, :, self.num_mask_tokens - 1 :] @ upscaled_embedding_hq).reshape(\n+            batch_size, point_batch_size, -1, height, width\n+        )\n+        masks = torch.cat([masks_sam, masks_hq], dim=2)\n+\n+        iou_pred = self.iou_prediction_head(iou_token_out)\n+\n+        if multimask_output:\n+            mask_slice = slice(1, self.num_mask_tokens - 1)\n+            iou_pred = iou_pred[:, :, mask_slice]\n+            # Sort the IoU scores in descending order and get indices\n+            iou_pred_sorted, sort_indices = torch.sort(iou_pred, dim=2, descending=True)\n+            # Reorder the masks according to sorted scores\n+            masks_sam = masks[:, :, mask_slice, :, :]\n+            masks_sam = torch.gather(\n+                masks_sam,\n+                2,\n+                sort_indices[..., None, None].expand(-1, -1, -1, masks_sam.shape[3], masks_sam.shape[4]),\n+            )\n+            # Update iou_pred with sorted scores\n+            iou_pred = iou_pred_sorted\n+        else:\n+            mask_slice = slice(0, 1)\n+            iou_pred = iou_pred[:, :, mask_slice]\n+            masks_sam = masks[:, :, mask_slice, :, :]\n+\n+        masks_hq = masks[:, :, slice(self.num_mask_tokens - 1, self.num_mask_tokens), :, :]\n+        if hq_token_only:\n+            masks = masks_hq\n+        else:\n+            masks = masks_sam + masks_hq\n+\n+        outputs = (masks, iou_pred)\n+        if output_attentions:\n+            outputs = outputs + (attentions,)\n+        else:\n+            outputs = outputs + (None,)\n+\n+        return outputs\n+\n+\n+class SamHQPreTrainedModel(SamPreTrainedModel):\n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, SamHQVisionEncoder):\n+            if module.pos_embed is not None:\n+                module.pos_embed.data.zero_()\n+\n+\n+SAM_HQ_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`SamHQConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The vision model from SAM-HQ without any head or projection on top.\"\"\",\n+    SAM_HQ_START_DOCSTRING,\n+)\n+class SamHQVisionModel(SamVisionModel):\n+    pass\n+\n+\n+@add_start_docstrings(\n+    \"Segment Anything Model HQ (SAM-HQ) for generating masks,given an input image and\",\n+    \" optional 2D location and bounding boxes.\",\n+    SAM_HQ_START_DOCSTRING,\n+)\n+class SamHQModel(SamModel):\n+    _tied_weights_keys = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+\n+    _keys_to_ignore_on_load_missing = [\"prompt_encoder.shared_embedding.positional_embedding\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vision_encoder = SamHQVisionEncoder(config.vision_config)\n+\n+        self.mask_decoder = SamHQMaskDecoder(config.mask_decoder_config)\n+\n+        self.post_init()\n+\n+    @torch.no_grad()\n+    def get_image_embeddings(\n+        self,\n+        pixel_values,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        Returns the image embeddings by passing the pixel values through the vision encoder.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Input pixel values\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\n+        \"\"\"\n+        vision_output = self.vision_encoder(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        image_embeddings = vision_output[0]\n+        intermediate_embeddings = vision_output[1]\n+\n+        return image_embeddings, intermediate_embeddings\n+\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_points: Optional[torch.FloatTensor] = None,\n+        input_labels: Optional[torch.LongTensor] = None,\n+        input_boxes: Optional[torch.FloatTensor] = None,\n+        input_masks: Optional[torch.LongTensor] = None,\n+        image_embeddings: Optional[torch.FloatTensor] = None,\n+        multimask_output: bool = True,\n+        hq_token_only: bool = False,\n+        attention_similarity: Optional[torch.FloatTensor] = None,\n+        target_embedding: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        intermediate_embeddings: Optional[List[torch.FloatTensor]] = None,\n+        **kwargs,\n+    ) -> List[Dict[str, torch.Tensor]]:\n+        r\"\"\"\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+                Pixel values. Pixel values can be obtained using [`SamHQProcessor`]. See [`SamHQProcessor.__call__`] for\n+                details.\n+            input_points (`torch.FloatTensor` of shape `(batch_size, num_points, 2)`):\n+                Input 2D spatial points, this is used by the prompt encoder to encode the prompt. Generally yields to much\n+                better results. The points can be obtained by passing a list of list of list to the processor that will\n+                create corresponding `torch` tensors of dimension 4. The first dimension is the image batch size, the\n+                second dimension is the point batch size (i.e. how many segmentation masks do we want the model to predict\n+                per input point), the third dimension is the number of points per segmentation mask (it is possible to pass\n+                multiple points for a single mask), and the last dimension is the x (vertical) and y (horizontal)\n+                coordinates of the point. If a different number of points is passed either for each image, or for each\n+                mask, the processor will create \"PAD\" points that will correspond to the (0, 0) coordinate, and the\n+                computation of the embedding will be skipped for these points using the labels.\n+            input_labels (`torch.LongTensor` of shape `(batch_size, point_batch_size, num_points)`):\n+                Input labels for the points, this is used by the prompt encoder to encode the prompt. According to the\n+                official implementation, there are 3 types of labels\n+\n+                - `1`: the point is a point that contains the object of interest\n+                - `0`: the point is a point that does not contain the object of interest\n+                - `-1`: the point corresponds to the background\n+\n+                We added the label:\n+\n+                - `-10`: the point is a padding point, thus should be ignored by the prompt encoder\n+\n+                The padding labels should be automatically done by the processor.\n+            input_boxes (`torch.FloatTensor` of shape `(batch_size, num_boxes, 4)`):\n+                Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n+                much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n+                that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n+                size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+                In the order (`x1`, `y1`, `x2`, `y2`):\n+\n+                - `x1`: the x coordinate of the top left point of the input box\n+                - `y1`: the y coordinate of the top left point of the input box\n+                - `x2`: the x coordinate of the bottom right point of the input box\n+                - `y2`: the y coordinate of the bottom right point of the input box\n+\n+            input_masks (`torch.FloatTensor` of shape `(batch_size, image_size, image_size)`):\n+                SAM_HQ model also accepts segmentation masks as input. The mask will be embedded by the prompt encoder to\n+                generate a corresponding embedding, that will be fed later on to the mask decoder. These masks needs to be\n+                manually fed by the user, and they need to be of shape (`batch_size`, `image_size`, `image_size`).\n+\n+            image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_channels, window_size, window_size)`):\n+                Image embeddings, this is used by the mask decder to generate masks and iou scores. For more memory\n+                efficient computation, users can first retrieve the image embeddings using the `get_image_embeddings`\n+                method, and then feed them to the `forward` method instead of feeding the `pixel_values`.\n+            multimask_output (`bool`, *optional*):\n+                In the original implementation and paper, the model always outputs 3 masks per image (or per point / per\n+                bounding box if relevant). However, it is possible to just output a single mask, that corresponds to the\n+                \"best\" mask, by specifying `multimask_output=False`.\n+            hq_token_only (`bool`, *optional*, defaults to `False`):\n+                Whether to use only the HQ token path for mask generation. When False, combines both standard and HQ paths.\n+                This is specific to SAM-HQ's architecture.\n+            attention_similarity (`torch.FloatTensor`, *optional*):\n+                Attention similarity tensor, to be provided to the mask decoder for target-guided attention in case the\n+                model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+            target_embedding (`torch.FloatTensor`, *optional*):\n+                Embedding of the target concept, to be provided to the mask decoder for target-semantic prompting in case\n+                the model is used for personalization as introduced in [PerSAM](https://arxiv.org/abs/2305.03048).\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+                tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+                more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+            intermediate_embeddings (`List[torch.FloatTensor]`, *optional*):\n+                Intermediate embeddings from vision encoder's non-windowed blocks, used by SAM-HQ for enhanced mask quality.\n+                Required when providing pre-computed image_embeddings instead of pixel_values.\n+        Example:\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoModel, AutoProcessor\n+\n+        >>> model = AutoModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        >>> processor = AutoProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        >>> img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/sam-car.png\"\n+        >>> raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        >>> input_points = [[[400, 650]]]  # 2D location of a window on the car\n+        >>> inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\")\n+\n+        >>> # Get high-quality segmentation mask\n+        >>> outputs = model(**inputs)\n+\n+        >>> # For high-quality mask only\n+        >>> outputs = model(**inputs, hq_token_only=True)\n+\n+        >>> # Postprocess masks\n+        >>> masks = processor.post_process_masks(\n+        ...     outputs.pred_masks, inputs[\"original_sizes\"], inputs[\"reshaped_input_sizes\"]\n+        ... )\n+        ```\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None and image_embeddings is None:\n+            raise ValueError(\"Either pixel_values or image_embeddings must be provided.\")\n+\n+        if pixel_values is not None and image_embeddings is not None:\n+            raise ValueError(\"Only one of pixel_values and image_embeddings can be provided.\")\n+\n+        if input_points is not None and len(input_points.shape) != 4:\n+            raise ValueError(\n+                \"The input_points must be a 4D tensor. Of shape `batch_size`, `point_batch_size`, `nb_points_per_image`, `2`.\"\n+                f\" got {input_points.shape}.\"\n+            )\n+\n+        if input_boxes is not None and len(input_boxes.shape) != 3:\n+            raise ValueError(\n+                \"The input_boxes must be a 3D tensor. Of shape `batch_size`, `nb_boxes`, `4`.\"\n+                f\" got {input_boxes.shape}.\"\n+            )\n+\n+        # Add validation for point and box batch sizes\n+        if input_points is not None and input_boxes is not None:\n+            point_batch_size = input_points.shape[1]\n+            box_batch_size = input_boxes.shape[1]\n+            if point_batch_size != box_batch_size:\n+                raise ValueError(\n+                    \"You should provide as many bounding boxes as input points per box. Got {} and {}.\".format(\n+                        point_batch_size, box_batch_size\n+                    )\n+                )\n+\n+        image_positional_embeddings = self.get_image_wide_positional_embeddings()\n+        # repeat with batch size\n+        batch_size = pixel_values.shape[0] if pixel_values is not None else image_embeddings.shape[0]\n+        image_positional_embeddings = image_positional_embeddings.repeat(batch_size, 1, 1, 1)\n+\n+        vision_attentions = None\n+        vision_hidden_states = None\n+\n+        if pixel_values is not None:\n+            vision_outputs = self.vision_encoder(\n+                pixel_values,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+            )\n+\n+            if return_dict:\n+                image_embeddings = vision_outputs.last_hidden_state\n+                intermediate_embeddings = vision_outputs.intermediate_embeddings\n+                if output_hidden_states:\n+                    vision_hidden_states = vision_outputs.hidden_states\n+                if output_attentions:\n+                    vision_attentions = vision_outputs.attentions\n+            else:\n+                image_embeddings = vision_outputs[0]\n+                intermediate_embeddings = vision_outputs[1]\n+                if output_hidden_states:\n+                    vision_hidden_states = vision_outputs[2]\n+                if output_attentions:\n+                    vision_attentions = vision_outputs[-1]\n+\n+        if input_points is not None and input_labels is None:\n+            input_labels = torch.ones_like(input_points[:, :, :, 0], dtype=torch.int, device=input_points.device)\n+\n+        sparse_embeddings, dense_embeddings = self.prompt_encoder(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            input_masks=input_masks,\n+        )\n+\n+        # Predict masks\n+        low_res_masks, iou_predictions, mask_decoder_attentions = self.mask_decoder(\n+            image_embeddings=image_embeddings,\n+            image_positional_embeddings=image_positional_embeddings,\n+            sparse_prompt_embeddings=sparse_embeddings,\n+            dense_prompt_embeddings=dense_embeddings,\n+            multimask_output=multimask_output,\n+            hq_token_only=hq_token_only,\n+            intermediate_embeddings=intermediate_embeddings,\n+            attention_similarity=attention_similarity,\n+            target_embedding=target_embedding,\n+            output_attentions=output_attentions,\n+        )\n+\n+        if not return_dict:\n+            output = (iou_predictions, low_res_masks)\n+            if output_hidden_states:\n+                output = output + (vision_hidden_states,)\n+\n+            if output_attentions:\n+                output = output + (vision_attentions, mask_decoder_attentions)\n+            return output\n+\n+        return SamHQImageSegmentationOutput(\n+            iou_scores=iou_predictions,\n+            pred_masks=low_res_masks,\n+            vision_hidden_states=vision_hidden_states,\n+            vision_attentions=vision_attentions,\n+            mask_decoder_attentions=mask_decoder_attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"SamHQVisionConfig\",\n+    \"SamHQMaskDecoderConfig\",\n+    \"SamHQPromptEncoderConfig\",\n+    \"SamHQConfig\",\n+    \"SamHQModel\",\n+    \"SamHQPreTrainedModel\",\n+    \"SamHQVisionModel\",\n+]"
        },
        {
            "sha": "1be26ce36210f3bbefd6bbd19a2d74e93a70f54c",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "added",
            "additions": 330,
            "deletions": 0,
            "changes": 330,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,330 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for SAMHQ.\n+\"\"\"\n+\n+from copy import deepcopy\n+from typing import List, Optional, Union\n+\n+import numpy as np\n+\n+from ...image_utils import ImageInput, VideoInput\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import AudioInput, BatchEncoding, PreTokenizedInput, TextInput\n+from ...utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class SamHQImagesKwargs(ImagesKwargs):\n+    segmentation_maps: Optional[ImageInput]\n+    input_points: Optional[List[List[float]]]\n+    input_labels: Optional[List[List[int]]]\n+    input_boxes: Optional[List[List[List[float]]]]\n+    point_pad_value: Optional[int]\n+\n+\n+class SamHQProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: SamHQImagesKwargs\n+    _defaults = {\n+        \"images_kwargs\": {\n+            \"point_pad_value\": None,\n+        }\n+    }\n+\n+\n+class SamHQProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a SAM HQ processor which wraps a SAM  image processor and an 2D points & Bounding boxes processor into a\n+    single processor.\n+\n+    [`SamHQProcessor`] offers all the functionalities of [`SamImageProcessor`]. See the docstring of\n+    [`~SamImageProcessor.__call__`] for more information.\n+\n+    Args:\n+        image_processor (`SamImageProcessor`):\n+            An instance of [`SamImageProcessor`]. The image processor is a required input.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\"]\n+    image_processor_class = \"SamImageProcessor\"\n+\n+    optional_call_args = [\n+        \"segmentation_maps\",\n+        \"input_points\",\n+        \"input_labels\",\n+        \"input_boxes\",\n+    ]\n+\n+    def __init__(self, image_processor):\n+        super().__init__(image_processor)\n+        # Ensure image_processor is properly initialized\n+        if not hasattr(self, \"image_processor\"):\n+            raise ValueError(\"image_processor was not properly initialized\")\n+        if not hasattr(self.image_processor, \"size\"):\n+            raise ValueError(\"image_processor.size is not set\")\n+        self.target_size = self.image_processor.size[\"longest_edge\"]\n+\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        # The following is to capture `segmentation_maps`, `input_points`, `input_labels` and `input_boxes`\n+        # arguments that may be passed as a positional argument.\n+        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n+        # or this conversation for more context:\n+        # https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n+        # This behavior is only needed for backward compatibility and will be removed in future versions.\n+        *args,  # to be deprecated\n+        text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio: Optional[AudioInput] = None,\n+        video: Optional[VideoInput] = None,\n+        **kwargs: Unpack[SamHQProcessorKwargs],\n+    ) -> BatchEncoding:\n+        \"\"\"\n+        This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n+        points and bounding boxes for the model if they are provided.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            SamHQProcessorKwargs,\n+            tokenizer_init_kwargs={},\n+            **kwargs,\n+            **self.prepare_and_validate_optional_call_args(*args),\n+        )\n+\n+        input_points = output_kwargs[\"images_kwargs\"].pop(\"input_points\", None)\n+        input_labels = output_kwargs[\"images_kwargs\"].pop(\"input_labels\", None)\n+        input_boxes = output_kwargs[\"images_kwargs\"].pop(\"input_boxes\", None)\n+\n+        encoding_image_processor = self.image_processor(\n+            images,\n+            **output_kwargs[\"images_kwargs\"],\n+        )\n+\n+        original_sizes = encoding_image_processor[\"original_sizes\"]\n+\n+        if hasattr(original_sizes, \"numpy\"):\n+            original_sizes = original_sizes.numpy()\n+\n+        input_points, input_labels, input_boxes = self._check_and_preprocess_points(\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+        )\n+\n+        encoding_image_processor = self._normalize_and_convert(\n+            encoding_image_processor,\n+            original_sizes,\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            input_boxes=input_boxes,\n+            return_tensors=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n+            point_pad_value=output_kwargs[\"images_kwargs\"].get(\"point_pad_value\"),\n+        )\n+\n+        return encoding_image_processor\n+\n+    def _normalize_and_convert(\n+        self,\n+        encoding_image_processor,\n+        original_sizes,\n+        input_points=None,\n+        input_labels=None,\n+        input_boxes=None,\n+        return_tensors=\"pt\",\n+        point_pad_value=-10,\n+    ):\n+        \"\"\"\n+        Normalize and convert the image processor output to the expected format.\n+        \"\"\"\n+        # Process input points\n+        if input_points is not None:\n+            input_points = self._normalize_batch_coordinates(input_points, original_sizes)\n+\n+            if not all(point.shape == input_points[0].shape for point in input_points):\n+                if input_labels is not None:\n+                    input_points, input_labels = self._pad_points_and_labels(\n+                        input_points, input_labels, point_pad_value\n+                    )\n+\n+            input_points = np.array(input_points)\n+\n+        # Process input labels\n+        if input_labels is not None:\n+            input_labels = np.array(input_labels)\n+\n+        # Process input boxes\n+        if input_boxes is not None:\n+            input_boxes = self._normalize_batch_coordinates(input_boxes, original_sizes, is_bounding_box=True)\n+            input_boxes = np.array(input_boxes)\n+\n+        # Update processor with converted inputs\n+        if input_boxes is not None:\n+            encoding_image_processor[\"input_boxes\"] = self._to_tensor(input_boxes, 3, return_tensors)\n+        if input_points is not None:\n+            encoding_image_processor[\"input_points\"] = self._to_tensor(input_points, 4, return_tensors)\n+        if input_labels is not None:\n+            encoding_image_processor[\"input_labels\"] = self._to_tensor(input_labels, 3, return_tensors)\n+\n+        return encoding_image_processor\n+\n+    def _pad_points_and_labels(self, input_points, input_labels, point_pad_value):\n+        r\"\"\"\n+        The method pads the 2D points and labels to the maximum number of points in the batch.\n+        \"\"\"\n+        expected_nb_points = max([point.shape[0] for point in input_points])\n+        processed_input_points = []\n+        for i, point in enumerate(input_points):\n+            if point.shape[0] != expected_nb_points:\n+                point = np.concatenate(\n+                    [point, np.zeros((expected_nb_points - point.shape[0], 2)) + point_pad_value], axis=0\n+                )\n+                input_labels[i] = np.append(input_labels[i], [point_pad_value])\n+            processed_input_points.append(point)\n+        input_points = processed_input_points\n+        return input_points, input_labels\n+\n+    def _normalize_coordinates(\n+        self, target_size: int, coords: np.ndarray, original_size, is_bounding_box=False\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Expects a numpy array of length 2 in the final dimension. Requires the original image size in (H,W) format.\n+        \"\"\"\n+        old_h, old_w = original_size\n+        new_h, new_w = self.image_processor._get_preprocess_shape(original_size, longest_edge=target_size)\n+        coords = deepcopy(coords).astype(float)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 2, 2)\n+\n+        coords[..., 0] = coords[..., 0] * (new_w / old_w)\n+        coords[..., 1] = coords[..., 1] * (new_h / old_h)\n+\n+        if is_bounding_box:\n+            coords = coords.reshape(-1, 4)\n+\n+        return coords\n+\n+    def _preprocess_input(self, inputs, error_message, expected_nesting=1, dtype=None):\n+        \"\"\"\n+        Preprocess input by converting torch tensors to numpy arrays and validating structure.\n+\n+        Args:\n+            inputs: The input to process\n+            error_message: Error message if validation fails\n+            expected_nesting: Expected nesting level (1 for points/labels, 2 for boxes)\n+            dtype: Optional data type for numpy array conversion\n+\n+        Returns:\n+            Processed input as list of numpy arrays or None\n+        \"\"\"\n+        if inputs is None:\n+            return None\n+\n+        # Convert torch tensor to list if applicable\n+        if hasattr(inputs, \"numpy\"):\n+            inputs = inputs.numpy().tolist()\n+\n+        # Validate structure based on expected nesting\n+        valid = isinstance(inputs, list)\n+        current = inputs\n+\n+        for _ in range(expected_nesting):\n+            if not valid or not current:\n+                break\n+            valid = valid and isinstance(current[0], list)\n+            current = current[0] if current else None\n+\n+        if not valid:\n+            raise ValueError(error_message)\n+\n+        # Convert to numpy arrays\n+        return [np.array(item, dtype=dtype) for item in inputs]\n+\n+    def _check_and_preprocess_points(\n+        self,\n+        input_points=None,\n+        input_labels=None,\n+        input_boxes=None,\n+    ):\n+        r\"\"\"\n+        Check and preprocesses the 2D points, labels and bounding boxes. It checks if the input is valid and if they\n+        are, it converts the coordinates of the points and bounding boxes. If a user passes directly a `torch.Tensor`,\n+        it is converted to a `numpy.ndarray` and then to a `list`.\n+        \"\"\"\n+        # Process each input type\n+        input_points = self._preprocess_input(input_points, \"Input points must be a list of list of floating points.\")\n+\n+        input_labels = self._preprocess_input(input_labels, \"Input labels must be a list of list integers.\")\n+\n+        input_boxes = self._preprocess_input(\n+            input_boxes,\n+            \"Input boxes must be a list of list of list of floating points.\",\n+            expected_nesting=2,\n+            dtype=np.float32,\n+        )\n+\n+        return input_points, input_labels, input_boxes\n+\n+    @property\n+    def model_input_names(self):\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(image_processor_input_names))\n+\n+    def post_process_masks(self, *args, **kwargs):\n+        return self.image_processor.post_process_masks(*args, **kwargs)\n+\n+    def _to_tensor(self, array, min_dim, return_tensors):\n+        \"\"\"\n+        Convert numpy array to tensor and ensure proper dimensionality.\n+        Args:\n+            array: The numpy array to convert\n+            min_dim: The minimum number of dimensions the result should have\n+            return_tensors: The type of tensors to return (e.g., \"pt\" for PyTorch tensors)\n+        Returns:\n+            The converted array or tensor with proper dimensions\n+        \"\"\"\n+        if return_tensors == \"pt\":\n+            array = torch.from_numpy(array)\n+            return array.unsqueeze(1) if array.ndim < min_dim else array\n+        return array\n+\n+    def _normalize_batch_coordinates(self, inputs, original_sizes, is_bounding_box=False):\n+        \"\"\"\n+        Normalize coordinates based on original sizes.\n+        Args:\n+            inputs: List of coordinate arrays\n+            original_sizes: Original sizes of the images\n+            is_bounding_box: Whether inputs are bounding boxes\n+        Returns:\n+            Normalized coordinates as list\n+        \"\"\"\n+        if len(original_sizes) != len(inputs):\n+            # Use first original size for all inputs\n+            return [\n+                self._normalize_coordinates(self.target_size, item, original_sizes[0], is_bounding_box=is_bounding_box)\n+                for item in inputs\n+            ]\n+        else:\n+            # Use paired original sizes for each input\n+            return [\n+                self._normalize_coordinates(self.target_size, item, size, is_bounding_box=is_bounding_box)\n+                for item, size in zip(inputs, original_sizes)\n+            ]\n+\n+\n+__all__ = [\"SamHQProcessor\"]"
        },
        {
            "sha": "5c0c5e72c811e2ef2509a83173d54348c4415fa0",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -189,7 +189,17 @@ def preprocess(\n                 inference_context = self.get_inference_context()\n                 with inference_context():\n                     model_inputs = self._ensure_tensor_on_device(model_inputs, device=self.device)\n-                    image_embeddings = self.model.get_image_embeddings(model_inputs.pop(\"pixel_values\"))\n+                    embeddings = self.model.get_image_embeddings(model_inputs.pop(\"pixel_values\"))\n+\n+                    # Handle both SAM (single tensor) and SAM-HQ (tuple) outputs\n+                    if isinstance(embeddings, tuple):\n+                        image_embeddings, intermediate_embeddings = embeddings\n+                        model_inputs[\"intermediate_embeddings\"] = intermediate_embeddings\n+                    else:\n+                        image_embeddings = embeddings\n+                    # TODO: Identifying the model by the type of its returned embeddings is brittle.\n+                    #       Consider using a more robust method for distinguishing model types here.\n+\n                     model_inputs[\"image_embeddings\"] = image_embeddings\n \n         n_points = grid_points.shape[1]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/sam_hq/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2F__init__.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1"
        },
        {
            "sha": "6c62d19746db3667d5af6e9f3722fbc494a5a46a",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "added",
            "additions": 1116,
            "deletions": 0,
            "changes": 1116,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,1116 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch SAM-HQ model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    SamHQConfig,\n+    SamHQMaskDecoderConfig,\n+    SamHQPromptEncoderConfig,\n+    SamHQVisionConfig,\n+    SamHQVisionModel,\n+    pipeline,\n+)\n+from transformers.testing_utils import cleanup, require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import SamHQModel, SamHQProcessor\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class SamHQVisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=36,\n+        intermediate_size=72,\n+        projection_dim=62,\n+        output_channels=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_channels=3,\n+        image_size=24,\n+        patch_size=2,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        dropout=0.0,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        qkv_bias=True,\n+        mlp_ratio=4.0,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        rel_pos_zero_init=False,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        num_pos_feats=16,\n+        mlp_dim=None,\n+        batch_size=2,\n+    ):\n+        self.parent = parent\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.projection_dim = projection_dim\n+        self.output_channels = output_channels\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.qkv_bias = qkv_bias\n+        self.mlp_ratio = mlp_ratio\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.rel_pos_zero_init = rel_pos_zero_init\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.num_pos_feats = num_pos_feats\n+        self.mlp_dim = mlp_dim\n+        self.batch_size = batch_size\n+\n+        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def get_config(self):\n+        return SamHQVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+            initializer_factor=self.initializer_factor,\n+            output_channels=self.output_channels,\n+            qkv_bias=self.qkv_bias,\n+            mlp_ratio=self.mlp_ratio,\n+            use_abs_pos=self.use_abs_pos,\n+            use_rel_pos=self.use_rel_pos,\n+            rel_pos_zero_init=self.rel_pos_zero_init,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+            num_pos_feats=self.num_pos_feats,\n+            mlp_dim=self.mlp_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = SamHQVisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        output_size = self.image_size // self.patch_size\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape, (self.batch_size, self.output_channels, output_size, output_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class SamHQVisionModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (SamHQVisionModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    test_torch_exportable = True\n+\n+    def setUp(self):\n+        self.model_tester = SamHQVisionModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=SamHQVisionConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        expected_attention_shape = (\n+            self.model_tester.batch_size * self.model_tester.num_attention_heads,\n+            196,\n+            196,\n+        )\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-4:]),\n+                list(expected_attention_shape),\n+            )\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamVisionModel does not support training\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in create_and_check_model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SAM model can't be compiled dynamic yet\")\n+\n+\n+class SamHQPromptEncoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        input_image_size=24,\n+        patch_size=2,\n+        mask_input_channels=4,\n+        num_point_embeddings=4,\n+        hidden_act=\"gelu\",\n+    ):\n+        self.hidden_size = hidden_size\n+        self.input_image_size = input_image_size\n+        self.patch_size = patch_size\n+        self.mask_input_channels = mask_input_channels\n+        self.num_point_embeddings = num_point_embeddings\n+        self.hidden_act = hidden_act\n+\n+    def get_config(self):\n+        return SamHQPromptEncoderConfig(\n+            image_size=self.input_image_size,\n+            patch_size=self.patch_size,\n+            mask_input_channels=self.mask_input_channels,\n+            hidden_size=self.hidden_size,\n+            num_point_embeddings=self.num_point_embeddings,\n+            hidden_act=self.hidden_act,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        dummy_points = floats_tensor([self.batch_size, 3, 2])\n+        config = self.get_config()\n+\n+        return config, dummy_points\n+\n+\n+class SamHQMaskDecoderTester:\n+    def __init__(\n+        self,\n+        hidden_size=32,\n+        hidden_act=\"relu\",\n+        mlp_dim=64,\n+        num_hidden_layers=12,\n+        num_attention_heads=4,\n+        attention_downsample_rate=2,\n+        num_multimask_outputs=3,\n+        iou_head_depth=3,\n+        iou_head_hidden_dim=32,\n+        layer_norm_eps=1e-6,\n+        vit_dim=36,\n+    ):\n+        self.hidden_size = hidden_size\n+        self.hidden_act = hidden_act\n+        self.mlp_dim = mlp_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_downsample_rate = attention_downsample_rate\n+        self.num_multimask_outputs = num_multimask_outputs\n+        self.iou_head_depth = iou_head_depth\n+        self.iou_head_hidden_dim = iou_head_hidden_dim\n+        self.layer_norm_eps = layer_norm_eps\n+        self.vit_dim = vit_dim\n+\n+    def get_config(self):\n+        return SamHQMaskDecoderConfig(\n+            hidden_size=self.hidden_size,\n+            hidden_act=self.hidden_act,\n+            mlp_dim=self.mlp_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            attention_downsample_rate=self.attention_downsample_rate,\n+            num_multimask_outputs=self.num_multimask_outputs,\n+            iou_head_depth=self.iou_head_depth,\n+            iou_head_hidden_dim=self.iou_head_hidden_dim,\n+            layer_norm_eps=self.layer_norm_eps,\n+            vit_dim=self.vit_dim,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        dummy_inputs = {\n+            \"image_embedding\": floats_tensor([self.batch_size, self.hidden_size]),\n+        }\n+        return config, dummy_inputs\n+\n+\n+class SamHQModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        hidden_size=36,\n+        intermediate_size=72,\n+        projection_dim=62,\n+        output_channels=32,\n+        num_hidden_layers=12,\n+        num_attention_heads=4,\n+        num_channels=3,\n+        image_size=24,\n+        patch_size=2,\n+        hidden_act=\"gelu\",\n+        layer_norm_eps=1e-06,\n+        dropout=0.0,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        qkv_bias=True,\n+        mlp_ratio=4.0,\n+        use_abs_pos=True,\n+        use_rel_pos=True,\n+        rel_pos_zero_init=False,\n+        window_size=14,\n+        global_attn_indexes=[2, 5, 8, 11],\n+        num_pos_feats=16,\n+        mlp_dim=None,\n+        batch_size=2,\n+    ):\n+        self.parent = parent\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.output_channels = output_channels\n+        self.num_channels = num_channels\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.hidden_act = hidden_act\n+        self.layer_norm_eps = layer_norm_eps\n+        self.qkv_bias = qkv_bias\n+        self.mlp_ratio = mlp_ratio\n+        self.use_abs_pos = use_abs_pos\n+        self.use_rel_pos = use_rel_pos\n+        self.rel_pos_zero_init = rel_pos_zero_init\n+        self.window_size = window_size\n+        self.global_attn_indexes = global_attn_indexes\n+        self.num_pos_feats = num_pos_feats\n+        self.mlp_dim = mlp_dim\n+        self.batch_size = batch_size\n+\n+        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+        self.prompt_encoder_tester = SamHQPromptEncoderTester()\n+        self.mask_decoder_tester = SamHQMaskDecoderTester()\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        vision_config = SamHQVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+            initializer_factor=self.initializer_factor,\n+            output_channels=self.output_channels,\n+            qkv_bias=self.qkv_bias,\n+            mlp_ratio=self.mlp_ratio,\n+            use_abs_pos=self.use_abs_pos,\n+            use_rel_pos=self.use_rel_pos,\n+            rel_pos_zero_init=self.rel_pos_zero_init,\n+            window_size=self.window_size,\n+            global_attn_indexes=self.global_attn_indexes,\n+            num_pos_feats=self.num_pos_feats,\n+            mlp_dim=self.mlp_dim,\n+        )\n+\n+        prompt_encoder_config = self.prompt_encoder_tester.get_config()\n+\n+        mask_decoder_config = self.mask_decoder_tester.get_config()\n+\n+        return SamHQConfig(\n+            vision_config=vision_config,\n+            prompt_encoder_config=prompt_encoder_config,\n+            mask_decoder_config=mask_decoder_config,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = SamHQModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            # Explicitly pass multimask_output=True\n+            result = model(pixel_values, multimask_output=True)\n+\n+        self.parent.assertEqual(result.iou_scores.shape, (self.batch_size, 1, 3))\n+        self.parent.assertEqual(result.pred_masks.shape[:3], (self.batch_size, 1, 3))\n+\n+    def create_and_check_get_image_features(self, config, pixel_values):\n+        model = SamHQModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            image_embeddings = model.get_image_embeddings(pixel_values)\n+        self.parent.assertEqual(image_embeddings[0][0].shape, (self.output_channels, 12, 12))\n+\n+    def create_and_check_get_image_and_intermediate_embeddings(self, config, pixel_values):\n+        model = SamHQModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            image_embeddings, intermediate_embeddings = model.get_image_embeddings(pixel_values)\n+\n+        self.parent.assertEqual(image_embeddings[0].shape, (self.output_channels, 12, 12))\n+        self.parent.assertEqual(intermediate_embeddings[0][0].shape, (12, 12, self.hidden_size))\n+\n+    def create_and_check_get_image_intermediate_embeddings(self, config, pixel_values):\n+        model = SamHQModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            image_embeddings, intermediate_embeddings = model.get_image_embeddings(pixel_values)\n+\n+        self.parent.assertIsInstance(intermediate_embeddings, list)\n+        self.parent.assertTrue(len(intermediate_embeddings) > 0)\n+        for embedding in intermediate_embeddings:\n+            self.parent.assertEqual(embedding.shape, (self.batch_size, 12, 12, self.hidden_size))\n+\n+    def create_and_check_get_image_hidden_states(self, config, pixel_values):\n+        model = SamHQModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model.vision_encoder(\n+                pixel_values,\n+                output_hidden_states=True,\n+                return_dict=True,\n+            )\n+\n+        # after computing the convolutional features\n+        expected_hidden_states_shape = (self.batch_size, 12, 12, 36)\n+        self.parent.assertEqual(len(result.hidden_states), self.num_hidden_layers + 1)\n+        self.parent.assertEqual(result[-1][0].shape, expected_hidden_states_shape)\n+\n+        with torch.no_grad():\n+            result = model.vision_encoder(\n+                pixel_values,\n+                output_hidden_states=True,\n+            )\n+\n+        # after computing the convolutional features\n+        expected_hidden_states_shape = (self.batch_size, 12, 12, 36)\n+        self.parent.assertEqual(len(result[1]), self.num_hidden_layers + 1)\n+        self.parent.assertEqual(result[1][0].shape, expected_hidden_states_shape)\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class SamHQModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SAM-HQ's vision encoder does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (SamHQModel,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": SamHQModel, \"mask-generation\": SamHQModel} if is_torch_available() else {}\n+    )\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    test_torchscript = False\n+    test_cpu_offload = False\n+    test_disk_offload_bin = False\n+    test_disk_offload_safetensors = False\n+\n+    # TODO: Fix me @Arthur: `run_batch_test` in `tests/test_pipeline_mixin.py` not working\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        return True\n+\n+    def setUp(self):\n+        self.model_tester = SamHQModelTester(self)\n+        common_properties = [\"initializer_range\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=SamHQConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SAM-HQ's vision encoder does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported in SamHQ models\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_get_image_features(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_get_image_features(*config_and_inputs)\n+\n+    def test_get_image_and_intermediate_embeddings(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_get_image_and_intermediate_embeddings(*config_and_inputs)\n+\n+    def test_get_image_intermediate_embeddings(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_get_image_intermediate_embeddings(*config_and_inputs)\n+\n+    def test_image_hidden_states(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_get_image_hidden_states(*config_and_inputs)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        expected_vision_attention_shape = (\n+            self.model_tester.batch_size * self.model_tester.num_attention_heads,\n+            196,\n+            196,\n+        )\n+        expected_mask_decoder_attention_shape = (self.model_tester.batch_size, 1, 144, 32)\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            vision_attentions = outputs.vision_attentions\n+            self.assertEqual(len(vision_attentions), self.model_tester.num_hidden_layers)\n+\n+            mask_decoder_attentions = outputs.mask_decoder_attentions\n+            self.assertEqual(len(mask_decoder_attentions), self.model_tester.mask_decoder_tester.num_hidden_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            vision_attentions = outputs.vision_attentions\n+            self.assertEqual(len(vision_attentions), self.model_tester.num_hidden_layers)\n+\n+            mask_decoder_attentions = outputs.mask_decoder_attentions\n+            self.assertEqual(len(mask_decoder_attentions), self.model_tester.mask_decoder_tester.num_hidden_layers)\n+\n+            self.assertListEqual(\n+                list(vision_attentions[0].shape[-4:]),\n+                list(expected_vision_attention_shape),\n+            )\n+\n+            self.assertListEqual(\n+                list(mask_decoder_attentions[0].shape[-4:]),\n+                list(expected_mask_decoder_attention_shape),\n+            )\n+\n+    @unittest.skip(reason=\"SamHQModel does not support training\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamHQModel does not support training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamHQModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamHQModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SamHQModel does not support training\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in create_and_check_model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    def check_pt_tf_outputs(self, tf_outputs, pt_outputs, model_class, tol=5e-5, name=\"outputs\", attributes=None):\n+        # Use a slightly higher default tol to make the tests non-flaky\n+        super().check_pt_tf_outputs(tf_outputs, pt_outputs, model_class, tol=tol, name=name, attributes=attributes)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"sushmanth/sam_hq_vit_b\"\n+        model = SamHQModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"SamHQModel can't be compiled dynamic yet\")\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        \"\"\"\n+        Tests if composite models dispatch correctly on SDPA/eager when requested so when loading the model.\n+        This tests only by looking at layer names, as usually SDPA layers are calles \"SDPAAttention\".\n+        In contrast to the above test, this one checks if the \"config._attn_implamentation\" is a dict after the model\n+        is loaded, because we manually replicate requested attn implementation on each sub-config when loading.\n+        See https://github.com/huggingface/transformers/pull/32238 for more info\n+\n+        The test tries to cover most general cases of composite models, VLMs with vision and text configs. Any model\n+        that has a different set of sub-configs has to overwrite this test.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+                # Root model determines SDPA support\n+                attn_impl = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_sdpa.vision_encoder.config._attn_implementation == attn_impl)\n+                self.assertTrue(model_sdpa.mask_decoder.config._attn_implementation == attn_impl)\n+\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.vision_encoder.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.mask_decoder.config._attn_implementation == \"eager\")\n+\n+                # Verify SDPA/eager layer presence\n+                has_sdpa = False\n+                for name, submodule in model_sdpa.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        has_sdpa = True\n+                        break\n+\n+                if not has_sdpa and attn_impl == \"sdpa\":\n+                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+def prepare_image():\n+    img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+def prepare_dog_img():\n+    img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/dog-sam.png\"\n+    raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+    return raw_image\n+\n+\n+@slow\n+class SamHQModelIntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        super().tearDown()\n+        # clean-up as much as possible GPU memory occupied by PyTorch\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_inference_mask_generation_no_point(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        inputs = processor(images=raw_image, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores\n+\n+        masks = outputs.pred_masks[0, 0, 0, 0, :3]\n+        self.assertTrue(torch.allclose(scores[0][0][-1], torch.tensor(0.4482), atol=2e-4))\n+        self.assertTrue(\n+            torch.allclose(masks, torch.tensor([-13.1695, -14.6201, -14.8989]).to(torch_device), atol=2e-3)\n+        )\n+\n+    def test_inference_mask_generation_one_point_one_bb(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_boxes = [[[650, 900, 1000, 1250]]]\n+        input_points = [[[820, 1080]]]\n+\n+        inputs = processor(\n+            images=raw_image, input_boxes=input_boxes, input_points=input_points, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores.squeeze()\n+        masks = outputs.pred_masks[0, 0, 0, 0, :3]\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9700), atol=2e-4))\n+        self.assertTrue(\n+            torch.allclose(masks, torch.tensor([-29.9144, -30.0546, -30.9526]).to(torch_device), atol=3e-2)\n+        )\n+\n+    def test_inference_mask_generation_batched_points_batched_images(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_points = [\n+            [[[820, 1080]], [[820, 1080]], [[820, 1080]], [[820, 1080]]],\n+            [[[510, 1080]], [[820, 1080]], [[820, 1080]], [[820, 1080]]],\n+        ]\n+\n+        inputs = processor(images=[raw_image, raw_image], input_points=input_points, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores.squeeze().cpu()\n+        masks = outputs.pred_masks[0, 0, 0, 0, :3].cpu()\n+        EXPECTED_SCORES = torch.tensor(\n+            [\n+                [\n+                    [0.9195, 0.8316, 0.6614],\n+                    [0.9195, 0.8316, 0.6614],\n+                    [0.9195, 0.8316, 0.6614],\n+                    [0.9195, 0.8316, 0.6614],\n+                ],\n+                [\n+                    [0.7598, 0.7388, 0.3110],\n+                    [0.9195, 0.8317, 0.6614],\n+                    [0.9195, 0.8317, 0.6614],\n+                    [0.9195, 0.8317, 0.6614],\n+                ],\n+            ]\n+        )\n+        EXPECTED_MASKS = torch.tensor([-40.2445, -37.4300, -38.1577])\n+\n+        self.assertTrue(torch.allclose(scores, EXPECTED_SCORES, atol=1e-3))\n+        self.assertTrue(torch.allclose(masks, EXPECTED_MASKS, atol=9e-3))\n+\n+    def test_inference_mask_generation_one_point_one_bb_zero(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_boxes = [[[620, 900, 1000, 1255]]]\n+        input_points = [[[820, 1080]]]\n+        labels = [[0]]\n+\n+        inputs = processor(\n+            images=raw_image,\n+            input_boxes=input_boxes,\n+            input_points=input_points,\n+            input_labels=labels,\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores.squeeze()\n+\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.8680), atol=1e-3))\n+\n+    def test_inference_mask_generation_with_labels(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_points = [[[400, 650]]]\n+        input_labels = [[1]]\n+\n+        inputs = processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9137), atol=1e-4))\n+\n+    def test_inference_mask_generation_without_labels(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_points = [[[400, 650]]]\n+\n+        inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9137), atol=1e-3))\n+\n+    def test_inference_mask_generation_two_points_with_labels(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_points = [[[400, 650], [800, 650]]]\n+        input_labels = [[1, 1]]\n+\n+        inputs = processor(\n+            images=raw_image, input_points=input_points, input_labels=input_labels, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.8859), atol=1e-3))\n+\n+    def test_inference_mask_generation_two_points_without_labels(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        input_points = [[[400, 650], [800, 650]]]\n+\n+        inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.8859), atol=1e-3))\n+\n+    def test_inference_mask_generation_two_points_batched(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+\n+        input_points = [[[400, 650], [800, 650]], [[400, 650]]]\n+        input_labels = [[1, 1], [1]]\n+\n+        inputs = processor(\n+            images=[raw_image, raw_image],\n+            input_points=input_points,\n+            input_labels=input_labels,\n+            images_kwargs={\"point_pad_value\": -10},\n+            return_tensors=\"pt\",\n+        ).to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[0][-1], torch.tensor(0.4482), atol=1e-4))\n+        self.assertTrue(torch.allclose(scores[1][-1], torch.tensor(0.4482), atol=1e-4))\n+\n+    def test_inference_mask_generation_one_box(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+\n+        input_boxes = [[[75, 275, 1725, 850]]]\n+\n+        inputs = processor(images=raw_image, input_boxes=input_boxes, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.6265), atol=1e-4))\n+\n+    def test_inference_mask_generation_batched_image_one_point(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+        raw_dog_image = prepare_dog_img()\n+\n+        input_points = [[[820, 1080]], [[220, 470]]]\n+\n+        inputs = processor(images=[raw_image, raw_dog_image], input_points=input_points, return_tensors=\"pt\").to(\n+            torch_device\n+        )\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores_batched = outputs.iou_scores.squeeze()\n+\n+        input_points = [[[220, 470]]]\n+\n+        inputs = processor(images=raw_dog_image, input_points=input_points, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+        scores_single = outputs.iou_scores.squeeze()\n+        self.assertTrue(torch.allclose(scores_batched[1, :], scores_single, atol=1e-4))\n+\n+    def test_inference_mask_generation_two_points_point_batch(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+\n+        input_points = torch.Tensor([[[400, 650]], [[220, 470]]]).cpu()  # fmt: skip\n+\n+        input_points = input_points.unsqueeze(0)\n+\n+        inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        iou_scores = outputs.iou_scores.cpu()\n+        self.assertTrue(iou_scores.shape == (1, 2, 3))\n+        torch.testing.assert_close(\n+            iou_scores, torch.tensor([[[0.9889, 0.9508, 0.9137], [0.8070, 0.7934, 0.7932]]]), atol=1e-3, rtol=1e-3\n+        )\n+\n+    def test_inference_mask_generation_three_boxes_point_batch(self):\n+        model = SamHQModel.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+        processor = SamHQProcessor.from_pretrained(\"sushmanth/sam_hq_vit_b\")\n+\n+        model.to(torch_device)\n+        model.eval()\n+\n+        raw_image = prepare_image()\n+\n+        # fmt: off\n+        input_boxes = torch.Tensor([[[620, 900, 1000, 1255]], [[75, 275, 1725, 850]],  [[75, 275, 1725, 850]]]).cpu()\n+        EXPECTED_IOU = torch.tensor([[[0.9850, 0.9730, 0.9726],\n+         [0.8891, 0.8017, 0.6265],\n+         [0.8891, 0.8017, 0.6265]]])\n+        # fmt: on\n+        input_boxes = input_boxes.unsqueeze(0)\n+\n+        inputs = processor(raw_image, input_boxes=input_boxes, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        iou_scores = outputs.iou_scores.cpu()\n+        self.assertTrue(iou_scores.shape == (1, 3, 3))\n+        torch.testing.assert_close(iou_scores, EXPECTED_IOU, atol=1e-4, rtol=1e-4)\n+\n+    def test_dummy_pipeline_generation(self):\n+        generator = pipeline(\"mask-generation\", model=\"sushmanth/sam_hq_vit_b\", device=torch_device)\n+        raw_image = prepare_image()\n+\n+        _ = generator(raw_image, points_per_batch=64)"
        },
        {
            "sha": "9f24cc7ab4ad0f73024e6402c341e9e1a344f9c9",
            "filename": "tests/models/sam_hq/test_processor_samhq.py",
            "status": "added",
            "additions": 167,
            "deletions": 0,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_processor_samhq.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -0,0 +1,167 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_torchvision, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoProcessor, SamHQProcessor, SamImageProcessor\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_vision\n+@require_torchvision\n+class SamHQProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = SamHQProcessor\n+\n+    @classmethod\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = SamImageProcessor()\n+        processor = SamHQProcessor(image_processor)\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    @classmethod\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    # Processor tester class can't use ProcessorTesterMixin atm because the processor is atypical e.g. only contains an image processor\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images.\"\"\"\n+        return prepare_image_inputs()\n+\n+    def prepare_mask_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+        mask_inputs = [np.random.randint(255, size=(30, 400), dtype=np.uint8)]\n+        mask_inputs = [Image.fromarray(x) for x in mask_inputs]\n+        return mask_inputs\n+\n+    def test_tokenizer_defaults_preserved_by_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_chat_template_save_loading(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_kwargs_overrides_default_tokenizer_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_unstructured_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_unstructured_kwargs_batched(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_doubly_passed_kwargs(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_structured_kwargs_nested(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_structured_kwargs_nested_from_dict(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_save_load_pretrained_additional_features(self):\n+        self.skipTest(\"SamHQProcessor does not have a tokenizer\")\n+\n+    def test_image_processor_no_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = SamHQProcessor(image_processor=image_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+\n+        input_feat_extract = image_processor(image_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, return_tensors=\"pt\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum().item(), input_processor[key].sum().item(), delta=1e-2)\n+\n+        for image in input_feat_extract.pixel_values:\n+            self.assertEqual(image.shape, (3, 1024, 1024))\n+\n+        for original_size in input_feat_extract.original_sizes:\n+            np.testing.assert_array_equal(original_size, np.array([30, 400]))\n+\n+        for reshaped_input_size in input_feat_extract.reshaped_input_sizes:\n+            np.testing.assert_array_equal(\n+                reshaped_input_size, np.array([77, 1024])\n+            )  # reshaped_input_size value is before padding\n+\n+    def test_image_processor_with_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = SamHQProcessor(image_processor=image_processor)\n+\n+        image_input = self.prepare_image_inputs()\n+        mask_input = self.prepare_mask_inputs()\n+\n+        input_feat_extract = image_processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+        input_processor = processor(images=image_input, segmentation_maps=mask_input, return_tensors=\"pt\")\n+\n+        for key in input_feat_extract.keys():\n+            self.assertAlmostEqual(input_feat_extract[key].sum().item(), input_processor[key].sum().item(), delta=1e-2)\n+\n+        for label in input_feat_extract.labels:\n+            self.assertEqual(label.shape, (256, 256))\n+\n+    @require_torch\n+    def test_post_process_masks(self):\n+        image_processor = self.get_image_processor()\n+\n+        processor = SamHQProcessor(image_processor=image_processor)\n+        dummy_masks = [torch.ones((1, 3, 5, 5))]\n+\n+        original_sizes = [[1764, 2646]]\n+\n+        reshaped_input_size = [[683, 1024]]\n+        masks = processor.post_process_masks(dummy_masks, original_sizes, reshaped_input_size)\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        masks = processor.post_process_masks(\n+            dummy_masks, torch.tensor(original_sizes), torch.tensor(reshaped_input_size)\n+        )\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        # should also work with np\n+        dummy_masks = [np.ones((1, 3, 5, 5))]\n+        masks = processor.post_process_masks(dummy_masks, np.array(original_sizes), np.array(reshaped_input_size))\n+\n+        self.assertEqual(masks[0].shape, (1, 3, 1764, 2646))\n+\n+        dummy_masks = [[1, 0], [0, 1]]\n+        with self.assertRaises(ValueError):\n+            masks = processor.post_process_masks(dummy_masks, np.array(original_sizes), np.array(reshaped_input_size))"
        },
        {
            "sha": "d6cb3f42c17efd73610e368ff45ad47b93d0f7e3",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -105,6 +105,8 @@\n     \"AutoformerConfig\": [\"num_static_real_features\", \"num_time_features\"],\n     # used internally to calculate `mlp_dim`\n     \"SamVisionConfig\": [\"mlp_ratio\"],\n+    # used internally to calculate `mlp_dim`\n+    \"SamHQVisionConfig\": [\"mlp_ratio\"],\n     # For (head) training, but so far not implemented\n     \"ClapAudioConfig\": [\"num_classes\"],\n     # Not used, but providing useful information to users"
        },
        {
            "sha": "e37894c4eabdce2d8bb85cce6ce3fc2187c81582",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -1040,6 +1040,7 @@ def _rep(match):\n     \"OpenAI GPT\": \"GPT\",\n     \"Perceiver\": \"Perceiver IO\",\n     \"SAM\": \"Segment Anything\",\n+    \"SAM_HQ\": \"Segment Anything High Quality\",\n     \"ViT\": \"Vision Transformer (ViT)\",\n }\n "
        },
        {
            "sha": "154eda0cf13e0bc158f5dda9f8340f45b393845f",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -469,6 +469,8 @@\n     \"SEWForCTC\",\n     \"SamConfig\",\n     \"SamPromptEncoderConfig\",\n+    \"SamHQConfig\",\n+    \"SamHQPromptEncoderConfig\",\n     \"SeamlessM4TConfig\",  # use of unconventional markdown\n     \"SeamlessM4Tv2Config\",  # use of unconventional markdown\n     \"Seq2SeqTrainingArguments\","
        },
        {
            "sha": "1164ac9db033627a250de14e87377efff7af670b",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -235,6 +235,7 @@\n     \"JukeboxVQVAE\",\n     \"JukeboxPrior\",\n     \"SamModel\",\n+    \"SamHQModel\",\n     \"DPTForDepthEstimation\",\n     \"DecisionTransformerGPT2Model\",\n     \"GLPNForDepthEstimation\","
        },
        {
            "sha": "4dfcaca525aeab2d03b740cbf3582d8baf6a0c4a",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/65e940208c38ebf82b5d7a2441eec361d2c968b1/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=65e940208c38ebf82b5d7a2441eec361d2c968b1",
            "patch": "@@ -209,6 +209,7 @@ docs/source/en/model_doc/roc_bert.md\n docs/source/en/model_doc/roformer.md\n docs/source/en/model_doc/rwkv.md\n docs/source/en/model_doc/sam.md\n+docs/source/en/model_doc/sam_hq.md\n docs/source/en/model_doc/segformer.md\n docs/source/en/model_doc/sew-d.md\n docs/source/en/model_doc/sew.md"
        }
    ],
    "stats": {
        "total": 4927,
        "additions": 4926,
        "deletions": 1
    }
}