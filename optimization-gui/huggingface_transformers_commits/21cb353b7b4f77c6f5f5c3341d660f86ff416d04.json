{
    "author": "casinca",
    "message": "[docs] Typos - Single GPU efficient training features (#38964)\n\n* Typos\n\n- corrected bf16 training argument\r\n- corrected header for SDPA\n\n* improved readability for SDPA suggested by @stevhliu\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "21cb353b7b4f77c6f5f5c3341d660f86ff416d04",
    "files": [
        {
            "sha": "e8cc6ec7579e435371b9508deb214e35d70e42e3",
            "filename": "docs/source/en/perf_train_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/21cb353b7b4f77c6f5f5c3341d660f86ff416d04/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/21cb353b7b4f77c6f5f5c3341d660f86ff416d04/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md?ref=21cb353b7b4f77c6f5f5c3341d660f86ff416d04",
            "patch": "@@ -31,7 +31,7 @@ Refer to the table below to quickly help you identify the features relevant to y\n | data preloading | yes | no |\n | torch_empty_cache_steps | no | yes |\n | torch.compile | yes | no |\n-| PEFT | no | yes |\n+| scaled dot production attention (SDPA) | yes | yes |\n \n ## Trainer\n \n@@ -128,7 +128,7 @@ fp16 isn't memory-optimized because the gradients that are computed in fp16 are\n \n [bf16](https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus) trades off some precision for a much larger dynamic range, which is helpful for avoiding overflow and underflow errors. You can use bf16 without adding any loss scaling methods like you would with fp16. bf16 is supported by NVIDIAs Ampere architecture or newer.\n \n-Configure [`~TrainingArguments.fp16`] in [`TrainingArguments`] to enable mixed precision training with the bf16 data type.\n+Configure [`~TrainingArguments.bf16`] in [`TrainingArguments`] to enable mixed precision training with the bf16 data type.\n \n ```py\n from transformers import TrainingArguments"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}