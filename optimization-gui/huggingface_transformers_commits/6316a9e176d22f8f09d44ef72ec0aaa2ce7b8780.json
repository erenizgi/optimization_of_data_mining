{
    "author": "3outeille",
    "message": "Fix MoE for V5 (#42456)\n\n* remove zero_like + scatter\n\n* fix mixtral moe\n\n* fix other moe models as well\n\n* fix ci\n\n* fix modular mixtral\n\n* fix qwen2_moe + qwen3_next\n\n* fix device mismatch for qwen3_vl_moe to pass tests\n\n* fix modular mixtral\n\n* fix other models\n\n* rm slow tokenizers (#40936)\n\n* fixes missed\n\n* gemma test fix\n\n* refactor\n\n* rm legacy from llama\n\n* added renaming\n\n* add _model\n\n* update legacy\n\n* update legacy\n\n* fix docstring\n\n* always load blank, then set _tokenizer if we have it\n\n* new toks\n\n* update all berttokenizer based models\n\n* apply feedback - delete bert duplicates\n\n* more models --> fast only\n\n* more convert_slow models\n\n* fix common test refs\n\n* updating fast only tokenizers\n\n* openai and pegasus\n\n* enable sentencepiecebackend\n\n* more models\n\n* code gen\n\n* t5\n\n* code gen tests\n\n* speecht5\n\n* mbart\n\n* mbart50\n\n* more models\n\n* more models\n\n* layouglmv2\n\n* update tests\n\n* update tests\n\n* update tests\n\n* pretrainedtokenizer\n\n* whisper\n\n* whisper\n\n* layoutxlm and storing backends\n\n* refactor sentencepiecebackend and additional_special_tokens\n\n* renaming tokenization_utils --> tokenization_python\n\n* udpate tests\n\n* bert test\n\n* blenderbot\n\n* clip\n\n* codegen\n\n* code_llama\n\n* cohere\n\n* deberata, deberat v2, funnel\n\n* gpt2\n\n* batch update tests\n\n* pegasus qwen2 roberta\n\n* more models\n\n* layout tests\n\n* some renaming\n\n* fix references to utils_fast\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* fix some tests\n\n* regression\n\n* fix refs\n\n* fix refs\n\n* missed the most crucial file in my last commit\n\n* fix refs\n\n* fix refs\n\n* fix refs\n\n* batch encode fix\n\n* fix some tests\n\n* BC for batch_decode bc too many refs\n\n* more tests\n\n* fix more tests\n\n* fix for processors\n\n* fixing more models\n\n* deleted mbart50 by accident\n\n* seamless m4t\n\n* albert fix\n\n* whisper\n\n* layout3\n\n* attempt to fix cached tokenizers on CI\n\n* trying another fix on CI\n\n* again try to work around CI\n\n* bertweet\n\n* tapas\n\n* mbart50\n\n* luke\n\n* mluke\n\n* markuplm\n\n* markuplm\n\n* fix some more auto tests\n\n* some random model failures\n\n* mistralcommontestser\n\n* more fixes\n\n* ref fix\n\n* siglip\n\n* marian\n\n* plbart\n\n* update utils toks\n\n* seamless m4t\n\n* roc bert\n\n* udpate byt5 test\n\n* xlm\n\n* esm\n\n* roformer\n\n* code llama\n\n* biogpt\n\n* m2m100\n\n* dpr and flaubert\n\n* xlm and speech to text\n\n* tok backend pass object\n\n* tokenizer object pass\n\n* wav2vec2\n\n* wav2vec2\n\n* cpmant\n\n* update utils tokenizers\n\n* cpmant\n\n* bartpho\n\n* test apply chat template assistant mask\n\n* apply chat template video\n\n* apply chat template assistant mask\n\n* test torch\n\n* update from slow in base and fix donut processor errors\n\n* auto to point to tokenizers backend, fix kosmos2\n\n* some non model fixes for old slow models that no longer have their own tokenizer file as they are the same as bert\n\n* missed file from last commit\n\n* idefics2\n\n* fixup\n\n* fixup\n\n* pretrained tokenizer fast test update\n\n* stash\n\n* bad merged\n\n* cherry pick more stuff that did not merge well\n\n* fix gptsw3\n\n* nit warn for now\n\n* update error raising\n\n* just ran fixup\n\n* bring back bert legacy\n\n* fix\n\n* nit\n\n* fix 56 errors on blenderbotsmall?\n\n* 18 for blenderbotsmall\n\n* tok auto\n\n* missed clip\n\n* fix tests\n\n* something missed\n\n* token healing\n\n* tok common tests update - nonmodel\n\n* try to fix non-model test in test_tokenization_utils\n\n* fix hub tests\n\n* try to fix hub tests\n\n* custom vocab related fixed\n\n* bert jap\n\n* BERT JAP\n\n* rename bert legacy to bert legacy\n\n* Wav2vec2\n\n* fix in tok python to update total vocab size - fixes speech t5\n\n* blender bot small\n\n* forgot test file\n\n* test failures\n\n* marian\n\n* gpt2 tiktoken\n\n* big bird / marian\n\n* udop\n\n* forgot couple changes\n\n* test_serve fix\n\n* missing import\n\n* a couple processors fixes\n\n* style partly\n\n* fix to fetch tests ci\n\n* Revert branch back to commit f5bc69ef state\n\n* revert branch to styling\n\n* update mistral after merge\n\n* fixes for non model tests\n\n* some processor test fixes\n\n* more processor test fixes\n\n* more processor fixes\n\n* hub tests\n\n* python tok utils\n\n* fix hub test\n\n* make style for now\n\n* remove problemattic fic copies\n\n* python utils/check_copies.py --fix_and_overwrite\n\n* more styling\n\n* fixup\n\n* silence docstirng\n\n* fix import?\n\n* fix imports\n\n* add the local test as well\n\n* throw spm error\n\n* llamas\n\n* fix a couple tests\n\n* broke ci\n\n* broke ci\n\n* broke ci\n\n* broke ci\n\n* add logs to debug gemma on ci\n\n* gemma and llama\n\n* gemma\n\n* revert las commit\n\n* gemma debug\n\n* gemma debug\n\n* gemma\n\n* safely import spiece backend\n\n* tok tests\n\n* check none\n\n* setup and qual\n\n* ruff\n\n* del dev files\n\n* tok auto\n\n* fill docstrings\n\n* update auto\n\n* blenderbot small nit\n\n* add migration guide\n\n* move mixtral patch to `TokenizersBackend`, move `TokenizerExtractor`\n\n* rename MistralCommonTokenizer to MistralCommonB ackend\n\n* nit\n\n* fix failures\n\n* fixup\n\n* remoove one old test\n\n* mark the slow one as slow\n\n* very small fixes\n\n* update auto mapping for missing ones\n\n* fixup lorsd\n\n* fixup doc and stuff\n\n* should be the final fixe\n\n* processing update\n\n* update\n\n* FIX or brute AI fix the llava test\n\n* style\n\n* slow?\n\n* fix is offline mode?\n\n* fix mt5\n\n* One tok utils (#42462)\n\n* consolidate python and utils tokenization files, they are copies\n\n* ruff and ref\n\n* Format\n\n* fix cohere\n\n* ?\n\n* up\n\n* am I dumbb?\n\n* grumble\n\n---------\n\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\n\n* [loading/saving] Reverse all loading operations when saving (#42396)\n\n* first shot\n\n* default to reversing\n\n* oupso\n\n* oupsi 2\n\n* oupsi 3\n\n* fix renamed kwargs\n\n* fix timm_wrapper\n\n* remove fix_state_dict methods\n\n* can do it all the time, with __init__ as well\n\n* doc\n\n* oupsi\n\n* fix\n\n* create helper\n\n* fix annotation annoying isue\n\n* small fix\n\n* small fixes\n\n* alright commit all that already\n\n* oupsi\n\n* the fix\n\n* update quantizers\n\n* this works\n\n* the hardcoded regex got me hard....\n\n* style\n\n* the final one\n\n* cleanup a bit\n\n* better\n\n* style\n\n* oupsi readded it\n\n* do it inside the ops instead - no need for full names anymore\n\n* reverse quantizers and simplify signatures\n\n* small thingy\n\n* add no_grad decorator\n\n* utils to rename keys\n\n* oupssii again\n\n* add test\n\n* simplify nicely\n\n* Fix T5 tests: use generation_config for generation parameters (#42419)\n\n* pass the generation parameters to generate()\n\n* fix use_task_specific_params to separate model.config and model.generation_config params\n\n* fix style\n\n* some fixes\n\n* remove redundant check\n\n* update expectation for llama_7b_bf16 on rocm\n\n* Update tests/models/llama/test_modeling_llama.py\n\nCo-authored-by: Rémi Ouazan <83456801+remi-or@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Rémi Ouazan <83456801+remi-or@users.noreply.github.com>\n\n* linting\n\n* more fix to pass the CI tests\n\n* fix lfm2 moe\n\n* fix docstring\n\n* fix docstring\n\n* fix qwen like model\n\n* fix flex olmo\n\n* revert lfm2 moe config\n\n* make fixup\n\n* fix docstring\n\n* fix conversion mapping\n\n* fix inference of gpt-oss\n\n* add some fixes to gpt-oss (but still not good)\n\n* fix modular\n\n* we need errors I think\n\n* fix config issue\n\n* this was fixed\n\n---------\n\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: BADAOUI Abdennacer <106801897+Abdennacer-Badaoui@users.noreply.github.com>\nCo-authored-by: Rémi Ouazan <83456801+remi-or@users.noreply.github.com>",
    "sha": "6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
    "files": [
        {
            "sha": "5968bd08d406339c264a16f78a83ce37f901b82d",
            "filename": "src/transformers/conversion_mapping.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fconversion_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fconversion_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconversion_mapping.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -175,6 +175,8 @@ def _build_checkpoint_conversion_mapping():\n     mapping[\"qwen3_vl_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"hunyuan_v1_moe\"] = mapping[\"qwen2_moe\"].copy()\n     mapping[\"minimax\"] = mapping[\"mixtral\"].copy()\n+    mapping[\"flex_olmo\"] = mapping[\"qwen2_moe\"].copy()\n+    mapping[\"olmoe\"] = mapping[\"qwen2_moe\"].copy()\n \n     return mapping\n "
        },
        {
            "sha": "f7d1d12ef44e60193e9ec77065b7453f6c6996f9",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -61,22 +61,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "97fe6cf3e84310430f7399d4f1de12fb0abe7d37",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -169,22 +169,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "15ec41de9f1ccc0c3be384d8c66a31d896736123",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -327,22 +327,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "f08a71fcb733043cbcdd11caa7d3c90d853fe96c",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -313,22 +313,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -351,8 +350,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class FlexOlmoSparseMoeBlock(nn.Module):\n@@ -364,7 +363,7 @@ def __init__(self, config):\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n             batch_size, sequence_length, hidden_dim\n         )"
        },
        {
            "sha": "b07d649b51d01bab4124dd06dcd06809fb73a460",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -350,22 +350,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "11c80fc502a339c2d64ebbaa7ff8d0806689f98e",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -414,22 +414,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "fac4f8b5680ac19432b9a4eaf4b821e935e18c05",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -95,12 +95,11 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n         \"\"\"\n         batch_size = hidden_states.shape[0]\n         hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n-        num_experts = routing_weights.shape[1]\n         if hidden_states.device.type == \"cpu\" or self.training:\n             next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(\n-                    router_indices, num_classes=num_experts + 1\n+                    router_indices, num_classes=self.num_experts\n                 )  # masking is also a class\n                 expert_mask = expert_mask.permute(2, 1, 0)\n                 # we sum on the top_k and on the sequence length to get which experts\n@@ -110,10 +109,10 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 # expert_idx only have 1 element, so we can use scale for fast indexing\n                 expert_idx = expert_idx[0]\n                 # skip masking index\n-                if expert_idx == num_experts:\n+                if expert_idx == self.num_experts:\n                     continue\n                 with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx])\n+                    top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n                 current_state = hidden_states[token_idx]\n                 gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n                 gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n@@ -122,21 +121,29 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 glu = gate * torch.sigmoid(gate * self.alpha)\n                 gated_output = (up + 1) * glu\n                 out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n-                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n+                weighted_output = out * routing_weights[token_idx, top_k_pos, None]\n                 next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n             next_states = next_states.view(batch_size, -1, self.hidden_size)\n         else:\n-            hidden_states = hidden_states.repeat(num_experts, 1)\n-            hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n+            num_tokens = hidden_states.shape[0]\n+            hidden_states = hidden_states.repeat(self.num_experts, 1)\n+            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n             gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n             gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n             gate = gate.clamp(min=None, max=self.limit)\n             up = up.clamp(min=-self.limit, max=self.limit)\n             glu = gate * torch.sigmoid(gate * self.alpha)\n             next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n             next_states = next_states + self.down_proj_bias[..., None, :]\n-            next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n-            next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n+            next_states = next_states.view(self.num_experts, batch_size, -1, self.hidden_size)\n+\n+            full_routing_weights = torch.zeros(\n+                num_tokens, self.num_experts, device=routing_weights.device, dtype=routing_weights.dtype\n+            )\n+            full_routing_weights.scatter_(1, router_indices, routing_weights)\n+            full_routing_weights = full_routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1, 1)\n+\n+            next_states = next_states * full_routing_weights\n             next_states = next_states.sum(dim=0)\n         return next_states\n \n@@ -155,8 +162,8 @@ def forward(self, hidden_states):\n         router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n @use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\")\n@@ -167,7 +174,7 @@ def __init__(self, config):\n         self.experts = GptOssExperts(config)\n \n     def forward(self, hidden_states):\n-        router_scores, router_indices = self.router(hidden_states)\n+        _, router_scores, router_indices = self.router(hidden_states)\n         routed_out = self.experts(hidden_states, router_indices, router_scores)\n         return routed_out, router_scores\n "
        },
        {
            "sha": "9432ac28b0fb15713ea05d7375116ff9af5f6f89",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 19,
            "deletions": 12,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -93,12 +93,11 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n         \"\"\"\n         batch_size = hidden_states.shape[0]\n         hidden_states = hidden_states.reshape(-1, self.hidden_size)  # (num_tokens, hidden_size)\n-        num_experts = routing_weights.shape[1]\n         if hidden_states.device.type == \"cpu\" or self.training:\n             next_states = torch.zeros_like(hidden_states, dtype=hidden_states.dtype, device=hidden_states.device)\n             with torch.no_grad():\n                 expert_mask = torch.nn.functional.one_hot(\n-                    router_indices, num_classes=num_experts + 1\n+                    router_indices, num_classes=self.num_experts\n                 )  # masking is also a class\n                 expert_mask = expert_mask.permute(2, 1, 0)\n                 # we sum on the top_k and on the sequence length to get which experts\n@@ -108,10 +107,10 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 # expert_idx only have 1 element, so we can use scale for fast indexing\n                 expert_idx = expert_idx[0]\n                 # skip masking index\n-                if expert_idx == num_experts:\n+                if expert_idx == self.num_experts:\n                     continue\n                 with torch.no_grad():\n-                    _, token_idx = torch.where(expert_mask[expert_idx])\n+                    top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n                 current_state = hidden_states[token_idx]\n                 gate_up = current_state @ self.gate_up_proj[expert_idx] + self.gate_up_proj_bias[expert_idx]\n                 gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n@@ -120,21 +119,29 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 glu = gate * torch.sigmoid(gate * self.alpha)\n                 gated_output = (up + 1) * glu\n                 out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n-                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n+                weighted_output = out * routing_weights[token_idx, top_k_pos, None]\n                 next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n             next_states = next_states.view(batch_size, -1, self.hidden_size)\n         else:\n-            hidden_states = hidden_states.repeat(num_experts, 1)\n-            hidden_states = hidden_states.view(num_experts, -1, self.hidden_size)\n+            num_tokens = hidden_states.shape[0]\n+            hidden_states = hidden_states.repeat(self.num_experts, 1)\n+            hidden_states = hidden_states.view(self.num_experts, -1, self.hidden_size)\n             gate_up = torch.bmm(hidden_states, self.gate_up_proj) + self.gate_up_proj_bias[..., None, :]\n             gate, up = gate_up[..., ::2], gate_up[..., 1::2]\n             gate = gate.clamp(min=None, max=self.limit)\n             up = up.clamp(min=-self.limit, max=self.limit)\n             glu = gate * torch.sigmoid(gate * self.alpha)\n             next_states = torch.bmm(((up + 1) * glu), self.down_proj)\n             next_states = next_states + self.down_proj_bias[..., None, :]\n-            next_states = next_states.view(num_experts, batch_size, -1, self.hidden_size)\n-            next_states = next_states * routing_weights.transpose(0, 1).view(num_experts, batch_size, -1)[..., None]\n+            next_states = next_states.view(self.num_experts, batch_size, -1, self.hidden_size)\n+\n+            full_routing_weights = torch.zeros(\n+                num_tokens, self.num_experts, device=routing_weights.device, dtype=routing_weights.dtype\n+            )\n+            full_routing_weights.scatter_(1, router_indices, routing_weights)\n+            full_routing_weights = full_routing_weights.transpose(0, 1).view(self.num_experts, batch_size, -1, 1)\n+\n+            next_states = next_states * full_routing_weights\n             next_states = next_states.sum(dim=0)\n         return next_states\n \n@@ -153,8 +160,8 @@ def forward(self, hidden_states):\n         router_logits = F.linear(hidden_states, self.weight, self.bias)  # (seq_len, num_experts)\n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         router_top_value = torch.nn.functional.softmax(router_top_value, dim=1, dtype=router_top_value.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n @use_kernel_forward_from_hub(\"MegaBlocksMoeMLP\")\n@@ -165,7 +172,7 @@ def __init__(self, config):\n         self.experts = GptOssExperts(config)\n \n     def forward(self, hidden_states):\n-        router_scores, router_indices = self.router(hidden_states)\n+        _, router_scores, router_indices = self.router(hidden_states)\n         routed_out = self.experts(hidden_states, router_indices, router_scores)\n         return routed_out, router_scores\n "
        },
        {
            "sha": "0f34c960f639867af21b15688c2d15a8bd3f0343",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -263,22 +263,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "cfb9f6eb4ab648c395cd217256535e4862e540ca",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -613,22 +613,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "3429e15a28722c2c79a57585d77e25be634b409d",
            "filename": "src/transformers/models/lfm2_moe/configuration_lfm2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fconfiguration_lfm2_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -162,6 +162,7 @@ def __init__(\n         self.routed_scaling_factor = routed_scaling_factor\n         self.norm_topk_prob = norm_topk_prob\n         self.layer_types = layer_types\n+        self.initializer_range = initializer_range\n \n         self.rope_parameters = rope_parameters\n         tie_word_embeddings = kwargs.get(\"tie_embedding\", tie_word_embeddings)  # to fit original config keys"
        },
        {
            "sha": "a8eb75fbd5f761c66fe6f9ea062c5ca52100da66",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -469,8 +469,8 @@ def forward(self, hidden_states):\n         router_logits = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class MiniMaxExperts(nn.Module):\n@@ -492,22 +492,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -526,7 +525,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n         hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n         return hidden_states"
        },
        {
            "sha": "5d9ef4d28e06a3d5b5079ced28f5c30ad1f1c8a4",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -74,22 +74,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -109,8 +108,8 @@ def forward(self, hidden_states):\n         router_logits = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class MixtralSparseMoeBlock(nn.Module):\n@@ -126,7 +125,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n         hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n         return hidden_states"
        },
        {
            "sha": "1796070fe6b610e3c239cfa2be60bb6c8b15382f",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -153,22 +153,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -188,8 +187,8 @@ def forward(self, hidden_states):\n         router_logits = torch.nn.functional.softmax(router_logits.float(), dim=-1)\n         router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=-1)  # (seq_len, top_k)\n         router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class MixtralSparseMoeBlock(nn.Module):\n@@ -205,7 +204,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         if self.training and self.jitter_noise > 0:\n             hidden_states *= torch.empty_like(hidden_states).uniform_(1.0 - self.jitter_noise, 1.0 + self.jitter_noise)\n         hidden_states = hidden_states.view(-1, hidden_states.shape[-1])\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         hidden_states = self.experts(hidden_states, top_k_index, top_k_weights)\n         hidden_states = hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n         return hidden_states"
        },
        {
            "sha": "b63df27608c291ad802302cdc55c0ccdad27b5e5",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -317,22 +317,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -355,8 +354,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class OlmoeSparseMoeBlock(nn.Module):\n@@ -368,7 +367,7 @@ def __init__(self, config):\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n             batch_size, sequence_length, hidden_dim\n         )"
        },
        {
            "sha": "e9399fac1a129c2f1f38281c9b9a98b80888bd73",
            "filename": "src/transformers/models/olmoe/modular_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -134,7 +134,7 @@ def __init__(self, config):\n     def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, hidden_dim)\n-        top_k_weights, top_k_index = self.gate(hidden_states)\n+        _, top_k_weights, top_k_index = self.gate(hidden_states)\n         final_hidden_states = self.experts(hidden_states, top_k_index, top_k_weights).reshape(\n             batch_size, sequence_length, hidden_dim\n         )"
        },
        {
            "sha": "20f2af0d7b3e696ed1fb08ffc77286b4d2ce2f03",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -346,22 +346,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states"
        },
        {
            "sha": "be2bcd4553af52cca1b5c50cf960820c9b55bb59",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -311,22 +311,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -349,8 +348,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen2MoeSparseMoeBlock(nn.Module):\n@@ -365,7 +364,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n         shared_expert_output = self.shared_expert(hidden_states_reshaped)\n-        routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n         shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output"
        },
        {
            "sha": "aae1805eefe88cb065c2f6a931b3d3e6267116df",
            "filename": "src/transformers/models/qwen2_moe/modular_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodular_qwen2_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -106,8 +106,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen2MoeSparseMoeBlock(nn.Module):\n@@ -122,7 +122,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n         shared_expert_output = self.shared_expert(hidden_states_reshaped)\n-        routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n         shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output"
        },
        {
            "sha": "7de0f845814961692ec2471e350fc13ce1a96b2e",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -231,22 +231,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -269,20 +268,20 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen3MoeSparseMoeBlock(nn.Module):\n     def __init__(self, config: Qwen3MoeConfig):\n         super().__init__()\n         self.experts = Qwen3MoeExperts(config)\n-        self.router = Qwen3MoeTopKRouter(config)\n+        self.gate = Qwen3MoeTopKRouter(config)\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n-        routing_weights, selected_experts = self.router(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n@@ -369,7 +368,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Qwen3MoeTopKRouter, layer_name=\"mlp.router\", index=0),\n+        \"router_logits\": OutputRecorder(Qwen3MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Qwen3MoeDecoderLayer,\n         \"attentions\": Qwen3MoeAttention,\n     }"
        },
        {
            "sha": "17b3c42f6ccfb2795e2a44863e90f01a4743e9ec",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -67,12 +67,12 @@ class Qwen3MoeSparseMoeBlock(nn.Module):\n     def __init__(self, config: Qwen3MoeConfig):\n         super().__init__()\n         self.experts = Qwen3MoeExperts(config)\n-        self.router = Qwen3MoeTopKRouter(config)\n+        self.gate = Qwen3MoeTopKRouter(config)\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n-        routing_weights, selected_experts = self.router(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n@@ -87,7 +87,7 @@ class Qwen3MoeDecoderLayer(Qwen2MoeDecoderLayer):\n \n class Qwen3MoePreTrainedModel(MixtralPreTrainedModel):\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Qwen3MoeTopKRouter, layer_name=\"mlp.router\", index=0),\n+        \"router_logits\": OutputRecorder(Qwen3MoeTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Qwen3MoeDecoderLayer,\n         \"attentions\": Qwen3MoeAttention,\n     }"
        },
        {
            "sha": "cd560fb667cdba29074bc72341f1877bd087d705",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -840,22 +840,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -878,8 +877,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen3NextSparseMoeBlock(nn.Module):\n@@ -894,7 +893,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n         shared_expert_output = self.shared_expert(hidden_states_reshaped)\n-        routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n         shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output"
        },
        {
            "sha": "10718fd1492979a379446ac73b53d13d716e3116",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -1113,7 +1113,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n-        device = grid_thw.device\n+        device = self.pos_embed.weight.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -1338,22 +1338,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -1376,20 +1375,20 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen3OmniMoeThinkerTextSparseMoeBlock(nn.Module):\n     def __init__(self, config: Qwen3OmniMoeThinkerConfig):\n         super().__init__()\n         self.experts = Qwen3OmniMoeThinkerTextExperts(config)\n-        self.router = Qwen3OmniMoeThinkerTextTopKRouter(config)\n+        self.gate = Qwen3OmniMoeThinkerTextTopKRouter(config)\n \n     def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n-        routing_weights, selected_experts = self.router(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         final_hidden_states = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n         return final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n \n@@ -1599,7 +1598,7 @@ class Qwen3OmniMoeThinkerTextPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Qwen3OmniMoeThinkerTextTopKRouter, layer_name=\"mlp.router\", index=0),\n+        \"router_logits\": OutputRecorder(Qwen3OmniMoeThinkerTextTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Qwen3OmniMoeThinkerTextDecoderLayer,\n         \"attentions\": Qwen3OmniMoeThinkerTextAttention,\n     }\n@@ -2767,22 +2766,21 @@ def forward(\n         top_k_weights: torch.Tensor,\n     ) -> torch.Tensor:\n         final_hidden_states = torch.zeros_like(hidden_states)\n-        num_experts = top_k_weights.shape[1]\n         with torch.no_grad():\n-            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=num_experts + 1)\n+            expert_mask = torch.nn.functional.one_hot(top_k_index, num_classes=self.num_experts)\n             expert_mask = expert_mask.permute(2, 1, 0)\n             expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n \n         for expert_idx in expert_hit:\n             expert_idx = expert_idx[0]\n-            if expert_idx == num_experts:\n+            if expert_idx == self.num_experts:\n                 continue\n-            _, token_idx = torch.where(expert_mask[expert_idx])\n+            top_k_pos, token_idx = torch.where(expert_mask[expert_idx])\n             current_state = hidden_states[token_idx]\n             gate, up = nn.functional.linear(current_state, self.gate_up_proj[expert_idx]).chunk(2, dim=-1)\n             current_hidden_states = self.act_fn(gate) * up\n             current_hidden_states = nn.functional.linear(current_hidden_states, self.down_proj[expert_idx])\n-            current_hidden_states = current_hidden_states * top_k_weights[token_idx, expert_idx, None]\n+            current_hidden_states = current_hidden_states * top_k_weights[token_idx, top_k_pos, None]\n             final_hidden_states.index_add_(0, token_idx, current_hidden_states.to(final_hidden_states.dtype))\n \n         return final_hidden_states\n@@ -2805,8 +2803,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n class Qwen3OmniMoeTalkerTextSparseMoeBlock(nn.Module):\n@@ -2823,7 +2821,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         batch_size, sequence_length, hidden_dim = hidden_states.shape\n         hidden_states_reshaped = hidden_states.view(-1, hidden_dim)\n         shared_expert_output = self.shared_expert(hidden_states_reshaped)\n-        routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n+        _, routing_weights, selected_experts = self.gate(hidden_states_reshaped)\n         expert_output = self.experts(hidden_states_reshaped, selected_experts, routing_weights)\n \n         shared_expert_output = F.sigmoid(self.shared_expert_gate(hidden_states_reshaped)) * shared_expert_output"
        },
        {
            "sha": "86500d1e8e247421e57907855b8faa8c2fbf913f",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -673,7 +673,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n-        device = grid_thw.device\n+        device = self.pos_embed.weight.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]"
        },
        {
            "sha": "bd9368a0c443032aa3a633d2be6f1f9949f1df54",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -569,7 +569,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n-        device = grid_thw.device\n+        device = self.pos_embed.weight.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]"
        },
        {
            "sha": "be87e08c8a248612e09469e37cc74076c9f4e9cf",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -385,8 +385,8 @@ def forward(self, hidden_states):\n         if self.norm_topk_prob:\n             router_top_value /= router_top_value.sum(dim=-1, keepdim=True)\n         router_top_value = router_top_value.to(router_logits.dtype)\n-        router_scores = torch.zeros_like(router_logits).scatter_(1, router_indices, router_top_value)\n-        return router_scores, router_indices\n+        router_scores = router_top_value\n+        return router_logits, router_scores, router_indices\n \n \n @auto_docstring\n@@ -402,7 +402,7 @@ class Qwen3VLMoePreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n     _supports_attention_backend = True\n     _can_record_outputs = {\n-        \"router_logits\": OutputRecorder(Qwen3VLMoeTextTopKRouter, layer_name=\"mlp.router\", index=0),\n+        \"router_logits\": OutputRecorder(Qwen3VLMoeTextTopKRouter, layer_name=\"mlp.gate\", index=0),\n         \"hidden_states\": Qwen3VLMoeTextDecoderLayer,\n         \"attentions\": Qwen3VLMoeTextAttention,\n     }\n@@ -687,7 +687,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n-        device = grid_thw.device\n+        device = self.pos_embed.weight.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]"
        },
        {
            "sha": "26bd2aa51978bca14c75e00129f82272924aa1b4",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -202,14 +202,21 @@ def tearDown(self):\n     # Non-distributed inference\n     # ------------------------\n     @staticmethod\n-    def load_and_forward(model_id, attn_implementation, input_text, **pretrained_kwargs):\n+    def load_and_forward(model_id, attn_implementation, input_text, mode=\"eval\", **pretrained_kwargs):\n         model = AutoModelForCausalLM.from_pretrained(\n             model_id,\n             dtype=torch.bfloat16,\n             device_map=\"auto\",\n             attn_implementation=attn_implementation,\n             **pretrained_kwargs,\n         )\n+\n+        # Set the correct mode\n+        if mode == \"train\":\n+            model.train()\n+        else:\n+            model.eval()\n+\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n \n         inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(model.device)\n@@ -308,6 +315,7 @@ def test_model_outputs(self, quantized, model, kernels, attn_impl, mode):\n             model_id,\n             attn_impl,\n             self.input_text,\n+            mode=mode,\n             use_kernels=kernels,\n         )\n "
        },
        {
            "sha": "8712690e5d173527fdf0e51d1d751428c8bd8d8b",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -205,21 +205,21 @@ class OlmoeIntegrationTest(unittest.TestCase):\n     def test_model_7b_logits(self):\n         input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n         model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", device_map=\"auto\")\n-        out = model(torch.tensor(input_ids)).logits.float()\n+        out = model(torch.tensor(input_ids, device=model.device)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-1.3814, -3.4450, -2.2990, -1.9542, -2.4387, -2.7941, -2.9312, -2.8309]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(out.mean(-1).cpu(), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-2.3874, -2.4076, -2.4995, 4.2278, 1.4004, -0.0252, 0.4189, -2.7560, 0.3531, 1.6678, -0.7941, -1.1818, -0.2920, 0.7131, -1.4173, 1.6723, 0.5406, 0.1345, -0.1800, 0.2304, 1.2791, 0.7489, 0.6341, -0.0151, -1.3693, -1.2532, -2.3921, 0.7376, 1.6876, 0.5483])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30].cpu(), EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_greedy_generation(self):\n         EXPECTED_TEXT_COMPLETION = \"\"\"Simply put, the theory of relativity states that \\nthe speed of light is the same for all observers, no matter \\nhow fast they are moving.  This is a very counter-intuitive \\nconcept, and it took Einstein a long time to come up with \\nthe theory.  The theory of relativity is based on two \\npostulates\"\"\"\n         prompt = \"Simply put, the theory of relativity states that \"\n         tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", device_map=\"auto\")\n-        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n         model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\", device_map=\"auto\")\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n \n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)"
        },
        {
            "sha": "0689fc90c775e61ba91969b16c5c6f15c18fb718",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6316a9e176d22f8f09d44ef72ec0aaa2ce7b8780",
            "patch": "@@ -2423,7 +2423,9 @@ def test_disk_offload_safetensors(self):\n                 max_memory = {0: max_size, \"cpu\": max_size}\n \n                 # This doesn't error out as it's in safetensors and doesn't need an offload folder\n-                new_model = model_class.from_pretrained(tmp_dir, device_map=\"auto\", max_memory=max_memory)\n+                new_model = model_class.from_pretrained(\n+                    tmp_dir, device_map=\"auto\", max_memory=max_memory, offload_folder=tmp_dir\n+                )\n \n                 self.check_device_map_is_respected(new_model, new_model.hf_device_map)\n                 torch.manual_seed(0)"
        }
    ],
    "stats": {
        "total": 345,
        "additions": 177,
        "deletions": 168
    }
}