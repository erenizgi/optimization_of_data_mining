{
    "author": "vasqu",
    "message": "[`Gemma Embedding`] Fix SWA (#40700)\n\n* fix gemma embedding flash attention\n\n* fix sdpa\n\n* fix atttempt number 2\n\n* alternative gemma fix\n\n* fix modular",
    "sha": "948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37",
    "files": [
        {
            "sha": "1f87edfc694e62a5f966c393a51ae1ecb7f735f5",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37",
            "patch": "@@ -226,6 +226,8 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        if use_bidirectional_attention:\n+            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n \n         self.rope_local_base_freq = rope_local_base_freq\n         self.rope_scaling = rope_scaling"
        },
        {
            "sha": "a7a805ebff50f04ccf565549a2d3a054da013743",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37",
            "patch": "@@ -279,7 +279,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.attention_dropout = self.config.attention_dropout\n-        self.is_causal = True\n+        self.is_causal = not self.config.use_bidirectional_attention\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -450,8 +450,8 @@ def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, in\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         \"\"\"A token can attend to any other token if their absolute distance is within\n-        half the sliding window size (distance <= sliding_window // 2).\"\"\"\n-        return abs(q_idx - kv_idx) <= sliding_window // 2\n+        the (exclusive) sliding window size (distance < sliding_window).\"\"\"\n+        return abs(q_idx - kv_idx) < sliding_window\n \n     return inner_mask\n "
        },
        {
            "sha": "2620d00c359325e7d85f90956beabb27b175ebef",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37",
            "patch": "@@ -237,6 +237,8 @@ def __init__(\n         self.attn_logit_softcapping = attn_logit_softcapping\n         self.layer_types = layer_types\n         self.use_bidirectional_attention = use_bidirectional_attention\n+        if use_bidirectional_attention:\n+            self.sliding_window = (self.sliding_window // 2) + 1  # due to fa we set exclusive bounds\n \n         self.rope_local_base_freq = rope_local_base_freq\n         self.rope_scaling = rope_scaling\n@@ -402,6 +404,7 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n \n         super().__init__(config, layer_idx)\n         self.sliding_window = config.sliding_window if self.is_sliding else None\n+        self.is_causal = not self.config.use_bidirectional_attention\n \n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n@@ -546,8 +549,8 @@ def _bidirectional_window_overlay(sliding_window: int) -> Callable[[int, int, in\n \n     def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n         \"\"\"A token can attend to any other token if their absolute distance is within\n-        half the sliding window size (distance <= sliding_window // 2).\"\"\"\n-        return abs(q_idx - kv_idx) <= sliding_window // 2\n+        the (exclusive) sliding window size (distance < sliding_window).\"\"\"\n+        return abs(q_idx - kv_idx) < sliding_window\n \n     return inner_mask\n "
        },
        {
            "sha": "619c295250fb1b6a608fbd4ba2d31fbc92dc00ec",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=948bc0fa34f56bc70477fcd35d7dc0d1cee3cf37",
            "patch": "@@ -1744,6 +1744,7 @@ def apply_rotary_pos_emb(\n class Gemma3nTextAttention(Gemma3Attention):\n     def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n+        self.is_causal = True\n         del self.attn_logit_softcapping\n         del self.scaling\n         self.v_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps, with_scale=False)"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 11,
        "deletions": 5
    }
}