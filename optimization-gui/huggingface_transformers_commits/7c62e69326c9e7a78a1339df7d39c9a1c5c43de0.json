{
    "author": "poedator",
    "message": "`GPT2Model` StaticCache support (#35761)\n\n* initial GPT2 changes\n\n* causal_mask support\n\n* return_legacy_cache\n\n* cleanup\n\n* fix1\n\n* outputs shape fixes\n\n* gpt2 return fix\n\n* pkv, attn fixes\n\n* fix dual_head\n\n* is_causal arg fix\n\n* decision transformer updated\n\n* style fix\n\n* batch_size from inputs_embeds\n\n* DecisionTransformerModel fixes\n\n* cross-attn support + cache warning\n\n* x-attn @decision\n\n* EDCache proper init\n\n* simplified logic in `if use_cache:` for GPT2Model\n\n* @deprecate_kwarg for DecisionTr attn fwd\n\n* @deprecate_kwarg in gpt2\n\n* deprecation version updated to 4.51\n\n* kwargs in gradient_checkpointing_fn\n\n* rename next_cache to past_key_values\n\n* attention_mask prep\n\n* +cache_position in GPT2DoubleHeadsModel\n\n* undo kwargs in gradient checkpointing\n\n* moved up `if self.gradient_checkpointing`\n\n* consistency in decision_transformer\n\n* pastkv, cache_pos in grad_checkpt args\n\n* rm _reorder_cache\n\n* output_attentions streamlined\n\n* decision_transformer consistency\n\n* return_legacy_cache improved\n\n* ClvpForCausalLM used for legacy cache test now\n\n* is_causal fixed\n\n* attn_output cleanup\n\n* consistency @ decision_transformer\n\n* Updated deprecation notice version to 4.52\n\n* upd deprecation\n\n* consistent legacy cache code in decision transformers\\\n\n* next_cache -> past_kv in decision_tr\n\n* cache support flags in decision_transf\n\n* rm legacy cache warning\n\n* consistency in cache init for decision transf\n\n* no Static Cache for Decision Transformer\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "7c62e69326c9e7a78a1339df7d39c9a1c5c43de0",
    "files": [
        {
            "sha": "a63e0df5122b3bdf2287b8caa29a00ee1d21dc05",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=7c62e69326c9e7a78a1339df7d39c9a1c5c43de0",
            "patch": "@@ -1589,7 +1589,6 @@ def forward(\n         )\n \n     @staticmethod\n-    # Copied from transformers.models.gpt2.modeling_gpt2.GPT2LMHeadModel._reorder_cache\n     def _reorder_cache(\n         past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n     ) -> Tuple[Tuple[torch.Tensor]]:"
        },
        {
            "sha": "54000b8f2438490f26aa6faf3a6ab006bd77d6a9",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 77,
            "deletions": 52,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=7c62e69326c9e7a78a1339df7d39c9a1c5c43de0",
            "patch": "@@ -24,6 +24,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import Conv1D, find_pruneable_heads_and_indices, prune_conv1d_layer\n@@ -34,6 +35,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n@@ -125,7 +127,8 @@ def eager_attention_forward(module, query, key, value, attention_mask, head_mask\n \n     if attention_mask is not None:\n         # Apply the attention mask\n-        attn_weights = attn_weights + attention_mask\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n@@ -257,19 +260,21 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n+    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n-        if encoder_hidden_states is not None:\n+        is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n@@ -289,17 +294,17 @@ def forward(\n         key_states = key_states.view(shape_kv).transpose(1, 2)\n         value_states = value_states.view(shape_kv).transpose(1, 2)\n \n-        if layer_past is not None:\n-            past_key, past_value = layer_past\n-            key_states = torch.cat((past_key, key_states), dim=-2)\n-            value_states = torch.cat((past_value, value_states), dim=-2)\n-\n-        if use_cache is True:\n-            present = (key_states, value_states)\n-        else:\n-            present = None\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                if is_cross_attention:\n+                    past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    past_key_value = past_key_value.self_attention_cache\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs\n+            )\n \n-        is_cross_attention = encoder_hidden_states is not None\n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n         using_eager = self.config._attn_implementation == \"eager\"\n@@ -338,11 +343,7 @@ def forward(\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.gpt2.modeling_gpt2.GPT2MLP with GPT2->DecisionTransformerGPT2\n@@ -383,10 +384,12 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = DecisionTransformerGPT2MLP(inner_dim, config)\n \n+    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n@@ -396,16 +399,15 @@ def forward(\n     ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n-        attn_outputs = self.attn(\n+        attn_output, self_attn_weights = self.attn(\n             hidden_states,\n-            layer_past=layer_past,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attn_outputs[1:]\n         # residual connection\n         hidden_states = attn_output + residual\n \n@@ -418,31 +420,31 @@ def forward(\n                 )\n             residual = hidden_states\n             hidden_states = self.ln_cross_attn(hidden_states)\n-            cross_attn_outputs = self.crossattention(\n+            cross_attn_output, cross_attn_weights = self.crossattention(\n                 hidden_states,\n+                past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n             )\n-            attn_output = cross_attn_outputs[0]\n             # residual connection\n-            hidden_states = residual + attn_output\n-            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n+            hidden_states = residual + cross_attn_output\n \n         residual = hidden_states\n         hidden_states = self.ln_2(hidden_states)\n         feed_forward_hidden_states = self.mlp(hidden_states)\n         # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n \n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+            if encoder_hidden_states is not None:\n+                outputs += (cross_attn_weights,)\n \n-        return outputs  # hidden_states, present, (attentions, cross_attentions)\n+        return outputs\n \n \n class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n@@ -456,6 +458,8 @@ class DecisionTransformerGPT2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     is_parallelizable = True\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = False\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n@@ -521,6 +525,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -558,14 +563,31 @@ def forward(\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.h))\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n+        # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder and similar addition in GPT2Model\n+        return_legacy_cache = False\n+        if use_cache:\n+            if past_key_values is None:\n+                return_legacy_cache = True\n+                past_key_values = DynamicCache()\n+            elif not isinstance(past_key_values, Cache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n+                    \"You should pass an instance of `Cache` instead, e.g. \"\n+                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+            if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n         if position_ids is None:\n-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n-            position_ids = position_ids.unsqueeze(0)\n+            position_ids = cache_position.unsqueeze(0)\n \n         # Attention mask.\n         if attention_mask is not None:\n@@ -624,17 +646,13 @@ def forward(\n                 )\n                 use_cache = False\n \n-        presents = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i, (block, layer_past) in enumerate(zip(self.h, past_key_values)):\n+        for i, block in enumerate(self.h):\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n-                # Ensure layer_past is on same device as hidden_states (might not be correct)\n-                if layer_past is not None:\n-                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n                 # Ensure that attention_mask is always on the same device as hidden_states\n                 if attention_mask is not None:\n                     attention_mask = attention_mask.to(hidden_states.device)\n@@ -648,6 +666,7 @@ def forward(\n                     block.__call__,\n                     hidden_states,\n                     None,\n+                    None,\n                     attention_mask,\n                     head_mask[i],\n                     encoder_hidden_states,\n@@ -658,7 +677,8 @@ def forward(\n             else:\n                 outputs = block(\n                     hidden_states,\n-                    layer_past=layer_past,\n+                    past_key_value=past_key_values,\n+                    cache_position=cache_position,\n                     attention_mask=attention_mask,\n                     head_mask=head_mask[i],\n                     encoder_hidden_states=encoder_hidden_states,\n@@ -668,13 +688,11 @@ def forward(\n                 )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                presents = presents + (outputs[1],)\n \n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n                 if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n+                    all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -689,16 +707,23 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        past_key_values = past_key_values if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = (\n+                past_key_values.self_attention_cache.to_legacy_cache()\n+                if self.config.add_cross_attention\n+                else past_key_values.to_legacy_cache()\n+            )\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions, all_cross_attentions]\n                 if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,"
        },
        {
            "sha": "6aec582b7c6cb901c9aedf13ba9ac4c064a65793",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 245,
            "deletions": 111,
            "changes": 356,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=7c62e69326c9e7a78a1339df7d39c9a1c5c43de0",
            "patch": "@@ -27,8 +27,9 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -46,6 +47,7 @@\n     logging,\n     replace_return_docstrings,\n )\n+from ...utils.deprecation import deprecate_kwarg\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gpt2 import GPT2Config\n \n@@ -136,7 +138,8 @@ def eager_attention_forward(module, query, key, value, attention_mask, head_mask\n \n     if attention_mask is not None:\n         # Apply the attention mask\n-        attn_weights = attn_weights + attention_mask\n+        causal_mask = attention_mask[:, :, :, : key.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n \n     attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n \n@@ -267,19 +270,21 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n \n         return attn_output, attn_weights\n \n+    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> Tuple[Union[torch.Tensor, Tuple[torch.Tensor]], ...]:\n-        if encoder_hidden_states is not None:\n+        is_cross_attention = encoder_hidden_states is not None\n+        if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n@@ -299,17 +304,17 @@ def forward(\n         key_states = key_states.view(shape_kv).transpose(1, 2)\n         value_states = value_states.view(shape_kv).transpose(1, 2)\n \n-        if layer_past is not None:\n-            past_key, past_value = layer_past\n-            key_states = torch.cat((past_key, key_states), dim=-2)\n-            value_states = torch.cat((past_value, value_states), dim=-2)\n-\n-        if use_cache is True:\n-            present = (key_states, value_states)\n-        else:\n-            present = None\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                if is_cross_attention:\n+                    past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    past_key_value = past_key_value.self_attention_cache\n+            cache_kwargs = {\"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(\n+                key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs\n+            )\n \n-        is_cross_attention = encoder_hidden_states is not None\n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n         using_eager = self.config._attn_implementation == \"eager\"\n@@ -348,11 +353,7 @@ def forward(\n         attn_output = self.c_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, present)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n+        return attn_output, attn_weights\n \n \n class GPT2MLP(nn.Module):\n@@ -388,10 +389,12 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = GPT2MLP(inner_dim, config)\n \n+    @deprecate_kwarg(\"layer_past\", new_name=\"past_key_value\", version=\"4.53.0\", raise_if_both_names=True)\n     def forward(\n         self,\n         hidden_states: Optional[Tuple[torch.FloatTensor]],\n-        layer_past: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.Tensor] = None,\n@@ -401,16 +404,15 @@ def forward(\n     ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n-        attn_outputs = self.attn(\n+        attn_output, self_attn_weights = self.attn(\n             hidden_states,\n-            layer_past=layer_past,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attn_outputs[1:]\n         # residual connection\n         hidden_states = attn_output + residual\n \n@@ -423,31 +425,31 @@ def forward(\n                 )\n             residual = hidden_states\n             hidden_states = self.ln_cross_attn(hidden_states)\n-            cross_attn_outputs = self.crossattention(\n+            cross_attn_output, cross_attn_weights = self.crossattention(\n                 hidden_states,\n+                past_key_value=past_key_value,\n                 attention_mask=attention_mask,\n                 head_mask=head_mask,\n                 encoder_hidden_states=encoder_hidden_states,\n                 encoder_attention_mask=encoder_attention_mask,\n                 output_attentions=output_attentions,\n             )\n-            attn_output = cross_attn_outputs[0]\n             # residual connection\n-            hidden_states = residual + attn_output\n-            outputs = outputs + cross_attn_outputs[2:]  # add cross attentions if we output attention weights\n+            hidden_states = residual + cross_attn_output\n \n         residual = hidden_states\n         hidden_states = self.ln_2(hidden_states)\n         feed_forward_hidden_states = self.mlp(hidden_states)\n         # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n \n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+            if encoder_hidden_states is not None:\n+                outputs += (cross_attn_weights,)\n \n-        return outputs  # hidden_states, present, (attentions, cross_attentions)\n+        return outputs\n \n \n # Copied from transformers.models.xlm.modeling_xlm.XLMSequenceSummary with XLM->GPT2\n@@ -565,6 +567,8 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n \n     def __init__(self, *inputs, **kwargs):\n         super().__init__(*inputs, **kwargs)\n@@ -669,10 +673,24 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        past_key_values (`Tuple[Tuple[torch.Tensor]]` of length `config.n_layers`):\n-            Contains precomputed hidden-states (key and values in the attention blocks) as computed by the model (see\n-            `past_key_values` output below). Can be used to speed up sequential decoding. The `input_ids` which have\n-            their past given to this model should not be passed as `input_ids` as they have already been computed.\n+        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n+            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+            Two formats are allowed:\n+            - a [`~cache_utils.Cache`] instance, see our\n+            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n+            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n+            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n+            cache format.\n+\n+            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n+            legacy cache format will be returned.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n+            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n+            of shape `(batch_size, sequence_length)`.\n         attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -721,6 +739,10 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n PARALLELIZE_DOCSTRING = r\"\"\"\n     This is an experimental feature and is a subject to change at a moment's notice.\n@@ -868,7 +890,8 @@ def _prune_heads(self, heads_to_prune):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Union[Tuple[Tuple[torch.Tensor]], Cache]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -906,51 +929,56 @@ def forward(\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.view(-1, input_shape[-1])\n \n-        if past_key_values is None:\n-            past_length = 0\n-            past_key_values = tuple([None] * len(self.h))\n-        else:\n-            past_length = past_key_values[0][0].size(-2)\n-        if position_ids is None:\n-            position_ids = torch.arange(past_length, input_shape[-1] + past_length, dtype=torch.long, device=device)\n-            position_ids = position_ids.unsqueeze(0)\n+        if self.gradient_checkpointing and self.training:\n+            if use_cache:\n+                logger.warning_once(\n+                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n+                )\n+                use_cache = False\n+\n+        # based on pattern from src/transformers/models/whisper/modeling_whisper.py::WhisperDecoder\n+        return_legacy_cache = False\n+        if use_cache:\n+            if past_key_values is None:\n+                return_legacy_cache = True\n+                past_key_values = DynamicCache()\n+            elif not isinstance(past_key_values, Cache):\n+                return_legacy_cache = True\n+                logger.warning_once(\n+                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n+                    \"You should pass an instance of `Cache` instead, e.g. \"\n+                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n+                )\n+                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+            if self.config.add_cross_attention and not isinstance(past_key_values, EncoderDecoderCache):\n+                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n \n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n         position_embeds = self.wpe(position_ids)\n         hidden_states = inputs_embeds + position_embeds.to(inputs_embeds.device)\n \n         # Attention mask.\n-        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n-        attention_mask = attention_mask.view(batch_size, -1) if attention_mask is not None else None\n-        if self._attn_implementation == \"flash_attention_2\":\n-            attention_mask = attention_mask if (attention_mask is not None and 0 in attention_mask) else None\n-        elif _use_sdpa:\n-            attention_mask = _prepare_4d_causal_attention_mask_for_sdpa(\n-                attention_mask=attention_mask,\n-                input_shape=(batch_size, input_shape[-1]),\n-                inputs_embeds=inputs_embeds,\n-                past_key_values_length=past_length,\n-            )\n-        else:\n-            if attention_mask is not None:\n-                # We create a 3D attention mask from a 2D tensor mask.\n-                # Sizes are [batch_size, 1, 1, to_seq_length]\n-                # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n-                # this attention mask is more simple than the triangular masking of causal attention\n-                # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n-                attention_mask = attention_mask[:, None, None, :]\n-\n-                # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n-                # masked positions, this operation will create a tensor which is 0.0 for\n-                # positions we want to attend and the dtype's smallest value for masked positions.\n-                # Since we are adding it to the raw scores before the softmax, this is\n-                # effectively the same as removing these entirely.\n-                attention_mask = attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n-                attention_mask = (1.0 - attention_mask) * torch.finfo(self.dtype).min\n+        # ._update_causal_mask() and ._prepare_4d_causal_attention_mask_with_cache_position() copied from LlamaModel\n+        if attention_mask is not None and attention_mask.ndim < 4:\n+            attention_mask = attention_mask.view(batch_size, -1)\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        )\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n         # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n+        _use_sdpa = self._attn_implementation == \"sdpa\" and output_attentions is False and head_mask is None\n         if self.config.add_cross_attention and encoder_hidden_states is not None:\n             encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n             encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n@@ -979,25 +1007,13 @@ def forward(\n \n         output_shape = (-1,) + input_shape[1:] + (hidden_states.size(-1),)\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        presents = () if use_cache else None\n         all_self_attentions = () if output_attentions else None\n         all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n         all_hidden_states = () if output_hidden_states else None\n-        for i in range(len(self.h)):\n-            block, layer_past = self.h[i], past_key_values[i]\n+        for i, block in enumerate(self.h):\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n-                # Ensure layer_past is on same device as hidden_states (might not be correct)\n-                if layer_past is not None:\n-                    layer_past = tuple(past_state.to(hidden_states.device) for past_state in layer_past)\n                 # Ensure that attention_mask is always on the same device as hidden_states\n                 if attention_mask is not None:\n                     attention_mask = attention_mask.to(hidden_states.device)\n@@ -1010,8 +1026,9 @@ def forward(\n                 outputs = self._gradient_checkpointing_func(\n                     block.__call__,\n                     hidden_states,\n-                    None,\n-                    attention_mask,\n+                    past_key_values,\n+                    cache_position,\n+                    causal_mask,\n                     head_mask[i],\n                     encoder_hidden_states,\n                     encoder_attention_mask,\n@@ -1021,8 +1038,9 @@ def forward(\n             else:\n                 outputs = block(\n                     hidden_states,\n-                    layer_past=layer_past,\n-                    attention_mask=attention_mask,\n+                    past_key_value=past_key_values,\n+                    cache_position=cache_position,\n+                    attention_mask=causal_mask,\n                     head_mask=head_mask[i],\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n@@ -1031,13 +1049,11 @@ def forward(\n                 )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                presents = presents + (outputs[1],)\n \n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n                 if self.config.add_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (outputs[3 if use_cache else 2],)\n+                    all_cross_attentions = all_cross_attentions + (outputs[2],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -1052,21 +1068,149 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n+        past_key_values = past_key_values if use_cache else None\n+        if return_legacy_cache:\n+            past_key_values = (\n+                past_key_values.self_attention_cache.to_legacy_cache()\n+                if self.config.add_cross_attention\n+                else past_key_values.to_legacy_cache()\n+            )\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, presents, all_hidden_states, all_self_attentions, all_cross_attentions]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions, all_cross_attentions]\n                 if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=presents,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n             cross_attentions=all_cross_attentions,\n         )\n \n+    def _update_causal_mask(\n+        self,\n+        attention_mask: torch.Tensor,\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype, device = input_tensor.dtype, input_tensor.device\n+        sequence_length = input_tensor.shape[1]\n+        if using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            device=device,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type == \"cuda\"\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        device: torch.device,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            device (`torch.device`):\n+                The device to plcae the 4D attention mask on.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n @add_start_docstrings(\n     \"\"\"\n@@ -1137,6 +1281,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1163,6 +1308,7 @@ def forward(\n             input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,\n+            cache_position=cache_position,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,\n             head_mask=head_mask,\n@@ -1206,20 +1352,6 @@ def forward(\n             cross_attentions=transformer_outputs.cross_attentions,\n         )\n \n-    @staticmethod\n-    def _reorder_cache(\n-        past_key_values: Tuple[Tuple[torch.Tensor]], beam_idx: torch.Tensor\n-    ) -> Tuple[Tuple[torch.Tensor]]:\n-        \"\"\"\n-        This function is used to re-order the `past_key_values` cache if [`~PreTrainedModel.beam_search`] or\n-        [`~PreTrainedModel.beam_sample`] is called. This is required to match `past_key_values` with the correct\n-        beam_idx at every generation step.\n-        \"\"\"\n-        return tuple(\n-            tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past)\n-            for layer_past in past_key_values\n-        )\n-\n \n @add_start_docstrings(\n     \"\"\"\n@@ -1292,6 +1424,7 @@ def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1350,6 +1483,7 @@ def forward(\n         transformer_outputs = self.transformer(\n             input_ids,\n             past_key_values=past_key_values,\n+            cache_position=cache_position,\n             attention_mask=attention_mask,\n             token_type_ids=token_type_ids,\n             position_ids=position_ids,"
        },
        {
            "sha": "96c757fd8f9b6ae68394cc9fc76d789a4098bf0f",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7c62e69326c9e7a78a1339df7d39c9a1c5c43de0/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=7c62e69326c9e7a78a1339df7d39c9a1c5c43de0",
            "patch": "@@ -40,9 +40,9 @@\n     from transformers import (\n         AutoModelForCausalLM,\n         AutoTokenizer,\n+        ClvpForCausalLM,\n         DynamicCache,\n         GenerationConfig,\n-        GPT2LMHeadModel,\n         LlamaConfig,\n         SinkCache,\n         StaticCache,\n@@ -103,7 +103,7 @@ def test_dynamic_cache_retrocompatibility(self):\n \n     def test_reorder_cache_retrocompatibility(self):\n         \"\"\"Tests that Cache.reorder_cache is retrocompatible with the legacy code path\"\"\"\n-        legacy_reorder_fn = GPT2LMHeadModel._reorder_cache  # An example of a legacy `_reorder_cache` function\n+        legacy_reorder_fn = ClvpForCausalLM._reorder_cache  # An example of a legacy `_reorder_cache` function\n \n         legacy_cache = ()\n         new_cache = DynamicCache()"
        }
    ],
    "stats": {
        "total": 490,
        "additions": 324,
        "deletions": 166
    }
}