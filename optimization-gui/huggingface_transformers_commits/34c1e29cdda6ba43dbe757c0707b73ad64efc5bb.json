{
    "author": "yao-matrix",
    "message": "enable autoround cases on XPU (#38167)\n\n* enable autoround cases on XPU\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>",
    "sha": "34c1e29cdda6ba43dbe757c0707b73ad64efc5bb",
    "files": [
        {
            "sha": "7837278f6f3d3c6ea5b6b5814487babb47320431",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/34c1e29cdda6ba43dbe757c0707b73ad64efc5bb/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34c1e29cdda6ba43dbe757c0707b73ad64efc5bb/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=34c1e29cdda6ba43dbe757c0707b73ad64efc5bb",
            "patch": "@@ -3013,6 +3013,11 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n         \"cpu\": 0,\n         \"default\": 0,\n     }\n+    BACKEND_SYNCHRONIZE = {\n+        \"cuda\": torch.cuda.synchronize,\n+        \"cpu\": None,\n+        \"default\": None,\n+    }\n     BACKEND_TORCH_ACCELERATOR_MODULE = {\n         \"cuda\": torch.cuda,\n         \"cpu\": None,\n@@ -3025,6 +3030,7 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED = {\"default\": None}\n     BACKEND_MAX_MEMORY_ALLOCATED = {\"default\": 0}\n     BACKEND_MEMORY_ALLOCATED = {\"default\": 0}\n+    BACKEND_SYNCHRONIZE = {\"default\": None}\n     BACKEND_TORCH_ACCELERATOR_MODULE = {\"default\": None}\n \n \n@@ -3052,6 +3058,7 @@ def _device_agnostic_dispatch(device: str, dispatch_table: dict[str, Callable],\n     BACKEND_RESET_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.reset_peak_memory_stats\n     BACKEND_MAX_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.max_memory_allocated\n     BACKEND_MEMORY_ALLOCATED[\"xpu\"] = torch.xpu.memory_allocated\n+    BACKEND_SYNCHRONIZE[\"xpu\"] = torch.xpu.synchronize\n     BACKEND_TORCH_ACCELERATOR_MODULE[\"xpu\"] = torch.xpu\n \n \n@@ -3085,6 +3092,10 @@ def backend_memory_allocated(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_MEMORY_ALLOCATED)\n \n \n+def backend_synchronize(device: str):\n+    return _device_agnostic_dispatch(device, BACKEND_SYNCHRONIZE)\n+\n+\n def backend_torch_accelerator_module(device: str):\n     return _device_agnostic_dispatch(device, BACKEND_TORCH_ACCELERATOR_MODULE)\n "
        },
        {
            "sha": "abdf6be5d601aa648431d6185713b18fe6a4da88",
            "filename": "tests/quantization/autoround/test_auto_round.py",
            "status": "modified",
            "additions": 20,
            "deletions": 10,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/34c1e29cdda6ba43dbe757c0707b73ad64efc5bb/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34c1e29cdda6ba43dbe757c0707b73ad64efc5bb/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoround%2Ftest_auto_round.py?ref=34c1e29cdda6ba43dbe757c0707b73ad64efc5bb",
            "patch": "@@ -17,11 +17,14 @@\n \n from transformers import AutoModelForCausalLM, AutoRoundConfig, AutoTokenizer\n from transformers.testing_utils import (\n+    backend_empty_cache,\n+    backend_synchronize,\n     require_accelerate,\n     require_auto_round,\n     require_intel_extension_for_pytorch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n@@ -33,7 +36,7 @@\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n @require_auto_round\n @require_accelerate\n class AutoRoundTest(unittest.TestCase):\n@@ -50,24 +53,27 @@ class AutoRoundTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\n         \"There is a girl who likes adventure, and she has been exploring the world for many years. She has visited every country in Europe and has even traveled to some of the most remote parts of Africa. She enjoys hiking through the mountains and discovering\"\n     )\n+    EXPECTED_OUTPUTS.add(\n+        \"There is a girl who likes adventure, and she has been exploring the world for many years. She has visited every country in Europe and has even traveled to some of the most remote parts of Africa. She has also climbed mountains and explored caves\"\n+    )\n \n-    device_map = \"cuda\"\n+    device_map = torch_device\n \n     # called only once for all test in this class\n     @classmethod\n     def setUpClass(cls):\n         \"\"\"\n         Setup quantized model\n         \"\"\"\n-        torch.cuda.synchronize()\n+        backend_synchronize(torch_device)\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name)\n         cls.quantized_model = AutoModelForCausalLM.from_pretrained(\n             cls.model_name, device_map=cls.device_map, torch_dtype=torch.float16\n         )\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n         gc.collect()\n \n     def test_quantized_model(self):\n@@ -128,14 +134,15 @@ def test_save_pretrained(self):\n             )\n \n             quantized_model.save_pretrained(tmpdirname)\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=\"cuda\")\n+            model = AutoModelForCausalLM.from_pretrained(tmpdirname, device_map=torch_device)\n \n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n \n             output = model.generate(**input_ids, max_new_tokens=40, do_sample=False)\n-            self.assertIn(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+            output_tokens = self.tokenizer.decode(output[0], skip_special_tokens=True)\n+            self.assertIn(output_tokens, self.EXPECTED_OUTPUTS)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n@@ -159,7 +166,7 @@ def test_convert_from_gptq(self):\n         quantization_config = AutoRoundConfig()\n \n         model = AutoModelForCausalLM.from_pretrained(\n-            model_name, device_map=\"cuda\", quantization_config=quantization_config, torch_dtype=\"auto\"\n+            model_name, device_map=torch_device, quantization_config=quantization_config, torch_dtype=\"auto\"\n         )\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n \n@@ -185,6 +192,7 @@ def test_convert_from_awq_cpu(self):\n         inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n         tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0])\n \n+    @require_torch_gpu\n     def test_mixed_bits(self):\n         \"\"\"\n         Simple test that checks if auto-round work properly with mixed bits\n@@ -203,7 +211,9 @@ def test_mixed_bits(self):\n         autoround = AutoRound(model, tokenizer, bits=bits, group_size=group_size, sym=sym, layer_config=layer_config)\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             autoround.quantize_and_save(output_dir=tmpdirname)\n-            model = AutoModelForCausalLM.from_pretrained(tmpdirname, torch_dtype=torch.float16, device_map=\"cuda\")\n+            model = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, torch_dtype=torch.float16, device_map=torch_device\n+            )\n             text = \"There is a girl who likes adventure,\"\n             inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n             tokenizer.decode(model.generate(**inputs, max_new_tokens=5)[0])"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 31,
        "deletions": 10
    }
}