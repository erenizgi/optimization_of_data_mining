{
    "author": "yuanwu2017",
    "message": "Gaudi: Fix the pipeline failed issue with hpu device (#36990)\n\n* Gaudi: fix the issue of is_torch_hpu_available() returns false\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Fix make fixup\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Add comments for the implicit behavior of import\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Update src/transformers/utils/import_utils.py\n\n* Update src/transformers/utils/import_utils.py\n\n---------\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>",
    "sha": "bd41b9c1ac35f81b7672d0b908bad6784dfd768b",
    "files": [
        {
            "sha": "714d69baf4c2c3b15dae6a58da01b81427587adb",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bd41b9c1ac35f81b7672d0b908bad6784dfd768b/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bd41b9c1ac35f81b7672d0b908bad6784dfd768b/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=bd41b9c1ac35f81b7672d0b908bad6784dfd768b",
            "patch": "@@ -947,12 +947,18 @@ def __init__(\n             if device == -1 and self.model.device is not None:\n                 device = self.model.device\n             if isinstance(device, torch.device):\n-                if device.type == \"xpu\" and not is_torch_xpu_available(check_device=True):\n+                if (device.type == \"xpu\" and not is_torch_xpu_available(check_device=True)) or (\n+                    device.type == \"hpu\" and not is_torch_hpu_available()\n+                ):\n                     raise ValueError(f'{device} is not available, you should use device=\"cpu\" instead')\n+\n                 self.device = device\n             elif isinstance(device, str):\n-                if \"xpu\" in device and not is_torch_xpu_available(check_device=True):\n+                if (\"xpu\" in device and not is_torch_xpu_available(check_device=True)) or (\n+                    \"hpu\" in device and not is_torch_hpu_available()\n+                ):\n                     raise ValueError(f'{device} is not available, you should use device=\"cpu\" instead')\n+\n                 self.device = torch.device(device)\n             elif device < 0:\n                 self.device = torch.device(\"cpu\")"
        },
        {
            "sha": "65a0d1bf374a6e46584cea87e0324bfa4622738d",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bd41b9c1ac35f81b7672d0b908bad6784dfd768b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bd41b9c1ac35f81b7672d0b908bad6784dfd768b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=bd41b9c1ac35f81b7672d0b908bad6784dfd768b",
            "patch": "@@ -811,6 +811,10 @@ def is_torch_hpu_available():\n \n     import torch\n \n+    if os.environ.get(\"PT_HPU_LAZY_MODE\", \"1\") == \"1\":\n+        # import habana_frameworks.torch in case of lazy mode to patch torch with torch.hpu\n+        import habana_frameworks.torch  # noqa: F401\n+\n     if not hasattr(torch, \"hpu\") or not torch.hpu.is_available():\n         return False\n "
        }
    ],
    "stats": {
        "total": 14,
        "additions": 12,
        "deletions": 2
    }
}