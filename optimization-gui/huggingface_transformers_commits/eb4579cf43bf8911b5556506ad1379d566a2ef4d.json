{
    "author": "ArthurZucker",
    "message": "`tokenizer` train from iterator without pre_tokenizers  (#35396)\n\n* fix if else issues\n\n* add a test\n\n* fix the test\n\n* style",
    "sha": "eb4579cf43bf8911b5556506ad1379d566a2ef4d",
    "files": [
        {
            "sha": "cc7edbd5328523479fd509676e186cba9bdf2eb1",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb4579cf43bf8911b5556506ad1379d566a2ef4d/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb4579cf43bf8911b5556506ad1379d566a2ef4d/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=eb4579cf43bf8911b5556506ad1379d566a2ef4d",
            "patch": "@@ -813,17 +813,17 @@ def train_new_from_iterator(\n             kwargs[\"end_of_word_suffix\"] = tokenizer_json[\"model\"][\"end_of_word_suffix\"]\n         if tokenizer_json[\"model\"][\"type\"] == \"Unigram\" and unk_token is not None:\n             kwargs[\"unk_token\"] = unk_token\n-        if (\n-            tokenizer_json[\"pre_tokenizer\"] is not None\n-            and tokenizer_json[\"pre_tokenizer\"][\"type\"] == \"ByteLevel\"\n-            or tokenizer_json[\"pre_tokenizer\"][\"type\"] == \"Sequence\"\n-            and \"pretokenizers\" in tokenizer_json[\"pre_tokenizer\"]\n-            and any(\n-                pretokenizer[\"type\"] == \"ByteLevel\"\n-                for pretokenizer in tokenizer_json[\"pre_tokenizer\"][\"pretokenizers\"]\n-            )\n-        ):\n-            kwargs[\"initial_alphabet\"] = pre_tokenizers_fast.ByteLevel.alphabet()\n+        if tokenizer_json[\"pre_tokenizer\"] is not None:\n+            if (\n+                tokenizer_json[\"pre_tokenizer\"][\"type\"] == \"ByteLevel\"\n+                or tokenizer_json[\"pre_tokenizer\"][\"type\"] == \"Sequence\"\n+                and \"pretokenizers\" in tokenizer_json[\"pre_tokenizer\"]\n+                and any(\n+                    pretokenizer[\"type\"] == \"ByteLevel\"\n+                    for pretokenizer in tokenizer_json[\"pre_tokenizer\"][\"pretokenizers\"]\n+                )\n+            ):\n+                kwargs[\"initial_alphabet\"] = pre_tokenizers_fast.ByteLevel.alphabet()\n \n         trainer_class = MODEL_TO_TRAINER_MAPPING[tokenizer_json[\"model\"][\"type\"]]\n         trainer = trainer_class(vocab_size=vocab_size, special_tokens=special_tokens, **kwargs)"
        },
        {
            "sha": "a9f2b1cd9b75e98854a506169527523af86dd279",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/eb4579cf43bf8911b5556506ad1379d566a2ef4d/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eb4579cf43bf8911b5556506ad1379d566a2ef4d/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=eb4579cf43bf8911b5556506ad1379d566a2ef4d",
            "patch": "@@ -48,6 +48,7 @@\n \n \n if is_tokenizers_available():\n+    import tokenizers\n     from tokenizers import Tokenizer\n     from tokenizers.models import WordPiece\n \n@@ -428,3 +429,21 @@ def test_sentencepiece_cohabitation(self):\n         # Now this will try to import sentencepiece_model_pb2_new.py. This should not fail even if the protobuf\n         # was already imported.\n         import_protobuf()\n+\n+    def test_training_new_tokenizer_edge_cases(self):\n+        _tokenizer = Tokenizer(tokenizers.models.BPE(vocab={\"a\": 1, \"b\": 2, \"ab\": 3}, merges=[(\"a\", \"b\")]))\n+        _tokenizer.pre_tokenizer = None\n+\n+        tokenizer = PreTrainedTokenizerFast(tokenizer_object=_tokenizer)\n+        toy_text_iterator = (\"a\" for _ in range(1000))\n+        tokenizer.train_new_from_iterator(text_iterator=toy_text_iterator, length=1000, vocab_size=50)\n+\n+        _tokenizer.normalizer = None\n+        tokenizer = PreTrainedTokenizerFast(tokenizer_object=_tokenizer)\n+        toy_text_iterator = (\"a\" for _ in range(1000))\n+        tokenizer.train_new_from_iterator(text_iterator=toy_text_iterator, length=1000, vocab_size=50)\n+\n+        _tokenizer.post_processor = None\n+        tokenizer = PreTrainedTokenizerFast(tokenizer_object=_tokenizer)\n+        toy_text_iterator = (\"a\" for _ in range(1000))\n+        tokenizer.train_new_from_iterator(text_iterator=toy_text_iterator, length=1000, vocab_size=50)"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 30,
        "deletions": 11
    }
}