{
    "author": "FightingZhen",
    "message": "[Feature] Support using FlashAttention2 on Ascend NPU (#36696)\n\n* [Feature] Support using flash-attention on Ascend NPU\n\n* Fix qwen3 and qwen3_moe moduler conversion mismatch",
    "sha": "e686fed6351767620d747e08fc82b045ac79e66f",
    "files": [
        {
            "sha": "a78166ed040b620c6e24a7e2c64c06c96f2d89d9",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -2,11 +2,10 @@\n \n import torch\n \n-from ..modeling_flash_attention_utils import _flash_attention_forward\n-from ..utils import is_flash_attn_greater_or_equal_2_10\n+from ..modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n \n \n-_use_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+_use_top_left_mask = flash_attn_supports_top_left_mask()\n \n \n def flash_attention_forward("
        },
        {
            "sha": "fa26379126fc09f9f56972741292c052a4895371",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "added",
            "additions": 233,
            "deletions": 0,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -0,0 +1,233 @@\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import os\n+\n+import torch\n+import torch.nn.functional as F\n+\n+from ..utils.import_utils import is_torch_npu_available\n+\n+\n+if is_torch_npu_available():\n+    import torch_npu\n+    from einops import rearrange, repeat\n+\n+\n+# FlashAttention2 is supported on Ascend NPU with down-right aligned causal mask by default.\n+# Set environment variable `NPU_FA2_SPARSE_MODE` to 2 when using top-left aligned causal mask.\n+TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE = 2\n+DOWN_RIGHT_ALIGNED_CAUSAL_MASK_MODE = 3\n+\n+SPARSE_MODE = int(os.getenv(\"NPU_FA2_SPARSE_MODE\", default=DOWN_RIGHT_ALIGNED_CAUSAL_MASK_MODE))\n+if SPARSE_MODE not in [TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE, DOWN_RIGHT_ALIGNED_CAUSAL_MASK_MODE]:\n+    raise ValueError(\n+        \"Environment variable `NPU_FA2_SPARSE_MODE` can only be set as 2 (top-left aligned causal mask) \"\n+        \"or 3 (down-right aligned causal mask).\"\n+    )\n+\n+\n+def is_npu_fa2_top_left_aligned_causal_mask():\n+    return SPARSE_MODE == TOP_LEFT_ALIGNED_CAUSAL_MASK_MODE if is_torch_npu_available() else False\n+\n+\n+# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n+class IndexFirstAxis(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, input, indices):\n+        ctx.save_for_backward(indices)\n+        assert input.ndim >= 2\n+        ctx.first_axis_dim, other_shape = input.shape[0], input.shape[1:]\n+        second_dim = other_shape.numel()\n+        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n+        # return input[indices]\n+        return torch.gather(\n+            rearrange(input, \"b ... -> b (...)\"), 0, repeat(indices, \"z -> z d\", d=second_dim)\n+        ).reshape(-1, *other_shape)\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        (indices,) = ctx.saved_tensors\n+        assert grad_output.ndim >= 2\n+        other_shape = grad_output.shape[1:]\n+        grad_output = rearrange(grad_output, \"b ... -> b (...)\")\n+        grad_input = torch.zeros(\n+            [ctx.first_axis_dim, grad_output.shape[1]],\n+            device=grad_output.device,\n+            dtype=grad_output.dtype,\n+        )\n+        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n+        # grad_input[indices] = grad_output\n+        grad_input.scatter_(0, repeat(indices, \"z -> z d\", d=grad_output.shape[1]), grad_output)\n+        return grad_input.reshape(ctx.first_axis_dim, *other_shape), None\n+\n+\n+index_first_axis = IndexFirstAxis.apply\n+\n+\n+# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n+class IndexPutFirstAxis(torch.autograd.Function):\n+    @staticmethod\n+    def forward(ctx, values, indices, first_axis_dim):\n+        ctx.save_for_backward(indices)\n+        assert indices.ndim == 1\n+        assert values.ndim >= 2\n+        output = torch.zeros(first_axis_dim, *values.shape[1:], device=values.device, dtype=values.dtype)\n+        # TD [2022-03-04] For some reason torch.scatter is a bit faster than indexing.\n+        output[indices] = values\n+        # output.scatter_(0, repeat(indices, 'z -> z d', d=values.shape[1]), values)\n+        return output\n+\n+    @staticmethod\n+    def backward(ctx, grad_output):\n+        (indices,) = ctx.saved_tensors\n+        # TD [2022-03-04] For some reason torch.gather is a bit faster than indexing.\n+        grad_values = grad_output[indices]\n+        # grad_values = torch.gather(grad_output, 0, repeat(indices, 'z -> z d', d=grad_output.shape[1]))\n+        return grad_values, None, None\n+\n+\n+index_put_first_axis = IndexPutFirstAxis.apply\n+\n+\n+# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n+def pad_input(hidden_states, indices, batch, seqlen):\n+    \"\"\"\n+    Arguments:\n+        hidden_states: (total_nnz, ...), where total_nnz = number of tokens in selected in attention_mask.\n+        indices: (total_nnz), the indices that represent the non-masked tokens of the original padded input sequence.\n+        batch: int, batch size for the padded sequence.\n+        seqlen: int, maximum sequence length for the padded sequence.\n+    Return:\n+        hidden_states: (batch, seqlen, ...)\n+    \"\"\"\n+    # dim = hidden_states.shape[-1]\n+    # output = torch.zeros((batch * seqlen), dim, device=hidden_states.device, dtype=hidden_states.dtype)\n+    # output[indices] = hidden_states\n+    output = index_put_first_axis(hidden_states, indices, batch * seqlen)\n+    return rearrange(output, \"(b s) ... -> b s ...\", b=batch)\n+\n+\n+# Copied from https://github.com/Dao-AILab/flash-attention/blob/main/flash_attn/bert_padding.py\n+def unpad_input(hidden_states, attention_mask, unused_mask=None):\n+    \"\"\"\n+    Arguments:\n+        hidden_states: (batch, seqlen, ...)\n+        attention_mask: (batch, seqlen), bool / int, 1 means valid and 0 means not valid.\n+        unused_mask: (batch, seqlen), bool / int, 1 means the element is allocated but unused.\n+    Return:\n+        hidden_states: (total_nnz, ...), where total_nnz = number of tokens selected in attention_mask + unused_mask.\n+        indices: (total_nnz), the indices of masked tokens from the flattened input sequence.\n+        cu_seqlens: (batch + 1), the cumulative sequence lengths, used to index into hidden_states.\n+        max_seqlen_in_batch: int\n+        seqused: (batch), returns the number of tokens selected in attention_mask + unused_mask.\n+    \"\"\"\n+    all_masks = (attention_mask + unused_mask) if unused_mask is not None else attention_mask\n+    seqlens_in_batch = all_masks.sum(dim=-1, dtype=torch.int32)\n+    used_seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n+    indices = torch.nonzero(all_masks.flatten(), as_tuple=False).flatten()\n+    max_seqlen_in_batch = seqlens_in_batch.max().item()\n+    cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n+    # TD [2022-03-04] We don't want to index with a bool mask, because Pytorch will expand the\n+    # bool mask, then call nonzero to get the indices, then index with those. The indices is @dim\n+    # times larger than it needs to be, wasting memory. It's faster and more memory-efficient to\n+    # index with integer indices. Moreover, torch's index is a bit slower than it needs to be,\n+    # so we write custom forward and backward to make it a bit faster.\n+    return (\n+        index_first_axis(rearrange(hidden_states, \"b s ... -> (b s) ...\"), indices),\n+        indices,\n+        cu_seqlens,\n+        max_seqlen_in_batch,\n+        used_seqlens_in_batch,\n+    )\n+\n+\n+def npu_flash_attn_func(\n+    q,\n+    k,\n+    v,\n+    dropout_p=0.0,\n+    softmax_scale=None,\n+    causal=False,\n+    **kwargs,\n+):\n+    keep_prob = 1.0 - dropout_p\n+\n+    if not causal:\n+        head_num = q.shape[2]\n+        output = torch_npu.npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n+    else:\n+        attn_mask_npu = torch.triu(torch.ones([2048, 2048]), diagonal=1).bool().to(q.device)\n+        head_num = q.shape[2]\n+        output = torch_npu.npu_fusion_attention(\n+            q,\n+            k,\n+            v,\n+            head_num,\n+            \"BSND\",\n+            keep_prob=keep_prob,\n+            scale=softmax_scale,\n+            atten_mask=attn_mask_npu,\n+            sparse_mode=SPARSE_MODE,\n+        )[0]\n+\n+    return output\n+\n+\n+def npu_flash_attn_varlen_func(\n+    q,\n+    k,\n+    v,\n+    cu_seqlens_q,\n+    cu_seqlens_k,\n+    dropout_p=0.0,\n+    softmax_scale=None,\n+    causal=False,\n+    **kwargs,\n+):\n+    keep_prob = 1.0 - dropout_p\n+\n+    if not causal:\n+        head_num = q.shape[1]\n+        output = torch_npu.npu_fusion_attention(\n+            q,\n+            k,\n+            v,\n+            head_num,\n+            pse=None,\n+            atten_mask=None,\n+            scale=softmax_scale,\n+            keep_prob=keep_prob,\n+            input_layout=\"TND\",\n+            actual_seq_qlen=tuple(cu_seqlens_q[1:].cpu().numpy().tolist()),\n+            actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),\n+        )[0]\n+    else:\n+        attn_mask_npu = torch.triu(torch.ones([2048, 2048]), diagonal=1).bool().to(q.device)\n+        head_num = q.shape[1]\n+        output = torch_npu.npu_fusion_attention(\n+            q,\n+            k,\n+            v,\n+            head_num,\n+            pse=None,\n+            padding_mask=None,\n+            atten_mask=attn_mask_npu,\n+            scale=softmax_scale,\n+            keep_prob=keep_prob,\n+            input_layout=\"TND\",\n+            actual_seq_qlen=tuple(cu_seqlens_q[1:].cpu().numpy().tolist()),\n+            actual_seq_kvlen=tuple(cu_seqlens_k[1:].cpu().numpy().tolist()),\n+            sparse_mode=SPARSE_MODE,\n+        )[0]\n+\n+    return output"
        },
        {
            "sha": "70516946c8943f00c13031b6484df5de93564708",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 50,
            "deletions": 1,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -19,19 +19,68 @@\n import torch\n import torch.nn.functional as F\n \n-from .utils import is_flash_attn_2_available, is_flash_attn_greater_or_equal, logging\n+from .utils import (\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal,\n+    is_flash_attn_greater_or_equal_2_10,\n+    is_torch_npu_available,\n+    logging,\n+)\n \n \n logger = logging.get_logger(__name__)\n+flash_attn_func = None\n \n \n if is_flash_attn_2_available():\n     from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n     from flash_attn import flash_attn_func, flash_attn_varlen_func\n+    from flash_attn.layers.rotary import apply_rotary_emb  # noqa\n \n+\n+# patch functions in package `flash-attn` when using flash-attention on Ascend NPU.\n+if is_torch_npu_available():\n+    from torch_npu import npu_rotary_mul as apply_rotary_emb  # noqa\n+\n+    from .integrations.npu_flash_attention import index_first_axis, pad_input, unpad_input\n+    from .integrations.npu_flash_attention import npu_flash_attn_func as flash_attn_func\n+    from .integrations.npu_flash_attention import npu_flash_attn_varlen_func as flash_attn_varlen_func\n+\n+\n+if flash_attn_func:\n     _flash_supports_window_size = \"window_size\" in list(inspect.signature(flash_attn_func).parameters)\n \n \n+def is_flash_attn_available():\n+    \"\"\"Determine whether flash-attention can be used or not.\"\"\"\n+\n+    # if package `flash-attn` is available, flash-attention can be used natively.\n+    if is_flash_attn_2_available():\n+        return True\n+\n+    # flash-attention can be used on Ascend NPU without package `flash-attn`\n+    if is_torch_npu_available():\n+        return True\n+\n+    return False\n+\n+\n+def flash_attn_supports_top_left_mask():\n+    \"\"\"Determine whether flash-attention uses top-left or down-right mask\"\"\"\n+\n+    if is_flash_attn_2_available():\n+        # top-left mask is used in package `flash-attn` with version lower than 2.1.0\n+        return not is_flash_attn_greater_or_equal_2_10()\n+\n+    if is_torch_npu_available():\n+        # down-right mask is used on Ascend NPU by default, set env `NPU_FA2_SPARSE_MODE=2` to activate top-left mask.\n+        from .integrations.npu_flash_attention import is_npu_fa2_top_left_aligned_causal_mask\n+\n+        return is_npu_fa2_top_left_aligned_causal_mask()\n+\n+    return False\n+\n+\n def _get_unpad_data(attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, int]:\n     \"\"\"\n     Retrieves indexing data required to repad unpadded (ragged) tensors."
        },
        {
            "sha": "98aa3206a7a3cc56886f0013d2063cb0741c64e9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -2276,11 +2276,13 @@ def _check_and_enable_flash_attn_2(\n             install_message = \"Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\"\n \n             if importlib.util.find_spec(\"flash_attn\") is None:\n+                # package `flash-attn` can not be installed on Ascend NPU, ignore related validation logic and early exit.\n                 if is_torch_npu_available():\n-                    recommend_message_npu = \"You should use attn_implementation='sdpa' instead when using NPU. \"\n-                    raise ImportError(\n-                        f\"{preface} the package flash_attn is not supported on Ascend NPU. {recommend_message_npu}\"\n-                    )\n+                    if not hard_check_only:\n+                        config._attn_implementation = \"flash_attention_2\"\n+\n+                    logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n+                    return config\n                 else:\n                     raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n "
        },
        {
            "sha": "06986e834c38ab5743d10ccf53b0c4cbf028286d",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -47,10 +47,7 @@\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_mamba_2_ssm_available,\n-)\n+from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n "
        },
        {
            "sha": "ccfae95058822fff64138a031a052880ba9171d1",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -29,14 +29,13 @@\n     SuppressTokensLogitsProcessor,\n )\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import CausalLMOutputWithPast, MaskedLMOutput\n from ...modeling_utils import PreTrainedModel, get_parameter_device\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_accelerate_available,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n )\n from ..auto import AutoModel\n@@ -54,7 +53,7 @@\n )\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -203,7 +202,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _split_heads(self, tensor, num_heads, attn_head_size):\n         \"\"\""
        },
        {
            "sha": "a519528491830dd56d158ad8344bba5eaf177700",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -32,6 +32,7 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -47,15 +48,13 @@\n     add_end_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_bart import BartConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -300,7 +299,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "aff97487bb1ecb742e22ae0ec455d5c715661680",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,7 +28,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -39,8 +39,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n@@ -55,10 +53,6 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n-\n-\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"ChameleonConfig\"\n@@ -385,7 +379,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Ignore copy\n     def forward("
        },
        {
            "sha": "f6da323a8a08a859dfdab878975cfdc7837c17fe",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -24,6 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import is_torch_greater_or_equal_than_2_2\n@@ -32,16 +33,14 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n )\n from .configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -414,7 +413,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n     def forward("
        },
        {
            "sha": "df312b047d0149d7a8fa13a6233f0d64df83098e",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,6 +27,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -40,15 +41,13 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_peft_available,\n     logging,\n )\n from .configuration_data2vec_audio import Data2VecAudioConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -495,7 +494,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "5303f84edcb5e2ce0813e02bca297fa773ccf2bd",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -25,13 +25,12 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -46,7 +45,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -331,7 +330,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "7cadf5673ff351044b1dad65846f13027529e0fe",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -32,7 +32,11 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs, _flash_attention_forward\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+    _flash_attention_forward,\n+    flash_attn_supports_top_left_mask,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -48,7 +52,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -254,7 +257,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "68102347904a66ab2015122bbd8945763086519d",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -23,11 +23,8 @@\n from torch import nn\n \n from ...cache_utils import Cache, StaticCache\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n-from ...utils import (\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-)\n+from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n+from ...utils import logging\n from ..gemma.modeling_gemma import GemmaForCausalLM\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -176,7 +173,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "b78050a01aebb24f84ad3abb7b2a0925653f0541",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -30,6 +30,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n@@ -49,15 +50,13 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_distilbert import DistilBertConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -251,7 +250,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "5fc8e43afa1575ee5114db7ffadd7d5549c5a5f0",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -32,7 +32,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -50,10 +49,6 @@\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n-\n-\n _CONFIG_FOR_DOC = \"Emu3Config\"\n _CHECKPOINT_FOR_DOC = \"BAAI/Emu3-Chat-hf\"\n "
        },
        {
            "sha": "e6cb36353dc58327e5b35857fb413d513ba1e29d",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -42,8 +43,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n@@ -53,7 +52,7 @@\n if TYPE_CHECKING:\n     from ...configuration_utils import PretrainedConfig\n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -472,7 +471,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "d9a84d6f463d979aa9b3c45e36ffe0ba6d73966d",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -24,6 +24,7 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -36,14 +37,12 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n )\n from .configuration_gpt_bigcode import GPTBigCodeConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -284,7 +283,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "12e45d918d885c083ebdc08dcc55e1f080eccf19",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -40,8 +41,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     is_torch_fx_available,\n     logging,\n@@ -55,7 +54,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -287,7 +286,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "fca8b0e5a167316ec6197995626829a7efac557b",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -38,8 +39,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     logging,\n@@ -54,7 +53,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -279,7 +278,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "c05a10504085caa439711bd3bb2b5c07de36af3a",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -24,7 +24,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     MoeCausalLMOutputWithPast,\n@@ -36,7 +36,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -534,7 +533,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "fd2cdf2843d3f3c25fcd906fe55eb95f4628c9cd",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -29,14 +29,13 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -411,7 +410,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "3f6e3a5a87394b01eaa781dff63b189b5b457517",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -26,21 +26,20 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_hubert import HubertConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -569,7 +568,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "81b77e57c1162fb1308a7a9305c3681f30388b4a",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,13 +27,12 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -42,7 +41,7 @@\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -279,7 +278,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,\n@@ -874,7 +873,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Ignore copy\n     def forward("
        },
        {
            "sha": "c3644ab6c785ca7e7bf2a3ae3aad2ec37e41b350",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -26,21 +26,20 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from ..auto import AutoModel\n from .configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -279,7 +278,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "4fba6ba89b9bb85c88b92228df12c40de6cc9f04",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -33,6 +33,7 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -48,14 +49,12 @@\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_mamba_ssm_available,\n )\n from .configuration_jamba import JambaConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -391,7 +390,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "c7457db3e9b5b3ab0bdd9a83d54f913c12dd116d",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -36,8 +37,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -52,7 +51,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -685,7 +684,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "cba5ead876c2b8e8772e5a7ab56d4ee3c9221237",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -31,6 +31,7 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -43,15 +44,13 @@\n     add_end_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_m2m_100 import M2M100Config\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -354,7 +353,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "522c5c8cfce760b1d2c1481314ab89966a67c546",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -31,6 +31,7 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -46,15 +47,13 @@\n     add_end_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_mbart import MBartConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -297,7 +296,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "4746755a4f5106aadabf39638c4736600f208ac1",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -25,22 +25,21 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_mimi import MimiConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -604,7 +603,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "cc352e76fb5f89bdd8bcf41e9ea1cc9c8a731e5d",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -30,6 +30,7 @@\n     GenerationMixin,\n )\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -42,8 +43,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -53,7 +52,7 @@\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -573,7 +572,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "add0eeba65440064f948f1362becde24f9440b54",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -40,6 +40,7 @@\n     _prepare_4d_causal_attention_mask,\n     _prepare_4d_causal_attention_mask_for_sdpa,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -51,8 +52,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -61,7 +60,7 @@\n from .configuration_musicgen import MusicgenConfig, MusicgenDecoderConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n if TYPE_CHECKING:\n@@ -330,7 +329,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "08af10d1a3d8dbb755c87edc333308a8a3737e65",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -35,6 +35,7 @@\n     StoppingCriteriaList,\n )\n from ...modeling_attn_mask_utils import _prepare_4d_causal_attention_mask, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     ModelOutput,\n@@ -43,8 +44,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -53,7 +52,7 @@\n from .configuration_musicgen_melody import MusicgenMelodyConfig, MusicgenMelodyDecoderConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n if TYPE_CHECKING:\n@@ -346,7 +345,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "b1cae89b0bae244a69219faa9d8004fccf991fa3",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,7 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -42,7 +42,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -324,7 +323,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Ignore copy\n     def forward("
        },
        {
            "sha": "e6246ac6001a108cf1606b51cd15e8d533e8f027",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -23,6 +23,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -33,16 +34,14 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmoe import OlmoeConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -404,7 +403,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "729f71a49aecf7bbef3d4bd9da308decd4045384",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,6 +27,7 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -38,8 +39,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -53,7 +52,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -208,7 +207,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "1065e801a4556fe5867ba0444219eab9cb5b4bed",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,19 +28,13 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from .configuration_paligemma import PaliGemmaConfig\n-\n-\n-if is_flash_attn_2_available():\n-    from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n-\n from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_paligemma import PaliGemmaConfig\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "7701a76e6c7ee0bd6c6ba879e7379731bc9d57b0",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -26,6 +26,7 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n+from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -36,7 +37,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -45,7 +45,7 @@\n from .configuration_phimoe import PhimoeConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n # This makes `_prepare_4d_causal_attention_mask` a leaf function in the FX graph."
        },
        {
            "sha": "62fb093bc91d083582d846cdc56a08ffeaf808bc",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 19,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -37,33 +37,20 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLVisionConfig\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_varlen_func\n-    from flash_attn.layers.rotary import apply_rotary_emb\n-\n-else:\n-    flash_attn_varlen_func = None\n-    apply_rotary_emb = None\n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n-else:\n-    flash_attn_varlen_func = None\n \n \n logger = logging.get_logger(__name__)\n@@ -819,7 +806,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "10100d8f3336cc8138d34ffc7cdbececf7428db8",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -47,18 +47,14 @@\n from ...configuration_utils import PretrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, VideoInput\n+from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_flash_attn_2_available, logging\n+from ...utils import logging\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_varlen_func\n-    from flash_attn.layers.rotary import apply_rotary_emb\n-\n-else:\n-    flash_attn_varlen_func = None\n-    apply_rotary_emb = None\n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import apply_rotary_emb, flash_attn_varlen_func\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "37d38c429a7c5f64bbaf030505989ab2755834e6",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -25,13 +25,12 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -40,7 +39,7 @@\n from .configuration_qwen2_audio import Qwen2AudioConfig, Qwen2AudioEncoderConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -210,7 +209,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     @deprecate_kwarg(\"key_value_states\", version=\"4.52\")\n     @deprecate_kwarg(\"past_key_value\", version=\"4.52\")"
        },
        {
            "sha": "40263884b1302f3647e2714bcad1421f10dcbda4",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -31,6 +31,7 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n@@ -44,16 +45,14 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -412,7 +411,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "b0c485a9e9e2c8eb10b138afc4debacfa399c645",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 9,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -33,26 +33,21 @@\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLVisionConfig\n \n \n-if is_flash_attn_2_available():\n-    from flash_attn import flash_attn_varlen_func\n-\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-else:\n-    flash_attn_varlen_func = None\n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_varlen_func\n \n \n logger = logging.get_logger(__name__)\n@@ -632,7 +627,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "069691cee2932aaa5ff6eeaf311303c4db5d05b7",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -19,6 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -588,7 +589,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "a697289993cbbcce9a16610cd85badd220e4e6d0",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -19,6 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import partial\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n@@ -685,7 +686,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     causal_mask,\n                     position_ids,"
        },
        {
            "sha": "e16a17dbf2bc6a56ff53bcd90cc7715efe38c0b1",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,20 +27,19 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n )\n from .configuration_sew import SEWConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -569,7 +568,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "34b0ee370cd3b485c0a34d27df25417f016e0a86",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,22 +28,21 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n     torch_int,\n )\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -451,7 +450,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n     def forward("
        },
        {
            "sha": "4fbe92148380c8a7760eceaa3ae8e769cab7ddf7",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -32,21 +32,20 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -342,7 +341,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n     def forward("
        },
        {
            "sha": "a0f43033d721801faafb9e629fc5009f65a56af2",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -30,21 +30,20 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from ..auto import AutoModel\n from .configuration_smolvlm import SmolVLMConfig, SmolVLMVisionConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -253,7 +252,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "a11e0627b8f14898966bce43da8b2363fa02f51a",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -30,6 +30,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n@@ -42,8 +43,6 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -58,7 +57,7 @@\n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -461,7 +460,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "7172618182a493f8fcbbc81a9d9c6049b7d28b48",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,22 +28,21 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import BaseModelOutput, CausalLMOutput, SequenceClassifierOutput, Wav2Vec2BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_unispeech import UniSpeechConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -601,7 +600,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "0472cde71565e821bcdfb3412168a5b6467c3b0b",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -42,16 +43,14 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_peft_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_unispeech_sat import UniSpeechSatConfig\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -618,7 +617,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "be6f5eeda6e32d494c20195a2c79339919c59bd9",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -28,6 +28,7 @@\n from ...activations import ACT2FN\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     CausalLMOutput,\n@@ -44,8 +45,6 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     cached_file,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_peft_available,\n     is_safetensors_available,\n     logging,\n@@ -61,7 +60,7 @@\n     from safetensors.torch import load_file as safe_load_file\n \n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n logger = logging.get_logger(__name__)\n@@ -664,7 +663,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def _reshape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim)"
        },
        {
            "sha": "5c85dc8f0128c15f661e065a48b97dbe2cad5b8e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -27,6 +27,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -39,8 +40,6 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -54,7 +53,7 @@\n \n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n-if is_flash_attn_2_available():\n+if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n \n@@ -366,7 +365,7 @@ def __init__(self, *args, **kwargs):\n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n         # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n     def forward(\n         self,"
        },
        {
            "sha": "5aa01be0d7c6602daa41ba3fae9066cdbf2c144b",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -1016,11 +1016,6 @@ def is_flash_attn_2_available():\n     if not (torch.cuda.is_available() or is_torch_mlu_available()):\n         return False\n \n-    # Ascend does not support \"flash_attn\".\n-    # If \"flash_attn\" is left in the env, is_flash_attn_2_available() should return False.\n-    if is_torch_npu_available():\n-        return False\n-\n     if torch.version.cuda:\n         return version.parse(importlib.metadata.version(\"flash_attn\")) >= version.parse(\"2.1.0\")\n     elif torch.version.hip:"
        },
        {
            "sha": "8abef0422705d22441a35f42cbaa92625d55cc91",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e686fed6351767620d747e08fc82b045ac79e66f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e686fed6351767620d747e08fc82b045ac79e66f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=e686fed6351767620d747e08fc82b045ac79e66f",
            "patch": "@@ -48,6 +48,7 @@\n     is_torch_available,\n     logging,\n )\n+from transformers.modeling_flash_attention_utils import is_flash_attn_available\n from transformers.testing_utils import (\n     TOKEN,\n     CaptureLogger,\n@@ -79,6 +80,7 @@\n     is_flash_attn_2_available,\n     is_flax_available,\n     is_tf_available,\n+    is_torch_npu_available,\n     is_torch_sdpa_available,\n     is_torchdynamo_available,\n )\n@@ -653,7 +655,7 @@ def test_model_from_pretrained_attn_implementation(self):\n         if is_torch_sdpa_available():\n             attn_implementation_available.append(\"sdpa\")\n \n-        if is_flash_attn_2_available():\n+        if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n \n         for requested_attn_implementation in attn_implementation_available:\n@@ -677,7 +679,7 @@ def test_model_from_config_attn_implementation(self):\n         if is_torch_sdpa_available():\n             attn_implementation_available.append(\"sdpa\")\n \n-        if is_flash_attn_2_available():\n+        if is_flash_attn_available():\n             attn_implementation_available.append(\"flash_attention_2\")\n \n         for requested_attn_implementation in attn_implementation_available:\n@@ -2676,6 +2678,11 @@ def test_not_available_flash(self):\n         if is_flash_attn_2_available():\n             self.skipTest(reason=\"Please uninstall flash-attn package to run test_not_available_flash\")\n \n+        if is_torch_npu_available():\n+            self.skipTest(\n+                reason=\"FlashAttention2 is supported on Ascend NPU without using package `flash-attn`, ignore this test case.\"\n+            )\n+\n         with self.assertRaises(ImportError) as cm:\n             _ = AutoModel.from_pretrained(\n                 \"hf-internal-testing/tiny-random-GPTBigCodeModel\", attn_implementation=\"flash_attention_2\"\n@@ -2686,6 +2693,11 @@ def test_not_available_flash_with_config(self):\n         if is_flash_attn_2_available():\n             self.skipTest(reason=\"Please uninstall flash-attn package to run test_not_available_flash\")\n \n+        if is_torch_npu_available():\n+            self.skipTest(\n+                reason=\"FlashAttention2 is supported on Ascend NPU without using package `flash-attn`, ignore this test case.\"\n+            )\n+\n         config = AutoConfig.from_pretrained(\"hf-internal-testing/tiny-random-GPTBigCodeModel\")\n \n         with self.assertRaises(ImportError) as cm:"
        }
    ],
    "stats": {
        "total": 681,
        "additions": 447,
        "deletions": 234
    }
}