{
    "author": "zucchini-nlp",
    "message": "Don't use default attn if pre-set in sub-config (#38526)\n\n* don't use default attn if pre-set in sib-config\n\n* style\n\n* add a test maybe",
    "sha": "55ec319de6a90c7b8db1218a5d73837fc6098371",
    "files": [
        {
            "sha": "6e2d7a8102f3e5e6f41cdad37b3ea41ee50d4cde",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55ec319de6a90c7b8db1218a5d73837fc6098371/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55ec319de6a90c7b8db1218a5d73837fc6098371/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=55ec319de6a90c7b8db1218a5d73837fc6098371",
            "patch": "@@ -2277,8 +2277,13 @@ def _autoset_attn_implementation(\n                 if not isinstance(requested_attn_implementation, dict)\n                 else requested_attn_implementation.get(key, None)\n             )\n-            # For models with backbone sub-config might be not initialized\n-            if sub_config is not None:\n+            # For models with backbone sub-config might be not initialized. Set the requested att\n+            # if the config hasn't got any attn pre-set and the requested attn in not `None` (i.e not the default attn)\n+            if (\n+                sub_config is not None\n+                and sub_config._attn_implementation_internal is None\n+                and curr_attn_implementation is not None\n+            ):\n                 sub_config._attn_implementation_internal = curr_attn_implementation\n \n         if config._attn_implementation == \"flash_attention_2\":"
        },
        {
            "sha": "446610db966a6ea8ea5ab973ed7e028b8a5c619c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/55ec319de6a90c7b8db1218a5d73837fc6098371/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55ec319de6a90c7b8db1218a5d73837fc6098371/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=55ec319de6a90c7b8db1218a5d73837fc6098371",
            "patch": "@@ -3425,6 +3425,13 @@ def test_attn_implementation_composite_models(self):\n                         f\"The eager model should not have SDPA/FA2 attention layers but got `{class_name}.config._attn_implementation={submodule.config._attn_implementation}`\"\n                     )\n \n+            # Set the attention to default `None` but the text config to `eager`\n+            # The model should load encoders in SDPA but not the text attention\n+            config._attn_implementation = None\n+            config.get_text_config(decoder=True)._attn_implementation = \"eager\"\n+            model = model_class(config)\n+            self.assertTrue(model.config.get_text_config(decoder=True)._attn_implementation == \"eager\")\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 16,
        "additions": 14,
        "deletions": 2
    }
}