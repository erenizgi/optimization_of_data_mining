{
    "author": "kylesayrs",
    "message": "[Bugfix] Fix reloading of pixtral/llava configs (#36077)\n\n* add is_composition flag to LlavaConfig\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\r\n\r\n* WIP: pixtral text config\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\r\n\r\n* fix style\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\r\n\r\n* add test\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\r\n\r\n* use is_composition for pixtral\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\r\n\r\n* Revert \"use is_composition for pixtral\"\r\n\r\nThis reverts commit a53d5f9fc5149c84419b0e9e03db6d99362add53.\r\n\r\n* Revert \"Revert \"use is_composition for pixtral\"\"\r\n\r\nThis reverts commit 3ab1c99404e2c2963fba0bcf94b9786d6365db0f.\r\n\r\n---------\r\n\r\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>",
    "sha": "bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0",
    "files": [
        {
            "sha": "74fa20cc7608926e6a556e5909b8a4e1e2b177f0",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0",
            "patch": "@@ -76,6 +76,7 @@ class LlavaConfig(PretrainedConfig):\n \n     model_type = \"llava\"\n     sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+    is_composition = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "458743887d3ac938284f7c9eb4aeb913622e4764",
            "filename": "tests/models/llava/test_configuration_llava.py",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_configuration_llava.py?ref=bcfc9d795e1330faaa8b39ffa18732f8b40fe7c0",
            "patch": "@@ -0,0 +1,70 @@\n+import tempfile\n+import unittest\n+\n+from transformers import LlavaConfig\n+\n+\n+class LlavaConfigTest(unittest.TestCase):\n+    def test_llava_reload(self):\n+        \"\"\"\n+        Simple test for reloading default llava configs\n+        \"\"\"\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = LlavaConfig()\n+            config.save_pretrained(tmp_dir)\n+\n+            reloaded = LlavaConfig.from_pretrained(tmp_dir)\n+            assert config.to_dict() == reloaded.to_dict()\n+\n+    def test_pixtral_reload(self):\n+        \"\"\"\n+        Simple test for reloading pixtral configs\n+        \"\"\"\n+        vision_config = {\n+            \"model_type\": \"pixtral\",\n+            \"head_dim\": 64,\n+            \"hidden_act\": \"silu\",\n+            \"image_size\": 1024,\n+            \"is_composition\": True,\n+            \"patch_size\": 16,\n+            \"rope_theta\": 10000.0,\n+            \"tie_word_embeddings\": False,\n+        }\n+\n+        text_config = {\n+            \"model_type\": \"mistral\",\n+            \"hidden_size\": 5120,\n+            \"head_dim\": 128,\n+            \"num_attention_heads\": 32,\n+            \"intermediate_size\": 14336,\n+            \"is_composition\": True,\n+            \"max_position_embeddings\": 1024000,\n+            \"num_hidden_layers\": 40,\n+            \"num_key_value_heads\": 8,\n+            \"rms_norm_eps\": 1e-05,\n+            \"rope_theta\": 1000000000.0,\n+            \"sliding_window\": None,\n+            \"vocab_size\": 131072,\n+        }\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = LlavaConfig(vision_config=vision_config, text_config=text_config)\n+            config.save_pretrained(tmp_dir)\n+\n+            reloaded = LlavaConfig.from_pretrained(tmp_dir)\n+            assert config.to_dict() == reloaded.to_dict()\n+\n+    def test_arbitrary_reload(self):\n+        \"\"\"\n+        Simple test for reloading arbirarily composed subconfigs\n+        \"\"\"\n+        default_values = LlavaConfig().to_dict()\n+        default_values[\"vision_config\"][\"model_type\"] = \"qwen2_vl\"\n+        default_values[\"text_config\"][\"model_type\"] = \"opt\"\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            config = LlavaConfig(**default_values)\n+            config.save_pretrained(tmp_dir)\n+\n+            reloaded = LlavaConfig.from_pretrained(tmp_dir)\n+            assert config.to_dict() == reloaded.to_dict()"
        }
    ],
    "stats": {
        "total": 71,
        "additions": 71,
        "deletions": 0
    }
}