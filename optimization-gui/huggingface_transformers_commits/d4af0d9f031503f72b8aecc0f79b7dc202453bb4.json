{
    "author": "gante",
    "message": "[generate] misc fixes (#40906)\n\nmisc fixes",
    "sha": "d4af0d9f031503f72b8aecc0f79b7dc202453bb4",
    "files": [
        {
            "sha": "2e312bcb3c79b1f441232d816bb024e5f058ef4c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/d4af0d9f031503f72b8aecc0f79b7dc202453bb4/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d4af0d9f031503f72b8aecc0f79b7dc202453bb4/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d4af0d9f031503f72b8aecc0f79b7dc202453bb4",
            "patch": "@@ -575,6 +575,9 @@ def prepare_inputs_for_generation(\n         # 2. Generic cache-dependent input preparation\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n+            # TODO (joao): handle the case where cache length == input_ids length. The function below results in an\n+            # exception because we get empty input_ids after slicing. In essence, we need to roll back the cache 1\n+            # token to recompute the logits for the first token to be generated (but not all caches support roll backs)\n             inputs_embeds, input_ids = self._cache_dependant_input_preparation(\n                 input_ids, inputs_embeds, cache_position\n             )\n@@ -2631,19 +2634,19 @@ def heal_tokens(\n         # replace bos with pad to not condition healing on it\n         input_ids = torch.where(input_ids == bos_token_id, pad_token_id, input_ids)\n \n-        \"\"\"\n-        the latter code assumes the input_ids is not empty,\n-        input_id has to be checked if contains elements\n-\t\t\"\"\"\n+        # the latter code assumes the input_ids is not empty, input_id has to be checked if contains elements\n         if input_ids.numel() == 0:\n             return input_ids\n \n         tail_ids = input_ids[:, -1].tolist()\n \n-        space_tok = tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(\" \"))[0]\n         # tail tokens are used for a prefix search, thus, whitespaces are replaced with\n         # their tokenization (e.g. 'Ä ') to enable search for tokens prefixed with a whitespace\n-        tail_toks = (tokenizer.decode(t).replace(\" \", space_tok) for t in tail_ids)\n+        if tokenizer.convert_tokens_to_ids(\" \") is not None:\n+            space_tok = tokenizer.convert_ids_to_tokens(tokenizer.convert_tokens_to_ids(\" \"))[0]\n+            tail_toks = (tokenizer.decode(t).replace(\" \", space_tok) for t in tail_ids)\n+        else:\n+            tail_toks = (tokenizer.decode(t) for t in tail_ids)\n \n         for batch_idx, (tail_id, tail_tok) in enumerate(zip(tail_ids, tail_toks)):\n             batch_ids = input_ids[batch_idx]"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}