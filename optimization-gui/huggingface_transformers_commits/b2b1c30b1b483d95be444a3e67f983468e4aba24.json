{
    "author": "McPatate",
    "message": "fix: continuous batching in `transformers serve` (#40479)\n\n* fix: continuous batching in `transformers serve`\n\n* fix: short circuit inner gen loop when prepare_next_batch prepared nothing\n\n* docs: add comment explaining FastAPI lifespan\n\n* test: add CB serving tests\n\n* refactor: remove gen cfg max new tokens override bc unnecessary\n\n* docs: add docstring for `ServeCommand::run`\n\n* feat: use new `DecodeStream` API",
    "sha": "b2b1c30b1b483d95be444a3e67f983468e4aba24",
    "files": [
        {
            "sha": "8a0ca22e7afbb95a28459587c8af645f62a0ec43",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 54,
            "deletions": 26,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=b2b1c30b1b483d95be444a3e67f983468e4aba24",
            "patch": "@@ -25,13 +25,15 @@\n import time\n from argparse import ArgumentParser, Namespace\n from collections.abc import Generator, Iterable\n+from contextlib import asynccontextmanager\n from dataclasses import dataclass, field\n from io import BytesIO\n from threading import Thread\n from typing import Optional, Union\n \n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n+from tokenizers.decoders import DecodeStream\n \n import transformers\n from transformers.models.auto.modeling_auto import (\n@@ -313,16 +315,16 @@ def __init__(\n         self._name_or_path = str(model.name_or_path)\n         self.processor = processor\n         self.timeout_seconds = timeout_seconds\n-        self._timer = threading.Timer(self.timeout_seconds, self._delete_model)\n+        self._timer = threading.Timer(self.timeout_seconds, self.timeout_reached)\n         self._timer.start()\n \n     def reset_timer(self):\n         \"\"\"Reset the timer for the deletion of the instances.\"\"\"\n         self._timer.cancel()\n-        self._timer = threading.Timer(self.timeout_seconds, self._delete_model)\n+        self._timer = threading.Timer(self.timeout_seconds, self.timeout_reached)\n         self._timer.start()\n \n-    def _delete_model(self):\n+    def delete_model(self):\n         \"\"\"Delete the wrapped model and processor and clean up resources.\"\"\"\n         if hasattr(self, \"model\") and self.model is not None:\n             del self.model\n@@ -335,9 +337,12 @@ def _delete_model(self):\n             if torch.cuda.is_available():\n                 torch.cuda.empty_cache()\n \n-            logger.info(\n-                f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\"\n-            )\n+            # XXX: in case we manually delete the model, like on server shutdown\n+            self._timer.cancel()\n+\n+    def timeout_reached(self):\n+        self.delete_model()\n+        logger.info(f\"{self._name_or_path} was removed from memory after {self.timeout_seconds} seconds of inactivity\")\n \n     def is_deleted(self):\n         \"\"\"Check if the instances have been deleted.\"\"\"\n@@ -353,6 +358,10 @@ class ServeArguments:\n     `transformers serve --help`\n     \"\"\"\n \n+    continuous_batching: bool = field(\n+        default=False,\n+        metadata={\"help\": \"Whether to use continuous batching for chat completions.\"},\n+    )\n     device: str = field(\n         default=\"auto\",\n         metadata={\n@@ -469,7 +478,7 @@ def __init__(self, args: ServeArguments):\n \n         # Store and process input arguments\n         self.args = args\n-        self.use_continuous_batching = self.args.attn_implementation == \"sdpa_paged\"\n+        self.use_continuous_batching = self.args.continuous_batching\n         self.enable_cors = self.args.enable_cors\n \n         if self.args.default_seed is not None:\n@@ -569,11 +578,13 @@ def validate_transcription_request(self, request: dict):\n     def build_chat_completion_chunk(\n         self,\n         request_id: Optional[str] = \"\",\n-        content: Optional[str] = None,\n+        content: Optional[int] = None,\n         model: Optional[str] = None,\n         role: Optional[str] = None,\n         finish_reason: Optional[str] = None,\n         tool_calls: Optional[list[\"ChoiceDeltaToolCall\"]] = None,\n+        decode_stream: Optional[DecodeStream] = None,\n+        tokenizer: Optional[PreTrainedTokenizerFast] = None,\n     ) -> str:\n         \"\"\"\n         Builds a chunk of a streaming OpenAI Chat Completion response.\n@@ -598,6 +609,8 @@ def build_chat_completion_chunk(\n         Returns:\n             `str`: The built chunk, a string containing a JSON string with the payload.\n         \"\"\"\n+        if decode_stream is not None and content is not None and tokenizer is not None:\n+            content = decode_stream.step(tokenizer._tokenizer, content)\n         chunk = ChatCompletionChunk(\n             id=request_id,\n             created=int(time.time()),\n@@ -635,7 +648,29 @@ def build_response_event(self, response: \"BaseModel\") -> str:\n         return f\"data: {response.model_dump_json(exclude_none=True)}\\n\\n\"\n \n     def run(self):\n-        app = FastAPI()\n+        \"\"\"\n+        Setup and run the FastAPI server for transformers serve.\n+\n+        Models will be loaded and unloaded automatically based on usage and a timeout.\n+\n+        The server will expose the following endpoints:\n+        - POST /v1/chat/completions: Generates chat completions.\n+        - POST /v1/responses: Generates responses.\n+        - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n+        - GET /v1/models: Lists available models for 3rd party tools.\n+\n+        Requires FastAPI and Uvicorn to be installed.\n+        \"\"\"\n+\n+        @asynccontextmanager\n+        async def lifespan(app: FastAPI):\n+            yield\n+            for model in self.loaded_models.values():\n+                model.delete_model()\n+            if self.running_continuous_batching_manager is not None:\n+                self.running_continuous_batching_manager.stop(block=True, timeout=5)\n+\n+        app = FastAPI(lifespan=lifespan)\n \n         # Some apps that make requests from external domains (e.g. Cursor) require CORS to be enabled. However, for\n         # security purposes, it's disabled by default\n@@ -774,10 +809,7 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n             eos_token_id=tokenizer.eos_token_id,\n             pad_token_id=tokenizer.pad_token_id,\n             use_cache=False,\n-            num_blocks=1,\n-            block_size=1024,\n             do_sample=False,\n-            max_batch_tokens=10,\n             scheduler=\"fifo\",\n         )\n \n@@ -798,34 +830,30 @@ def continuous_batching_chat_completion(self, req: dict) -> Generator[str, None,\n \n         def stream_chat_completion(_inputs):\n             try:\n+                decode_stream = DecodeStream([id.item() for id in _inputs], False)\n                 request_id = self.running_continuous_batching_manager.add_request(\n                     _inputs, request_id=req.get(\"request_id\"), max_new_tokens=generation_config.max_new_tokens\n                 )\n \n-                queue_is_flushed = False\n-\n                 # Emit the assistant role to start the stream. Other chunks won't have a role, as it is implicit\n                 # they come from the assistant.\n                 yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n \n-                for result in self.running_continuous_batching_manager:\n-                    if result.request_id != request_id:\n-                        continue\n-                    if req.get(\"request_id\") is not None and not queue_is_flushed:\n-                        if result.status == RequestStatus.FINISHED:\n-                            continue\n-                        else:\n-                            queue_is_flushed = True\n-\n-                    finish_reason = \"stop\" if result.status == RequestStatus.FINISHED else None\n+                for result in self.running_continuous_batching_manager.request_id_iter(request_id):\n                     if result.status == RequestStatus.FINISHED:\n                         yield self.build_chat_completion_chunk(\n-                            request_id, finish_reason=finish_reason, model=model_id_and_revision\n+                            request_id,\n+                            finish_reason=\"stop\",\n+                            model=model_id_and_revision,\n                         )\n                         break\n                     else:\n                         yield self.build_chat_completion_chunk(\n-                            request_id=request_id, content=result.next_token, model=model_id_and_revision\n+                            request_id=request_id,\n+                            content=result.generated_tokens[-1],\n+                            model=model_id_and_revision,\n+                            decode_stream=decode_stream,\n+                            tokenizer=tokenizer,\n                         )\n \n             except Exception as e:"
        },
        {
            "sha": "91bc849a7a41f35fec7950604ce9f3d4f61d1484",
            "filename": "src/transformers/generation/continuous_batching/classes.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fclasses.py?ref=b2b1c30b1b483d95be444a3e67f983468e4aba24",
            "patch": "@@ -75,7 +75,6 @@ class GenerationOutput:\n         error (Optional[str]): Any error message associated with the request. When None, the request was successful.\n         status (RequestStatus): The status of the request.\n         created_time (float): The time the request was created.\n-        next_token (Optional[int]): The next token to be generated.\n     \"\"\"\n \n     request_id: str\n@@ -85,7 +84,6 @@ class GenerationOutput:\n     error: Optional[str] = None\n     status: RequestStatus = RequestStatus.PENDING\n     created_time: float = field(default_factory=time.time)\n-    next_token: Optional[int] = field(default_factory=int)\n \n \n @dataclass\n@@ -106,7 +104,6 @@ class RequestState:\n         eos_token_id (int): The ID of the end-of-sequence token.\n         created_time (float): The time the request was created.\n         error (Optional[str]): Any error message associated with the request. When None, has had no error yet.\n-        next_token (Optional[str]): The next token to be generated.\n     \"\"\"\n \n     # Required fields\n@@ -122,7 +119,6 @@ class RequestState:\n     eos_token_id: int = -1  # ID of the end-of-sequence token\n     created_time: float = field(default_factory=time.time)  # Time the request was created\n     error: Optional[str] = None  # Error message if the request failed\n-    next_token: Optional[str] = None  # Next token to be generated\n     lifespan: tuple[float, float] = (-1, -1)  # (time request was no longer pending, time request finished)\n \n     @property\n@@ -206,5 +202,4 @@ def to_generation_output(self):\n             generated_tokens=self.static_outputs,\n             logprobs=[],\n             error=self.error,\n-            next_token=self.next_token,\n         )"
        },
        {
            "sha": "b46880fa4ce1aff07ad3094e611e9e9306d0b27c",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 25,
            "deletions": 22,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b1c30b1b483d95be444a3e67f983468e4aba24/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=b2b1c30b1b483d95be444a3e67f983468e4aba24",
            "patch": "@@ -20,13 +20,11 @@\n from typing import Optional\n \n import torch\n-from tokenizers.decoders import DecodeStream\n from torch import nn\n from tqdm import tqdm\n \n from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n-from ...tokenization_utils_fast import PreTrainedTokenizerFast\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n@@ -102,9 +100,6 @@ def __init__(\n \n         self.setup_static_tensors()\n \n-        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.config._name_or_path)\n-        self.decode_stream = DecodeStream(skip_special_tokens=True)\n-\n     def return_attention_mask(self) -> bool:\n         return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n \n@@ -227,18 +222,18 @@ def _handle_request_error(self, error, state: RequestState):\n         self.output_queue.put(state.to_generation_output())\n \n     @traced\n-    def prepare_next_batch(self):\n+    def prepare_next_batch(self) -> bool:\n         \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n         # Get new requests from the queue\n         self._get_new_requests()\n         if not self.scheduler.has_pending_requests():\n-            return None\n+            return False\n \n         self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n \n         self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n         if not self.requests_in_batch:\n-            return None\n+            return False\n \n         # Get the request objects for this batch\n         self.reset_static_tensors()\n@@ -291,6 +286,8 @@ def prepare_next_batch(self):\n \n         self.metrics.record_kv_cache_memory_metrics(self.cache)\n \n+        return True\n+\n     @traced\n     def _build_tensors(\n         self,\n@@ -357,7 +354,6 @@ def _sync(self):\n     def _maybe_send_output(self, state: RequestState, token: int):\n         \"\"\"Send output to the queue based on streaming mode and request state.\"\"\"\n         if self.streaming:\n-            state.next_token = self.decode_stream.step(self.tokenizer, state.static_outputs[-1])\n             self.output_queue.put(state.to_generation_output())\n         elif state.status == RequestStatus.FINISHED:\n             self.output_queue.put(state.to_generation_output())\n@@ -463,7 +459,6 @@ def __init__(\n         self.profile = getattr(generation_config, \"profile\", False)\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n-        self.decode_stream = DecodeStream(skip_special_tokens=True)\n         self.slice_inputs = slice_inputs\n \n     @traced\n@@ -534,6 +529,7 @@ def add_request(\n \n         max_new_tokens = self.generation_config.max_new_tokens if max_new_tokens is None else max_new_tokens\n \n+        # NOTE: do we want to handle a case when the user wants token ids returned instead of decoded text?\n         state = RequestState(\n             request_id=request_id,\n             prompt_ids=list(input_ids),\n@@ -548,35 +544,41 @@ def add_request(\n         return request_id\n \n     def add_requests(self, inputs: list[list[int]], **kwargs):\n-        for i, input_ids in enumerate(inputs):\n-            # Assign a predictable request ID for ordering results later\n-            req_id = f\"batch_req_{i}\"\n-            self.add_request(input_ids, request_id=req_id, **kwargs)\n+        for input_ids in inputs:\n+            self.add_request(input_ids, **kwargs)\n \n-    def get_result(self, timeout=None) -> Optional[GenerationOutput]:\n+    def get_result(self, request_id=None, timeout=None) -> Optional[GenerationOutput]:\n         \"\"\"Retrieve one result from the output queue.\n \n         Args:\n             timeout: Maximum time to wait for a result\n \n         Returns:\n-            Optional[Dict]: The result data or None if timeout\n+            Optional[GenerationOutput]: The result data or None if timeout\n         \"\"\"\n         if self._generation_thread is None and self.output_queue.empty():\n             return None\n         try:\n             result = self.output_queue.get(block=True, timeout=timeout)\n+            if request_id is not None and result.request_id != request_id:\n+                self.output_queue.put(result)\n+                return None\n             logger.debug(f\"Retrieved result for request {result.request_id}\")\n             return result\n         except queue.Empty:\n             return None\n \n     def __iter__(self):\n         \"\"\"Iterate over results as they become available.\"\"\"\n-        while (\n-            self._generation_thread is not None and self._generation_thread.is_alive() or not self.output_queue.empty()\n-        ):\n-            result = self.get_result(timeout=0.1)  # allow the model to run for 10 seconds\n+        while self._generation_thread is not None and self._generation_thread.is_alive():\n+            result = self.get_result(timeout=0.1)\n+            if result is not None:\n+                yield result\n+\n+    def request_id_iter(self, request_id):\n+        \"\"\"Iterate over results matching a specific request id as they become available.\"\"\"\n+        while self._generation_thread is not None and self._generation_thread.is_alive():\n+            result = self.get_result(request_id=request_id, timeout=0.1)\n             if result is not None:\n                 yield result\n \n@@ -637,6 +639,7 @@ def _run_generation_loop(self):\n                 self.generation_config,\n                 self.model.device,\n                 self.model.dtype,\n+                # FIXME: this is unused, why was it added?\n                 num_requests=len(self.input_queue.queue),\n                 tp_size=getattr(self.model, \"_tp_size\", None),  # Use model's actual TP setting\n             )\n@@ -681,7 +684,8 @@ def _run_generation_loop(self):\n     def _inner_generation_loop(self, batch_processor: ContinuousBatchProcessor):\n         if torch.cuda.is_available():\n             torch.cuda.synchronize()\n-        batch_processor.prepare_next_batch()\n+        if not batch_processor.prepare_next_batch():\n+            return\n         device, total, reserved, allocated = get_device_and_memory_breakdown()\n         logger.debug(f\"[Memory] Device: {device}, Total: {total}, Reserved: {reserved}, Allocated: {allocated}\")\n         if torch.cuda.is_available() and self.use_cuda_graph:\n@@ -829,7 +833,6 @@ def generate_batch(\n                                 results[req_id] = result\n                                 finished_count += 1\n                                 pbar.update(1)\n-                            logger.debug(manager.batch_processor.tokenizer.decode(result.generated_tokens))\n                         else:\n                             if not manager.is_running():\n                                 logger.error(\"Generation thread terminated unexpectedly.\")"
        },
        {
            "sha": "f83060013214cc4e8493dfb487187fe4f6531bf8",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 57,
            "deletions": 5,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/b2b1c30b1b483d95be444a3e67f983468e4aba24/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b2b1c30b1b483d95be444a3e67f983468e4aba24/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=b2b1c30b1b483d95be444a3e67f983468e4aba24",
            "patch": "@@ -19,7 +19,7 @@\n from unittest.mock import patch\n \n import aiohttp.client_exceptions\n-from huggingface_hub import AsyncInferenceClient\n+from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n from parameterized import parameterized\n \n import transformers.commands.transformers_cli as cli\n@@ -501,12 +501,65 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(port=cls.port, attn_implementation=\"sdpa_paged\")  # important: toggle continuous batching\n-        serve_command = ServeCommand(args)\n-        thread = Thread(target=serve_command.run)\n+        args = ServeArguments(\n+            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa_paged\", default_seed=42\n+        )\n+        cls.serve_command = ServeCommand(args)\n+        thread = Thread(target=cls.serve_command.run)\n         thread.daemon = True\n         thread.start()\n \n+    def test_full_request(self):\n+        \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n+\n+        request = {\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"messages\": [\n+                {\"role\": \"system\", \"content\": \"You are a sports assistant designed to craft sports programs.\"},\n+                {\"role\": \"user\", \"content\": \"Tell me what you can do.\"},\n+            ],\n+            \"stream\": True,\n+            \"max_tokens\": 30,\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+\n+        full_text = \"\"\n+        for token in all_payloads:\n+            if isinstance(token, ChatCompletionStreamOutput) and token.choices and len(token.choices) > 0:\n+                content = token.choices[0].delta.get(\"content\", \"\")\n+                full_text += content if content is not None else \"\"\n+\n+        # Verify that the system prompt went through.\n+        self.assertTrue(\n+            full_text.startswith(\n+                \"I can assist you with a wide range of tasks, from answering questions to providing information on various sports topics.\"\n+            )\n+        )\n+\n+    def test_max_tokens_not_set_in_req(self):\n+        request = {\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"messages\": [\n+                {\"role\": \"system\", \"content\": \"You are a sports assistant designed to craft sports programs.\"},\n+                {\"role\": \"user\", \"content\": \"Tell me what you can do.\"},\n+            ],\n+            \"stream\": True,\n+        }\n+        all_payloads = asyncio.run(self.run_server(request))\n+\n+        full_text = \"\"\n+        for token in all_payloads:\n+            if isinstance(token, ChatCompletionStreamOutput) and token.choices and len(token.choices) > 0:\n+                content = token.choices[0].delta.get(\"content\", \"\")\n+                full_text += content if content is not None else \"\"\n+\n+        # Verify that the system prompt went through.\n+        self.assertTrue(\n+            full_text.startswith(\n+                \"I can assist you with a wide range of tasks, from answering questions to providing information on various sports topics.\"\n+            )\n+        )\n+\n \n @require_openai\n class ServeResponsesMixin:\n@@ -537,7 +590,6 @@ def test_request(self):\n             \"max_output_tokens\": 1,\n         }\n         all_payloads = asyncio.run(self.run_server(request))\n-        print(\"ok\")\n \n         order_of_payloads = [\n             ResponseCreatedEvent,"
        }
    ],
    "stats": {
        "total": 194,
        "additions": 136,
        "deletions": 58
    }
}