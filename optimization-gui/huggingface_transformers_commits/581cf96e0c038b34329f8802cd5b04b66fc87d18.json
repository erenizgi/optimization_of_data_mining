{
    "author": "ydshieh",
    "message": "fix tied weigths issue  (#37031)\n\n* fix\n\n* comment\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "581cf96e0c038b34329f8802cd5b04b66fc87d18",
    "files": [
        {
            "sha": "ac8429728059ce386c81dadd3076c17d7307f1cb",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/581cf96e0c038b34329f8802cd5b04b66fc87d18/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/581cf96e0c038b34329f8802cd5b04b66fc87d18/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=581cf96e0c038b34329f8802cd5b04b66fc87d18",
            "patch": "@@ -784,6 +784,9 @@ def _load_state_dict_into_meta_model(\n     if is_meta_state_dict:\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n \n+    # Used to fix the issue mentioned in #37031: when loading a model with tied weights in state_dict + `tie_word_embeddings = False`,\n+    # we need to make sure they are not loaded as tied weights!\n+    data_ptrs = set()\n     for param_name, empty_param in state_dict.items():\n         if param_name not in expected_keys:\n             continue\n@@ -853,11 +856,19 @@ def _load_state_dict_into_meta_model(\n                 if is_fsdp_enabled():\n                     param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n                 module, param_type = get_module_from_name(model, param_name)\n+\n+                # avoid tied weights\n+                if param.data_ptr() in data_ptrs:\n+                    param = param.clone()\n+\n                 module.load_state_dict(\n                     {param_type: param.to(param_device)},\n                     strict=False,\n                     assign=True,\n                 )\n+\n+                # Add `data_ptr` of `model.state_dict()[param_name]` to avoid tied weights\n+                data_ptrs.add(model.state_dict()[param_name].data_ptr())\n             else:\n                 hf_quantizer.create_quantized_param(\n                     model, param, param_name, param_device, state_dict, unexpected_keys"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 11,
        "deletions": 0
    }
}