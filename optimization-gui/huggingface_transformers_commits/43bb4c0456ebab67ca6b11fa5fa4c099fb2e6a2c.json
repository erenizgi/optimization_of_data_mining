{
    "author": "rphmeier",
    "message": "Fix qwen2_5 get_rope_index tensor device locations (#37597)\n\n* Fix qwen2_5 get_rope_index tensor device locations\n\n* simpler fix\n\n* edit right file for modular model\n\n* add a test\n\n* try normalizing type to fix non-video\n\n* fix some imports\n\n* add a video forward test with dummy input",
    "sha": "43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c",
    "files": [
        {
            "sha": "4da0f59bf469af237bfca2b0c1a4d13a7f7a9683",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c",
            "patch": "@@ -1663,6 +1663,11 @@ def get_rope_index(\n                     range_tensor = torch.arange(llm_grid_t).view(-1, 1)\n                     expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)\n \n+                    ## normalize type, send to device.\n+                    second_per_grid_t = torch.as_tensor(\n+                        second_per_grid_t, dtype=range_tensor.dtype, device=range_tensor.device\n+                    )\n+\n                     time_tensor = expanded_range * second_per_grid_t * self.config.vision_config.tokens_per_second\n \n                     time_tensor_long = time_tensor.long()"
        },
        {
            "sha": "e34724c7906a4c76f7f216d65ef0c99314d841bb",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c",
            "patch": "@@ -559,6 +559,11 @@ def get_rope_index(\n                     range_tensor = torch.arange(llm_grid_t).view(-1, 1)\n                     expanded_range = range_tensor.expand(-1, llm_grid_h * llm_grid_w)\n \n+                    ## normalize type, send to device.\n+                    second_per_grid_t = torch.as_tensor(\n+                        second_per_grid_t, dtype=range_tensor.dtype, device=range_tensor.device\n+                    )\n+\n                     time_tensor = expanded_range * second_per_grid_t * self.config.vision_config.tokens_per_second\n \n                     time_tensor_long = time_tensor.long()"
        },
        {
            "sha": "21947dca3552e1e385303bf327fcef6c26b96c65",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 108,
            "deletions": 0,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=43bb4c0456ebab67ca6b11fa5fa4c099fb2e6a2c",
            "patch": "@@ -14,6 +14,7 @@\n \"\"\"Testing suite for the PyTorch Qwen2.5-VL model.\"\"\"\n \n import gc\n+import tempfile\n import unittest\n \n import requests\n@@ -27,12 +28,14 @@\n )\n from transformers.testing_utils import (\n     is_flaky,\n+    require_cv2,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n     slow,\n     torch_device,\n )\n+from transformers.utils import is_cv2_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -44,6 +47,9 @@\n )\n \n \n+if is_cv2_available():\n+    import cv2\n+\n if is_torch_available():\n     import torch\n \n@@ -262,6 +268,59 @@ def test_mismatching_num_image_tokens(self):\n                 image_grid_thw=image_grid_thw,\n             )\n \n+    def test_video_forward(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        B = self.model_tester.batch_size\n+        C = config.vision_config.in_chans\n+        T = config.vision_config.temporal_patch_size\n+        P = config.vision_config.patch_size\n+\n+        input_ids = ids_tensor([B, self.model_tester.seq_length], self.model_tester.vocab_size)\n+\n+        F = 4\n+        patch_H = self.model_tester.image_size // P\n+        patch_W = self.model_tester.image_size // P\n+        patch_T = F // T\n+        patches_per_video = patch_T * patch_H * patch_W\n+        pixel_values_videos = floats_tensor(\n+            [\n+                # first dim: batch_size * num_patches\n+                B * patches_per_video,\n+                # second dim: in_channels * temporal_patch_size * patch_size^2\n+                C * T * (P**2),\n+            ]\n+        )\n+        video_grid_thw = torch.tensor([[patch_T, patch_H, patch_W]] * B)\n+\n+        # sanity check\n+        assert pixel_values_videos.shape[0] == video_grid_thw.prod(dim=1).sum().item()\n+\n+        # Insert video token sequence\n+        input_ids[:, -1] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.video_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.image_token_id] = self.model_tester.pad_token_id\n+        input_ids[input_ids == self.model_tester.vision_start_token_id] = self.model_tester.pad_token_id\n+        input_ids[:, self.model_tester.num_image_tokens] = self.model_tester.video_token_id\n+\n+        insertion_point = self.model_tester.num_image_tokens\n+\n+        assert (B * patches_per_video) + insertion_point <= self.model_tester.seq_length\n+        for b in range(B):\n+            input_ids[b, insertion_point - 1] = self.model_tester.vision_start_token_id\n+            input_ids[b, insertion_point : insertion_point + patches_per_video] = self.model_tester.video_token_id\n+\n+        for model_class in self.all_model_classes:\n+            second_per_grid_ts = torch.tensor([1.0] * B, device=torch_device)\n+            model = model_class(config).to(torch_device)\n+            outputs = model(\n+                input_ids=input_ids,\n+                pixel_values_videos=pixel_values_videos,\n+                video_grid_thw=video_grid_thw,\n+                second_per_grid_ts=second_per_grid_ts,\n+            )\n+            self.assertIsNotNone(outputs)\n+\n     @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n     def test_feed_forward_chunking(self):\n         pass\n@@ -534,3 +593,52 @@ def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n+\n+    @slow\n+    @require_cv2\n+    def test_small_model_integration_test_with_video(self):\n+        model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n+        )\n+\n+        video_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\"\n+        messages2 = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                ],\n+            }\n+        ]\n+        text = self.processor.apply_chat_template(messages2, tokenize=False, add_generation_prompt=True)\n+\n+        with tempfile.NamedTemporaryFile(suffix=\".mp4\") as f:\n+            f.write(requests.get(video_url).content)\n+            f.flush()\n+            cap = cv2.VideoCapture(f.name)\n+\n+            frames = []\n+            while True:\n+                ret, frame = cap.read()\n+                if not ret:\n+                    break\n+                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n+                frames.append(Image.fromarray(frame_rgb).resize((224, 224), Image.BICUBIC))\n+\n+            cap.release()\n+\n+        inputs = self.processor(text=[text], videos=[frames], return_tensors=\"pt\").to(torch_device)\n+\n+        # it should not matter whether two images are the same size or not\n+        output = model.generate(**inputs, max_new_tokens=30)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            'system\\nYou are a helpful assistant.\\nuser\\nWhat is shown in this video?\\nassistant\\nThe video shows an indoor tennis court with a person standing on one side, preparing to serve the ball. The individual is dressed in athletic attire, including',\n+        ]  # fmt: skip\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        }
    ],
    "stats": {
        "total": 118,
        "additions": 118,
        "deletions": 0
    }
}