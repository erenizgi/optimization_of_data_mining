{
    "author": "Cyrilvallez",
    "message": "Remove `ALL_LAYERNORM_LAYERS` (#38922)\n\n* remove it everywhere\n\n* Update trainer_pt_utils.py\n\n* Update trainer_pt_utils.py\n\n* style\n\n* sort list in test\n\n* CIs\n\n* use recursion same way as before (for intermediate layer names)",
    "sha": "aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
    "files": [
        {
            "sha": "ecb5c89207e629e49669e5f9823e6dd9b7fb0598",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -27,8 +27,6 @@\n from ..utils.generic import GeneralInterface\n \n \n-ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n-\n logger = logging.get_logger(__name__)\n \n # Cache this result has it's a C FFI call which can be pretty time-consuming"
        },
        {
            "sha": "25b9923620a90c8406a9fa623aa0293256f0f56a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     LossKwargs,\n     auto_docstring,\n@@ -72,9 +71,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(ChameleonRMSNorm)\n-\n-\n # copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Chameleon\n # TODO(joao): add me back asap :)\n class ChameleonRotaryEmbedding(nn.Module):"
        },
        {
            "sha": "930f3f45e8827db26e3f41e88af9f46b5b91de7f",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_rope_utils import dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import LossKwargs, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -69,9 +68,6 @@ def forward(self, hidden_states):\n         return hidden_states.to(input_dtype)\n \n \n-ALL_LAYERNORM_LAYERS.append(CohereLayerNorm)\n-\n-\n class CohereRotaryEmbedding(LlamaRotaryEmbedding):\n     @torch.no_grad()\n     @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)"
        },
        {
            "sha": "24e0b557f95d0a62188ed1d000d6c9f2f46d1b50",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -34,7 +34,6 @@\n     TokenClassifierOutput,\n )\n from ....modeling_utils import PreTrainedModel\n-from ....pytorch_utils import ALL_LAYERNORM_LAYERS\n from ....utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -311,10 +310,6 @@ def forward(self, input):\n             return self.norm(input)\n \n \n-# add this layernorm class to ALL_LAYERNORM_LAYERS\n-ALL_LAYERNORM_LAYERS.append(MegaSequenceNorm)\n-\n-\n class MegaMultiDimensionDampedEma(nn.Module):\n     \"\"\"\n     Mega's Exponential Moving Average layer, largely left unmodified from the original repo with the exception of"
        },
        {
            "sha": "5672bef39c3723b754a99172ab1a04b09cd4d428",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_outputs import BaseModelOutputWithPast, MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from .configuration_granitemoe import GraniteMoeConfig\n \n@@ -145,9 +144,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(GraniteMoeRMSNorm)\n-\n-\n # Copied from transformers.models.granite.modeling_granite.GraniteRotaryEmbedding with Granite->GraniteMoe\n class GraniteMoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteMoeConfig, device=None):"
        },
        {
            "sha": "d3bba25a56428b211f9e5f874f23bfb5b87bf7f4",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_idefics import IdeficsConfig\n from .perceiver import IdeficsPerceiverResampler\n@@ -386,9 +385,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(IdeficsRMSNorm)\n-\n-\n # this was adapted from LlamaRotaryEmbedding\n class IdeficsEmbedding(torch.nn.Module):\n     def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):"
        },
        {
            "sha": "3a200ad988b8cf0ca89841ed39bdf94894861681",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_llama import LlamaConfig\n \n@@ -69,9 +68,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(LlamaRMSNorm)\n-\n-\n class LlamaRotaryEmbedding(nn.Module):\n     def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()"
        },
        {
            "sha": "8efc2f7416c763913a01d78b23abbeff5dfbc2aa",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -34,7 +34,7 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -258,8 +258,6 @@ def forward(self, hidden_states):\n     logger.warning(\"discovered apex but it failed to load, falling back to LongT5LayerNorm\")\n     pass\n \n-ALL_LAYERNORM_LAYERS.append(LongT5LayerNorm)\n-\n \n # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->LongT5\n class LongT5DenseActDense(nn.Module):"
        },
        {
            "sha": "c06fd27e368d7ea59a46fc51b8e485d3dc067522",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput, Seq2SeqLMOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n@@ -234,9 +233,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(MoshiRMSNorm)\n-\n-\n class MoshiFlexibleLinear(nn.Module):\n     def __init__(self, input_size, output_size, num_layers):\n         super().__init__()"
        },
        {
            "sha": "2cb0cdc5f0e57501b680db3a4c943d042f68ffee",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -37,7 +37,6 @@\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n from .configuration_nemotron import NemotronConfig\n \n@@ -85,9 +84,6 @@ def forward(self, input: Tensor) -> Tensor:\n             return F.layer_norm(*args)\n \n \n-ALL_LAYERNORM_LAYERS.append(NemotronLayerNorm1P)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n class NemotronRotaryEmbedding(nn.Module):\n     # Ignore copy"
        },
        {
            "sha": "f12bc2b87d35354eb2d65d740bc5c7ab8b5ab521",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -5,7 +5,6 @@\n \n from ...cache_utils import Cache\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import logging\n from ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRMSNorm, eager_attention_forward\n from ..olmo.configuration_olmo import OlmoConfig\n@@ -176,9 +175,6 @@ def forward(self, hidden_states):\n         return (self.weight * hidden_states).to(input_dtype)\n \n \n-ALL_LAYERNORM_LAYERS.append(Olmo2RMSNorm)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]"
        },
        {
            "sha": "d904deb149308950ca4362a85577c931b6a0fbcd",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, logging\n from .configuration_olmoe import OlmoeConfig\n \n@@ -142,9 +141,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(OlmoeRMSNorm)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmoe\n class OlmoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: OlmoeConfig, device=None):"
        },
        {
            "sha": "4f9e42cc94c89caa6d6314ff57ea773621c49601",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -33,7 +33,6 @@\n     Seq2SeqModelOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -96,8 +95,6 @@ def forward(self, hidden_states):\n     logger.warning(\"Discovered apex but it failed to load, falling back to Pix2StructLayerNorm\")\n     pass\n \n-ALL_LAYERNORM_LAYERS.append(Pix2StructLayerNorm)\n-\n \n class Pix2StructVisionEmbeddings(nn.Module):\n     r\"\"\""
        },
        {
            "sha": "48f930dacb6e0a079ee3046c64d49c69251df58a",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -30,7 +30,7 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPastAndCrossAttentions, Seq2SeqLMOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torch_fx_proxy, is_torchdynamo_compiling, logging\n from .configuration_pop2piano import Pop2PianoConfig\n \n@@ -88,8 +88,6 @@ def forward(self, hidden_states):\n if not _load_pop2piano_layer_norm:\n     Pop2PianoLayerNorm = FusedRMSNorm  # noqa\n \n-ALL_LAYERNORM_LAYERS.append(Pop2PianoLayerNorm)\n-\n \n # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->Pop2Piano,t5->pop2piano\n class Pop2PianoDenseActDense(nn.Module):"
        },
        {
            "sha": "50d19a33b7315a1a6afd45558747a2bc1d95f4ee",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithNoAttention, CausalLMOutput\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import is_torchdynamo_compiling\n from .configuration_recurrent_gemma import RecurrentGemmaConfig\n@@ -58,9 +57,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(RecurrentGemmaRMSNorm)\n-\n-\n class RecurrentGemmaRotaryEmbedding(nn.Module):\n     def __init__(self, dim, base=10000, device=None):\n         super().__init__()"
        },
        {
            "sha": "cf613ee5b82bc5ffddd78edb0675d06636744762",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -34,7 +34,7 @@\n     Seq2SeqMoEOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -240,9 +240,6 @@ def forward(self, hidden_states):\n         return self.weight * hidden_states\n \n \n-ALL_LAYERNORM_LAYERS.append(SwitchTransformersLayerNorm)\n-\n-\n # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->SwitchTransformers\n class SwitchTransformersDenseActDense(nn.Module):\n     def __init__(self, config: SwitchTransformersConfig):"
        },
        {
            "sha": "b099d6cea02a2809dc62fba211a671cf157e7256",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -38,7 +38,7 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS, find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     DUMMY_INPUTS,\n     DUMMY_MASK,\n@@ -273,8 +273,6 @@ def forward(self, hidden_states):\n     logger.warning(\"discovered apex but it failed to load, falling back to T5LayerNorm\")\n     pass\n \n-ALL_LAYERNORM_LAYERS.append(T5LayerNorm)\n-\n \n class T5DenseActDense(nn.Module):\n     def __init__(self, config: T5Config):"
        },
        {
            "sha": "6d77801d4ef67db4003182b954b4682c29d6402b",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba import ZambaConfig\n@@ -81,9 +80,6 @@ def extra_repr(self):\n         return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n-ALL_LAYERNORM_LAYERS.append(ZambaRMSNorm)\n-\n-\n # Copied from transformers.models.llama.modeling_llama.repeat_kv\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\""
        },
        {
            "sha": "a342f17059d51913eadb17923cf8f04ee4296eb3",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -73,7 +73,6 @@\n from .optimization import Adafactor, get_scheduler\n from .processing_utils import ProcessorMixin\n from .pytorch_utils import (\n-    ALL_LAYERNORM_LAYERS,\n     is_torch_greater_or_equal_than_2_3,\n )\n from .tokenization_utils_base import PreTrainedTokenizerBase\n@@ -1186,9 +1185,10 @@ def get_decay_parameter_names(self, model) -> list[str]:\n \n         This function filters out parameters in two ways:\n         1. By layer type (instances of layers specified in ALL_LAYERNORM_LAYERS)\n-        2. By parameter name patterns (containing 'bias', 'layernorm', or 'rmsnorm')\n+        2. By parameter name patterns (containing 'bias', or variation of 'norm')\n         \"\"\"\n-        decay_parameters = get_parameter_names(model, ALL_LAYERNORM_LAYERS, [\"bias\", \"layernorm\", \"rmsnorm\"])\n+        forbidden_name_patterns = [r\"bias\", r\"layernorm\", r\"rmsnorm\", r\"(?:^|\\.)norm(?:$|\\.)\", r\"_norm(?:$|\\.)\"]\n+        decay_parameters = get_parameter_names(model, [nn.LayerNorm], forbidden_name_patterns)\n         return decay_parameters\n \n     def create_optimizer(self):"
        },
        {
            "sha": "2fb362605855962f5d0514a41e87d82869c4a806",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa42987c1e3a0cf1c18a5783274f0d8cc8409b53/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
            "patch": "@@ -21,6 +21,7 @@\n import json\n import math\n import os\n+import re\n import sys\n import warnings\n from collections.abc import Iterator, Mapping\n@@ -1124,21 +1125,25 @@ def get_parameter_names(model, forbidden_layer_types, forbidden_layer_names=None\n     \"\"\"\n     Returns the names of the model parameters that are not inside a forbidden layer.\n     \"\"\"\n-    if forbidden_layer_names is None:\n-        forbidden_layer_names = []\n+    forbidden_layer_patterns = (\n+        [re.compile(pattern) for pattern in forbidden_layer_names] if forbidden_layer_names is not None else []\n+    )\n     result = []\n     for name, child in model.named_children():\n         child_params = get_parameter_names(child, forbidden_layer_types, forbidden_layer_names)\n         result += [\n             f\"{name}.{n}\"\n             for n in child_params\n             if not isinstance(child, tuple(forbidden_layer_types))\n-            and not any(forbidden in f\"{name}.{n}\".lower() for forbidden in forbidden_layer_names)\n+            and not any(pattern.search(f\"{name}.{n}\".lower()) for pattern in forbidden_layer_patterns)\n         ]\n     # Add model specific parameters that are not in any child\n     result += [\n-        k for k in model._parameters.keys() if not any(forbidden in k.lower() for forbidden in forbidden_layer_names)\n+        k\n+        for k in model._parameters.keys()\n+        if not any(pattern.search(k.lower()) for pattern in forbidden_layer_patterns)\n     ]\n+\n     return result\n \n "
        }
    ],
    "stats": {
        "total": 90,
        "additions": 16,
        "deletions": 74
    }
}