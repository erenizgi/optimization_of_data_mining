{
    "author": "yonigozlan",
    "message": "Update SAM/SAM HQ attention implementation + fix Cuda sync issues (#39386)\n\n* update attention implementation and improve inference speed\n\n* modular sam_hq + fix integration tests on A10\n\n* fixup\n\n* fix after review\n\n* softmax in correct place\n\n* return attn_weights in sam/sam_hq",
    "sha": "433d2a23d734914e0a9903c6b7039d28daa6cbc1",
    "files": [
        {
            "sha": "8a607f237c43d850ae4a864e276f89f523b67258",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 70,
            "deletions": 91,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=433d2a23d734914e0a9903c6b7039d28daa6cbc1",
            "patch": "@@ -16,7 +16,7 @@\n \n import collections\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     ModelOutput,\n@@ -178,6 +178,28 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class SamAttention(nn.Module):\n     \"\"\"\n     SAM's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n@@ -186,6 +208,7 @@ class SamAttention(nn.Module):\n \n     def __init__(self, config, downsample_rate=None):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n \n         downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n@@ -194,25 +217,32 @@ def __init__(self, config, downsample_rate=None):\n         self.num_attention_heads = config.num_attention_heads\n         if self.internal_dim % config.num_attention_heads != 0:\n             raise ValueError(\"num_attention_heads must divide hidden_size.\")\n+        self.scaling = (self.internal_dim // config.num_attention_heads) ** -0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.out_proj = nn.Linear(self.internal_dim, self.hidden_size)\n \n+        self.is_causal = False\n+\n     def _separate_heads(self, hidden_states: Tensor, num_attention_heads: int) -> Tensor:\n         batch, point_batch_size, n_tokens, channel = hidden_states.shape\n         c_per_head = channel // num_attention_heads\n         hidden_states = hidden_states.reshape(batch * point_batch_size, n_tokens, num_attention_heads, c_per_head)\n         return hidden_states.transpose(1, 2)\n \n     def _recombine_heads(self, hidden_states: Tensor, point_batch_size: int) -> Tensor:\n-        batch, n_heads, n_tokens, c_per_head = hidden_states.shape\n-        hidden_states = hidden_states.transpose(1, 2)\n+        batch, n_tokens, n_heads, c_per_head = hidden_states.shape\n         return hidden_states.reshape(batch // point_batch_size, point_batch_size, n_tokens, n_heads * c_per_head)\n \n     def forward(\n-        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+        self,\n+        query: Tensor,\n+        key: Tensor,\n+        value: Tensor,\n+        attention_similarity: Optional[Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Tensor:\n         # Input projections\n         query = self.q_proj(query)\n@@ -226,64 +256,26 @@ def forward(\n         value = self._separate_heads(value, self.num_attention_heads)\n \n         # SamAttention\n-        _, _, _, c_per_head = query.shape\n-        attn = query @ key.permute(0, 1, 3, 2)  # batch_size * point_batch_size  x N_heads x N_tokens x N_tokens\n-        attn = attn / (c_per_head**0.5)\n-        attn = torch.softmax(attn, dim=-1)\n-\n-        if attention_similarity is not None:\n-            attn = attn + attention_similarity\n-            attn = torch.softmax(attn, dim=-1)\n-\n-        # Get output\n-        out = attn @ value\n-        out = self._recombine_heads(out, point_batch_size)\n-        out = self.out_proj(out)\n-\n-        return out\n-\n-\n-class SamSdpaAttention(SamAttention):\n-    \"\"\"\n-    SAM's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n-    values. Using SDPA instead of the default attention.\n-    \"\"\"\n-\n-    def __init__(self, config, downsample_rate=None):\n-        super().__init__(config, downsample_rate)\n-\n-    def forward(\n-        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n-    ) -> Tensor:\n-        # Input projections\n-        query = self.q_proj(query)\n-        key = self.k_proj(key)\n-        value = self.v_proj(value)\n-\n-        point_batch_size = query.shape[1]\n-        # Separate into heads\n-        query = self._separate_heads(query, self.num_attention_heads)\n-        key = self._separate_heads(key, self.num_attention_heads)\n-        value = self._separate_heads(value, self.num_attention_heads)\n-\n-        # Scaled dot product attention\n-        attn_mask = None\n-        if attention_similarity is not None:\n-            attn_mask = attention_similarity.unsqueeze(1).expand(-1, self.num_attention_heads, -1, -1)\n-\n-        out = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)\n-\n-        # Get output\n-        out = self._recombine_heads(out, point_batch_size)\n-        out = self.out_proj(out)\n-\n-        return out\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_similarity,\n+            dropout=0.0 if not self.training else self.dropout_p,\n+            scaling=self.scaling,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n \n+        attn_output = self._recombine_heads(attn_output, point_batch_size)\n+        attn_output = self.out_proj(attn_output)\n \n-SAM_ATTENTION_CLASSES = {\n-    \"eager\": SamAttention,\n-    \"sdpa\": SamSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class SamTwoWayAttentionBlock(nn.Module):\n@@ -306,21 +298,17 @@ def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_\n         self.hidden_size = config.hidden_size\n         self.layer_norm_eps = config.layer_norm_eps\n \n-        self.self_attn = SAM_ATTENTION_CLASSES[config._attn_implementation](config, downsample_rate=1)\n+        self.self_attn = SamAttention(config, downsample_rate=1)\n         self.layer_norm1 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n-        self.cross_attn_token_to_image = SAM_ATTENTION_CLASSES[config._attn_implementation](\n-            config, downsample_rate=attention_downsample_rate\n-        )\n+        self.cross_attn_token_to_image = SamAttention(config, downsample_rate=attention_downsample_rate)\n         self.layer_norm2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.mlp = SamMLPBlock(config)\n         self.layer_norm3 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.layer_norm4 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n-        self.cross_attn_image_to_token = SAM_ATTENTION_CLASSES[config._attn_implementation](\n-            config, downsample_rate=attention_downsample_rate\n-        )\n+        self.cross_attn_image_to_token = SamAttention(config, downsample_rate=attention_downsample_rate)\n         self.skip_first_layer_pe = skip_first_layer_pe\n \n     def forward(\n@@ -330,21 +318,22 @@ def forward(\n         query_point_embedding: Tensor,\n         key_point_embedding: Tensor,\n         attention_similarity: Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         # Self attention block\n         if self.skip_first_layer_pe:\n-            queries = self.self_attn(query=queries, key=queries, value=queries)\n+            queries, _ = self.self_attn(query=queries, key=queries, value=queries)\n         else:\n             query = queries + query_point_embedding\n-            attn_out = self.self_attn(query=query, key=query, value=queries)\n+            attn_out, _ = self.self_attn(query=query, key=query, value=queries)\n             queries = queries + attn_out\n         queries = self.layer_norm1(queries)\n \n         # Cross attention block, tokens attending to image embedding\n         query = queries + query_point_embedding\n         key = keys + key_point_embedding\n \n-        attn_out = self.cross_attn_token_to_image(\n+        attn_out, _ = self.cross_attn_token_to_image(\n             query=query, key=key, value=keys, attention_similarity=attention_similarity\n         )\n         queries = queries + attn_out\n@@ -360,7 +349,7 @@ def forward(\n         query = queries + query_point_embedding\n         key = keys + key_point_embedding\n \n-        attn_out = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n+        attn_out, _ = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n         keys = keys + attn_out\n \n         keys = self.layer_norm4(keys)\n@@ -378,7 +367,7 @@ def __init__(self, config: SamMaskDecoderConfig):\n         for i in range(self.num_hidden_layers):\n             self.layers.append(SamTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n \n-        self.final_attn_token_to_image = SAM_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.final_attn_token_to_image = SamAttention(config)\n         self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n \n     def forward(\n@@ -388,6 +377,7 @@ def forward(\n         image_positional_embeddings: Tensor,\n         attention_similarity: Tensor,\n         target_embedding=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         if image_embeddings is None:\n             raise ValueError(\"You have to specify an image_embedding\")\n@@ -410,12 +400,13 @@ def forward(\n                 query_point_embedding=point_embeddings,\n                 key_point_embedding=image_positional_embeddings,\n                 attention_similarity=attention_similarity,\n+                **kwargs,\n             )\n         # Apply the final attenion layer from the points to the image\n         query = queries + point_embeddings\n         key = keys + image_positional_embeddings\n \n-        attn_out = self.final_attn_token_to_image(query=query, key=key, value=keys)\n+        attn_out, _ = self.final_attn_token_to_image(query=query, key=key, value=keys)\n \n         queries = queries + attn_out\n         queries = self.layer_norm_final_attn(queries)\n@@ -501,12 +492,12 @@ def forward(\n                 Whether to return multiple masks or a single mask.\n         \"\"\"\n         batch_size, num_channels, height, width = image_embeddings.shape\n-        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        point_batch_size = sparse_prompt_embeddings.shape[1] if sparse_prompt_embeddings is not None else 1\n         # Concatenate output tokens\n         output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight], dim=0)\n         output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n \n-        if sparse_prompt_embeddings.sum().item() != 0:\n+        if sparse_prompt_embeddings is not None:\n             tokens = torch.cat((output_tokens, sparse_prompt_embeddings), dim=2)\n         else:\n             tokens = output_tokens\n@@ -611,7 +602,7 @@ def forward(self, masks):\n \n \n class SamPromptEncoder(nn.Module):\n-    def __init__(self, config: SamPromptEncoderConfig):\n+    def __init__(self, config: SamConfig):\n         super().__init__()\n         self.shared_embedding = SamPositionalEmbedding(config.vision_config)\n         config = config.prompt_encoder_config\n@@ -645,11 +636,7 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n \n         # This is required for the ONNX export. The dtype, device need to be explicitly\n         # specified as otherwise torch.onnx.export interprets as double\n-        point_embedding = torch.where(\n-            labels[..., None] != -10,\n-            point_embedding,\n-            torch.tensor(0.0, dtype=point_embedding.dtype, device=point_embedding.device),\n-        )\n+        point_embedding = torch.where(labels[..., None] != -10, point_embedding, torch.zeros_like(point_embedding))\n \n         point_embedding = torch.where(\n             (labels == 0)[:, :, :, None],\n@@ -696,9 +683,8 @@ def forward(\n         \"\"\"\n         sparse_embeddings = None\n         batch_size = 1\n-        target_device = self.shared_embedding.positional_embedding.device\n         if input_points is not None:\n-            batch_size, point_batch_size = input_points.shape[:2]\n+            batch_size = input_points.shape[0]\n             if input_labels is None:\n                 raise ValueError(\"If points are provided, labels must also be provided.\")\n             point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))\n@@ -717,9 +703,6 @@ def forward(\n                 batch_size, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n             )\n \n-        if sparse_embeddings is None:\n-            sparse_embeddings = torch.zeros((batch_size, 1, 1, self.hidden_size), device=target_device)\n-\n         return sparse_embeddings, dense_embeddings\n \n \n@@ -1184,10 +1167,6 @@ def get_image_embeddings(self, pixel_values, **kwargs: Unpack[TransformersKwargs\n         Args:\n             pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n                 Input pixel values\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers.\n         \"\"\"\n         vision_output = self.vision_encoder(\n             pixel_values,"
        },
        {
            "sha": "288d4134d2058e8eb41a9e75b1046c7e424fba5a",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 72,
            "deletions": 89,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=433d2a23d734914e0a9903c6b7039d28daa6cbc1",
            "patch": "@@ -21,7 +21,7 @@\n # limitations under the License.\n import collections\n from dataclasses import dataclass\n-from typing import Optional, Union\n+from typing import Callable, Optional, Union\n \n import numpy as np\n import torch\n@@ -34,7 +34,7 @@\n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n from .configuration_sam_hq import SamHQConfig, SamHQMaskDecoderConfig, SamHQPromptEncoderConfig, SamHQVisionConfig\n@@ -601,6 +601,28 @@ def forward(self, x: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class SamHQAttention(nn.Module):\n     \"\"\"\n     SAM_HQ's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n@@ -609,6 +631,7 @@ class SamHQAttention(nn.Module):\n \n     def __init__(self, config, downsample_rate=None):\n         super().__init__()\n+        self.config = config\n         self.hidden_size = config.hidden_size\n \n         downsample_rate = config.attention_downsample_rate if downsample_rate is None else downsample_rate\n@@ -617,25 +640,32 @@ def __init__(self, config, downsample_rate=None):\n         self.num_attention_heads = config.num_attention_heads\n         if self.internal_dim % config.num_attention_heads != 0:\n             raise ValueError(\"num_attention_heads must divide hidden_size.\")\n+        self.scaling = (self.internal_dim // config.num_attention_heads) ** -0.5\n \n         self.q_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.k_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.v_proj = nn.Linear(self.hidden_size, self.internal_dim)\n         self.out_proj = nn.Linear(self.internal_dim, self.hidden_size)\n \n+        self.is_causal = False\n+\n     def _separate_heads(self, hidden_states: Tensor, num_attention_heads: int) -> Tensor:\n         batch, point_batch_size, n_tokens, channel = hidden_states.shape\n         c_per_head = channel // num_attention_heads\n         hidden_states = hidden_states.reshape(batch * point_batch_size, n_tokens, num_attention_heads, c_per_head)\n         return hidden_states.transpose(1, 2)\n \n     def _recombine_heads(self, hidden_states: Tensor, point_batch_size: int) -> Tensor:\n-        batch, n_heads, n_tokens, c_per_head = hidden_states.shape\n-        hidden_states = hidden_states.transpose(1, 2)\n+        batch, n_tokens, n_heads, c_per_head = hidden_states.shape\n         return hidden_states.reshape(batch // point_batch_size, point_batch_size, n_tokens, n_heads * c_per_head)\n \n     def forward(\n-        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+        self,\n+        query: Tensor,\n+        key: Tensor,\n+        value: Tensor,\n+        attention_similarity: Optional[Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Tensor:\n         # Input projections\n         query = self.q_proj(query)\n@@ -649,64 +679,26 @@ def forward(\n         value = self._separate_heads(value, self.num_attention_heads)\n \n         # SamHQAttention\n-        _, _, _, c_per_head = query.shape\n-        attn = query @ key.permute(0, 1, 3, 2)  # batch_size * point_batch_size  x N_heads x N_tokens x N_tokens\n-        attn = attn / (c_per_head**0.5)\n-        attn = torch.softmax(attn, dim=-1)\n-\n-        if attention_similarity is not None:\n-            attn = attn + attention_similarity\n-            attn = torch.softmax(attn, dim=-1)\n-\n-        # Get output\n-        out = attn @ value\n-        out = self._recombine_heads(out, point_batch_size)\n-        out = self.out_proj(out)\n-\n-        return out\n-\n-\n-class SamHQSdpaAttention(SamHQAttention):\n-    \"\"\"\n-    SAM_HQ's attention layer that allows for downscaling the size of the embedding after projection to queries, keys, and\n-    values. Using SDPA instead of the default attention.\n-    \"\"\"\n-\n-    def __init__(self, config, downsample_rate=None):\n-        super().__init__(config, downsample_rate)\n-\n-    def forward(\n-        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n-    ) -> Tensor:\n-        # Input projections\n-        query = self.q_proj(query)\n-        key = self.k_proj(key)\n-        value = self.v_proj(value)\n-\n-        point_batch_size = query.shape[1]\n-        # Separate into heads\n-        query = self._separate_heads(query, self.num_attention_heads)\n-        key = self._separate_heads(key, self.num_attention_heads)\n-        value = self._separate_heads(value, self.num_attention_heads)\n-\n-        # Scaled dot product attention\n-        attn_mask = None\n-        if attention_similarity is not None:\n-            attn_mask = attention_similarity.unsqueeze(1).expand(-1, self.num_attention_heads, -1, -1)\n-\n-        out = F.scaled_dot_product_attention(query, key, value, attn_mask=attn_mask)\n-\n-        # Get output\n-        out = self._recombine_heads(out, point_batch_size)\n-        out = self.out_proj(out)\n-\n-        return out\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query,\n+            key,\n+            value,\n+            attention_mask=attention_similarity,\n+            dropout=0.0 if not self.training else self.dropout_p,\n+            scaling=self.scaling,\n+            is_causal=self.is_causal,\n+            **kwargs,\n+        )\n \n+        attn_output = self._recombine_heads(attn_output, point_batch_size)\n+        attn_output = self.out_proj(attn_output)\n \n-SAM_HQ_ATTENTION_CLASSES = {\n-    \"eager\": SamHQAttention,\n-    \"sdpa\": SamHQSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class SamHQTwoWayAttentionBlock(nn.Module):\n@@ -729,21 +721,17 @@ def __init__(self, config, attention_downsample_rate: int = 2, skip_first_layer_\n         self.hidden_size = config.hidden_size\n         self.layer_norm_eps = config.layer_norm_eps\n \n-        self.self_attn = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](config, downsample_rate=1)\n+        self.self_attn = SamHQAttention(config, downsample_rate=1)\n         self.layer_norm1 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n-        self.cross_attn_token_to_image = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](\n-            config, downsample_rate=attention_downsample_rate\n-        )\n+        self.cross_attn_token_to_image = SamHQAttention(config, downsample_rate=attention_downsample_rate)\n         self.layer_norm2 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.mlp = SamHQMLPBlock(config)\n         self.layer_norm3 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n \n         self.layer_norm4 = nn.LayerNorm(self.hidden_size, eps=self.layer_norm_eps)\n-        self.cross_attn_image_to_token = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](\n-            config, downsample_rate=attention_downsample_rate\n-        )\n+        self.cross_attn_image_to_token = SamHQAttention(config, downsample_rate=attention_downsample_rate)\n         self.skip_first_layer_pe = skip_first_layer_pe\n \n     def forward(\n@@ -753,21 +741,22 @@ def forward(\n         query_point_embedding: Tensor,\n         key_point_embedding: Tensor,\n         attention_similarity: Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         # Self attention block\n         if self.skip_first_layer_pe:\n-            queries = self.self_attn(query=queries, key=queries, value=queries)\n+            queries, _ = self.self_attn(query=queries, key=queries, value=queries)\n         else:\n             query = queries + query_point_embedding\n-            attn_out = self.self_attn(query=query, key=query, value=queries)\n+            attn_out, _ = self.self_attn(query=query, key=query, value=queries)\n             queries = queries + attn_out\n         queries = self.layer_norm1(queries)\n \n         # Cross attention block, tokens attending to image embedding\n         query = queries + query_point_embedding\n         key = keys + key_point_embedding\n \n-        attn_out = self.cross_attn_token_to_image(\n+        attn_out, _ = self.cross_attn_token_to_image(\n             query=query, key=key, value=keys, attention_similarity=attention_similarity\n         )\n         queries = queries + attn_out\n@@ -783,7 +772,7 @@ def forward(\n         query = queries + query_point_embedding\n         key = keys + key_point_embedding\n \n-        attn_out = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n+        attn_out, _ = self.cross_attn_image_to_token(query=key, key=query, value=queries)\n         keys = keys + attn_out\n \n         keys = self.layer_norm4(keys)\n@@ -801,7 +790,7 @@ def __init__(self, config: SamHQMaskDecoderConfig):\n         for i in range(self.num_hidden_layers):\n             self.layers.append(SamHQTwoWayAttentionBlock(config, skip_first_layer_pe=(i == 0)))\n \n-        self.final_attn_token_to_image = SAM_HQ_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.final_attn_token_to_image = SamHQAttention(config)\n         self.layer_norm_final_attn = nn.LayerNorm(config.hidden_size)\n \n     def forward(\n@@ -811,6 +800,7 @@ def forward(\n         image_positional_embeddings: Tensor,\n         attention_similarity: Tensor,\n         target_embedding=None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         if image_embeddings is None:\n             raise ValueError(\"You have to specify an image_embedding\")\n@@ -833,12 +823,13 @@ def forward(\n                 query_point_embedding=point_embeddings,\n                 key_point_embedding=image_positional_embeddings,\n                 attention_similarity=attention_similarity,\n+                **kwargs,\n             )\n         # Apply the final attenion layer from the points to the image\n         query = queries + point_embeddings\n         key = keys + image_positional_embeddings\n \n-        attn_out = self.final_attn_token_to_image(query=query, key=key, value=keys)\n+        attn_out, _ = self.final_attn_token_to_image(query=query, key=key, value=keys)\n \n         queries = queries + attn_out\n         queries = self.layer_norm_final_attn(queries)\n@@ -957,7 +948,7 @@ def forward(\n                 - (Optional) A tuple containing attention tensors if output_attentions is True.\n         \"\"\"\n         batch_size, num_channels, height, width = image_embeddings.shape\n-        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        point_batch_size = sparse_prompt_embeddings.shape[1] if sparse_prompt_embeddings is not None else 1\n \n         has_intermediate = intermediate_embeddings is not None and len(intermediate_embeddings) > 0\n \n@@ -980,7 +971,7 @@ def forward(\n         output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight, self.hq_token.weight], dim=0)\n         output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n \n-        if torch.any(sparse_prompt_embeddings != 0):\n+        if sparse_prompt_embeddings is not None:\n             tokens = torch.cat([output_tokens, sparse_prompt_embeddings], dim=2)\n         else:\n             tokens = output_tokens\n@@ -1147,7 +1138,7 @@ def forward(self, masks):\n \n \n class SamHQPromptEncoder(nn.Module):\n-    def __init__(self, config: SamHQPromptEncoderConfig):\n+    def __init__(self, config: SamHQConfig):\n         super().__init__()\n         self.shared_embedding = SamHQPositionalEmbedding(config.vision_config)\n         config = config.prompt_encoder_config\n@@ -1181,11 +1172,7 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n \n         # This is required for the ONNX export. The dtype, device need to be explicitly\n         # specified as otherwise torch.onnx.export interprets as double\n-        point_embedding = torch.where(\n-            labels[..., None] != -10,\n-            point_embedding,\n-            torch.tensor(0.0, dtype=point_embedding.dtype, device=point_embedding.device),\n-        )\n+        point_embedding = torch.where(labels[..., None] != -10, point_embedding, torch.zeros_like(point_embedding))\n \n         point_embedding = torch.where(\n             (labels == 0)[:, :, :, None],\n@@ -1232,9 +1219,8 @@ def forward(\n         \"\"\"\n         sparse_embeddings = None\n         batch_size = 1\n-        target_device = self.shared_embedding.positional_embedding.device\n         if input_points is not None:\n-            batch_size, point_batch_size = input_points.shape[:2]\n+            batch_size = input_points.shape[0]\n             if input_labels is None:\n                 raise ValueError(\"If points are provided, labels must also be provided.\")\n             point_embeddings = self._embed_points(input_points, input_labels, pad=(input_boxes is None))\n@@ -1253,9 +1239,6 @@ def forward(\n                 batch_size, -1, self.image_embedding_size[0], self.image_embedding_size[1]\n             )\n \n-        if sparse_embeddings is None:\n-            sparse_embeddings = torch.zeros((batch_size, 1, 1, self.hidden_size), device=target_device)\n-\n         return sparse_embeddings, dense_embeddings\n \n \n@@ -1517,8 +1500,8 @@ def forward(\n         return SamHQImageSegmentationOutput(\n             iou_scores=mask_decoder_output[1],\n             pred_masks=mask_decoder_output[0],\n-            vision_hidden_states=vision_outputs.hidden_states,\n-            vision_attentions=vision_outputs.attentions,\n+            vision_hidden_states=vision_outputs.hidden_states if pixel_values is not None else None,\n+            vision_attentions=vision_outputs.attentions if pixel_values is not None else None,\n         )\n \n "
        },
        {
            "sha": "67399295c6693538ef5a36b18a215ff4ca77e5a5",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/433d2a23d734914e0a9903c6b7039d28daa6cbc1/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=433d2a23d734914e0a9903c6b7039d28daa6cbc1",
            "patch": "@@ -327,7 +327,7 @@ def forward(\n                 - (Optional) A tuple containing attention tensors if output_attentions is True.\n         \"\"\"\n         batch_size, num_channels, height, width = image_embeddings.shape\n-        point_batch_size = sparse_prompt_embeddings.shape[1]\n+        point_batch_size = sparse_prompt_embeddings.shape[1] if sparse_prompt_embeddings is not None else 1\n \n         has_intermediate = intermediate_embeddings is not None and len(intermediate_embeddings) > 0\n \n@@ -350,7 +350,7 @@ def forward(\n         output_tokens = torch.cat([self.iou_token.weight, self.mask_tokens.weight, self.hq_token.weight], dim=0)\n         output_tokens = output_tokens.repeat(batch_size, point_batch_size, 1, 1)\n \n-        if torch.any(sparse_prompt_embeddings != 0):\n+        if sparse_prompt_embeddings is not None:\n             tokens = torch.cat([output_tokens, sparse_prompt_embeddings], dim=2)\n         else:\n             tokens = output_tokens\n@@ -641,8 +641,8 @@ def forward(\n         return SamHQImageSegmentationOutput(\n             iou_scores=mask_decoder_output[1],\n             pred_masks=mask_decoder_output[0],\n-            vision_hidden_states=vision_outputs.hidden_states,\n-            vision_attentions=vision_outputs.attentions,\n+            vision_hidden_states=vision_outputs.hidden_states if pixel_values is not None else None,\n+            vision_attentions=vision_outputs.attentions if pixel_values is not None else None,\n         )\n \n "
        },
        {
            "sha": "192f7c8b02d0eeae19dadf3496aef0cb7c7ff1fd",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/433d2a23d734914e0a9903c6b7039d28daa6cbc1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/433d2a23d734914e0a9903c6b7039d28daa6cbc1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=433d2a23d734914e0a9903c6b7039d28daa6cbc1",
            "patch": "@@ -806,7 +806,7 @@ def test_inference_mask_generation_no_point(self):\n         expectations = Expectations(\n             {\n                 (None, None): [-13.1695, -14.6201, -14.8989],\n-                (\"cuda\", 8): [-13.1668, -14.6182, -14.8970],\n+                (\"cuda\", 8): [-7.6769, -9.6935, -9.8773],\n             }\n         )\n         EXPECTED_MASKS = torch.tensor(expectations.get_expectation()).to(torch_device)\n@@ -831,9 +831,9 @@ def test_inference_mask_generation_one_point_one_bb(self):\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n         masks = outputs.pred_masks[0, 0, 0, 0, :3]\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9700), atol=2e-4))\n-        self.assertTrue(\n-            torch.allclose(masks, torch.tensor([-29.9144, -30.0546, -30.9526]).to(torch_device), atol=3e-2)\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9700).to(torch_device), atol=2e-4, rtol=2e-4)\n+        torch.testing.assert_close(\n+            masks, torch.tensor([-9.2033, -8.5505, -7.1361]).to(torch_device), atol=3e-2, rtol=3e-2\n         )\n \n     def test_inference_mask_generation_batched_points_batched_images(self):\n@@ -895,7 +895,7 @@ def test_inference_mask_generation_batched_points_batched_images(self):\n         expectations = Expectations(\n             {\n                 (None, None): [-40.2445, -37.4300, -38.1577],\n-                (\"cuda\", 8): [-40.2351, -37.4334, -38.1526],\n+                (\"cuda\", 8): [-14.1195, -17.2663, -13.7805],\n             }\n         )\n         EXPECTED_MASKS = torch.tensor(expectations.get_expectation()).to(torch_device)"
        }
    ],
    "stats": {
        "total": 340,
        "additions": 151,
        "deletions": 189
    }
}