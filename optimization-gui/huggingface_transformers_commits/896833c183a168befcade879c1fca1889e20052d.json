{
    "author": "Cyrilvallez",
    "message": "Fix some tests (especially compile with fullgraph=True on Python<3.11) (#38319)\n\n* fix tests\n\n* better fix for python<3.11\n\n* fixes\n\n* style",
    "sha": "896833c183a168befcade879c1fca1889e20052d",
    "files": [
        {
            "sha": "286d03a55982c2146d1bc58ca588722b5af1ef6c",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -2232,7 +2232,7 @@ def _prefetch_next_layer(self, layer_idx: int) -> None:\n \n     def _prefetch_layer_in_context(self, layer_idx: int) -> None:\n         \"\"\"Performs the actual copy of the layer to device cache.\"\"\"\n-        if len(self.key_cache) >= layer_idx:\n+        if len(self.key_cache) > layer_idx:\n             self.device_key_cache[self.active_device_layer].copy_(self.key_cache[layer_idx], non_blocking=True)\n             self.device_value_cache[self.active_device_layer].copy_(self.value_cache[layer_idx], non_blocking=True)\n         # The layer was not yet initialized"
        },
        {
            "sha": "bd4b30a3d125380c74e2945fedda83ed82a0f335",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 34,
            "deletions": 58,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -11,7 +11,6 @@\n # specific language governing permissions and limitations under the License.\n \n import logging\n-from contextlib import contextmanager\n from typing import Callable, Optional\n \n import torch\n@@ -110,14 +109,13 @@ def export(\n         example_input_ids = input_ids if input_ids is not None else torch.tensor([[1]], dtype=torch.long)\n         example_cache_position = cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long)\n \n-        with patch_mask_interface():\n-            exported_program = torch.export.export(\n-                self.model,\n-                args=(example_input_ids, example_cache_position),\n-                kwargs={},\n-                dynamic_shapes=dynamic_shapes,\n-                strict=strict if strict is not None else True,\n-            )\n+        exported_program = torch.export.export(\n+            self.model,\n+            args=(example_input_ids, example_cache_position),\n+            kwargs={},\n+            dynamic_shapes=dynamic_shapes,\n+            strict=strict if strict is not None else True,\n+        )\n         return exported_program\n \n     @staticmethod\n@@ -456,24 +454,6 @@ def forward(\n         return outputs.logits\n \n \n-@contextmanager\n-def patch_mask_interface():\n-    \"\"\"\n-    Context manager to locally use a simple dict instead of `AttentionMaskInterface`, as otherwise export will fail\n-    with `strict=True` due to dynamo skip rules, i.e. `torch._dynamo.exc.Unsupported: 'inline in skipfiles:\n-    Mapping.__contains__ | __contains__, skipped according trace_rules.lookup SKIP_DIRS'`.\n-    Note that this seem to be an issue only for python<3.11.\n-    \"\"\"\n-    import transformers\n-\n-    original = transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS\n-    transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS = ALL_MASK_ATTENTION_FUNCTIONS._global_mapping\n-    try:\n-        yield\n-    finally:\n-        transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS = original\n-\n-\n def convert_and_export_with_cache(\n     model: PreTrainedModel,\n     example_input_ids: Optional[torch.Tensor] = None,\n@@ -515,14 +495,13 @@ def convert_and_export_with_cache(\n         )\n \n         if is_torch_greater_or_equal(\"2.6.0\"):\n-            with patch_mask_interface():\n-                exported_program = torch.export.export(\n-                    TorchExportableModuleWithStaticCache(model),\n-                    args=(example_input_ids, example_cache_position),\n-                    kwargs={},\n-                    dynamic_shapes=dynamic_shapes,\n-                    strict=strict if strict is not None else True,\n-                )\n+            exported_program = torch.export.export(\n+                TorchExportableModuleWithStaticCache(model),\n+                args=(example_input_ids, example_cache_position),\n+                kwargs={},\n+                dynamic_shapes=dynamic_shapes,\n+                strict=strict if strict is not None else True,\n+            )\n         else:\n             if dynamic_shapes is not None:\n                 logging.warning(\n@@ -534,14 +513,13 @@ def convert_and_export_with_cache(\n             #\n             # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n             # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n-            with patch_mask_interface():\n-                exported_program = torch.export._trace._export(\n-                    TorchExportableModuleWithStaticCache(model),\n-                    args=(example_input_ids,),\n-                    kwargs={\"cache_position\": example_cache_position},\n-                    pre_dispatch=False,\n-                    strict=True,\n-                )\n+            exported_program = torch.export._trace._export(\n+                TorchExportableModuleWithStaticCache(model),\n+                args=(example_input_ids,),\n+                kwargs={\"cache_position\": example_cache_position},\n+                pre_dispatch=False,\n+                strict=True,\n+            )\n         return exported_program\n \n \n@@ -634,10 +612,9 @@ def _export_encoder(self, encoder_input_ids):\n \n         # Export the encoder\n         with torch.no_grad():\n-            with patch_mask_interface():\n-                exported_encoder = torch.export.export(\n-                    wrapped_encoder, (encoder_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n-                )\n+            exported_encoder = torch.export.export(\n+                wrapped_encoder, (encoder_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n+            )\n \n         return exported_encoder\n \n@@ -657,17 +634,16 @@ def _export_decoder(self, decoder_input_ids, encoder_hidden_states, cache_positi\n \n         # Export the decoder\n         with torch.no_grad():\n-            with patch_mask_interface():\n-                exported_decoder = torch.export.export(\n-                    wrapped_decoder,\n-                    (decoder_input_ids, encoder_hidden_states, cache_position),\n-                    dynamic_shapes={\n-                        \"decoder_input_ids\": None,\n-                        \"encoder_hidden_states\": {1: encoder_seq_len_dim},\n-                        \"cache_position\": None,\n-                    },\n-                    strict=True,\n-                )\n+            exported_decoder = torch.export.export(\n+                wrapped_decoder,\n+                (decoder_input_ids, encoder_hidden_states, cache_position),\n+                dynamic_shapes={\n+                    \"decoder_input_ids\": None,\n+                    \"encoder_hidden_states\": {1: encoder_seq_len_dim},\n+                    \"cache_position\": None,\n+                },\n+                strict=True,\n+            )\n \n         return exported_decoder\n "
        },
        {
            "sha": "53a81e1daaf51821a76feea7483847678c02256a",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -623,7 +623,11 @@ def _preprocess_mask_arguments(\n         return True, attention_mask, None, None\n \n     # For TGI/vLLM backends, or other custom attention without equivalent mask creation: we don't need a mask!\n-    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS:\n+    # Note: it's not ideal to check the `_global_mapping` attribute instead of the object itself, however otherwise\n+    # full graph dynamo tracing (i.e. torch.export or compile with `fullgraph=True`) will fail on Python<3.11\n+    # with `torch._dynamo.exc.Unsupported: 'inline in skipfiles:Mapping.__contains__ | __contains__, skipped\n+    # according trace_rules.lookup SKIP_DIRS'` -- can be removed when we require Python>=3.11\n+    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS._global_mapping:\n         return True, None, None, None\n \n     # Move the mask to correct device, and potentially switch dtype for efficiency"
        },
        {
            "sha": "ff7963ae7e0d0437e73693a3a28bbfadd63dac2f",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -232,8 +232,8 @@ def test_batched_small_model_logits(self):\n \n         EXPECTED_LOGITS = torch.Tensor(\n             [\n-                [[0.0000, 0.1866, -0.1997], [0.0000, -0.0736, 0.1785], [0.0000, -0.1965, -0.0569]],\n-                [[0.0000, -0.0302, 0.1488], [0.0000, -0.0402, 0.1351], [0.0000, -0.0341, 0.1116]],\n+                [[0.0000, 0.0285, 0.0322], [0.0000, 0.0011, 0.1105], [0.0000, -0.0018, -0.1019]],\n+                [[0.0000, 0.1080, 0.0454], [0.0000, -0.1808, -0.1553], [0.0000, 0.0452, 0.0369]],\n             ]\n         ).to(device=torch_device, dtype=torch.float16)\n \n@@ -251,4 +251,4 @@ def test_batched_small_model_logits(self):\n             output = model(**inputs)\n \n         logits = output.logits\n-        torch.testing.assert_close(EXPECTED_LOGITS, logits[:, :3, :3], rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(EXPECTED_LOGITS, logits[:, -3:, :3], rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "26442ef84588754abab320c82f7a230304b59a52",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -150,7 +150,6 @@ class CsmForConditionalGenerationTest(ModelTesterMixin, GenerationTesterMixin, u\n     test_headmasking = False\n     test_resize_embeddings = False\n     test_resize_embeddings_untied = False\n-    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = CsmModelTester(self)"
        },
        {
            "sha": "8f4215c7205f11a53b6293660fa677d96cb6808a",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 19,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -402,24 +402,12 @@ def test_small_model_logits_batched(self):\n         #\n         # Note: Key 9 is currently set for MI300, but may need potential future adjustments for H100s,\n         # considering differences in hardware processing and potential deviations in generated text.\n-        EXPECTED_LOGITS_LEFT = {\n-            7: torch.Tensor(\n-                [[0.1904, 0.0500, 0.7187], [0.1933, 0.0515, 0.7187], [0.2001, 0.0559, 0.7148]],\n-            ).to(torch_device),\n-            8: torch.Tensor([[0.1914, 0.0508, 0.7188], [0.1953, 0.0510, 0.7227], [0.1973, 0.0562, 0.7148]]).to(\n-                torch_device\n-            ),\n-            9: torch.Tensor([[0.1904, 0.0513, 0.7227], [0.1943, 0.0518, 0.7227], [0.1982, 0.0557, 0.7148]]).to(\n-                torch_device\n-            ),\n-        }\n-\n         EXPECTED_LOGITS_LEFT_UNPADDED = {\n             7: torch.Tensor(\n                 [[0.2236, 0.5195, -0.3828], [0.8203, -0.2275, 0.6054], [0.2656, -0.7070, 0.2460]],\n             ).to(torch_device),\n-            8: torch.Tensor([[0.2217, 0.5195, -0.3828], [0.8203, -0.2295, 0.6055], [0.2676, -0.7109, 0.2461]]).to(\n-                torch_device\n+            8: torch.Tensor([[0.2207, 0.5234, -0.3828], [0.8203, -0.2285, 0.6055], [0.2656, -0.7109, 0.2451]]).to(\n+                torch_device,\n             ),\n             9: torch.Tensor([[0.2236, 0.5195, -0.3828], [0.8203, -0.2285, 0.6055], [0.2637, -0.7109, 0.2451]]).to(\n                 torch_device\n@@ -430,8 +418,8 @@ def test_small_model_logits_batched(self):\n             7: torch.Tensor([[0.2167, 0.1269, -0.1640], [-0.3496, 0.2988, -1.0312], [0.0688, 0.7929, 0.8007]]).to(\n                 torch_device\n             ),\n-            8: torch.Tensor([[0.2178, 0.1260, -0.1621], [-0.3496, 0.2988, -1.0312], [0.0693, 0.7930, 0.8008]]).to(\n-                torch_device\n+            8: torch.Tensor([[0.2178, 0.1270, -0.1621], [-0.3496, 0.3008, -1.0312], [0.0693, 0.7930, 0.7969]]).to(\n+                torch_device,\n             ),\n             9: torch.Tensor([[0.2197, 0.1250, -0.1611], [-0.3516, 0.3008, -1.0312], [0.0684, 0.7930, 0.8008]]).to(\n                 torch_device\n@@ -442,9 +430,6 @@ def test_small_model_logits_batched(self):\n             logits = model(dummy_input, attention_mask=attention_mask).logits\n         logits = logits.float()\n \n-        torch.testing.assert_close(\n-            logits[0, :3, :3], EXPECTED_LOGITS_LEFT[self.cuda_compute_capability_major_version], atol=1e-3, rtol=1e-3\n-        )\n         torch.testing.assert_close(\n             logits[0, -3:, -3:],\n             EXPECTED_LOGITS_LEFT_UNPADDED[self.cuda_compute_capability_major_version],"
        },
        {
            "sha": "ddef77eef13e82c12ca51d8cde0fee283d763975",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/896833c183a168befcade879c1fca1889e20052d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/896833c183a168befcade879c1fca1889e20052d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=896833c183a168befcade879c1fca1889e20052d",
            "patch": "@@ -4461,6 +4461,7 @@ def test_torch_compile_for_training(self):\n         del loss\n \n         model = torch.compile(model, fullgraph=True, mode=\"reduce-overhead\")\n+\n         # forward compilation\n         set_seed(42)\n         loss = model(**inputs).loss"
        }
    ],
    "stats": {
        "total": 131,
        "additions": 48,
        "deletions": 83
    }
}