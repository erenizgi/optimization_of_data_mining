{
    "author": "gante",
    "message": "[tests] fix flaky pattern in `test_generate_continue_from_past_key_values` (#37724)",
    "sha": "3a1acc36ed7dd4c599d6fbfbe9609eac51ea894c",
    "files": [
        {
            "sha": "64f728224ae0e53ea6c7b9c13594e45adcd561b3",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 31,
            "deletions": 52,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a1acc36ed7dd4c599d6fbfbe9609eac51ea894c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a1acc36ed7dd4c599d6fbfbe9609eac51ea894c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=3a1acc36ed7dd4c599d6fbfbe9609eac51ea894c",
            "patch": "@@ -1097,25 +1097,18 @@ def test_contrastive_generate_low_memory(self):\n \n             # test output equality of low versus high memory\n             model = model_class(config).to(torch_device).eval()\n+            generate_kwargs = {\n+                \"top_k\": 4,\n+                \"penalty_alpha\": 0.6,\n+                \"max_new_tokens\": self.max_new_tokens,\n+                \"use_cache\": True,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n \n-            low_output = model.generate(\n-                top_k=4,\n-                penalty_alpha=0.6,\n-                low_memory=True,\n-                max_new_tokens=self.max_new_tokens,\n-                **inputs_dict,\n-                use_cache=True,\n-            )\n-\n-            high_output = model.generate(\n-                top_k=4,\n-                penalty_alpha=0.6,\n-                low_memory=False,\n-                max_new_tokens=self.max_new_tokens,\n-                **inputs_dict,\n-                use_cache=True,\n-            )\n-            self.assertListEqual(low_output.tolist(), high_output.tolist())\n+            low_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=True)\n+            high_output = model.generate(**inputs_dict, **generate_kwargs, low_memory=False)\n+            self._check_similar_generate_outputs(low_output, high_output)\n \n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n@@ -1863,22 +1856,29 @@ def test_generate_continue_from_past_key_values(self):\n \n             model = model_class(config).to(torch_device)\n             model.eval()\n-            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n-            model.generation_config.forced_eos_token_id = None\n-            model.generation_config.encoder_no_repeat_ngram_size = 0\n-            model.generation_config.use_cache = True\n \n             # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n             outputs = model(**inputs)\n             if \"past_key_values\" not in outputs:\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n             # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n-            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=4)\n \n             # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n             # inputs may need to be tweaked across `generate` calls (like the attention mask).\n-            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n \n             # Continue from the tokens generated above, preparing the inputs accordingly\n             inputs[\"past_key_values\"] = outputs_cached.past_key_values\n@@ -1901,10 +1901,13 @@ def test_generate_continue_from_past_key_values(self):\n                         mode=\"constant\",\n                         value=1,\n                     )\n-            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n \n             # The two sets of generated text and past kv should be equal to each other\n-            self.assertListEqual(outputs.sequences.tolist(), outputs_cached.sequences.tolist())\n+            self._check_similar_generate_outputs(outputs, outputs_cached)\n             for layer_idx in range(len(outputs_cached.past_key_values)):\n                 for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n                     self.assertTrue(\n@@ -1930,6 +1933,8 @@ def test_generate_continue_from_inputs_embeds(self):\n \n             if config.is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder\")\n+            # TODO (joao, raushan): the correct line below is `if not hasattr(config.get_text_config(), \"use_cache\")`,\n+            # but it breaks a few models. Fix and then apply `_check_similar_generate_outputs` pattern\n             if not hasattr(config, \"use_cache\"):\n                 self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n \n@@ -1990,32 +1995,6 @@ def test_generate_continue_from_inputs_embeds(self):\n                         )\n                     )\n \n-    @parameterized.expand([(\"offloaded\",)])  # (\"offloaded_static\",) TODO: @raushan fixme in some models (eg T5)\n-    @require_torch_accelerator\n-    @pytest.mark.generate\n-    def test_offloaded_cache_implementation(self, cache_implementation):\n-        \"\"\"Tests we can generate by indicating `cache_implementation` for each possible cache class\"\"\"\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_cache_class:\n-                self.skipTest(reason=\"This model does not support the new cache format\")\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-\n-            model = model_class(config).to(torch_device).eval()\n-            generation_kwargs = {\n-                \"max_new_tokens\": 5,\n-                \"use_cache\": True,\n-                \"cache_implementation\": cache_implementation,\n-            }\n-\n-            legacy_results = model.generate(**generation_kwargs, **inputs_dict)\n-\n-            # Most cache classes have their own tests except for some that are tested here\n-            # The ones here do not need special treatment when passing `cache_implementation`\n-            # and are not bound to specific models only\n-            new_results = model.generate(**generation_kwargs, **inputs_dict)\n-            self.assertListEqual(legacy_results.tolist(), new_results.tolist())\n-\n     @pytest.mark.generate\n     def test_generate_with_static_cache(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 83,
        "additions": 31,
        "deletions": 52
    }
}