{
    "author": "fanqiNO1",
    "message": "Fix Break change of AWQ FusedModules due to Attention Refactor (#41909)\n\n* fix awq bc due to attention refactor\n\n* feat: support more rope_types for awq fusion\n\n* feat: add test for llama3\n\n* fix ruff format\n\n* propagate changes in modeling_llama",
    "sha": "75e39856f877e75b3a15849d19151a73ed3a1159",
    "files": [
        {
            "sha": "b541083a571f5b8a9dc0171f32e3b52573da6f6c",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 97,
            "deletions": 5,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -18,6 +18,7 @@\n from packaging import version\n \n from ..activations import ACT2FN\n+from ..modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ..modeling_utils import PreTrainedModel\n from ..utils import is_auto_awq_available, is_ipex_available, is_torch_available, logging\n from ..utils.quantization_config import (\n@@ -46,7 +47,6 @@\n         \"mlp\": [\"w1\", \"w3\", \"w2\"],\n         \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n         \"use_alibi\": False,\n-        \"rope_theta\": 1000000.0,\n     },\n     \"llama\": {\n         \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n@@ -60,6 +60,18 @@\n         \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n         \"use_alibi\": False,\n     },\n+    \"qwen2\": {\n+        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n+        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n+        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n+        \"use_alibi\": False,\n+    },\n+    \"qwen3\": {\n+        \"attention\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"q_norm\", \"k_norm\"],\n+        \"mlp\": [\"gate_proj\", \"up_proj\", \"down_proj\"],\n+        \"layernorm\": [\"input_layernorm\", \"post_attention_layernorm\", \"norm\"],\n+        \"use_alibi\": False,\n+    },\n }\n \n AWQ_SCALES_MAPPINGS = {\n@@ -74,6 +86,53 @@\n }\n \n \n+if is_auto_awq_available():\n+    from awq.modules.fused.attn import RoPE\n+\n+    class AWQRoPE(RoPE):\n+        \"\"\"\n+        AWQRoPE module for hacking rope implementation in AWQ fused attention modules to support more models.\n+\n+        Args:\n+            rope_type (`str`):\n+                The rope type to use.\n+            head_dim (`int`):\n+                The head dimension.\n+            max_seq_len (`int`):\n+                The maximum sequence length.\n+            config (`PreTrainedConfig`):\n+                The model config object.\n+            device (`torch.device`):\n+                The device to put the module on.\n+        \"\"\"\n+\n+        def __init__(self, rope_type, head_dim, max_seq_len, config, device):\n+            rope_init_fn = ROPE_INIT_FUNCTIONS[rope_type]\n+            self.inv_freq, self.attention_scaling = rope_init_fn(config, device)\n+            # Use fake rope_theta to initialize the parent class\n+            super().__init__(head_dim=head_dim, max_seq_len=max_seq_len, device=device, rope_theta=-1)\n+\n+        def precompute_freqs_cis(self, dim: int, end: int, theta=-1):\n+            t = torch.arange(end, device=self.inv_freq.device)\n+            freqs = torch.outer(t, self.inv_freq).float()\n+            freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n+            del self.inv_freq  # free the memory\n+            return freqs_cis\n+\n+        def forward(\n+            self,\n+            xq: torch.Tensor,\n+            xk: torch.Tensor,\n+            start_pos: int,\n+            seqlen: int,\n+            partial: bool = False,\n+        ):\n+            xq_out, xk_out = super().forward(xq, xk, start_pos, seqlen, partial)\n+            xq_out = (xq_out * self.attention_scaling).type_as(xq)\n+            xk_out = (xk_out * self.attention_scaling).type_as(xk)\n+            return xq_out, xk_out\n+\n+\n def replace_quantization_scales(model, model_type):\n     from awq.modules.act import ScaledActivation\n \n@@ -219,15 +278,17 @@ def get_modules_to_fuse(model, quantization_config):\n         # Properly deal with the case where we have a multi-modal model as well (e.g. Llava)\n         config = model.config.get_text_config(decoder=True)\n \n-        # Handle hidden_size, num_attention_heads, num_key_value_heads on our own.\n+        # Handle hidden_size, num_attention_heads, num_key_value_heads, rope_parameters on our own.\n         hidden_size = config.hidden_size\n         num_attention_heads = config.num_attention_heads\n         num_key_value_heads = getattr(config, \"num_key_value_heads\", num_attention_heads)\n+        rope_parameters = config.rope_parameters\n \n         # Fill `current_fused_mapping` with the expected values\n         current_fused_mapping[\"hidden_size\"] = hidden_size\n         current_fused_mapping[\"num_attention_heads\"] = num_attention_heads\n         current_fused_mapping[\"num_key_value_heads\"] = num_key_value_heads\n+        current_fused_mapping[\"rope_parameters\"] = rope_parameters\n         current_fused_mapping[\"max_seq_len\"] = quantization_config.fuse_max_seq_len\n     else:\n         raise ValueError(\n@@ -261,6 +322,15 @@ def fuse_awq_modules(model, quantization_config):\n         from awq.modules.fused.attn import QuantAttentionFused\n         from awq.modules.fused.mlp import QuantFusedMLP\n         from awq.modules.fused.norm import FasterTransformerRMSNorm\n+\n+        # Hack QuantAttentionFused to modify the return value of forward function to avoid returning past_key_value\n+        old_quant_attention_fused_forward = QuantAttentionFused.forward\n+\n+        def new_quant_attention_fused_forward(self, *args, **kwargs):\n+            attn_output, attention_weight, _ = old_quant_attention_fused_forward(self, *args, **kwargs)\n+            return attn_output, attention_weight\n+\n+        QuantAttentionFused.forward = new_quant_attention_fused_forward\n     else:\n         raise ValueError(\"Fusing is only supported for the AutoAWQ backend\")\n \n@@ -376,7 +446,7 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n             The pytorch parent module that has layernorm modules to fuse\n         modules_to_fuse (`list[str]`):\n             The module fusing mapping. The dictionary has to contain a field `attention` with attention module names\n-            in the correct order: q, k, v, o layer\n+            in the correct order: q, k, v, o layer, (q_norm, k_norm) optional\n         current_module_name (`str`):\n             The current submodule name\n         target_cls (`~autoawq.QuantAttentionFused`):\n@@ -415,6 +485,14 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n         v_proj = getattr(module, modules_to_fuse[\"attention\"][2])\n         o_proj = getattr(module, modules_to_fuse[\"attention\"][3])\n \n+        # maybe there are q_norm and k_norm layers\n+        if len(modules_to_fuse[\"attention\"]) > 4:\n+            q_norm = getattr(module, modules_to_fuse[\"attention\"][4])\n+            k_norm = getattr(module, modules_to_fuse[\"attention\"][5])\n+        else:\n+            q_norm = None\n+            k_norm = None\n+\n         bias = torch.cat([q_proj.bias, k_proj.bias, v_proj.bias], dim=0) if q_proj.bias is not None else None\n \n         qkv_layer = linear_target_cls(\n@@ -445,16 +523,30 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n             modules_to_fuse[\"max_seq_len\"],\n             use_alibi=modules_to_fuse[\"use_alibi\"],\n             # The default value in autoawq is set to 10000.0\n-            rope_theta=modules_to_fuse.get(\"rope_theta\", 10000.0),\n+            rope_theta=modules_to_fuse[\"rope_parameters\"].get(\"rope_theta\", 10000.0),\n+            q_norm=q_norm,\n+            k_norm=k_norm,\n         )\n \n+        # Hack the rope module if not using alibi and rope_type is not default\n+        # As the default rope implementation in autoawq only supports the \"default\" rope type\n+        rope_type = modules_to_fuse[\"rope_parameters\"].get(\"rope_type\", \"default\")\n+        if not modules_to_fuse[\"use_alibi\"] and rope_type != \"default\":\n+            fused_attention_layer.rope = AWQRoPE(\n+                rope_type,\n+                modules_to_fuse[\"hidden_size\"] // modules_to_fuse[\"num_attention_heads\"],\n+                modules_to_fuse[\"max_seq_len\"],\n+                model.config.get_text_config(decoder=True),\n+                previous_device,\n+            )\n+\n         fused_attention_layer.is_hf_transformers = True\n \n         parent_name, child_name = current_module_name.rsplit(\".\", 1)\n         parent = model.get_submodule(parent_name)\n         setattr(parent, child_name, fused_attention_layer.to(previous_device))\n \n-        del q_proj, k_proj, v_proj, o_proj\n+        del q_proj, k_proj, v_proj, o_proj, q_norm, k_norm\n         module_has_been_fused = True\n \n     return module_has_been_fused"
        },
        {
            "sha": "77a3d65478d654af2cb86e1b35c9cb5b36f403f5",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -416,6 +416,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "779f4a63e37821923efd38dbe44dc952099b0894",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -421,6 +421,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "96a6a82da91db7abd2043f807567b323ee725622",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -750,6 +750,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "73597dd98d82858c18dfc5449b8cefa41c0e5d9a",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -420,6 +420,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "9fc2593d3175e23b456b94a4d7b4fae5c00b508a",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -453,6 +453,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "0a1da6cf7ed7b621b93f7a1f4a7107f007fe5363",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -754,6 +754,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "5424b48a7725dbf1028bbf31af3a214c03fee2a4",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -537,6 +537,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "f1f11cbcf1b37c957ddbc46a77a057c9eec43540",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -626,6 +626,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "99524915b9f6c6fbcc6118eb83c969b3897fd199",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -676,6 +676,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "65671913b27ffe0b83598b150b22fcb3cb408a8e",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -1247,6 +1247,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "b53ddf923e70f6da5ed8264f70089e98624eaea1",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -419,6 +419,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "8a508e2de54c1e8e540b9d6fe27d3edb36e536e1",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -437,6 +437,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "c982c36f9aab7a786f589e1475c051143320ed03",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -441,6 +441,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "7bd4ca5a3d2d7138a07b014fb5bac77be6501b73",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -570,6 +570,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "e616da3cd07bd7aa73e3eba954dbdd23090e0f9e",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -420,6 +420,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "4d184a0b19824bd70ee2166f25261d34e4a19a77",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -434,6 +434,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "6293fc7bce939a6d5fcdaa69aab94cae78653ec0",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -522,6 +522,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "e3adac5d117d7a1027d887d78f068b08f0ebfa23",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -425,6 +425,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "2ba7a25f71b50b8bd55b73773c46e416a640f9af",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -423,6 +423,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "44e3592157af49c5cebe2a6ba969fe8a8a49eedd",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -428,6 +428,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "682193ca8d516b895068aae967845c89cc74601d",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -426,6 +426,7 @@ def forward(\n                 position_embeddings=position_embeddings,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n+                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )"
        },
        {
            "sha": "5770bdbed13a6e4f2fad5f12f0615387d62165f1",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/75e39856f877e75b3a15849d19151a73ed3a1159/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75e39856f877e75b3a15849d19151a73ed3a1159/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=75e39856f877e75b3a15849d19151a73ed3a1159",
            "patch": "@@ -305,6 +305,9 @@ class AwqFusedTest(unittest.TestCase):\n     multi_modal_model_name = \"ybelkada/llava-1.5-7b-hf-awq\"\n     multi_modal_model_code_revision = \"ad108a50f5b9e681bdd7378409f57b7fa59a7442\"\n \n+    awq_rope_model_name = \"hugging-quants/Meta-Llama-3.1-8B-Instruct-AWQ-INT4\"\n+    awq_rope_model_revision = \"db1f81ad4b8c7e39777509fac66c652eb0a52f91\"\n+\n     prompt = (\n         \"You're standing on the surface of the Earth. \"\n         \"You walk one mile south, one mile west and one mile north. \"\n@@ -314,6 +317,7 @@ class AwqFusedTest(unittest.TestCase):\n     EXPECTED_GENERATION = prompt + \"\\n\\nYou're at the center of a square.\"\n     EXPECTED_GENERATION_CUSTOM_MODEL = \"Hello,\\n\\nI have a problem with my 20\"\n     EXPECTED_GENERATION_MIXTRAL = prompt + \" You're on the North Pole.\\n\\nThe\"\n+    EXPECTED_GENERATION_AWQ_ROPE = prompt + \" [Note: You can't be in a city, and\"\n \n     def tearDown(self):\n         gc.collect()\n@@ -513,6 +517,33 @@ def test_generation_mixtral_fused(self):\n         outputs = model.generate(**inputs, max_new_tokens=12)\n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_MIXTRAL)\n \n+    @pytest.mark.flash_attn_test\n+    @require_flash_attn\n+    @require_torch_multi_gpu\n+    @unittest.skipIf(\n+        get_device_properties()[0] == \"cuda\" and get_device_properties()[1] < 8,\n+        \"Skipping because RuntimeError: FlashAttention only supports Ampere GPUs or newer, so not supported on GPU with capability < 8.0\",\n+    )\n+    def test_generation_awq_rope_fused(self):\n+        \"\"\"\n+        Text generation test for AWQ model with special RoPE implementation (e.g. LLaMA3) + fused\n+        \"\"\"\n+        quantization_config = AwqConfig(bits=4, fuse_max_seq_len=1024, do_fuse=True)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.awq_rope_model_name,\n+            quantization_config=quantization_config,\n+            device_map=\"auto\",\n+            revision=self.awq_rope_model_revision,\n+        )\n+\n+        tokenizer = AutoTokenizer.from_pretrained(self.awq_rope_model_name)\n+        tokenizer.pad_token = tokenizer.eos_token\n+\n+        inputs = tokenizer([self.prompt, self.prompt], return_tensors=\"pt\", padding=True).to(torch_device)\n+\n+        outputs = model.generate(**inputs, max_new_tokens=12, do_sample=False)\n+        self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_AWQ_ROPE)\n+\n \n @slow\n @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 154,
        "additions": 149,
        "deletions": 5
    }
}