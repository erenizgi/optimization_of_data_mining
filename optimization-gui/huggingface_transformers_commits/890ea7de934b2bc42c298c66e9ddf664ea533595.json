{
    "author": "MekkCyber",
    "message": "Fix failling GGML test (#34871)\n\nfix_test",
    "sha": "890ea7de934b2bc42c298c66e9ddf664ea533595",
    "files": [
        {
            "sha": "1171e82e5285d5f7d1a97131f059f18b9bc6658e",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/890ea7de934b2bc42c298c66e9ddf664ea533595/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/890ea7de934b2bc42c298c66e9ddf664ea533595/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=890ea7de934b2bc42c298c66e9ddf664ea533595",
            "patch": "@@ -623,8 +623,8 @@ def test_falcon7b_q2_k(self):\n             torch_dtype=torch.float16,\n         )\n \n-        text = tokenizer(self.example_text, return_tensors=\"pt\").to(torch_device)\n-        out = model.generate(**text, max_new_tokens=10)\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"].to(torch_device)\n+        out = model.generate(text, max_new_tokens=10)\n \n         EXPECTED_TEXT = \"Hello All,\\nI am new to this forum.\"\n         self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}