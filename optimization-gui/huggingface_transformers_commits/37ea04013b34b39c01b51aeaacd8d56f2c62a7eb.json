{
    "author": "gante",
    "message": "Generate: Fix modern llm `generate` calls with `synced_gpus` (#34095)",
    "sha": "37ea04013b34b39c01b51aeaacd8d56f2c62a7eb",
    "files": [
        {
            "sha": "68b8b598ec0978d47b0c88fdd5280b2c90603207",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 63,
            "deletions": 71,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/37ea04013b34b39c01b51aeaacd8d56f2c62a7eb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37ea04013b34b39c01b51aeaacd8d56f2c62a7eb/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=37ea04013b34b39c01b51aeaacd8d56f2c62a7eb",
            "patch": "@@ -379,9 +379,10 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case\n         if past_key_values is not None:\n             model_inputs[\"past_key_values\"] = past_key_values\n-            if inputs_embeds is not None:  # Exception 1\n+            if inputs_embeds is not None or cache_position[-1] >= input_ids.shape[1]:  # Exception 1 or Exception 3\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n@@ -2609,8 +2610,14 @@ def _dola_decoding(\n                     outputs.hidden_states[candidate_premature_layer][:, -1, :]\n                 ).to(final_logits.device)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             next_token_logits = _dola_select_contrast(\n                 candidate_premature_layers, candidate_premature_logits, final_logits\n@@ -2652,11 +2659,6 @@ def _dola_decoding(\n             input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n             if streamer is not None:\n                 streamer.put(next_tokens.cpu())\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n \n             # stop when each sentence is finished\n             unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n@@ -3016,8 +3018,14 @@ def _contrastive_search(\n                 )\n             # contrastive_search main logic end\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             # finished sentences should have their next token be a padding token\n             if has_eos_stopping_criteria:\n@@ -3027,11 +3035,6 @@ def _contrastive_search(\n             input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n             if streamer is not None:\n                 streamer.put(next_tokens.cpu())\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n \n             # stop when each sentence is finished\n             unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n@@ -3168,8 +3171,14 @@ def _sample(\n             # forward pass to get next token\n             outputs = self(**model_inputs, return_dict=True)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n@@ -3214,11 +3223,6 @@ def _sample(\n             input_ids = torch.cat([input_ids, next_tokens[:, None]], dim=-1)\n             if streamer is not None:\n                 streamer.put(next_tokens.cpu())\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n \n             unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n             this_peer_finished = unfinished_sequences.max() == 0\n@@ -3415,9 +3419,15 @@ def _beam_search(\n             else:  # Unchanged original behavior\n                 outputs = self(**model_inputs, return_dict=True)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n                 cur_len = cur_len + 1\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n@@ -3491,12 +3501,6 @@ def _beam_search(\n \n             input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n \n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n-\n             # This is needed to properly delete outputs.logits which may be very large for first iteration\n             # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n             # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n@@ -3670,9 +3674,15 @@ def _group_beam_search(\n \n             outputs = self(**model_inputs, return_dict=True)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n                 cur_len = cur_len + 1\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             if output_scores:\n                 processed_score = torch.zeros_like(outputs.logits[:, -1, :])\n@@ -3782,12 +3792,6 @@ def _group_beam_search(\n \n             input_ids = torch.cat([input_ids, current_tokens.unsqueeze(-1)], dim=-1)\n \n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n-\n             # This is needed to properly delete outputs.logits which may be very large for first iteration\n             # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n             # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n@@ -3948,9 +3952,15 @@ def _constrained_beam_search(\n \n             outputs = self(**model_inputs, return_dict=True)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+            )\n             if synced_gpus and this_peer_finished:\n                 cur_len = cur_len + 1\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n             # (the clone itself is always small)\n@@ -4018,11 +4028,6 @@ def _constrained_beam_search(\n             beam_idx = beam_outputs[\"next_beam_indices\"]\n \n             input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-            )\n \n             # This is needed to properly delete outputs.logits which may be very large for first iteration\n             # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n@@ -4162,17 +4167,8 @@ def _assisted_decoding(\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n-        # This is needed if return_dict_in_generate is True\n-        start_from_empty_dynamic_cache = False\n-        past_key_values = model_kwargs.get(\"past_key_values\", None)\n-        if isinstance(past_key_values, DynamicCache) or (\n-            isinstance(past_key_values, EncoderDecoderCache)\n-            and isinstance(past_key_values.self_attention_cache, DynamicCache)\n-        ):\n-            if past_key_values.get_seq_length() == 0:\n-                start_from_empty_dynamic_cache = True\n-\n         this_peer_finished = False\n+        is_first_iteration = True  # to preserve the same API in the output as other generation methods\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             cur_len = input_ids.shape[-1]\n \n@@ -4271,63 +4267,59 @@ def _assisted_decoding(\n             # 5. Update the candidate generation strategy if needed\n             candidate_generator.update_candidate_strategy(input_ids, new_logits, n_matches)\n \n+            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            model_kwargs = self._update_model_kwargs_for_generation(\n+                outputs,\n+                model_kwargs,\n+                is_encoder_decoder=self.config.is_encoder_decoder,\n+                num_new_tokens=n_matches + 1,\n+            )\n             if synced_gpus and this_peer_finished:\n-                continue  # don't waste resources running the code we don't need\n+                continue\n \n             # Store scores, attentions and hidden_states when required\n             # Assistant: modified to append one tuple element per token, as in the other generation methods.\n             if return_dict_in_generate:\n+                newly_added_length = n_matches + 1\n                 if output_scores:\n-                    scores += tuple(new_logits[:, i, :] for i in range(n_matches + 1))\n+                    scores += tuple(new_logits[:, i, :] for i in range(newly_added_length))\n                 if output_logits:\n-                    raw_logits += (next_token_logits,)\n-\n-                if \"past_key_values\" not in model_kwargs or start_from_empty_dynamic_cache:\n-                    added_len = new_cur_len\n-                    # set it to false for other iterations\n-                    start_from_empty_dynamic_cache = False\n-                else:\n-                    added_len = n_matches + 1\n+                    raw_logits += tuple(next_token_logits[:, i, :] for i in range(newly_added_length))\n \n+                newly_added_length = new_cur_len if is_first_iteration else newly_added_length\n                 if output_attentions:\n                     if self.config.is_encoder_decoder:\n                         cross_attentions = _split_model_outputs(\n-                            cross_attentions, outputs.cross_attentions, cur_len, added_len\n+                            cross_attentions, outputs.cross_attentions, cur_len, newly_added_length\n                         )\n                         decoder_attentions = _split_model_outputs(\n                             decoder_attentions,\n                             outputs.decoder_attentions,\n                             cur_len,\n-                            added_len,\n+                            newly_added_length,\n                             is_decoder_attention=True,\n                         )\n                     else:\n                         decoder_attentions = _split_model_outputs(\n                             decoder_attentions,\n                             outputs.attentions,\n                             cur_len,\n-                            added_len,\n+                            newly_added_length,\n                             is_decoder_attention=True,\n                         )\n                 if output_hidden_states:\n                     if self.config.is_encoder_decoder:\n                         decoder_hidden_states = _split_model_outputs(\n-                            decoder_hidden_states, outputs.decoder_hidden_states, cur_len, added_len\n+                            decoder_hidden_states, outputs.decoder_hidden_states, cur_len, newly_added_length\n                         )\n                     else:\n                         decoder_hidden_states = _split_model_outputs(\n-                            decoder_hidden_states, outputs.hidden_states, cur_len, added_len\n+                            decoder_hidden_states, outputs.hidden_states, cur_len, newly_added_length\n                         )\n \n-            model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n-                model_kwargs,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                num_new_tokens=n_matches + 1,\n-            )\n-\n             unfinished_sequences = unfinished_sequences & ~stopping_criteria(input_ids, scores)\n             this_peer_finished = unfinished_sequences.max() == 0\n+            is_first_iteration = False\n \n         if streamer is not None:\n             streamer.end()"
        }
    ],
    "stats": {
        "total": 134,
        "additions": 63,
        "deletions": 71
    }
}