{
    "author": "gante",
    "message": "[tests] re-enable aria fast tests (#40846)\n\n* rise from the dead\n\n* test",
    "sha": "4cb41ad2a283036a245d22141a9b08788c0071d8",
    "files": [
        {
            "sha": "ea79003fbd74f2b5895b97cd517405949949f7c8",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=4cb41ad2a283036a245d22141a9b08788c0071d8",
            "patch": "@@ -481,6 +481,7 @@ def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:"
        },
        {
            "sha": "5d302015a7c901b71efb8d832266ded66a59a4ae",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=4cb41ad2a283036a245d22141a9b08788c0071d8",
            "patch": "@@ -368,6 +368,7 @@ def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:"
        },
        {
            "sha": "94d842eee8269fb928def1ec077d1d2fd1e099b9",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 18,
            "deletions": 4,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cb41ad2a283036a245d22141a9b08788c0071d8/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=4cb41ad2a283036a245d22141a9b08788c0071d8",
            "patch": "@@ -813,12 +813,11 @@ def wrapper(*args, **kwargs):\n \n class TransformersKwargs(TypedDict, total=False):\n     \"\"\"\n-    Keyword arguments to be passed to the loss function\n+    Keyword arguments to be passed to the forward pass of a `PreTrainedModel`.\n \n     Attributes:\n         num_items_in_batch (`Optional[torch.Tensor]`, *optional*):\n-            Number of items in the batch. It is recommended to pass it when\n-            you are doing gradient accumulation.\n+            Number of items in the batch. It is recommended to pass it when you are doing gradient accumulation.\n         output_hidden_states (`Optional[bool]`, *optional*):\n             Most of the models support outputting all hidden states computed during the forward pass.\n         output_attentions (`Optional[bool]`, *optional*):\n@@ -1059,7 +1058,22 @@ def wrapped_forward(*args, **kwargs):\n                         module.forward = make_capture_wrapper(module, original_forward, key, specs.index)\n                         monkey_patched_layers.append((module, original_forward))\n \n-        outputs = func(self, *args, **kwargs)\n+        try:\n+            outputs = func(self, *args, **kwargs)\n+        except TypeError as original_exception:\n+            # If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\n+            # Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\n+            # Otherwise -> we're probably missing `**kwargs` in the decorated function\n+            kwargs_without_recordable = {k: v for k, v in kwargs.items() if k not in recordable_keys}\n+            try:\n+                outputs = func(self, *args, **kwargs_without_recordable)\n+            except TypeError:\n+                raise original_exception\n+            raise TypeError(\n+                \"Missing `**kwargs` in the signature of the `@check_model_inputs`-decorated function \"\n+                f\"({func.__qualname__})\"\n+            )\n+\n         # Restore original forward methods\n         for module, original_forward in monkey_patched_layers:\n             module.forward = original_forward"
        },
        {
            "sha": "e55c37eca80082f126d0a388bf464abc0ab7394e",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cb41ad2a283036a245d22141a9b08788c0071d8/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cb41ad2a283036a245d22141a9b08788c0071d8/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4cb41ad2a283036a245d22141a9b08788c0071d8",
            "patch": "@@ -2131,12 +2131,12 @@ def _check_encoder_hidden_states_for_generate(self, hidden_states, batch_size, c\n     def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_values, cache_length, config):\n         self.assertIsInstance(decoder_past_key_values, (tuple, Cache))\n \n-        # (batch, head, seq_length, head_features)\n+        # (batch, # kv heads, seq_length, head_features)\n         expected_shape = (\n             batch_size,\n-            config.num_key_value_heads if hasattr(config, \"num_key_value_heads\") else config.num_attention_heads,\n+            getattr(config, \"num_key_value_heads\", None) or config.num_attention_heads,\n             cache_length,\n-            config.hidden_size // config.num_attention_heads,\n+            getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads,\n         )\n \n         if isinstance(decoder_past_key_values, Cache):"
        },
        {
            "sha": "17259a5effa84edff0e4a2b47cd2bb0aefe0a7d6",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 21,
            "deletions": 66,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cb41ad2a283036a245d22141a9b08788c0071d8/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cb41ad2a283036a245d22141a9b08788c0071d8/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=4cb41ad2a283036a245d22141a9b08788c0071d8",
            "patch": "@@ -15,7 +15,6 @@\n \n import unittest\n \n-import pytest\n import requests\n \n from transformers import (\n@@ -61,6 +60,10 @@ class AriaVisionText2TextModelTester:\n     def __init__(\n         self,\n         parent,\n+        batch_size=13,\n+        num_channels=3,\n+        image_size=16,\n+        num_image_tokens=4,\n         ignore_index=-100,\n         image_token_index=9,\n         projector_hidden_act=\"gelu\",\n@@ -83,32 +86,32 @@ def __init__(\n             num_choices=4,\n             pad_token_id=1,\n             hidden_size=32,\n-            intermediate_size=64,\n+            intermediate_size=16,\n             max_position_embeddings=60,\n             model_type=\"aria_moe_lm\",\n             moe_intermediate_size=4,\n-            moe_num_experts=4,\n+            moe_num_experts=3,\n             moe_topk=2,\n-            num_attention_heads=8,\n+            num_attention_heads=2,\n             num_experts_per_tok=3,\n             num_hidden_layers=2,\n-            num_key_value_heads=8,\n+            num_key_value_heads=2,\n             rope_theta=5000000,\n             vocab_size=99,\n             eos_token_id=2,\n             head_dim=4,\n         ),\n         is_training=True,\n         vision_config=Idefics3VisionConfig(\n-            image_size=358,\n-            patch_size=10,\n+            image_size=16,\n+            patch_size=8,\n             num_channels=3,\n             is_training=True,\n             hidden_size=32,\n-            projection_dim=20,\n+            projection_dim=4,\n             num_hidden_layers=2,\n-            num_attention_heads=16,\n-            intermediate_size=10,\n+            num_attention_heads=2,\n+            intermediate_size=4,\n             dropout=0.1,\n             attention_dropout=0.1,\n             initializer_range=0.02,\n@@ -130,11 +133,14 @@ def __init__(\n         self.num_attention_heads = text_config.num_attention_heads\n         self.is_training = is_training\n \n-        self.batch_size = 10\n-        self.num_channels = 3\n-        self.image_size = 358\n-        self.num_image_tokens = 128\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.num_image_tokens = num_image_tokens\n         self.seq_length = seq_length + self.num_image_tokens\n+        self.projector_patch_to_query_dict = {\n+            vision_config.image_size**2 // vision_config.patch_size**2: vision_config.projection_dim\n+        }\n \n     def get_config(self):\n         return AriaConfig(\n@@ -146,6 +152,7 @@ def get_config(self):\n             vision_feature_select_strategy=self.vision_feature_select_strategy,\n             vision_feature_layer=self.vision_feature_layer,\n             eos_token_id=self.eos_token_id,\n+            projector_patch_to_query_dict=self.projector_patch_to_query_dict,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -176,7 +183,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@slow\n @require_torch\n class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     \"\"\"\n@@ -193,61 +199,10 @@ def setUp(self):\n         self.model_tester = AriaVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=AriaConfig, has_text_modality=False)\n \n-    @unittest.skip(\n-        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant_false(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    @pytest.mark.torch_compile_test\n-    def test_sdpa_can_compile_dynamic(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n-    def test_sdpa_can_dispatch_on_flash(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n-    def test_feed_forward_chunking(self):\n-        pass\n-\n     @unittest.skip(reason=\"Unstable test\")\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n-    def test_cpu_offload(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n-    def test_disk_offload_bin(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n-    def test_disk_offload_safetensors(self):\n-        pass\n-\n \n SKIP = False\n torch_accelerator_module = getattr(torch, torch_device)"
        }
    ],
    "stats": {
        "total": 117,
        "additions": 44,
        "deletions": 73
    }
}