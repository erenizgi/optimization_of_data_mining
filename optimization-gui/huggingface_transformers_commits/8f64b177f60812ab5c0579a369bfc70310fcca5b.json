{
    "author": "stas00",
    "message": "[ForCausalLMLoss] allow users to pass shifted labels (#36607)\n\n* [ForCausalLMLoss] allow users to pass shifted labels\n\nSigned-off-by: Stas Bekman <stas@stason.org>\n\n* style\n\nSigned-off-by: Stas Bekman <stas@stason.org>\n\n---------\n\nSigned-off-by: Stas Bekman <stas@stason.org>",
    "sha": "8f64b177f60812ab5c0579a369bfc70310fcca5b",
    "files": [
        {
            "sha": "b75305d5ae0e930ac6e7a0cce1e1a615b35fa01d",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f64b177f60812ab5c0579a369bfc70310fcca5b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f64b177f60812ab5c0579a369bfc70310fcca5b/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8f64b177f60812ab5c0579a369bfc70310fcca5b",
            "patch": "@@ -31,14 +31,22 @@ def fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_i\n \n \n def ForCausalLMLoss(\n-    logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs\n+    logits,\n+    labels,\n+    vocab_size: int,\n+    num_items_in_batch: int = None,\n+    ignore_index: int = -100,\n+    shift_labels=None,\n+    **kwargs,\n ):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()\n-    labels = labels.to(logits.device)\n-    # Shift so that tokens < n predict n\n-    labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n-    shift_labels = labels[..., 1:].contiguous()\n+\n+    if shift_labels is None:\n+        labels = labels.to(logits.device)\n+        # Shift so that tokens < n predict n\n+        labels = nn.functional.pad(labels, (0, 1), value=ignore_index)\n+        shift_labels = labels[..., 1:].contiguous()\n \n     # Flatten the tokens\n     logits = logits.view(-1, vocab_size)"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 13,
        "deletions": 5
    }
}