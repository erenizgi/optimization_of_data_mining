{
    "author": "guangy10",
    "message": "Llama3 and Llama2 are ExecuTorch compatible (#34101)\n\nLlama3_1b and Llama2_7b are ExecuTorch compatible\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "9470c00042d0ff37e52a6e442970547b42d29b6c",
    "files": [
        {
            "sha": "fe521ea410913ceebc4d881600d740336b82e78a",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 69,
            "deletions": 0,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/9470c00042d0ff37e52a6e442970547b42d29b6c/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9470c00042d0ff37e52a6e442970547b42d29b6c/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=9470c00042d0ff37e52a6e442970547b42d29b6c",
            "patch": "@@ -23,6 +23,7 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n+from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_bitsandbytes,\n@@ -916,6 +917,74 @@ def test_compile_static_cache(self):\n         static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n+    @slow\n+    @require_read_token\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        llama_models = {\n+            \"meta-llama/Llama-3.2-1B\": [\n+                \"Simply put, the theory of relativity states that 1) the speed of light is the same for all \"\n+                \"observers, regardless of their location, and 2) the laws of physics are the same for all observers\"\n+            ],\n+            \"meta-llama/Llama-3.2-3B\": [\n+                \"Simply put, the theory of relativity states that 1. the speed of light is constant, and 2. \"\n+                \"the speed of light is the fastest speed possible\"\n+            ],\n+            \"meta-llama/Llama-2-7b-hf\": [\n+                \"Simply put, the theory of relativity states that 1) the speed of light is a constant, and 2) \"\n+                \"the laws of physics are the same for all\",\n+            ],\n+        }\n+\n+        for llama_model_ckp, EXPECTED_TEXT_COMPLETION in llama_models.items():\n+            # Load tokenizer\n+            tokenizer = AutoTokenizer.from_pretrained(llama_model_ckp, pad_token=\"</s>\", padding_side=\"right\")\n+            max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+                \"input_ids\"\n+            ].shape[-1]\n+\n+            # Load model\n+            device = \"cpu\"\n+            dtype = torch.bfloat16\n+            cache_implementation = \"static\"\n+            attn_implementation = \"sdpa\"\n+            batch_size = 1\n+            model = LlamaForCausalLM.from_pretrained(\n+                llama_model_ckp,\n+                device_map=device,\n+                torch_dtype=dtype,\n+                attn_implementation=attn_implementation,\n+                generation_config=GenerationConfig(\n+                    use_cache=True,\n+                    cache_implementation=cache_implementation,\n+                    max_length=max_generation_length,\n+                    cache_config={\n+                        \"batch_size\": batch_size,\n+                        \"max_cache_len\": max_generation_length,\n+                    },\n+                ),\n+            )\n+\n+            prompts = [\"Simply put, the theory of relativity states that \"]\n+            prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+            prompt_token_ids = prompt_tokens[\"input_ids\"]\n+            max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+            # Static Cache + export\n+            exported_program = convert_and_export_with_cache(model)\n+            ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+                exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+            )\n+            ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+            self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n+\n \n @slow\n @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 69,
        "deletions": 0
    }
}