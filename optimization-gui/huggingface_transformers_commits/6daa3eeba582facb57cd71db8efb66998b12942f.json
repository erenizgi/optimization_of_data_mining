{
    "author": "yonigozlan",
    "message": "Fix InternVL attention when using qk_norm (38B and 78B) (#37620)\n\n* fix internvlvision attention when using qk_norm\n\n* nit\n\n* modular",
    "sha": "6daa3eeba582facb57cd71db8efb66998b12942f",
    "files": [
        {
            "sha": "b6af62afd57fe792d064ff28bd9f3f3ca99d5a95",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/6daa3eeba582facb57cd71db8efb66998b12942f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6daa3eeba582facb57cd71db8efb66998b12942f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=6daa3eeba582facb57cd71db8efb66998b12942f",
            "patch": "@@ -150,10 +150,7 @@ def forward(\n         key_states = self.k_proj(hidden_states)\n         value_states = self.v_proj(hidden_states)\n \n-        query_states = query_states.reshape(-1, self.num_heads, self.head_dim)\n         query_states = self.q_norm(query_states)\n-\n-        key_states = key_states.reshape(-1, self.num_heads, self.head_dim)\n         key_states = self.k_norm(key_states)\n \n         query_states = query_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n@@ -860,13 +857,13 @@ def get_image_features(\n     @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "00b516cf041e2ffac2e00c68f33d1e4d20bdb7a2",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 55,
            "deletions": 41,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/6daa3eeba582facb57cd71db8efb66998b12942f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6daa3eeba582facb57cd71db8efb66998b12942f/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=6daa3eeba582facb57cd71db8efb66998b12942f",
            "patch": "@@ -16,15 +16,17 @@\n \n import collections.abc\n from dataclasses import dataclass\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -92,6 +94,55 @@ def __init__(self, config: InternVLVisionConfig):\n         self.q_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n         self.k_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n \n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ):\n+        batch_size, seq_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = self.q_norm(query_states)\n+        key_states = self.k_norm(key_states)\n+\n+        query_states = query_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            is_causal=False,\n+            **kwargs,\n+        )\n+        attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)\n+\n+        output = self.projection_layer(attn_output)\n+        output = self.projection_dropout(output)\n+\n+        outputs = (output, attn_weights) if output_attentions else (output, None)\n+        return outputs\n+\n \n class InternVLVisionPreTrainedModel(PreTrainedModel):\n     \"\"\"\n@@ -609,26 +660,7 @@ def get_image_features(\n \n     @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    def forward(\n-        self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: Optional[int] = None,\n-        vision_feature_select_strategy: Optional[str] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n-        **lm_kwargs,\n-    ) -> Union[Tuple, InternVLCausalLMOutputWithPast]:\n+    def forward(**super_kwargs):\n         r\"\"\"\n         Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n@@ -679,25 +711,7 @@ def forward(\n         >>> print(processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True))\n         The images depict the Statue of Liberty and the Golden Gate Bridge.\n         ```\"\"\"\n-        super().forward(\n-            input_ids=input_ids,\n-            pixel_values=pixel_values,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            vision_feature_layer=vision_feature_layer,\n-            vision_feature_select_strategy=vision_feature_select_strategy,\n-            labels=labels,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-            cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n-            image_sizes=image_sizes,\n-            **lm_kwargs,\n-        )\n+        super().forward(**super_kwargs)\n \n \n __all__ = ["
        }
    ],
    "stats": {
        "total": 105,
        "additions": 58,
        "deletions": 47
    }
}