{
    "author": "SunMarc",
    "message": "Fix quantization base class  (#41613)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "bc9900562d20ddb6fefbc0d3d52633ac23787932",
    "files": [
        {
            "sha": "5ba372a41fcb46b28f43e44ae00ec4bf3a09e15f",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc9900562d20ddb6fefbc0d3d52633ac23787932/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc9900562d20ddb6fefbc0d3d52633ac23787932/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=bc9900562d20ddb6fefbc0d3d52633ac23787932",
            "patch": "@@ -211,6 +211,9 @@ def update_ep_plan(self, config):\n         \"updates the tp plan for the scales\"\n         return config\n \n+    def _process_model_before_weight_loading(self, model, **kwargs):\n+        return model\n+\n     def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpoint_files=None, **kwargs):\n         \"\"\"\n         Setting model attributes and/or converting model before weights loading. At this point\n@@ -345,12 +348,6 @@ def update_state_dict_with_metadata(self, state_dict, metadata):\n         \"\"\"Update state dict with metadata. Default behaviour returns state_dict\"\"\"\n         return state_dict\n \n-    @abstractmethod\n-    def _process_model_before_weight_loading(self, model, **kwargs): ...\n-\n-    @abstractmethod\n-    def _process_model_after_weight_loading(self, model, **kwargs): ...\n-\n     @abstractmethod\n     def is_serializable(self, safe_serialization=None): ...\n "
        },
        {
            "sha": "959e97fdec5325a367cf48e7d909ef73e9ad5e75",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc9900562d20ddb6fefbc0d3d52633ac23787932/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc9900562d20ddb6fefbc0d3d52633ac23787932/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=bc9900562d20ddb6fefbc0d3d52633ac23787932",
            "patch": "@@ -364,7 +364,7 @@ def preprocess_model(self, model: \"PreTrainedModel\", config, dtype=None, checkpo\n         \"\"\"\n         super().preprocess_model(model, config, dtype, checkpoint_files, **kwargs)\n         # Torchao needs access to all metadata later\n-        model.set_metadata(checkpoint_files)\n+        self.set_metadata(checkpoint_files)\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\""
        },
        {
            "sha": "ba436504de7ae82f304e627dd6019b91128d038c",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc9900562d20ddb6fefbc0d3d52633ac23787932/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc9900562d20ddb6fefbc0d3d52633ac23787932/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=bc9900562d20ddb6fefbc0d3d52633ac23787932",
            "patch": "@@ -645,7 +645,7 @@ def test_pipeline(self):\n         self.pipe = pipeline(\n             \"text-generation\",\n             model=self.model_name,\n-            model_kwargs={\"device_map\": \"auto\", \"load_in_8bit\": True},\n+            model_kwargs={\"device_map\": \"auto\", \"quantization_config\": BitsAndBytesConfig(load_in_8bit=True)},\n             max_new_tokens=self.MAX_NEW_TOKENS,\n         )\n \n@@ -851,8 +851,7 @@ def test_cpu_accelerator_disk_loading_custom_device_map_kwargs(self):\n             model_8bit = AutoModelForCausalLM.from_pretrained(\n                 self.model_name,\n                 device_map=device_map,\n-                quantization_config=BitsAndBytesConfig(load_in_8bit=True),\n-                llm_int8_enable_fp32_cpu_offload=True,\n+                quantization_config=BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True),\n                 offload_folder=tmpdirname,\n             )\n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 6,
        "deletions": 10
    }
}