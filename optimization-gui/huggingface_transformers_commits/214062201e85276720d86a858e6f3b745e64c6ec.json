{
    "author": "co63oc",
    "message": "Fix typos in strings and comments (#37784)\n\n* Fix typos in strings and comments\n\n* Fix",
    "sha": "214062201e85276720d86a858e6f3b745e64c6ec",
    "files": [
        {
            "sha": "863fd67bddcc7f24a3f442d452e7e28a4629615e",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -162,7 +162,7 @@ def _is_chinese_char(self, cp):\n class TextIteratorStreamer(TextStreamer):\n     \"\"\"\n     Streamer that stores print-ready text in a queue, to be used by a downstream application as an iterator. This is\n-    useful for applications that benefit from acessing the generated text in a non-blocking way (e.g. in an interactive\n+    useful for applications that benefit from accessing the generated text in a non-blocking way (e.g. in an interactive\n     Gradio demo).\n \n     <Tip warning={true}>\n@@ -233,7 +233,7 @@ def __next__(self):\n class AsyncTextIteratorStreamer(TextStreamer):\n     \"\"\"\n     Streamer that stores print-ready text in a queue, to be used by a downstream application as an async iterator.\n-    This is useful for applications that benefit from acessing the generated text asynchronously (e.g. in an\n+    This is useful for applications that benefit from accessing the generated text asynchronously (e.g. in an\n     interactive Gradio demo).\n \n     <Tip warning={true}>"
        },
        {
            "sha": "b2da6f18baf97005cb948de3e0a12cb7fd6825a6",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -2082,7 +2082,7 @@ def _gather_beams(nested, beam_indices, batch_axis=0):\n \n         def gather_fn(tensor):\n             if batch_axis > 0:\n-                # pushes all dimentions before the batch to the end, so we get (batch, beam_id, ...)\n+                # pushes all dimensions before the batch to the end, so we get (batch, beam_id, ...)\n                 perm = tf.concat((tf.range(tf.rank(tensor))[batch_axis:], tf.range(batch_axis)), axis=0)\n                 tensor = tf.transpose(tensor, perm=perm)\n "
        },
        {
            "sha": "995b556b7ea020b373d16b01b13a30218c496aa3",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1285,7 +1285,7 @@ def _merge_criteria_processor_list(\n         Merge user-defined processors/criteria with the ones instantiated inside `generate`. In case the same\n         processor/criteria is present on both lists, use the user-defined one.\n \n-        (Note: up to v4.49.0, this funtion threw an exception is the same logit processor was found twice.)\n+        (Note: up to v4.49.0, this function threw an exception is the same logit processor was found twice.)\n         \"\"\"\n         if len(custom_list) == 0:\n             return default_list\n@@ -3852,7 +3852,7 @@ def _beam_search(\n \n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n-        # (joao) feature lost in the refactor. Probably won't implement, hurts readbility with minimal gains (there\n+        # (joao) feature lost in the refactor. Probably won't implement, hurts readability with minimal gains (there\n         # are newer low-memory alternatives like the offloaded cache)\n         sequential = generation_config.low_memory\n         if sequential:"
        },
        {
            "sha": "139d8cf2c7d09c2a37b38fdb18c9a7e5e068c665",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -538,7 +538,7 @@ def __call__(self, tokenized_outputs: torch.Tensor):\n         context_repetition_mask = self.logits_processor.compute_context_repetition_mask(\n             input_ids=tokenized_outputs,\n         )\n-        # context repitition mask shape [batch_size, output_len - (ngram_len - 1)]\n+        # context repetition mask shape [batch_size, output_len - (ngram_len - 1)]\n \n         combined_mask = context_repetition_mask * eos_token_mask\n "
        },
        {
            "sha": "cdbed289bd74ef27155ed6ebeed3dfe950ec7607",
            "filename": "src/transformers/integrations/aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faqlm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -30,7 +30,7 @@ def replace_with_aqlm_linear(\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with AQLM quantized layers.\n     `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successfull or not.\n+    conversion has been successful or not.\n \n     Args:\n         model (`torch.nn.Module`):"
        },
        {
            "sha": "23a418ead5552e15e158a066bd9aeac9a8ce6f0c",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -101,7 +101,7 @@ def replace_with_awq_linear(\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with AWQ quantized layers.\n     `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successfull or not.\n+    conversion has been successful or not.\n \n     During the module replacement, we also infer the backend to use through the `quantization_config` object.\n "
        },
        {
            "sha": "aafca87856b248e8eaa5954e2a4950b6411049d1",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -201,7 +201,7 @@ def _replace_with_bitnet_linear(\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n     \"\"\"\n \n     if current_key_name is None:"
        },
        {
            "sha": "7a6f6e107a74e28e1e474fd699e8df1cfc5b5557",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -158,7 +158,7 @@ def _replace_with_bnb_linear(\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n     \"\"\"\n     for name, module in model.named_children():\n         if current_key_name is None:\n@@ -280,7 +280,7 @@ def replace_8bit_linear(*args, **kwargs):\n     return replace_with_bnb_linear(*args, **kwargs)\n \n \n-# For backward compatiblity\n+# For backward compatibility\n def set_module_8bit_tensor_to_device(*args, **kwargs):\n     warnings.warn(\n         \"`set_module_8bit_tensor_to_device` will be deprecated in a future version, please use `set_module_quantized_tensor_to_device` instead\",\n@@ -403,7 +403,7 @@ def _dequantize_and_replace(\n     some performance drop compared to the original model before quantization - use it only for specific usecases\n     such as QLoRA adapters merging.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n     \"\"\"\n     quant_method = quantization_config.quantization_method()\n "
        },
        {
            "sha": "a3d124aa4b7a8f86ffa86d505a9b66a09f3e76df",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -36,7 +36,7 @@ def _replace_with_eetq_linear(\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n     \"\"\"\n     if current_key_name is None:\n         current_key_name = []"
        },
        {
            "sha": "ba4faa96c22048e327138f964c4710ee176618ec",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -167,7 +167,7 @@ def _replace_with_fbgemm_fp8_linear(\n     \"\"\"\n     Private method that wraps the recursion for module replacement.\n \n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n     \"\"\"\n \n     import re\n@@ -196,7 +196,7 @@ def _replace_with_fbgemm_fp8_linear(\n \n                     # Force requires grad to False to avoid unexpected errors\n                     model._modules[name].requires_grad_(False)\n-                # set non persistant buffer outside of init_empty_weights\n+                # set non persistent buffer outside of init_empty_weights\n                 model._modules[name].input_scale_ub = torch.tensor(\n                     [quantization_config.activation_scale_ub],\n                     dtype=torch.float,"
        },
        {
            "sha": "51bdc88608bf998fe1f15eff3fd542e9c62904ab",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -424,7 +424,7 @@ def converted(self):\n         if post_processor:\n             tokenizer.post_processor = post_processor\n \n-        # HACK: patch the llama-3 tokenizer to use the correspinding pre-tokenizer\n+        # HACK: patch the llama-3 tokenizer to use the corresponding pre-tokenizer\n         # and normalizer\n         if self.is_llama_3_tokenizer:\n             tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel("
        },
        {
            "sha": "02c9a23dc6877e3dd823b07af0a332561f16d856",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -558,7 +558,7 @@ def replace_with_higgs_linear(\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with HIGGS quantized layers.\n     `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successfull or not.\n+    conversion has been successful or not.\n \n     Args:\n         model (`torch.nn.Module`):"
        },
        {
            "sha": "f18eb43c7893b050457c3ee99c5b0ef2e74ccaf9",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1093,7 +1093,7 @@ def setup(self, args, state, model):\n             if state.is_hyper_param_search:\n                 if mode is not None:\n                     logger.warning(\n-                        \"Hyperparameter Search is enabled, forcing the creation of new experimetns, COMET_MODE value %r  is ignored\",\n+                        \"Hyperparameter Search is enabled, forcing the creation of new experiments, COMET_MODE value %r  is ignored\",\n                         comet_old_mode,\n                     )\n                 mode = \"create\""
        },
        {
            "sha": "0c9402abe68a0f1ca7f519d9df2c4025c0867244",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -350,7 +350,7 @@ def set_adapter(self, adapter_name: Union[List[str], str]) -> None:\n \n         for _, module in self.named_modules():\n             if isinstance(module, (BaseTunerLayer, ModulesToSaveWrapper)):\n-                # For backward compatbility with previous PEFT versions\n+                # For backward compatibility with previous PEFT versions\n                 if hasattr(module, \"set_adapter\"):\n                     module.set_adapter(adapter_name)\n                 else:"
        },
        {
            "sha": "c63b9b3b0c9a4b0e029d209ffadb3de8b0e36155",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -30,7 +30,7 @@ def replace_with_quanto_layers(\n ):\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with Quanto quantized layers.\n-    Returns the converted model and a boolean that indicates if the conversion has been successfull or not.\n+    Returns the converted model and a boolean that indicates if the conversion has been successful or not.\n \n     Args:\n         model (`torch.nn.Module`):"
        },
        {
            "sha": "61bf29224d6666f29d1fcefa85b5a1a6ab4d7bc3",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -611,14 +611,14 @@ def __init__(self):\n                 f\"Trying to prepare {layer_name}, but it's not supported. Corresponding module: {module} Fix it's TP plan: {e}\"\n             )\n \n-    # 2. We add hooks to the parrent module if needed\n+    # 2. We add hooks to the parent module if needed\n     if \".\" in layer_name:\n-        parrent_layer_name = layer_name.rsplit(\".\", 1)[0]\n-        generic_name = re.sub(r\"\\d+\", \"*\", parrent_layer_name)\n+        parent_layer_name = layer_name.rsplit(\".\", 1)[0]\n+        generic_name = re.sub(r\"\\d+\", \"*\", parent_layer_name)\n         # The module itself needs hooks\n         if module_plan := tp_plan.get(generic_name, False):\n             tp_layer = translate_to_torch_parallel_style(module_plan)\n-            module_to_tp_ = model.get_submodule(parrent_layer_name)\n+            module_to_tp_ = model.get_submodule(parent_layer_name)\n             tp_layer.prepare_module_tp(module_to_tp_, device_mesh)\n \n "
        },
        {
            "sha": "f76bd70377211da62dd145d820ce84f839b265e0",
            "filename": "src/transformers/integrations/vptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fvptq.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -28,7 +28,7 @@ def replace_with_vptq_linear(\n     \"\"\"\n     Public method that recursively replaces the Linear layers of the given model with VPTQ quantized layers.\n     `accelerate` is needed to use this method. Returns the converted model and a boolean that indicates if the\n-    conversion has been successfull or not.\n+    conversion has been successful or not.\n \n     Args:\n         model (`torch.nn.Module`):"
        },
        {
            "sha": "a65d06de108b29d600284884f742edb9820ef604",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -112,7 +112,7 @@ def forward(self, outputs, targets):\n \n         # Compute the L1 cost between boxes\n         bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n-        # Compute the giou cost betwen boxes\n+        # Compute the giou cost between boxes\n         giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n         # Compute the final cost matrix\n         cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost"
        },
        {
            "sha": "5fbf51c297050a2fc6714c057d28149bd6a199b8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1395,7 +1395,7 @@ def _find_mismatched_keys(\n         for key in new_state_dict.keys():\n             if key in model_state_dict and new_state_dict[key].shape != model_state_dict[key].shape:\n                 # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n-                # Without matching with module type or paramter type it seems like a practical way to detect valid 4bit weights.\n+                # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n                 if not (\n                     new_state_dict[key].shape[-1] == 1\n                     and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()"
        },
        {
            "sha": "a6220fbf926a87e22b0800093d5d6b2e539035e7",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -463,7 +463,7 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n             projection_size,\n             bias=self.use_bias,\n         )\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n \n         # time step projection (discretization)\n         # instantiate once and copy inv_dt in init_weights of PretrainedModel\n@@ -1541,7 +1541,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n+        # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "3ee6e72711e90aafb031fa57613c4a1f6aba3aa2",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -260,7 +260,7 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n             projection_size,\n             bias=self.use_bias,\n         )\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n \n         # time step projection (discretization)\n         # instantiate once and copy inv_dt in init_weights of PretrainedModel\n@@ -1257,7 +1257,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n+        # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "2357fd537504080b1f7b36d5c9bfd3f6a058be51",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1296,7 +1296,7 @@ def tie_weights(self):\n     @add_start_docstrings_to_model_forward(BARK_FINE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        codebook_idx: int,  # an additionnal idx corresponding to the id of the codebook that will be predicted\n+        codebook_idx: int,  # an additional idx corresponding to the id of the codebook that will be predicted\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n@@ -1547,7 +1547,7 @@ def generate(\n     - [`BarkSemanticModel`] (also referred to as the 'text' model): a causal auto-regressive transformer model that\n       takes\n     as input tokenized text, and predicts semantic text tokens that capture the meaning of the text.\n-    - [`BarkCoarseModel`] (also refered to as the 'coarse acoustics' model), also a causal autoregressive transformer,\n+    - [`BarkCoarseModel`] (also referred to as the 'coarse acoustics' model), also a causal autoregressive transformer,\n     that takes into input the results of the last model. It aims at regressing the first two audio codebooks necessary\n     to `encodec`.\n     - [`BarkFineModel`] (the 'fine acoustics' model), this time a non-causal autoencoder transformer, which iteratively\n@@ -1640,7 +1640,7 @@ def enable_cpu_offload(\n             self.to(\"cpu\")\n             torch_accelerator_module.empty_cache()  # otherwise we don't see the memory savings (but they probably exist)\n \n-        # this layer is used outside the first foward pass of semantic so need to be loaded before semantic\n+        # this layer is used outside the first forward pass of semantic so need to be loaded before semantic\n         self.semantic.input_embeds_layer, _ = cpu_offload_with_hook(self.semantic.input_embeds_layer, device)\n \n         hook = None"
        },
        {
            "sha": "f39ed47a19b83d63dccf62dea375bd06af448946",
            "filename": "src/transformers/models/bert/convert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fconvert_bert_token_dropping_original_tf2_checkpoint_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -67,10 +67,10 @@ def get_encoder_layer_array(layer_index: int, name: str):\n \n         return torch.from_numpy(array)\n \n-    def get_encoder_attention_layer_array(layer_index: int, name: str, orginal_shape):\n+    def get_encoder_attention_layer_array(layer_index: int, name: str, original_shape):\n         full_name = f\"encoder/_transformer_layers/{layer_index}/_attention_layer/{name}/.ATTRIBUTES/VARIABLE_VALUE\"\n         array = tf.train.load_variable(tf_checkpoint_path, full_name)\n-        array = array.reshape(orginal_shape)\n+        array = array.reshape(original_shape)\n \n         if \"kernel\" in name:\n             array = array.transpose()"
        },
        {
            "sha": "141f10464aa0949209d3bec3c7758907eb40c390",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -460,7 +460,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "3a2b6f46f8e96abf53fdd3fc627e2736a4b69af5",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -667,7 +667,7 @@ def tokenize(self, text):\n         \"\"\"\n         Tokenizes a piece of text into characters.\n \n-        For example, `input = \"apple\"\"` wil return as output `[\"a\", \"p\", \"p\", \"l\", \"e\"]`.\n+        For example, `input = \"apple\"\"` will return as output `[\"a\", \"p\", \"p\", \"l\", \"e\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens.\n@@ -866,7 +866,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "3b250a056ee380c94ebbb47a7b4affcde379d5d7",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1171,7 +1171,7 @@ def _bigbird_block_rand_mask_with_head(\n             if plan_idx > 0:\n                 # set the row for all from_blocks starting from 0 to\n                 # plan_block_length[plan_idx-1]\n-                # column indx start fromm plan_block_length[plan_idx-1] and ends at\n+                # column indx start from plan_block_length[plan_idx-1] and ends at\n                 # plan_block_length[plan_idx]\n                 if plan_num_rand_blocks[plan_idx] > 0:\n                     rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))"
        },
        {
            "sha": "e3bdfc38daf781888cf7bf6902a22e7b593d3750",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1055,7 +1055,7 @@ def _bigbird_block_rand_mask_with_head(\n             from_block_size: int. size of block in from sequence.\n             to_block_size: int. size of block in to sequence.\n             num_heads: int. total number of heads.\n-            plan_from_length: list. plan from length where num_random_blocks are choosen from.\n+            plan_from_length: list. plan from length where num_random_blocks are chosen from.\n             plan_num_rand_blocks: list. number of rand blocks within the plan.\n             indices_prng_key: jax.random.PRNGKey. PRNG key that is used to perform random jax operations.\n             deterministic: bool. When False random attention will be used.\n@@ -1104,7 +1104,7 @@ def _bigbird_block_rand_mask_with_head(\n             if plan_idx > 0:\n                 # set the row for all from_blocks starting from 0 to\n                 # plan_block_length[plan_idx-1]\n-                # column indx start fromm plan_block_length[plan_idx-1] and ends at\n+                # column indx start from plan_block_length[plan_idx-1] and ends at\n                 # plan_block_length[plan_idx]\n                 if plan_num_rand_blocks[plan_idx] > 0:\n                     rnd_r_cnt = int(sum(plan_num_rand_blocks[:plan_idx]))"
        },
        {
            "sha": "ee81c6b3af73e6c00568595d564238d33e88bfa9",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -970,7 +970,7 @@ def _bigbird_block_rand_mask_with_head(\n             if plan_idx > 0:\n                 # set the row for all from_blocks starting from 0 to\n                 # plan_block_length[plan_idx-1]\n-                # column indx start fromm plan_block_length[plan_idx-1] and ends at\n+                # column indx start from plan_block_length[plan_idx-1] and ends at\n                 # plan_block_length[plan_idx]\n                 if plan_num_rand_blocks[plan_idx] > 0:\n                     rnd_r_cnt = int(np.sum(plan_num_rand_blocks[:plan_idx]))"
        },
        {
            "sha": "3b4d44afcdbf10fea2e325994cd6fef21efa3045",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -64,7 +64,7 @@ class BlipForConditionalGenerationModelOutput(ModelOutput):\n \n     Args:\n         loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n             Prediction scores of the language modeling head of the text decoder model.\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*):\n@@ -109,7 +109,7 @@ class BlipTextVisionModelOutput(ModelOutput):\n \n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n             The image embeddings obtained by applying the projection layer to the pooler_output.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -145,7 +145,7 @@ class BlipImageTextMatchingModelOutput(ModelOutput):\n         itm_score (`torch.FloatTensor`):\n             The image-text similarity scores.\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n             The image embeddings obtained by applying the projection layer to the pooler_output.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):"
        },
        {
            "sha": "36e8a2da7a6a0faf04035db28c90bb88ff753d61",
            "filename": "src/transformers/models/blip/modeling_tf_blip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -73,7 +73,7 @@ class TFBlipForConditionalGenerationModelOutput(ModelOutput):\n \n     Args:\n         loss (`tf.Tensor`, *optional*, returned when `labels` is provided, `tf.Tensor` of shape `(1,)`):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         logits (`tf.Tensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n             Prediction scores of the language modeling head of the text decoder model.\n         image_embeds (`tf.Tensor` of shape `(batch_size, output_dim)`, *optional*):\n@@ -118,7 +118,7 @@ class TFBlipTextVisionModelOutput(ModelOutput):\n \n     Args:\n         loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         image_embeds (`tf.Tensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n             The image embeddings obtained by applying the projection layer to the pooler_output.\n         last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -154,7 +154,7 @@ class TFBlipImageTextMatchingModelOutput(ModelOutput):\n         itm_score (`tf.Tensor`):\n             The image-text similarity scores.\n         loss (`tf.Tensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Languge modeling loss from the text decoder.\n+            Language modeling loss from the text decoder.\n         image_embeds (`tf.Tensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n             The image embeddings obtained by applying the projection layer to the pooler_output.\n         last_hidden_state (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`):"
        },
        {
            "sha": "c4aa6f27c963afcdc4801d80168378d02e433e38",
            "filename": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -116,12 +116,12 @@ def convert_bloom_checkpoint_to_pytorch(\n                 else:\n                     for key in tensors.keys():\n                         if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n-                            # We average (sum and then divide) some weights accross TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n+                            # We average (sum and then divide) some weights across TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n                             tensors[key] += temp[key]\n                         else:\n                             # Some weights are RowParallelLinear in Megatron-Deepspeed, others are ColumnParallel\n                             cat_dim = 1 if any(text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN) else 0\n-                            # We concatenate these weights accross TP ranks\n+                            # We concatenate these weights across TP ranks\n                             tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n \n             # Divide by the number of TP the weights we want to average\n@@ -175,13 +175,13 @@ def convert_bloom_checkpoint_to_pytorch(\n                     tensors = temp\n                 else:\n                     for key in tensors.keys():\n-                        # We average (sum and then divide) some weights accross TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n+                        # We average (sum and then divide) some weights across TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n                         if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n                             tensors[key] += temp[key]\n                         else:\n                             # Some weights are RowParallelLinear in Megatron-Deepspeed, others are ColumnParallel\n                             cat_dim = 1 if any(text in key for text in WEIGHTS_WITH_ROW_PARALLELISM_CONTAIN) else 0\n-                            # We concatenate these weights accross TP ranks\n+                            # We concatenate these weights across TP ranks\n                             tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n \n             # Divide by the number of TP the weights we want to average"
        },
        {
            "sha": "76f95b5f8cdfa310775987b0b53a2e213198b8af",
            "filename": "src/transformers/models/camembert/tokenization_camembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Ftokenization_camembert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -181,7 +181,7 @@ def _tokenize(self, text: str) -> List[str]:\n \n     def _convert_token_to_id(self, token):\n         \"\"\"Converts a token (str) in an id using the vocab.\"\"\"\n-        # specifi to camembert, both 3 and 4 point to the unk token.\n+        # specific to camembert, both 3 and 4 point to the unk token.\n         if self.sp_model.PieceToId(token) == 0:\n             # Convert sentence piece unk token to fairseq unk token index\n             return self.unk_token_id"
        },
        {
            "sha": "59b253b5ece12ca17973e18570f8cd232d0cb9be",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -384,7 +384,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         tokenizer_file=os.path.join(input_base_path, \"tokenizer/text_tokenizer_modified.json\"), legacy=False\n     )\n     tokenizer.sep_token_id = 8710  # assign <reserved08706> to sep so that we can append it after input text\n-    tokenizer.pad_token_id = 1  # assing <pad> to special pad_token\n+    tokenizer.pad_token_id = 1  # assign <pad> to special pad_token\n     image_processor = ChameleonImageProcessor()\n     processor = ChameleonProcessor(image_processor=image_processor, tokenizer=tokenizer)\n     processor.save_pretrained(model_path)"
        },
        {
            "sha": "b03336ce7ea59cf50c1fdbbe249bf413f4d3c521",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -124,7 +124,7 @@ class ChameleonLinearScalingRotaryEmbedding(ChameleonRotaryEmbedding):\n     \"\"\"ChameleonRotaryEmbedding extended with linear scaling. Credits to the Reddit user /u/kaiokendev\"\"\"\n \n     def forward(self, x, position_ids):\n-        # difference to the original RoPE: a scaling factor is aplied to the position ids\n+        # difference to the original RoPE: a scaling factor is applied to the position ids\n         position_ids = position_ids.float() / self.scaling_factor\n         cos, sin = super().forward(x, position_ids)\n         return cos, sin"
        },
        {
            "sha": "4f89deed499d66afdc7947a4bee3d298c777c998",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -864,24 +864,24 @@ def reshape_mel2img(self, normalized_input_features):\n         _, _, time_length, freq_length = normalized_input_features.shape\n \n         spec_width = int(self.spec_size * self.freq_ratio)\n-        spec_heigth = self.spec_size // self.freq_ratio\n+        spec_height = self.spec_size // self.freq_ratio\n \n-        if time_length > spec_width or freq_length > spec_heigth:\n+        if time_length > spec_width or freq_length > spec_height:\n             raise ValueError(\"the wav size should be less than or equal to the swin input size\")\n \n         # to avoid bicubic zero error\n         if time_length < spec_width:\n             normalized_input_features = nn.functional.interpolate(\n                 normalized_input_features, (spec_width, freq_length), mode=\"bicubic\", align_corners=True\n             )\n-        if freq_length < spec_heigth:\n+        if freq_length < spec_height:\n             normalized_input_features = nn.functional.interpolate(\n-                normalized_input_features, (time_length, spec_heigth), mode=\"bicubic\", align_corners=True\n+                normalized_input_features, (time_length, spec_height), mode=\"bicubic\", align_corners=True\n             )\n \n         batch, channels, time, freq = normalized_input_features.shape\n \n-        # batch_size, channels, spec_width, spec_heigth --> batch_size, channels, spec_heigth * freq_ratio, spec_width // freq_ratio\n+        # batch_size, channels, spec_width, spec_height --> batch_size, channels, spec_height * freq_ratio, spec_width // freq_ratio\n         normalized_input_features = normalized_input_features.reshape(\n             batch, channels * self.freq_ratio, time // self.freq_ratio, freq\n         )"
        },
        {
            "sha": "6a4965971c74ebcf7b6263c3d01b3e78afb47f38",
            "filename": "src/transformers/models/clvp/feature_extraction_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ffeature_extraction_clvp.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -49,9 +49,9 @@ class ClvpFeatureExtractor(SequenceFeatureExtractor):\n             The default length of raw audio in seconds. If `max_length` is not set during `__call__` then it will\n             automatically be set to default_audio_length * `self.sampling_rate`.\n         hop_length (`int`, *optional*, defaults to 256):\n-            Length of the overlaping windows for the STFT used to obtain the Mel Frequency coefficients.\n+            Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n         chunk_length (`int`, *optional*, defaults to 30):\n-            The maximum number of chuncks of `sampling_rate` samples used to trim and pad longer or shorter audio\n+            The maximum number of chunks of `sampling_rate` samples used to trim and pad longer or shorter audio\n             sequences.\n         n_fft (`int`, *optional*, defaults to 1024):\n             Size of the Fourier transform."
        },
        {
            "sha": "13fde28d020ac7788786ac708af5747a541b1832",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -147,7 +147,7 @@ class ConditionalDetrObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1550,8 +1550,8 @@ def forward(\n         flattened_mask = mask.flatten(1)\n \n         # Fourth, sent flattened_features + flattened_mask + object_queries through encoder\n-        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (batch_size, height*width, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 inputs_embeds=flattened_features,\n@@ -1908,8 +1908,8 @@ def forward(\n         flattened_mask = mask.flatten(1)\n \n         # Fourth, sent flattened_features + flattened_mask + object_queries through encoder\n-        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (batch_size, height*width, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.conditional_detr.model.encoder(\n                 inputs_embeds=flattened_features,\n@@ -2046,7 +2046,7 @@ def __init__(self, dim, fpn_dims, context_dim):\n                 nn.init.constant_(m.bias, 0)\n \n     def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n-        # here we concatenate x, the projected feature map, of shape (batch_size, d_model, heigth/32, width/32) with\n+        # here we concatenate x, the projected feature map, of shape (batch_size, d_model, height/32, width/32) with\n         # the bbox_mask = the attention maps of shape (batch_size, n_queries, n_heads, height/32, width/32).\n         # We expand the projected feature map to match the number of heads.\n         x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)"
        },
        {
            "sha": "1593e77ef46552fe08085374096a0954c0d03395",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -465,7 +465,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "a587a11f87f8faecfb8d2183b984cf341bb06596",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -454,7 +454,7 @@ def forward(\n                 )\n             if querylen != query_segment.size(1):\n                 raise AssertionError(\n-                    f\"querylen should be equal to query_segment.size(1), but got {querylen} and {query_segment.szie(1)}!\"\n+                    f\"querylen should be equal to query_segment.size(1), but got {querylen} and {query_segment.size(1)}!\"\n                 )\n \n             key_pos = key_pos.view(batch, -1, keylen)"
        },
        {
            "sha": "3a60a07edf61b3a2d367371579ed8baa8ab1ba4c",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -143,7 +143,7 @@ class DabDetrObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~DabDetrImageProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1415,8 +1415,8 @@ def forward(\n         reference_position_embeddings = self.query_refpoint_embeddings.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n \n         # Fourth, sent flattened_features + flattened_mask + object_queries through encoder\n-        # flattened_features is a Tensor of shape (heigth*width, batch_size, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (height*width, batch_size, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 inputs_embeds=flattened_features,"
        },
        {
            "sha": "bab37cc6c1175562845d31cd1c673bf6e9a36f63",
            "filename": "src/transformers/models/data2vec/configuration_data2vec_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fconfiguration_data2vec_audio.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -90,7 +90,7 @@ class Data2VecAudioConfig(PretrainedConfig):\n             Number of groups of 1D convolutional positional embeddings layer.\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n         mask_time_length (`int`, *optional*, defaults to 10):\n@@ -101,7 +101,7 @@ class Data2VecAudioConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "9a41ed6fb064066524fcaaf871f411cf70e13e34",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1666,7 +1666,7 @@ def reshape_features(x):\n             features[i + 2] = ops[i + 2](features[i + 2])\n \n         logits = self.decode_head(features)\n-        # Tranpose the logits to maintain consistency in the output formats.\n+        # Transpose the logits to maintain consistency in the output formats.\n         transposed_logits = tf.transpose(logits, perm=[0, 3, 1, 2])\n \n         auxiliary_logits = None"
        },
        {
            "sha": "436834c7e5e038b79d9cf6ef21e34ba5eaf6ae64",
            "filename": "src/transformers/models/decision_transformer/configuration_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fconfiguration_decision_transformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -26,7 +26,7 @@ class DecisionTransformerConfig(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`DecisionTransformerModel`]. It is used to\n     instantiate a Decision Transformer model according to the specified arguments, defining the model architecture.\n     Instantiating a configuration with the defaults will yield a similar configuration to that of the standard\n-    DecisionTransformer architecture. Many of the config options are used to instatiate the GPT2 model that is used as\n+    DecisionTransformer architecture. Many of the config options are used to instantiate the GPT2 model that is used as\n     part of the architecture.\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the"
        },
        {
            "sha": "22501ee50866301c173d1f23e10e918f81ff8f2f",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -319,7 +319,7 @@ def forward(\n             else:\n                 # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n                 # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n-                # not necessarily to eager (if mentionned options are provided).\n+                # not necessarily to eager (if mentioned options are provided).\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         if using_eager and self.reorder_and_upcast_attn:"
        },
        {
            "sha": "a540bdc5d788931c7b3c4f176cb14ba3fad9e6c8",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -226,7 +226,7 @@ class DeformableDetrObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~DeformableDetrProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n@@ -1578,8 +1578,8 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n \n             scale = torch.cat([valid_width.unsqueeze(-1), valid_height.unsqueeze(-1)], 1).view(batch_size, 1, 1, 2)\n             grid = (grid.unsqueeze(0).expand(batch_size, -1, -1, -1) + 0.5) / scale\n-            width_heigth = torch.ones_like(grid) * 0.05 * (2.0**level)\n-            proposal = torch.cat((grid, width_heigth), -1).view(batch_size, -1, 4)\n+            width_height = torch.ones_like(grid) * 0.05 * (2.0**level)\n+            proposal = torch.cat((grid, width_height), -1).view(batch_size, -1, 4)\n             proposals.append(proposal)\n             _cur += height * width\n         output_proposals = torch.cat(proposals, 1)"
        },
        {
            "sha": "ef4f0da573d61ab07790f76e3a7fa2f5d764c75b",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -267,7 +267,7 @@ class DetaObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~DetaProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n@@ -1570,8 +1570,8 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n \n             scale = torch.cat([valid_width.unsqueeze(-1), valid_height.unsqueeze(-1)], 1).view(batch_size, 1, 1, 2)\n             grid = (grid.unsqueeze(0).expand(batch_size, -1, -1, -1) + 0.5) / scale\n-            width_heigth = torch.ones_like(grid) * 0.05 * (2.0**level)\n-            proposal = torch.cat((grid, width_heigth), -1).view(batch_size, -1, 4)\n+            width_height = torch.ones_like(grid) * 0.05 * (2.0**level)\n+            proposal = torch.cat((grid, width_height), -1).view(batch_size, -1, 4)\n             proposals.append(proposal)\n             _cur += height * width\n             level_ids.append(grid.new_ones(height * width, dtype=torch.long) * level)\n@@ -2293,7 +2293,7 @@ def forward(self, outputs, targets):\n         else:\n             indices = self.matcher(outputs_without_aux, targets)\n \n-        # Compute the average number of target boxes accross all nodes, for normalization purposes\n+        # Compute the average number of target boxes across all nodes, for normalization purposes\n         num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n         num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n         # Check that we have initialized the distributed state"
        },
        {
            "sha": "80f16881b5ea4c4859cca93ac003cf70eaae8e5e",
            "filename": "src/transformers/models/deprecated/efficientformer/convert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -194,9 +194,9 @@ def convert_efficientformer_checkpoint(\n     # Save Checkpoints\n     Path(pytorch_dump_path).mkdir(exist_ok=True)\n     model.save_pretrained(pytorch_dump_path)\n-    print(f\"Checkpoint successfuly converted. Model saved at {pytorch_dump_path}\")\n+    print(f\"Checkpoint successfully converted. Model saved at {pytorch_dump_path}\")\n     processor.save_pretrained(pytorch_dump_path)\n-    print(f\"Processor successfuly saved at {pytorch_dump_path}\")\n+    print(f\"Processor successfully saved at {pytorch_dump_path}\")\n \n     if push_to_hub:\n         print(\"Pushing model to the hub...\")"
        },
        {
            "sha": "8aa927d821a087c5caee7f557bb8a776b9bd3dbf",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/convert_gptsan_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -47,7 +47,7 @@ def convert_tf_gptsan_to_pt(args):\n                     player = int(key_name[9])\n                 elif key_name.startswith(\"pasts/out\"):\n                     player = 8\n-                name = \"model.sqout.%d.weight\" % (player * 2)  # enter to nn.Sequencial with Tanh, so 2 at a time\n+                name = \"model.sqout.%d.weight\" % (player * 2)  # enter to nn.Sequential with Tanh, so 2 at a time\n                 state = vnp.transpose([1, 0]).copy()  # Mesh-Tensorflow is a diagonal matrix\n                 new_state[name] = torch.tensor(state)\n             elif key_name.startswith(\"model/moe\"):"
        },
        {
            "sha": "17da733be978c60be040c08944f0177de44ec099",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -258,12 +258,12 @@ def forward(self, hidden_states):\n         expert the corresponding hidden states.\n \n         \"\"\"\n-        # Step 1: Get the router_mask from the router as wel as the probabilities\n+        # Step 1: Get the router_mask from the router as well as the probabilities\n         router_mask, router_probs, router_logits = self.router(hidden_states)\n         expert_index = torch.argmax(router_mask, dim=-1)\n \n         # The routers introduced might not always map all the tokens, to a router, which means that some hidden states\n-        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the seleced ones.\n+        # can be unchanged from one layer to another. That is why the hidden states are cloned before updating only the selected ones.\n \n         next_states = hidden_states.clone()\n         for idx, expert in enumerate(self.experts.values()):\n@@ -905,7 +905,7 @@ def forward(\n \n         Returns:\n             `MoEModelOutputWithPastAndCrossAttentions` or `tuple` if `return_dict` returns\n-            MoEModelOutputWithPastAndCrossAttentions insted of tuple\n+            MoEModelOutputWithPastAndCrossAttentions instead of tuple\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         device = self.position_embeddings.weight.device\n@@ -1006,7 +1006,7 @@ def forward(\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids.unsqueeze(1).unsqueeze(2)\n             prefix_lm_mask = ((prefix_lm_mask + token_type_ids) > 0).float()\n-        # Marge prefix_lm_mask and attention_mask\n+        # Merge prefix_lm_mask and attention_mask\n         extended_attention_mask = prefix_lm_mask * attention_mask.unsqueeze(1).unsqueeze(2)\n \n         # Prepare head mask if needed\n@@ -1130,7 +1130,7 @@ def forward(\n             labels in `[0, ..., config.vocab_size]`\n \n         Returns:\n-            `MoECausalLMOutputWithPast` or `tuple` if `return_dict` returns MoECausalLMOutputWithPast insted of tuple\n+            `MoECausalLMOutputWithPast` or `tuple` if `return_dict` returns MoECausalLMOutputWithPast instead of tuple\n \n         Example:\n "
        },
        {
            "sha": "9ffe4d9b14aac44347cbb5d5da83e51516a25105",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -125,7 +125,7 @@ class GPTSanJapaneseTokenizer(PreTrainedTokenizer):\n         emoji_file (`str`):\n             File containing the emoji.\n         unk_token (`str`, *optional*, defaults to `\"<|nottoken|>\"`):\n-            The token used for unknown charactor\n+            The token used for unknown character\n         pad_token (`str`, *optional*, defaults to `\"<|separator|>\"`):\n             The token used for padding\n         bos_token (`str`, *optional*, defaults to `\"<|startoftext|>\"`):\n@@ -372,7 +372,7 @@ class SubWordJapaneseTokenizer:\n     - Decoding byte0~byte255 tokens correctly\n     - Added bagofword token handling\n \n-    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT Lisence according to the\n+    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT License according to the\n     original repository.\n \n     MIT License"
        },
        {
            "sha": "7b91a429b0d3fb059a8d94b32222764bbcb64ee6",
            "filename": "src/transformers/models/deprecated/graphormer/modeling_graphormer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fmodeling_graphormer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -740,7 +740,7 @@ def _init_weights(\n         Initialize the weights\n         \"\"\"\n         if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            # We might be missing part of the Linear init, dependant on the layer num\n+            # We might be missing part of the Linear init, dependent on the layer num\n             module.weight.data.normal_(mean=0.0, std=0.02)\n             if module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "3380e386931a7c94e144c8ad6a2620fbb671bf50",
            "filename": "src/transformers/models/deprecated/jukebox/convert_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -197,7 +197,7 @@ def fix_jukebox_keys(state_dict, model_state_dict, key_prefix, mapping):\n         if f\"{key_prefix}.{key}\" not in model_state_dict or key is None:\n             print(f\"failed converting {original_key} to {key}, does not match\")\n \n-        # handle missmatched shape\n+        # handle mismatched shape\n         elif value.shape != model_state_dict[f\"{key_prefix}.{key}\"].shape:\n             val = model_state_dict[f\"{key_prefix}.{key}\"]\n             print(f\"{original_key}-> {key} : \\nshape {val.shape} and {value.shape}, do not match\")"
        },
        {
            "sha": "3bff1d83bafb9ef2f7b146ee0c318b8b8dff3a63",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1308,11 +1308,11 @@ def __init__(\n                 Number of tokens or lyrics tokens provided in a single pass.\n             embed_dim (`int`, *optional*):\n                 Either equals to the dimension of the codebook, or the sum of n_vocab (lyrics) and codebook dimension,\n-                if the model combines lyrics and music tokens, or simply n_vocab if the model is a seperate encoder\n+                if the model combines lyrics and music tokens, or simply n_vocab if the model is a separate encoder\n             audio_conditioning (`bool`, *optional*, defaults to `False`):\n-                Whether or not the prior supports conditionning on audio.\n+                Whether or not the prior supports conditioning on audio.\n             metadata_conditioning (`bool`, *optional*, defaults to `False`):\n-                Whether or not the prior supports conditionning on artitst, genres, lyrics and timing.\n+                Whether or not the prior supports conditioning on artitst, genres, lyrics and timing.\n             is_encoder (`bool`, *optional*, defaults to `False`):\n                 Whether the model is an encoder only model.\n         \"\"\"\n@@ -1392,7 +1392,7 @@ def forward(\n         hidden_states = self.transformer(\n             hidden_states, last_encoder_hidden_states=last_encoder_hidden_states\n         )  # Transformer\n-        if self.add_cond_after_transformer:  # Piped doesnt add x_cond\n+        if self.add_cond_after_transformer:  # Piped doesn't add x_cond\n             hidden_states = hidden_states + audio_conditioning\n \n         activations = hidden_states\n@@ -1535,7 +1535,7 @@ def primed_sample(\n             if get_preds:\n                 preds = []\n \n-            # Fill up key/value cache for past context by runing forward pass.\n+            # Fill up key/value cache for past context by running forward pass.\n             # We do so in chunks instead of doing the whole past in one forward pass to reduce max memory usage.\n             if chunk_size is None:\n                 chunk_size = len(sampled_audio)\n@@ -1617,7 +1617,7 @@ def primed_sample(\n \n class JukeboxMusicTokenConditioner(nn.Module):\n     \"\"\"\n-    The `JukeboxMusicTokenConditioner` takes music tokens as an input (coresponding to the codes of the VQVAE's\n+    The `JukeboxMusicTokenConditioner` takes music tokens as an input (corresponding to the codes of the VQVAE's\n     codebook) and upsamples it using a single layer of decoder convolution block (the same is used in the VQVAE).\n     \"\"\"\n \n@@ -1637,20 +1637,20 @@ def __init__(self, config, level):\n         )\n         self.layer_norm = JukeboxLayerNorm(config.hidden_size)\n \n-    def forward(self, music_tokens, raw_audio_conditionning=None):\n+    def forward(self, music_tokens, raw_audio_conditioning=None):\n         \"\"\"\n         Args:\n             music_tokens (`torch.LongTensor`):\n-                Music tokens form the uper level in range(nb_discrete_codes)\n-            raw_audio_conditionning (`torch.LongTensor`, *optional*):\n+                Music tokens form the upper level in range(nb_discrete_codes)\n+            raw_audio_conditioning (`torch.LongTensor`, *optional*):\n                 Audio used when primed sampling, raw audio information that conditions the generation\n         \"\"\"\n-        if raw_audio_conditionning is None:\n-            raw_audio_conditionning = 0.0\n+        if raw_audio_conditioning is None:\n+            raw_audio_conditioning = 0.0\n         # Embed music_tokens\n         music_tokens = music_tokens.long()\n         hidden_states = self.embed_tokens(music_tokens)\n-        hidden_states = hidden_states + raw_audio_conditionning\n+        hidden_states = hidden_states + raw_audio_conditioning\n \n         # Run conditioner\n         hidden_states = hidden_states.permute(0, 2, 1)\n@@ -1768,7 +1768,7 @@ class JukeboxPrior(PreTrainedModel):\n     \"\"\"\n     The JukeboxPrior class, which is a wrapper around the various conditioning and the transformer. JukeboxPrior can be\n     seen as language models trained on music. They model the next `music token` prediction task. If a (lyric) `encoder\n-    is defined, it also models the `next character` prediction on the lyrics. Can be conditionned on timing, artist,\n+    is defined, it also models the `next character` prediction on the lyrics. Can be conditioned on timing, artist,\n     genre, lyrics and codes from lower-levels Priors.\n \n     Args:\n@@ -1809,7 +1809,7 @@ def _init_weights(self, module):\n         elif isinstance(module, JukeboxConditionalAutoregressive) and hasattr(module, \"start_token\"):\n             module.start_token.data.normal_(mean=0.0, std=0.01 * init_scale)\n         elif isinstance(module, JukeboxResConv1DBlock) and self.config.zero_out:\n-            module.conv1d_2.weigth.data.zero_()\n+            module.conv1d_2.weight.data.zero_()\n             module.conv1d_2.bias.data.zero_()\n         if isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n@@ -1931,7 +1931,7 @@ def set_metadata_lyric_tokens(self, labels):\n             tokens_list = torch.zeros(\n                 (labels.shape[0], self.nb_relevant_lyric_tokens), dtype=torch.long, device=labels.device\n             )\n-            indices_list = []  # whats the index of each current character in original array\n+            indices_list = []  # what's the index of each current character in original array\n             for idx in range(labels.shape[0]):\n                 full_tokens = labels.clone()[:, 4 + self.metadata_embedding.max_nb_genres :]\n                 total_length, offset, duration = labels[idx, 0], labels[idx, 1], labels[idx, 2]\n@@ -2073,12 +2073,12 @@ def sample(\n             n_samples (`int`):\n                 Number of samples to generate.\n             music_tokens (`List[torch.LongTensor]`, *optional*):\n-                Previously gemerated tokens at the current level. Used as context for the generation.\n+                Previously generated tokens at the current level. Used as context for the generation.\n             music_tokens_conds (`List[torch.FloatTensor]`, *optional*):\n                 Upper-level music tokens generated by the previous prior model. Is `None` if the generation is not\n-                conditionned on the upper-level tokens.\n+                conditioned on the upper-level tokens.\n             metadata (`List[torch.LongTensor]`, *optional*):\n-                List containing the metatdata tensor with the artist, genre and the lyric tokens.\n+                List containing the metadata tensor with the artist, genre and the lyric tokens.\n             temp (`float`, *optional*, defaults to 1.0):\n                 Sampling temperature.\n             top_k (`int`, *optional*, defaults to 0):\n@@ -2237,11 +2237,11 @@ def forward(\n             hidden_states (`torch.Tensor`):\n                 Hidden states which should be raw audio\n             metadata (`List[torch.LongTensor]`, *optional*):\n-                List containing the metadata conditioning tensorwith the lyric and the metadata tokens.\n+                List containing the metadata conditioning tensor with the lyric and the metadata tokens.\n             decode (`bool`, *optional*, defaults to `False`):\n                 Whether or not to decode the encoded to tokens.\n             get_preds (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the actual predicitons of the model.\n+                Whether or not to return the actual predictions of the model.\n         \"\"\"\n         batch_size = hidden_states.shape[0]\n         music_tokens, *music_tokens_conds = self.encode(hidden_states, bs_chunks=batch_size)\n@@ -2466,10 +2466,10 @@ def _sample(\n             metas (`List[Any]`, *optional*):\n                 Metadatas used to generate the `labels`\n             chunk_size (`int`, *optional*, defaults to 32):\n-                Size of a chunk of audio, used to fill up the memory in chuncks to prevent OOM erros. Bigger chunks\n+                Size of a chunk of audio, used to fill up the memory in chunks to prevent OOM errors. Bigger chunks\n                 means faster memory filling but more consumption.\n             sampling_temperature (`float`, *optional*, defaults to 0.98):\n-                Temperature used to ajust the randomness of the sampling.\n+                Temperature used to adjust the randomness of the sampling.\n             lower_batch_size (`int`, *optional*, defaults to 16):\n                 Maximum batch size for the lower level priors\n             max_batch_size (`int`, *optional*, defaults to 16):"
        },
        {
            "sha": "e873111cb2a61c1f150a1fd8b835bf04c2e4fb6f",
            "filename": "src/transformers/models/deprecated/mctct/modeling_mctct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fmodeling_mctct.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -467,7 +467,7 @@ def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n \n     def _get_feature_vector_attention_mask(self, feature_vector_length, attention_mask):\n         # generate creates 3D attention mask, because of the shape of input_features\n-        # convert it to 2D if thats the case\n+        # convert it to 2D if that's the case\n         if len(attention_mask.shape) > 2:\n             attention_mask = attention_mask[:, :, -1]\n "
        },
        {
            "sha": "85d10156103b0bbc0a98331717172c124de8a8da",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -463,7 +463,7 @@ def forward(\n             prev_state (`torch.Tensor` of shape `(batch_size, config.ndim)`, *optional*):\n                 The hidden state returned from the previous timestep during incremental decoding.\n             use_cache (`bool`, default `False`):\n-                Whether to perfom incremental decoding; uses `prev_state` as the prior timestep, and returns the\n+                Whether to perform incremental decoding; uses `prev_state` as the prior timestep, and returns the\n                 updated EMA hidden state for use in the next step\n \n         Returns:\n@@ -652,7 +652,7 @@ def forward(\n             output_attentions (`bool`, defaults to `False`):\n                 Whether or not to return the cross-attention weights.\n             use_cache (`bool`, defaults to `False`):\n-                Whether to perfom incremental decoding; uses `prev_state` as the prior timestep, and returns the\n+                Whether to perform incremental decoding; uses `prev_state` as the prior timestep, and returns the\n                 updated EMA hidden state for use in the next step\n \n         Returns:\n@@ -936,7 +936,7 @@ def forward(\n             output_attentions (`bool`, default `False`):\n                 Whether to return self-attention weights\n             use_cache (`bool`, default `False`):\n-                Whether to perfom incremental decoding; uses `past_key_values` as prior state, and returns the updated\n+                Whether to perform incremental decoding; uses `past_key_values` as prior state, and returns the updated\n                 states for use in the next step\n \n         Returns:\n@@ -1214,7 +1214,7 @@ def forward(\n             output_attentions (`bool`, default `False`):\n                 Whether to return self-attention weights\n             use_cache (`bool`, default `False`):\n-                Whether to perfom incremental decoding; uses `past_key_value` as prior state, and returns the updated\n+                Whether to perform incremental decoding; uses `past_key_value` as prior state, and returns the updated\n                 states for use in the next step\n \n         Returns:"
        },
        {
            "sha": "b5e47abb11791aedccd5621677713136a037be8c",
            "filename": "src/transformers/models/deprecated/realm/retrieval_realm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fretrieval_realm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -76,7 +76,7 @@ class RealmRetriever:\n \n         Parameters:\n             block_records (`np.ndarray`):\n-                A numpy array which cantains evidence texts.\n+                A numpy array which contains evidence texts.\n             tokenizer ([`RealmTokenizer`]):\n                 The tokenizer to encode retrieved texts.\n     \"\"\""
        },
        {
            "sha": "5c3c7a196ff4f258a9c02e2510ab4ed839724dfd",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -516,7 +516,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "7f2206f0d90b648da294e337450a9934c98ce6d3",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -457,7 +457,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "7d82659b5a63e6a882ffdc9bb1c67a03eec4955a",
            "filename": "src/transformers/models/deprecated/transfo_xl/configuration_transfo_xl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fconfiguration_transfo_xl.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -49,7 +49,7 @@ class TransfoXLConfig(PretrainedConfig):\n         d_inner (`int`, *optional*, defaults to 4096):\n             Inner dimension in FF\n         div_val (`int`, *optional*, defaults to 4):\n-            Divident value for adapative input and softmax\n+            Divident value for adaptive input and softmax\n         pre_lnorm (`boolean`, *optional*, defaults to `False`):\n             Whether or not to apply LayerNorm to the input instead of the output in the blocks.\n         n_layer (`int`, *optional*, defaults to 18):"
        },
        {
            "sha": "6e1c49d708cc81db9b8971d4f97a1f903c9485e3",
            "filename": "src/transformers/models/deprecated/tvlt/feature_extraction_tvlt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Ffeature_extraction_tvlt.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -46,7 +46,7 @@ class TvltFeatureExtractor(SequenceFeatureExtractor):\n         sampling_rate (`int`, *optional*, defaults to 44100):\n             The sampling rate at which the audio files should be digitalized expressed in Hertz (Hz).\n         hop_length_to_sampling_rate (`int`, *optional*, defaults to 86):\n-            Hop length is length of the overlaping windows for the STFT used to obtain the Mel Frequency coefficients.\n+            Hop length is length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n             For example, with sampling rate 44100, the hop length is 512, with 44100 / 512 = 86\n         n_fft (`int`, *optional*, defaults to 2048):\n             Size of the Fourier transform.\n@@ -141,7 +141,7 @@ def __call__(\n \n                 <Tip>\n \n-                For TvltTransformer models, `attention_mask` should alwys be passed for batched inference, to avoid\n+                For TvltTransformer models, `attention_mask` should always be passed for batched inference, to avoid\n                 subtle bugs.\n \n                 </Tip>"
        },
        {
            "sha": "02d78c93407324ae99ec6a02f06f2135263cc3e8",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -356,10 +356,10 @@ def preprocess(\n \n             - **pixel_mask** -- Pixel masks to be fed to a model, of shape (batch_size, num_pixel_patches).\n \n-            - **pixel_values_mixed** -- Pixel values with both postive or negative to be fed to a model, of shape\n+            - **pixel_values_mixed** -- Pixel values with both positive or negative to be fed to a model, of shape\n               (batch_size, num_channels, height, width).\n \n-            - **pixel_mask_mixed** -- Pixel masks with both postive or negative to be fed to a model, of shape\n+            - **pixel_mask_mixed** -- Pixel masks with both positive or negative to be fed to a model, of shape\n               (batch_size, num_pixel_patches).\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize"
        },
        {
            "sha": "b509d60d124578dca56c270661e40e4b8b25feaf",
            "filename": "src/transformers/models/deprecated/van/convert_van_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -91,7 +91,7 @@ def __call__(self, x: Tensor):\n         for dest_m, src_m in zip(dest_traced, src_traced):\n             dest_m.load_state_dict(src_m.state_dict())\n             if self.verbose == 1:\n-                print(f\"Transfered from={src_m} to={dest_m}\")\n+                print(f\"Transferred from={src_m} to={dest_m}\")\n \n \n def copy_parameters(from_model: nn.Module, our_model: nn.Module) -> nn.Module:"
        },
        {
            "sha": "f07a76b2b2353446dd6245adf79f593af08039ab",
            "filename": "src/transformers/models/depth_anything/convert_depth_anything_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_depth_anything_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_depth_anything_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconvert_depth_anything_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -87,7 +87,7 @@ def create_rename_keys(config):\n     rename_keys.append((\"pretrained.patch_embed.proj.weight\", \"backbone.embeddings.patch_embeddings.projection.weight\"))\n     rename_keys.append((\"pretrained.patch_embed.proj.bias\", \"backbone.embeddings.patch_embeddings.projection.bias\"))\n \n-    # Transfomer encoder\n+    # Transformer encoder\n     for i in range(config.backbone_config.num_hidden_layers):\n         rename_keys.append((f\"pretrained.blocks.{i}.ls1.gamma\", f\"backbone.encoder.layer.{i}.layer_scale1.lambda1\"))\n         rename_keys.append((f\"pretrained.blocks.{i}.ls2.gamma\", f\"backbone.encoder.layer.{i}.layer_scale2.lambda1\"))"
        },
        {
            "sha": "d1aa64d2f60fcf15d065f40a837181f803d3dd03",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -902,14 +902,14 @@ def __init__(self, config):\n         for _ in range(self.num_layers - 1):\n             self.intermediate.append(DepthProFeatureFusionLayer(config))\n \n-        # final layer doesnot require deconvolution\n+        # final layer does not require deconvolution\n         self.final = DepthProFeatureFusionLayer(config, use_deconv=False)\n \n     def forward(self, hidden_states: List[torch.Tensor]) -> List[torch.Tensor]:\n         if self.num_layers != len(hidden_states):\n             raise ValueError(\n                 f\"num_layers={self.num_layers} in DepthProFeatureFusionStage\"\n-                f\"doesnot match len(hidden_states)={len(hidden_states)}\"\n+                f\"does not match len(hidden_states)={len(hidden_states)}\"\n             )\n \n         fused_hidden_states = []"
        },
        {
            "sha": "989d68e75d866ae17c893d7f5ab4650a35202cc3",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -138,7 +138,7 @@ class DetrObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~DetrImageProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1278,8 +1278,8 @@ def forward(\n         flattened_mask = mask.flatten(1)\n \n         # Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\n-        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (batch_size, height*width, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 inputs_embeds=flattened_features,\n@@ -1603,8 +1603,8 @@ def forward(\n         flattened_mask = mask.flatten(1)\n \n         # Fourth, sent flattened_features + flattened_mask + position embeddings through encoder\n-        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (batch_size, height*width, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.detr.model.encoder(\n                 inputs_embeds=flattened_features,\n@@ -1739,7 +1739,7 @@ def __init__(self, dim, fpn_dims, context_dim):\n                 nn.init.constant_(m.bias, 0)\n \n     def forward(self, x: Tensor, bbox_mask: Tensor, fpns: List[Tensor]):\n-        # here we concatenate x, the projected feature map, of shape (batch_size, d_model, heigth/32, width/32) with\n+        # here we concatenate x, the projected feature map, of shape (batch_size, d_model, height/32, width/32) with\n         # the bbox_mask = the attention maps of shape (batch_size, n_queries, n_heads, height/32, width/32).\n         # We expand the projected feature map to match the number of heads.\n         x = torch.cat([_expand(x, bbox_mask.shape[1]), bbox_mask.flatten(0, 1)], 1)"
        },
        {
            "sha": "e5f1a20ae52e51f58e069158a6083d5c4a669f49",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -475,7 +475,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "21aa2b4897ebe8ab2e17367798a4be5326bf53e3",
            "filename": "src/transformers/models/dpt/convert_dinov2_depth_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dinov2_depth_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dinov2_depth_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dinov2_depth_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -123,7 +123,7 @@ def create_rename_keys_backbone(config):\n     rename_keys.append((\"patch_embed.proj.weight\", \"backbone.embeddings.patch_embeddings.projection.weight\"))\n     rename_keys.append((\"patch_embed.proj.bias\", \"backbone.embeddings.patch_embeddings.projection.bias\"))\n \n-    # Transfomer encoder\n+    # Transformer encoder\n     for i in range(config.backbone_config.num_hidden_layers):\n         # layernorms\n         rename_keys.append((f\"blocks.{i}.norm1.weight\", f\"backbone.encoder.layer.{i}.norm1.weight\"))"
        },
        {
            "sha": "c4ff8a3eb7bf80cb2fd13aa8b194a42fedf9d9f4",
            "filename": "src/transformers/models/dpt/convert_dpt_beit_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_beit_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_beit_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_beit_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -77,7 +77,7 @@ def create_rename_keys(config):\n     rename_keys.append((\"pretrained.model.patch_embed.proj.weight\", \"backbone.embeddings.patch_embeddings.projection.weight\"))\n     rename_keys.append((\"pretrained.model.patch_embed.proj.bias\", \"backbone.embeddings.patch_embeddings.projection.bias\"))\n \n-    # Transfomer encoder\n+    # Transformer encoder\n     for i in range(config.backbone_config.num_hidden_layers):\n         rename_keys.append((f\"pretrained.model.blocks.{i}.gamma_1\", f\"backbone.encoder.layer.{i}.lambda_1\"))\n         rename_keys.append((f\"pretrained.model.blocks.{i}.gamma_2\", f\"backbone.encoder.layer.{i}.lambda_2\"))"
        },
        {
            "sha": "9a35ee4b4a3a3053438ae47d04ec00d7b71b3459",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -111,22 +111,22 @@ class DPTImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions. Can be overidden by `do_resize` in `preprocess`.\n+            Whether to resize the image's (height, width) dimensions. Can be overridden by `do_resize` in `preprocess`.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the image after resizing. Can be overidden by `size` in `preprocess`.\n+            Size of the image after resizing. Can be overridden by `size` in `preprocess`.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Defines the resampling filter to use if resizing the image. Can be overidden by `resample` in `preprocess`.\n+            Defines the resampling filter to use if resizing the image. Can be overridden by `resample` in `preprocess`.\n         keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n             If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-            be overidden by `keep_aspect_ratio` in `preprocess`.\n+            be overridden by `keep_aspect_ratio` in `preprocess`.\n         ensure_multiple_of (`int`, *optional*, defaults to 1):\n-            If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+            If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n             by `ensure_multiple_of` in `preprocess`.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overidden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             `preprocess`.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overidden by `rescale_factor` in `preprocess`.\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in `preprocess`.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method."
        },
        {
            "sha": "7943b5d1175043636b5fbfac58af74054549f164",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -383,7 +383,7 @@ def forward(self, hidden_states: torch.FloatTensor) -> torch.Tensor:\n \n class EfficientNetEncoder(nn.Module):\n     r\"\"\"\n-    Forward propogates the embeddings through each EfficientNet block.\n+    Forward propagates the embeddings through each EfficientNet block.\n \n     Args:\n         config ([`EfficientNetConfig`]):"
        },
        {
            "sha": "4bf75ff33e439c3ccb9dce9c2c56e3763be686b3",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1237,7 +1237,7 @@ def __call__(self, hidden_states, cls_index=None, deterministic: bool = True):\n         Returns:\n             `jnp.ndarray`: The summary of the sequence hidden states.\n         \"\"\"\n-        # NOTE: this doest \"first\" type summary always\n+        # NOTE: this does \"first\" type summary always\n         output = hidden_states[:, 0]\n         output = self.first_dropout(output, deterministic=deterministic)\n         output = self.summary(output)"
        },
        {
            "sha": "365274d7ed046ce2801fabc07bc0d589f3a2ea35",
            "filename": "src/transformers/models/electra/tokenization_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -464,7 +464,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "d2d0b56a8d38bc5a7296a59d0c199ac5de0cbd8e",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1780,7 +1780,7 @@ def forward(\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compilable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "52d32dbdeeaa27125c60ce062876bb745698c62b",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1123,7 +1123,7 @@ def forward(**super_kwargs):\n \n class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_model.lm_head.weight\"]\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compilable\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "c00b5ebd1ca1a3334b3a1e60db0ce97a9f58fb42",
            "filename": "src/transformers/models/encodec/configuration_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fconfiguration_encodec.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -38,7 +38,7 @@ class EncodecConfig(PretrainedConfig):\n \n     Args:\n         target_bandwidths (`List[float]`, *optional*, defaults to `[1.5, 3.0, 6.0, 12.0, 24.0]`):\n-            The range of diffent bandwiths the model can encode audio with.\n+            The range of different bandwidths the model can encode audio with.\n         sampling_rate (`int`, *optional*, defaults to 24000):\n             The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).\n         audio_channels (`int`, *optional*, defaults to 1):"
        },
        {
            "sha": "ba699d745e8aaad815395fad67f939b4a5c790a4",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -503,7 +503,7 @@ def _init_weights(self, module):\n ENCODEC_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_values (`torch.FloatTensor` of shape `(batch_size, channels, sequence_length)`, *optional*):\n-            Raw audio input converted to Float and padded to the approriate length in order to be encoded using chunks\n+            Raw audio input converted to Float and padded to the appropriate length in order to be encoded using chunks\n             of length self.chunk_length and a stride of `config.chunk_stride`.\n         padding_mask (`torch.BoolTensor` of shape `(batch_size, channels, sequence_length)`, *optional*):\n             Mask to avoid computing scaling factors on padding token indices (can we avoid computing conv on these+)."
        },
        {
            "sha": "af57b2596cee99eefe0493cc4aea51c845036d2e",
            "filename": "src/transformers/models/encoder_decoder/configuration_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fconfiguration_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -78,7 +78,7 @@ def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         if \"encoder\" not in kwargs or \"decoder\" not in kwargs:\n             raise ValueError(\n-                f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n+                f\"A configuration of type {self.model_type} cannot be instantiated because \"\n                 f\"both `encoder` and `decoder` sub-configurations were not passed, only {kwargs}\"\n             )\n         encoder_config = kwargs.pop(\"encoder\")"
        },
        {
            "sha": "ccb0aa0a6d2d0a2a5c89b22abcb647574b7582a2",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -784,7 +784,7 @@ def from_encoder_decoder_pretrained(\n                       [`~FlaxPreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "9926f8d10fd6a83b36e52410c209088f7c286163",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -341,7 +341,7 @@ def from_encoder_decoder_pretrained(\n                       `decoder_from_pt` should be set to `True`.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "8526771d79c41c9d6207cdbd0a080b312df5d816",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -604,7 +604,7 @@ def __init__(self, config: FlavaPossibleConfigs) -> None:\n         self.intermediate = FlavaIntermediate(config)\n         self.output = FlavaOutput(config)\n \n-        # TODO: Check fp32 layer norm possiblity\n+        # TODO: Check fp32 layer norm possibility\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n "
        },
        {
            "sha": "d1c624a88d370f0ee3ace862317ec45f596c8004",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -495,7 +495,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "314fba427a70fa9e35838e650d17977ca32c2eeb",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -329,7 +329,7 @@ def forward(\n             else:\n                 # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n                 # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n-                # not necessarily to eager (if mentionned options are provided).\n+                # not necessarily to eager (if mentioned options are provided).\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         if using_eager and self.reorder_and_upcast_attn:"
        },
        {
            "sha": "194fff7dd68a303d065afb26789b5346780cf763",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -195,7 +195,7 @@ def save_vocabulary(self, save_directory: str, filename_prefix: Optional[str] =\n \n class SubWordJapaneseTokenizer:\n     \"\"\"\n-    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT Lisence according to the\n+    https://github.com/tanreinama/Japanese-BPEEncoder_V2 This tokenizer class is under MIT License according to the\n     original repository.\n \n     MIT License"
        },
        {
            "sha": "79f3d72cc85bdae14767306fbdf3d3442596a34d",
            "filename": "src/transformers/models/granitemoe/configuration_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fconfiguration_granitemoe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -97,9 +97,9 @@ class GraniteMoeConfig(PretrainedConfig):\n         num_local_experts (`int`, *optional*, defaults to 8): total number of experts\n         num_experts_per_tok (`int`, *optional*, defaults to 2): number of experts per token\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss.\n-        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxialiary loss coefficient\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n \n     ```python\n     >>> from transformers import GraniteMoeModel, GraniteMoeConfig"
        },
        {
            "sha": "26496f7d0e2417ac717d53ef6f1654904fee9e47",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -237,11 +237,12 @@ class GraniteMoeParallelExperts(nn.Module):\n     def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n         \"\"\"\n         Initialize the GraniteMoeParallelExperts module.\n-        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's comptible with\n+        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's compatible with\n         many MoE libraries, such as [Megablock](https://github.com/databricks/megablocks) and\n         [ScatterMoE](https://github.com/shawntan/scattermoe), as well as the\n         [MoE kernel](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py)\n         used in vllm.\n+\n         Args:\n             num_experts (int):\n                 Number of experts.\n@@ -259,11 +260,13 @@ def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n     def forward(self, inputs, expert_size):\n         \"\"\"\n         Forward pass of the GraniteMoeParallelExperts module.\n+\n         Args:\n             inputs (Tensor):\n                 Input tensor.\n             expert_size:\n                 Expert size information.\n+\n         Returns:\n             Tensor: Output tensor.\n         \"\"\""
        },
        {
            "sha": "32b55c69f373b2776ec10477412589cc7a3c0b1c",
            "filename": "src/transformers/models/granitemoeshared/configuration_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fconfiguration_granitemoeshared.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -97,9 +97,9 @@ class GraniteMoeSharedConfig(PretrainedConfig):\n         num_local_experts (`int`, *optional*, defaults to 8): total number of experts\n         num_experts_per_tok (`int`, *optional*, defaults to 2): number of experts per token\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss.\n-        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxialiary loss coefficient\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001): router auxiliary loss coefficient\n         shared_intermediate_size (`int`, *optional*, defaults to 0): intermediate size for shared experts. 0 implies\n             no shared experts.\n "
        },
        {
            "sha": "2ca60e007bd77d681dcf1c5963008a56356e7a71",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -105,11 +105,12 @@ class GraniteMoeSharedParallelExperts(nn.Module):\n     def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n         \"\"\"\n         Initialize the GraniteMoeSharedParallelExperts module.\n-        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's comptible with\n+        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's compatible with\n         many MoE libraries, such as [Megablock](https://github.com/databricks/megablocks) and\n         [ScatterMoE](https://github.com/shawntan/scattermoe), as well as the\n         [MoE kernel](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py)\n         used in vllm.\n+\n         Args:\n             num_experts (int):\n                 Number of experts.\n@@ -127,11 +128,13 @@ def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n     def forward(self, inputs, expert_size):\n         \"\"\"\n         Forward pass of the GraniteMoeSharedParallelExperts module.\n+\n         Args:\n             inputs (Tensor):\n                 Input tensor.\n             expert_size:\n                 Expert size information.\n+\n         Returns:\n             Tensor: Output tensor.\n         \"\"\""
        },
        {
            "sha": "6d2a2a8df23450bc25c7477ea82e921f5eb815cb",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -262,7 +262,7 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~GroundingDinoProcessor.post_process_grounded_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`List[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n@@ -2098,9 +2098,9 @@ def get_valid_ratio(self, mask):\n         _, height, width = mask.shape\n         valid_height = torch.sum(mask[:, :, 0], 1)\n         valid_width = torch.sum(mask[:, 0, :], 1)\n-        valid_ratio_heigth = valid_height.float() / height\n+        valid_ratio_height = valid_height.float() / height\n         valid_ratio_width = valid_width.float() / width\n-        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_heigth], -1)\n+        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_height], -1)\n         return valid_ratio\n \n     def generate_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes):\n@@ -2136,8 +2136,8 @@ def generate_encoder_output_proposals(self, enc_output, padding_mask, spatial_sh\n \n             scale = torch.cat([valid_width.unsqueeze(-1), valid_height.unsqueeze(-1)], 1).view(batch_size, 1, 1, 2)\n             grid = (grid.unsqueeze(0).expand(batch_size, -1, -1, -1) + 0.5) / scale\n-            width_heigth = torch.ones_like(grid) * 0.05 * (2.0**level)\n-            proposal = torch.cat((grid, width_heigth), -1).view(batch_size, -1, 4)\n+            width_height = torch.ones_like(grid) * 0.05 * (2.0**level)\n+            proposal = torch.cat((grid, width_height), -1).view(batch_size, -1, 4)\n             proposals.append(proposal)\n             current_position += height * width\n "
        },
        {
            "sha": "36d41bfc57b5fe8465ea0f1c645d832ebc7846b0",
            "filename": "src/transformers/models/hubert/configuration_hubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -106,7 +106,7 @@ class HubertConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -118,7 +118,7 @@ class HubertConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "c243ebde9e2ebe94eb4de4dddb0a3cbc047e1c7d",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -199,7 +199,7 @@ def freeze_model(model, module_exceptions=[]):\n     module_exceptions_mapped = [mapping[m] for m in module_exceptions]\n     for module in model.modules():\n         if module_exceptions and any(isinstance(module, t) for t in module_exceptions_mapped):\n-            module.requires_grad_(True)  # Explicitely setting it to true to avoid any mistakes\n+            module.requires_grad_(True)  # Explicitly setting it to true to avoid any mistakes\n         else:\n             module.requires_grad_(False)\n     return model\n@@ -1235,7 +1235,7 @@ def forward(\n             image_attention_mask = None\n \n         # cross_attention_gate:\n-        # For any tokens attending to no images, the hidden_states comming out of the cross-attention should be zeroed-out.\n+        # For any tokens attending to no images, the hidden_states coming out of the cross-attention should be zeroed-out.\n         # `image_attention_mask` has shape [bsz, 1, num_images, hidden_size] with elements equal to either 0.0 or a very negative number.\n         # If any of the elements are 0.0, then the token is attending to at least one image and the gate value is 1. Otherwise the gate value is 0.\n         # `cross_attention_gate` has shape [bsz, seq_len] with elements equal to either 0.0 or 1.0."
        },
        {
            "sha": "37876080dfcfea5bf949bcc45817619b675c1c7b",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -318,7 +318,7 @@ def __call__(\n         and the two images will be massaged using [`IdeficsImageProcessor.__call__`] method and placed inside the\n         `pixel_values` dict entry of the return value.\n \n-        This example also examplifies that images can be passed as objects or as text urls. It can be seen that the\n+        This example also exemplifies that images can be passed as objects or as text urls. It can be seen that the\n         first image is passed as object and the second one as a url.\n \n         To do training do:"
        },
        {
            "sha": "b72b6f0b538786e2e41c9c4f152960e984acec48",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -598,7 +598,7 @@ def __init__(self, config: JambaConfig, layer_idx):\n \n         # projection of the input hidden states\n         self.in_proj = nn.Linear(self.hidden_size, self.intermediate_size * 2, bias=self.use_bias)\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n         self.x_proj = nn.Linear(self.intermediate_size, self.time_step_rank + self.ssm_state_size * 2, bias=False)\n         # time step projection (discretization)\n         self.dt_proj = nn.Linear(self.time_step_rank, self.intermediate_size, bias=True)\n@@ -1547,7 +1547,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n+        # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "5846ea369da7adf7f1a96410ca05808583cc478b",
            "filename": "src/transformers/models/jetmoe/configuration_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fconfiguration_jetmoe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -57,7 +57,7 @@ class JetMoeConfig(PretrainedConfig):\n         num_experts_per_tok (`int, *optional*, defaults to 2):\n             The number of experts to route per-token and for MoE and MoA.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss.\n         aux_loss_coef (`float`, *optional*, defaults to 0.01):\n             The coefficient for the auxiliary loss."
        },
        {
            "sha": "180f90676ba4d0b5d0b2bd65ee62c15a09cfa8fe",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -147,7 +147,7 @@ class JetMoeParallelExperts(nn.Module):\n     def __init__(self, num_experts: int, input_size: int, output_size: int) -> None:\n         \"\"\"\n         Initialize the JetMoeParallelExperts module.\n-        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's comptible with\n+        The experts weights are stored in [num_experts, output_size, input_size] format. Such that it's compatible with\n         many MoE libraries, such as [Megablock](https://github.com/databricks/megablocks) and\n         [ScatterMoE](https://github.com/shawntan/scattermoe), as well as the\n         [MoE kernel](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/fused_moe/fused_moe.py)"
        },
        {
            "sha": "135ca2f68a194cc7eb2cfd90ee43c2217e7ca753",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -465,7 +465,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "1fa23c32ae4cbeb65d4b60d8099dc507d2fd2979",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1521,7 +1521,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "d55a9d9b0e465e47baaa8f28e4cf96a542c75a04",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -448,7 +448,7 @@ def forward(\n             attention_scores = attention_scores + attention_mask\n \n         # Normalize the attention scores to probabilities.\n-        # Use the trick of the CogView paper to stablize training\n+        # Use the trick of the CogView paper to stabilize training\n         attention_probs = self.cogview_attention(attention_scores)\n \n         # This is actually dropping out entire tokens to attend to, which might"
        },
        {
            "sha": "be56ef1bb1c91ec1d94ca35583e34757e3d8b167",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -2467,7 +2467,7 @@ class LEDForSequenceClassification(LEDPreTrainedModel):\n     def __init__(self, config: LEDConfig, **kwargs):\n         warnings.warn(\n             \"The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of\"\n-            \" Transformers. No actual method were provided in the original paper on how to perfom\"\n+            \" Transformers. No actual method were provided in the original paper on how to perform\"\n             \" sequence classification.\",\n             FutureWarning,\n         )"
        },
        {
            "sha": "923d9ffc63bf0437097dc95d28a04cf1ac3858a5",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -239,7 +239,7 @@ def write_model(\n \n     config_kwargs = {}\n     if params[\"use_scaled_rope\"]:\n-        # some constans from original code\n+        # some constants from original code\n         rope_scaling = {\n             \"rope_type\": \"llama3\",\n             \"factor\": 8.0,\n@@ -288,7 +288,7 @@ def write_model(\n         for_llm_compressor=_OFFLINE_QUANT_COMPATIBLE,\n         **config_kwargs,\n     )\n-    # default vision config frmo params\n+    # default vision config from params\n \n     vision_params = params[\"vision_args\"]\n     vision_dim = vision_params[\"dim\"]"
        },
        {
            "sha": "0959199c2efd811f879679c60ffdf54f4732850c",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -857,7 +857,7 @@ def create_chunked_attention_mask(\n         '?'         :  5           |\n \n         If the chunk size is 3.\n-        This can just be appplied over the already created attention mask\n+        This can just be applied over the already created attention mask\n         \"\"\"\n         arange_vector = torch.arange(start, end, device=device)\n         block_pos = torch.abs(\n@@ -894,7 +894,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "33dbe37d581c049804ff719872bed29e2d604ff9",
            "filename": "src/transformers/models/llava/convert_llava_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconvert_llava_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -72,7 +72,7 @@ def load_original_state_dict(model_id):\n                 for key in f.keys():\n                     original_state_dict[key] = f.get_tensor(key)\n \n-    # tied wieghts so lm.head is not saved. Let's clone to load state dict\n+    # tied weights so lm.head is not saved. Let's clone to load state dict\n     if \"lm_head.weight\" not in original_state_dict:\n         original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n \n@@ -127,7 +127,7 @@ def convert_llava_llama_to_hf(text_model_id, vision_model_id, output_hub_path, o\n         vision_config=vision_config,\n     )\n \n-    # llms-lab interleeave models do not use any selection startegy except for last hidden state\n+    # llms-lab interleave models do not use any selection strategy except for last hidden state\n     if \"Qwen\" in text_model_id:\n         config.image_token_id = 151646\n         if \"siglip\" in vision_model_id:"
        },
        {
            "sha": "be98c0a3e565ca8f75e1655f52e734e7f24d55a1",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -53,7 +53,7 @@ class LlavaProcessor(ProcessorMixin):\n             Patch size from the vision tower.\n         vision_feature_select_strategy (`str`, *optional*):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n-            Shoudl be same as in model's config\n+            Should be same as in model's config\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):"
        },
        {
            "sha": "63246e8a53aba102a510937cd3f829df0e13b4cb",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -688,7 +688,7 @@ def preprocess(\n         image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n         for image in images:\n             # convert image into a list of patches\n-            # we intentially use the same data format as the input data format\n+            # we intentionally use the same data format as the input data format\n             image_patches = self.get_image_patches(\n                 image,\n                 image_grid_pinpoints,"
        },
        {
            "sha": "e1409b5d1dac96a6526dea4a192dd689ac0bb4cb",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -56,7 +56,7 @@ class LlavaNextProcessor(ProcessorMixin):\n             Patch size from the vision tower.\n         vision_feature_select_strategy (`str`, *optional*):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n-            Shoudl be same as in model's config\n+            Should be same as in model's config\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):"
        },
        {
            "sha": "12ba442175332ee2ce37d749967cff5373dfea2c",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -64,7 +64,7 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n             Patch size from the vision tower.\n         vision_feature_select_strategy (`str`, *optional*):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n-            Shoudl be same as in model's config\n+            Should be same as in model's config\n         video_token (`str`, *optional*, defaults to `\"<video>\"`):\n             Special token used to denote video location.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):"
        },
        {
            "sha": "8b0ebe03bf503b520633e2d313862937fbbfff45",
            "filename": "src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -69,7 +69,7 @@ def load_original_state_dict(model_id):\n                 for key in f.keys():\n                     original_state_dict[key] = f.get_tensor(key)\n \n-    # tied wieghts so lm.head is not saved. Let's clone to load state dict\n+    # tied weights so lm.head is not saved. Let's clone to load state dict\n     if \"lm_head.weight\" not in original_state_dict:\n         original_state_dict[\"lm_head.weight\"] = original_state_dict[\"model.embed_tokens.weight\"].clone()\n "
        },
        {
            "sha": "5a9bb5e3eaef90d92c6d3f3ca15e8b199a192188",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -132,7 +132,7 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n         image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n             A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n             based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processinf videos.\n+            method. Not used for processing videos.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n@@ -647,7 +647,7 @@ def preprocess(\n         image_sizes = [get_image_size(image, channel_dim=input_data_format) for image in images]\n         for image in images:\n             # convert image into a list of patches\n-            # we intentially use the same data format as the input data format\n+            # we intentionally use the same data format as the input data format\n             size_tuple = (\n                 (size[\"height\"], size[\"width\"])\n                 if \"height\" in size and \"width\" in size"
        },
        {
            "sha": "753abc924ac514b3851266c83fae3e567ed318e5",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -63,7 +63,7 @@ class LlavaOnevisionProcessor(ProcessorMixin):\n             Number of image tokens for one imagethat will be returned by vision tower.\n         vision_feature_select_strategy (`str`, *optional*):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n-            Shoudl be same as in model's config\n+            Should be same as in model's config\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):"
        },
        {
            "sha": "f54ec03c8f3e9bfdf6f975414be26106af9e1ba5",
            "filename": "src/transformers/models/longt5/configuration_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fconfiguration_longt5.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -54,7 +54,7 @@ class LongT5Config(PretrainedConfig):\n         local_radius (`int`, *optional*, defaults to 127)\n             Number of tokens to the left/right for each token to locally self-attend in a local attention mechanism.\n         global_block_size (`int`, *optional*, defaults to 16)\n-            Lenght of blocks an input sequence is divided into for a global token representation. Used only for\n+            Length of blocks an input sequence is divided into for a global token representation. Used only for\n             `encoder_attention_type = \"transient-global\"`.\n         relative_attention_num_buckets (`int`, *optional*, defaults to 32):\n             The number of buckets to use for each attention layer."
        },
        {
            "sha": "d99797107363d2d0175d3b29033fb415a8073a25",
            "filename": "src/transformers/models/longt5/convert_longt5x_checkpoint_to_flax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fconvert_longt5x_checkpoint_to_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fconvert_longt5x_checkpoint_to_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fconvert_longt5x_checkpoint_to_flax.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -198,7 +198,7 @@ def convert_t5x_checkpoint_to_flax(t5x_checkpoint_path, config_name, flax_dump_f\n         flax_model.params[\"lm_head\"][\"kernel\"] = t5x_model[\"target\"][\"decoder\"][\"logits_dense\"][\"kernel\"]\n \n     flax_model.save_pretrained(flax_dump_folder_path)\n-    print(\"T5X Model was sucessfully converted!\")\n+    print(\"T5X Model was successfully converted!\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "130170441430bc5f05cd4e881845c596e6f02fa8",
            "filename": "src/transformers/models/longt5/modeling_flax_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -145,7 +145,7 @@ def _get_local_attention_mask(attention_mask: np.ndarray, block_len: int) -> jnp\n def _make_global_fixed_block_ids(attention_mask: np.ndarray, global_block_size: int) -> Tuple[jnp.ndarray, np.ndarray]:\n     \"\"\"Obtain the \"fixed block\" global id corresponding to each input token.\n \n-    This implementation is a simlified version of the original Flaxformr implementation adopted from:\n+    This implementation is a simplified version of the original Flaxformr implementation adopted from:\n     https://github.com/google/flaxformer/blob/main/flaxformer/architectures/longt5/long_attention.py.\n \n     In our scenario, as we use this strategy only for a decoder, orphan tokens, i.e. those tokens which do not make for"
        },
        {
            "sha": "a509df9df2bbbdd1ee5cba9cd82dccabb810420e",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -158,7 +158,7 @@ def _make_global_fixed_block_ids(\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n     \"\"\"Obtain the \"fixed block\" global id corresponding to each input token.\n \n-    This implementation is a simlified version of the original Flaxformr implementation adopted from:\n+    This implementation is a simplified version of the original Flaxformr implementation adopted from:\n     https://github.com/google/flaxformer/blob/main/flaxformer/architectures/longt5/long_attention.py.\n \n     In our scenario, as we use this strategy only for a decoder, orphan tokens, i.e. those tokens which do not make for"
        },
        {
            "sha": "18d3d2e60d7b41cb9303f7b09f279c34a9b0d134",
            "filename": "src/transformers/models/lxmert/configuration_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fconfiguration_lxmert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -78,7 +78,7 @@ class LxmertConfig(PretrainedConfig):\n             This represents the last dimension of the pooled-object features used as input for the model, representing\n             the size of each object feature itself.\n         visual_pos_dim (`int`, *optional*, defaults to 4):\n-            This represents the number of spacial features that are mixed into the visual features. The default is set\n+            This represents the number of spatial features that are mixed into the visual features. The default is set\n             to 4 because most commonly this will represent the location of a bounding box. i.e., (x, y, width, height)\n         visual_loss_normalizer (`float`, *optional*, defaults to 6.67):\n             This represents the scaling factor in which each visual loss is multiplied by if during pretraining, one"
        },
        {
            "sha": "4f25a67177cd0169583a35dbdbf341c2664679ff",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -832,8 +832,8 @@ def _init_weights(self, module):\n \n             These are currently not provided by the transformers library.\n         visual_pos (`torch.FloatTensor` of shape `(batch_size, num_visual_features, visual_pos_dim)`):\n-            This input represents spacial features corresponding to their relative (via index) visual features. The\n-            pre-trained LXMERT model expects these spacial features to be normalized bounding boxes on a scale of 0 to\n+            This input represents spatial features corresponding to their relative (via index) visual features. The\n+            pre-trained LXMERT model expects these spatial features to be normalized bounding boxes on a scale of 0 to\n             1.\n \n             These are currently not provided by the transformers library."
        },
        {
            "sha": "0efea463291e145972e8ed33ffe6a748a5bce178",
            "filename": "src/transformers/models/lxmert/modeling_tf_lxmert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_tf_lxmert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1036,8 +1036,8 @@ def input_signature(self):\n \n             These are currently not provided by the transformers library.\n         visual_pos (`tf.Tensor` of shape `(batch_size, num_visual_features, visual_feat_dim)`):\n-            This input represents spacial features corresponding to their relative (via index) visual features. The\n-            pre-trained LXMERT model expects these spacial features to be normalized bounding boxes on a scale of 0 to\n+            This input represents spatial features corresponding to their relative (via index) visual features. The\n+            pre-trained LXMERT model expects these spatial features to be normalized bounding boxes on a scale of 0 to\n             1.\n \n             These are currently not provided by the transformers library."
        },
        {
            "sha": "dcf2f80061ba71fdb030d9997cfc872985015b9a",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -464,7 +464,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "4f6c17af01f7e92572064ce6726be3afb7bb6852",
            "filename": "src/transformers/models/mamba/configuration_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fconfiguration_mamba.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -80,7 +80,7 @@ class MambaConfig(PretrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the cache should be used.\n         use_mambapy (`bool`, *optional*, defaults to `False`):\n-            Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not avaiable. If `True`, the mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n+            Determines the fallback strategy during training if the CUDA-based official implementation of Mamba is not available. If `True`, the mamba.py implementation is used. If `False`, the naive and slower implementation is used. Consider switching to the naive version if memory is limited.\n \n \n     Example:"
        },
        {
            "sha": "99f7ebbb88cd99df23f784f9a078a00090c82e89",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -274,7 +274,7 @@ def __init__(self, config: Mamba2Config, layer_idx: int):\n             projection_size,\n             bias=config.use_bias,\n         )\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n \n         # time step projection (discretization)\n         # instantiate once and copy inv_dt in init_weights of PretrainedModel\n@@ -1032,7 +1032,7 @@ def prepare_inputs_for_generation(\n         attention_mask: Optional[torch.Tensor] = None,\n         **kwargs,\n     ):\n-        # Overwitten -- uses `cache_params` as opposed to `past_key_values`\n+        # Overwritten -- uses `cache_params` as opposed to `past_key_values`\n \n         if use_cache:\n             # `cache_position` should have been initialized in `generate`"
        },
        {
            "sha": "fd38b2e17cb42048938ece109c82b6a7eecc2189",
            "filename": "src/transformers/models/marian/convert_marian_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fconvert_marian_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -68,7 +68,7 @@ def find_pretrained_model(src_lang: str, tgt_lang: str) -> List[str]:\n     model_ids = [x.id for x in model_list if x.id.startswith(\"Helsinki-NLP\")]\n     src_and_targ = [\n         remove_prefix(m, prefix).lower().split(\"-\") for m in model_ids if \"+\" not in m\n-    ]  # + cant be loaded.\n+    ]  # + can't be loaded.\n     matching = [f\"{prefix}{a}-{b}\" for (a, b) in src_and_targ if src_lang in a and tgt_lang in b]\n     return matching\n "
        },
        {
            "sha": "5eeab498a52764fa784878f66e44bb05745ec36e",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1257,7 +1257,7 @@ def resize_token_embeddings(\n             self._resize_final_logits_bias(new_num_tokens)\n         return new_embeddings\n \n-    # NOTE: `_resize_token_embeddings` was rewriten in the base class, *args exists to absorb the extra arg\n+    # NOTE: `_resize_token_embeddings` was rewritten in the base class, *args exists to absorb the extra arg\n     def _resize_token_embeddings(self, new_num_tokens: int, pad_to_multiple_of=None, *args) -> nn.Embedding:\n         old_embeddings = self.get_input_embeddings()\n         new_embeddings = self._get_resized_embeddings(old_embeddings, new_num_tokens, pad_to_multiple_of)"
        },
        {
            "sha": "bdd48bc8f36ab4b69157d7d9ac37c352b1b2daa5",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -448,7 +448,7 @@ def forward(\n             pred_probs = class_queries_logits[i].softmax(-1)\n             pred_mask = masks_queries_logits[i]\n \n-            # Compute the classification cost. Contrary to the loss, we don't use the NLL, but approximate it in 1 - proba[target class]. The 1 is a constant that doesn't change the matching, it can be ommitted.\n+            # Compute the classification cost. Contrary to the loss, we don't use the NLL, but approximate it in 1 - proba[target class]. The 1 is a constant that doesn't change the matching, it can be omitted.\n             cost_class = -pred_probs[:, class_labels[i]]\n             target_mask = mask_labels[i].to(pred_mask)\n             target_mask = target_mask[:, None]\n@@ -465,15 +465,15 @@ def forward(\n \n             # compute the cross entropy loss between each mask pairs -> shape (num_queries, num_labels)\n             cost_mask = pair_wise_sigmoid_cross_entropy_loss(pred_mask, target_mask)\n-            # Compute the dice loss betwen each mask pairs -> shape (num_queries, num_labels)\n+            # Compute the dice loss between each mask pairs -> shape (num_queries, num_labels)\n             cost_dice = pair_wise_dice_loss(pred_mask, target_mask)\n             # final cost matrix\n             cost_matrix = self.cost_mask * cost_mask + self.cost_class * cost_class + self.cost_dice * cost_dice\n             # eliminate infinite values in cost_matrix to avoid the error ``ValueError: cost matrix is infeasible``\n             cost_matrix = torch.minimum(cost_matrix, torch.tensor(1e10))\n             cost_matrix = torch.maximum(cost_matrix, torch.tensor(-1e10))\n             cost_matrix = torch.nan_to_num(cost_matrix, 0)\n-            # do the assigmented using the hungarian algorithm in scipy\n+            # do the assignment using the hungarian algorithm in scipy\n             assigned_indices: Tuple[np.array] = linear_sum_assignment(cost_matrix.cpu())\n             indices.append(assigned_indices)\n \n@@ -1275,9 +1275,9 @@ def get_valid_ratio(self, mask, dtype=torch.float32):\n         _, height, width = mask.shape\n         valid_height = torch.sum(~mask[:, :, 0], 1)\n         valid_width = torch.sum(~mask[:, 0, :], 1)\n-        valid_ratio_heigth = valid_height.to(dtype) / height\n+        valid_ratio_height = valid_height.to(dtype) / height\n         valid_ratio_width = valid_width.to(dtype) / width\n-        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_heigth], -1)\n+        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_height], -1)\n         return valid_ratio\n \n     def forward("
        },
        {
            "sha": "e354c21c401dd42f91dab1701c8ab5668a9a88cf",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -871,19 +871,19 @@ def forward(self, masks_queries_logits, class_queries_logits, mask_labels, class\n             pred_probs = pred_probs.softmax(-1)\n             # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n             # but approximate it in 1 - proba[target class].\n-            # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+            # The 1 is a constant that doesn't change the matching, it can be omitted.\n             cost_class = -pred_probs[:, labels]\n             # flatten spatial dimension \"q h w -> q (h w)\"\n             pred_mask_flat = pred_mask.flatten(1)  # [num_queries, height*width]\n             # same for target_mask \"c h w -> c (h w)\"\n             target_mask_flat = target_mask[:, 0].flatten(1)  # [num_total_labels, height*width]\n             # compute the focal loss between each mask pairs -> shape (num_queries, num_labels)\n             cost_mask = pair_wise_sigmoid_focal_loss(pred_mask_flat, target_mask_flat)\n-            # Compute the dice loss betwen each mask pairs -> shape (num_queries, num_labels)\n+            # Compute the dice loss between each mask pairs -> shape (num_queries, num_labels)\n             cost_dice = pair_wise_dice_loss(pred_mask_flat, target_mask_flat)\n             # final cost matrix\n             cost_matrix = self.cost_mask * cost_mask + self.cost_class * cost_class + self.cost_dice * cost_dice\n-            # do the assigmented using the hungarian algorithm in scipy\n+            # do the assignment using the hungarian algorithm in scipy\n             assigned_indices: Tuple[np.array] = linear_sum_assignment(cost_matrix.cpu())\n             indices.append(assigned_indices)\n \n@@ -923,7 +923,7 @@ def __init__(\n             num_labels (`int`):\n                 The number of classes.\n             matcher (`MaskFormerHungarianMatcher`):\n-                A torch module that computes the assigments between the predictions and labels.\n+                A torch module that computes the assignments between the predictions and labels.\n             weight_dict (`Dict[str, float]`):\n                 A dictionary of weights to be applied to the different losses.\n             eos_coef (`float`):\n@@ -1085,7 +1085,7 @@ def forward(\n             - **loss_mask** -- The loss computed using sigmoid focal loss on the predicted and ground truth masks.\n             - **loss_dice** -- The loss computed using dice loss on the predicted on the predicted and ground truth\n               masks.\n-            if `use_auxiliary_loss` was set to `true` in [`MaskFormerConfig`], the dictionary contains addional losses\n+            if `use_auxiliary_loss` was set to `true` in [`MaskFormerConfig`], the dictionary contains additional losses\n             for each auxiliary predictions.\n         \"\"\"\n "
        },
        {
            "sha": "3cb72c6c8463e10980cb20204182bca4fa23aab0",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -935,7 +935,7 @@ def forward(\n             zip(hidden_states, self.stage_names[1:], spatial_dimensions)\n         ):\n             norm = self.hidden_states_norms[i]\n-            # the last element corespond to the layer's last block output but before patch merging\n+            # the last element correspond to the layer's last block output but before patch merging\n             hidden_state_unpolled = hidden_state[-1]\n             hidden_state_norm = norm(hidden_state_unpolled)\n             # the pixel decoder (FPN) expects 3D tensors (features)"
        },
        {
            "sha": "52f076a05ac424fc1f89a79e975e21f1aa376e96",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -326,7 +326,7 @@ def call(\n         )\n \n         if past_key_value is not None:\n-            # resue k, v, self_attention\n+            # reuse k, v, self_attention\n             key_states = tf.concat([past_key_value[0], key_states], axis=2)\n             value_states = tf.concat([past_key_value[1], value_states], axis=2)\n "
        },
        {
            "sha": "066b045ee1ef2976bf92cf36685f758592154310",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -87,7 +87,7 @@ class MixtralConfig(PretrainedConfig):\n         num_local_experts (`int`, *optional*, defaults to 8):\n             Number of experts per Sparse MLP layer.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss. See [here]() for more details\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "f5f338fd8ddb4e16a083ad1ba1d78230a67b4fee",
            "filename": "src/transformers/models/mllama/convert_mllama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -234,7 +234,7 @@ def write_model(\n     text_rope_theta = params[\"rope_theta\"]\n     cross_attention_num_layers = params[\"vision_num_cross_attention_layers\"]\n \n-    # some constans from original code\n+    # some constants from original code\n     rope_scaling = {\n         \"rope_type\": \"llama3\",\n         \"factor\": 8.0,"
        },
        {
            "sha": "1f9de552706b4d703d405a42d79cbec5e426797d",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -466,7 +466,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "974a4dc1e4c75cb3cf5c906c7b42158aadff5248",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -160,7 +160,7 @@ def __init__(\n         \"\"\"\n         max_seqlen: if max_seqlen, device, and dtype are provided, we precompute the cos_sin_cache\n             up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n-            the cos_sin_cache wll be recomputed during the forward pass.\n+            the cos_sin_cache will be recomputed during the forward pass.\n         \"\"\"\n         super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n         self.max_seqlen = max_seqlen"
        },
        {
            "sha": "9e74b4d7870f3fe7798568a6a56d572f76911108",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -423,7 +423,7 @@ def __init__(\n         \"\"\"\n         max_seqlen: if max_seqlen, device, and dtype are provided, we precompute the cos_sin_cache\n             up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n-            the cos_sin_cache wll be recomputed during the forward pass.\n+            the cos_sin_cache will be recomputed during the forward pass.\n         \"\"\"\n         super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n         self.max_seqlen = max_seqlen"
        },
        {
            "sha": "0aca1cf2614a5c884abf988917142101a675ab4e",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -490,7 +490,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "bff4c0b06c35cf6f159174d40f5c52021cf1b904",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -69,7 +69,7 @@ def build_mpt_alibi_tensor(num_heads, sequence_length, alibi_bias_max=8, device=\n \n class MptAttention(nn.Module):\n     \"\"\"Multi-head self attention.\n-    Using torch or triton attention implemetation enables user to also use additive bias.\n+    Using torch or triton attention implementation enables user to also use additive bias.\n     \"\"\"\n \n     def __init__(self, config: MptConfig):"
        },
        {
            "sha": "cdee89efdaf444efa830eb6043e25c8808bc0ef3",
            "filename": "src/transformers/models/mra/convert_mra_pytorch_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -87,7 +87,7 @@ def convert_mra_checkpoint(checkpoint_path, mra_config_file, pytorch_dump_path):\n     model.eval()\n     model.save_pretrained(pytorch_dump_path)\n \n-    print(f\"Checkpoint successfuly converted. Model saved at {pytorch_dump_path}\")\n+    print(f\"Checkpoint successfully converted. Model saved at {pytorch_dump_path}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "9170f947fed37c74a6a60a7fc9333230285d30ed",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1693,7 +1693,7 @@ def get_text_encoder(self):\n         return self.text_encoder\n \n     def get_encoder(self):\n-        # get the text encoder to compute the conditionning hidden-states for generation\n+        # get the text encoder to compute the conditioning hidden-states for generation\n         return self.get_text_encoder()\n \n     def get_decoder(self):"
        },
        {
            "sha": "7be48691814eb72359f867b6e2e591470ba89992",
            "filename": "src/transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -88,7 +88,7 @@ def convert_nystromformer_checkpoint(checkpoint_path, nystromformer_config_file,\n     model.eval()\n     model.save_pretrained(pytorch_dump_path)\n \n-    print(f\"Checkpoint successfuly converted. Model saved at {pytorch_dump_path}\")\n+    print(f\"Checkpoint successfully converted. Model saved at {pytorch_dump_path}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "2b85ea55e3dc1c605a73ab9d9367d720987cac39",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -86,7 +86,7 @@ class OlmoeConfig(PretrainedConfig):\n         num_experts (`int`, *optional*, defaults to 64):\n             Number of routed experts.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.01):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "47438cde419756c9cdf87afb16c6ea13d00126b0",
            "filename": "src/transformers/models/oneformer/configuration_oneformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -74,7 +74,7 @@ class OneFormerConfig(PretrainedConfig):\n         importance_sample_ratio (`float`, *optional*, defaults to 0.75):\n             Ratio of points that are sampled via importance sampling.\n         init_std (`float`, *optional*, defaults to 0.02):\n-            Standard deviation for normal intialization.\n+            Standard deviation for normal initialization.\n         init_xavier_std (`float`, *optional*, defaults to 1.0):\n             Standard deviation for xavier uniform initialization.\n         layer_norm_eps (`float`, *optional*, defaults to 1e-05):"
        },
        {
            "sha": "212edc1c854af0f5e7b44c098362d377e82937da",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -261,7 +261,7 @@ def __init__(\n         \"\"\"\n         super().__init__()\n         if cost_class == 0 and cost_mask == 0 and cost_dice == 0:\n-            raise ValueError(\"All costs cant be 0\")\n+            raise ValueError(\"All costs can't be 0\")\n         self.cost_class = cost_class\n         self.cost_mask = cost_mask\n         self.cost_dice = cost_dice\n@@ -304,7 +304,7 @@ def forward(self, masks_queries_logits, class_queries_logits, mask_labels, class\n             pred_probs = pred_probs.softmax(-1)\n             # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n             # but approximate it in 1 - proba[target class].\n-            # The 1 is a constant that doesn't change the matching, it can be ommitted.\n+            # The 1 is a constant that doesn't change the matching, it can be omitted.\n             cost_class = -pred_probs[:, labels]\n \n             pred_mask = pred_mask[:, None]\n@@ -371,7 +371,7 @@ def __init__(\n             num_labels (`int`):\n                 The number of classes.\n             matcher (`OneFormerHungarianMatcher`):\n-                A torch module that computes the assigments between the predictions and labels.\n+                A torch module that computes the assignments between the predictions and labels.\n             weight_dict (`Dict[str, float]`):\n                 A dictionary of weights to be applied to the different losses.\n             eos_coef (`float`):\n@@ -684,7 +684,7 @@ def forward(\n             - **loss_dice** -- The loss computed using dice loss on the predicted on the predicted and ground truth\n               masks.\n             - **loss_contrastive** -- The query-text contrstive loss computed using object and text queries.\n-            if `use_auxiliary_loss` was set to `true` in [`OneFormerConfig`], the dictionary contains addional losses\n+            if `use_auxiliary_loss` was set to `true` in [`OneFormerConfig`], the dictionary contains additional losses\n             for each auxiliary predictions.\n         \"\"\"\n \n@@ -1367,9 +1367,9 @@ def get_valid_ratio(self, mask, dtype=torch.float32):\n         _, height, width = mask.shape\n         valid_height = torch.sum(~mask[:, :, 0], 1)\n         valid_width = torch.sum(~mask[:, 0, :], 1)\n-        valid_ratio_heigth = valid_height.to(dtype) / height\n+        valid_ratio_height = valid_height.to(dtype) / height\n         valid_ratio_width = valid_width.to(dtype) / width\n-        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_heigth], -1)\n+        valid_ratio = torch.stack([valid_ratio_width, valid_ratio_height], -1)\n         return valid_ratio\n \n     def forward("
        },
        {
            "sha": "bc211d1fb4b30e21c12c721002cea39f04ece97e",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -88,7 +88,7 @@ def _scale_boxes(boxes, target_sizes):\n     else:\n         raise ValueError(\"`target_sizes` must be a list, tuple or torch.Tensor\")\n \n-    # for owlv2 image is padded to max size unlike owlvit, thats why we have to scale boxes to max size\n+    # for owlv2 image is padded to max size unlike owlvit, that's why we have to scale boxes to max size\n     max_size = torch.max(image_height, image_width)\n \n     scale_factor = torch.stack([max_size, max_size, max_size, max_size], dim=1)"
        },
        {
            "sha": "ce4a3ed49d289766e9199f8c6ad2bb61784842f7",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -58,7 +58,7 @@ class Owlv2ProcessorKwargs(ProcessingKwargs, total=False):\n class Owlv2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs an Owlv2 processor which wraps [`Owlv2ImageProcessor`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`] into\n-    a single processor that interits both the image processor and tokenizer functionalities. See the\n+    a single processor that inherits both the image processor and tokenizer functionalities. See the\n     [`~OwlViTProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more information.\n \n     Args:"
        },
        {
            "sha": "564b198ac1476f878fa9a26d099d6b0c0cfcc7cb",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -58,7 +58,7 @@ class OwlViTProcessorKwargs(ProcessingKwargs, total=False):\n class OwlViTProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an OWL-ViT processor which wraps [`OwlViTImageProcessor`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`]\n-    into a single processor that interits both the image processor and tokenizer functionalities. See the\n+    into a single processor that inherits both the image processor and tokenizer functionalities. See the\n     [`~OwlViTProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more information.\n \n     Args:"
        },
        {
            "sha": "95897db85facb20e7f5e63c06dd92b93fa4ca992",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -669,7 +669,7 @@ def __init__(self, config: PatchTSTConfig, num_patches: int):\n             # cls_token: [1 x num_input_channels x 1 x d_model]\n             self.cls_token = nn.Parameter(torch.zeros(1, 1, 1, config.d_model))\n             num_patches += 1\n-        # postional encoding: [num_patches x d_model]\n+        # positional encoding: [num_patches x d_model]\n         self.position_enc = self._init_pe(config, num_patches)\n         # Positional dropout\n         self.positional_dropout = ("
        },
        {
            "sha": "e4e2d27909b41ccdce6a99096bbfdafa66ace013",
            "filename": "src/transformers/models/phi4_multimodal/convert_phi4_multimodal_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -191,7 +191,7 @@ def convert_and_save_processor(input_dir: str, output_dir: str):\n     # Save the processor\n     converted_processor.save_pretrained(output_dir)\n \n-    # we need to rename a few tokens but tokenizers doesn't allow doing that programatically\n+    # we need to rename a few tokens but tokenizers doesn't allow doing that programmatically\n     # To avoid consufion and manual renaming, the below part load and re-saved each json file\n     vocab = json.load(open(f\"{output_dir}/vocab.json\", \"r\"))\n     vocab[\"<|endoftext11|>\"] = \"<|audio|>\""
        },
        {
            "sha": "6f1a19fab65e7f6ce9b254f28fbc0ac877d4f1b9",
            "filename": "src/transformers/models/phi4_multimodal/feature_extraction_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Ffeature_extraction_phi4_multimodal.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -56,7 +56,7 @@ def speechlib_mel(sample_rate, n_fft, n_mels, fmin=None, fmax=None):\n         fmax = sample_rate / 2\n     if fmin is None:\n         fmin = 0\n-    assert fmin >= 0, \"fmin cannot be negtive\"\n+    assert fmin >= 0, \"fmin cannot be negative\"\n     assert fmin < fmax <= sample_rate / 2, \"fmax must be between (fmin, samplerate / 2]\"\n \n     def mel(f):\n@@ -74,7 +74,7 @@ def f2bin(f):\n \n     khi = max(khi, klo)\n \n-    # Spec 2: SpeechLib uses trianges in Mel space\n+    # Spec 2: SpeechLib uses triangles in Mel space\n     mlo = mel(fmin)\n     mhi = mel(fmax)\n     m_centers = np.linspace(mlo, mhi, n_mels + 2)"
        },
        {
            "sha": "f2f691139d7b3ed947ac0908299262f13ce98ea9",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1128,7 +1128,7 @@ def forward_embeddings(self, hidden_states, masks):\n         seq_len = math.ceil(hidden_states.shape[1] / self.config.time_reduction)\n         if seq_len <= 0:\n             raise ValueError(\n-                f\"The squence length after time reduction is invalid: {seq_len}. Your input feature is too short.\"\n+                f\"The sequence length after time reduction is invalid: {seq_len}. Your input feature is too short.\"\n             )\n \n         batch_size = hidden_states.shape[0]\n@@ -1173,7 +1173,7 @@ def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n \n         unfolded = False\n         bs, seq_len, _ = hidden_states.shape\n-        max_seq_len = 500  # maxium position for absolute positional encoding\n+        max_seq_len = 500  # maximum position for absolute positional encoding\n         if seq_len > max_seq_len:\n             # audio sequence is longer than max_seq_len, unfold it into chunks of max_seq_len\n             unfolded = True"
        },
        {
            "sha": "d13fa8dc8b0d5959ec2e27d0305c8a4c251086c2",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1186,7 +1186,7 @@ def forward_embeddings(self, hidden_states, masks):\n         seq_len = math.ceil(hidden_states.shape[1] / self.config.time_reduction)\n         if seq_len <= 0:\n             raise ValueError(\n-                f\"The squence length after time reduction is invalid: {seq_len}. Your input feature is too short.\"\n+                f\"The sequence length after time reduction is invalid: {seq_len}. Your input feature is too short.\"\n             )\n \n         batch_size = hidden_states.shape[0]\n@@ -1231,7 +1231,7 @@ def forward(self, hidden_states: torch.Tensor, mask: Optional[torch.Tensor]):\n \n         unfolded = False\n         bs, seq_len, _ = hidden_states.shape\n-        max_seq_len = 500  # maxium position for absolute positional encoding\n+        max_seq_len = 500  # maximum position for absolute positional encoding\n         if seq_len > max_seq_len:\n             # audio sequence is longer than max_seq_len, unfold it into chunks of max_seq_len\n             unfolded = True"
        },
        {
            "sha": "e4a29de6a339f2693d4af6632cfd97afa7343d6e",
            "filename": "src/transformers/models/pop2piano/feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ffeature_extraction_pop2piano.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -258,7 +258,7 @@ def _pad(self, features: np.ndarray, add_zero_line=True):\n             )\n \n             if add_zero_line:\n-                # if it is batched then we seperate each examples using zero array\n+                # if it is batched then we separate each examples using zero array\n                 zero_array_len = max([*zip(*features_shapes)][1])\n \n                 # we concatenate the zero array line here\n@@ -304,14 +304,14 @@ def pad(\n             to it:\n             - **attention_mask** numpy.ndarray of shape `(batch_size, max_input_features_seq_length)` --\n                 Example :\n-                    1, 1, 1, 0, 0 (audio 1, also here it is padded to max length of 5 thats why there are 2 zeros at\n+                    1, 1, 1, 0, 0 (audio 1, also here it is padded to max length of 5 that's why there are 2 zeros at\n                     the end indicating they are padded)\n \n-                    0, 0, 0, 0, 0 (zero pad to seperate audio 1 and 2)\n+                    0, 0, 0, 0, 0 (zero pad to separate audio 1 and 2)\n \n                     1, 1, 1, 1, 1 (audio 2)\n \n-                    0, 0, 0, 0, 0 (zero pad to seperate audio 2 and 3)\n+                    0, 0, 0, 0, 0 (zero pad to separate audio 2 and 3)\n \n                     1, 1, 1, 1, 1 (audio 3)\n             - **attention_mask_beatsteps** numpy.ndarray of shape `(batch_size, max_beatsteps_seq_length)`\n@@ -333,7 +333,7 @@ def pad(\n                     processed_features_dict[f\"attention_mask_{feature_name}\"] = attention_mask\n \n         # If we are processing only one example, we should remove the zero array line since we don't need it to\n-        # seperate examples from each other.\n+        # separate examples from each other.\n         if not is_batched and not return_attention_mask:\n             processed_features_dict[\"input_features\"] = processed_features_dict[\"input_features\"][:-1, ...]\n "
        },
        {
            "sha": "22bf21e2f4ba800e8afd22eb2666a6d0231b6777",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -669,7 +669,7 @@ def batch_decode(\n                 )\n \n         if attention_masks_present:\n-            # check for zeros(since token_ids are seperated by zero arrays)\n+            # check for zeros(since token_ids are separated by zero arrays)\n             batch_idx = np.where(feature_extractor_output[\"attention_mask\"][:, 0] == 0)[0]\n         else:\n             batch_idx = [token_ids.shape[0]]"
        },
        {
            "sha": "2940932fe53e7f14a95e112775595e722ef2c05e",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -102,22 +102,22 @@ class PromptDepthAnythingImageProcessor(BaseImageProcessor):\n \n     Args:\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions. Can be overidden by `do_resize` in `preprocess`.\n+            Whether to resize the image's (height, width) dimensions. Can be overridden by `do_resize` in `preprocess`.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the image after resizing. Can be overidden by `size` in `preprocess`.\n+            Size of the image after resizing. Can be overridden by `size` in `preprocess`.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Defines the resampling filter to use if resizing the image. Can be overidden by `resample` in `preprocess`.\n+            Defines the resampling filter to use if resizing the image. Can be overridden by `resample` in `preprocess`.\n         keep_aspect_ratio (`bool`, *optional*, defaults to `False`):\n             If `True`, the image is resized to the largest possible size such that the aspect ratio is preserved. Can\n-            be overidden by `keep_aspect_ratio` in `preprocess`.\n+            be overridden by `keep_aspect_ratio` in `preprocess`.\n         ensure_multiple_of (`int`, *optional*, defaults to 1):\n-            If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overidden\n+            If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n             by `ensure_multiple_of` in `preprocess`.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overidden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             `preprocess`.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overidden by `rescale_factor` in `preprocess`.\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in `preprocess`.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method."
        },
        {
            "sha": "51257f76dbae4d0b1e4dad301fe7a54dd06b6b06",
            "filename": "src/transformers/models/prophetnet/tokenization_prophetnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -213,7 +213,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "5e4509b24915f20f8590616ce7a1b3ee82374090",
            "filename": "src/transformers/models/pvt_v2/convert_pvt_v2_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt_v2%2Fconvert_pvt_v2_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -35,7 +35,7 @@\n def create_rename_keys(config):\n     rename_keys = []\n     for i in range(config.num_encoder_blocks):\n-        # Remane embedings' paramters\n+        # Rename embeddings' parameters\n         rename_keys.append(\n             (f\"patch_embed{i + 1}.proj.weight\", f\"pvt_v2.encoder.layers.{i}.patch_embedding.proj.weight\")\n         )"
        },
        {
            "sha": "132b562f9ad0c5375e5241bbfbedbbdcda3baf75",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -154,7 +154,7 @@ class Qwen2_5OmniAudioEncoderConfig(PretrainedConfig):\n         n_window (`int`, *optional*, defaults to 100):\n             The chunk for conv and flash attn in AudioEncoder.\n         output_dim (`int`, *optional*, defaults to 3584):\n-            The output dimention of AudioEncoder.\n+            The output dimension of AudioEncoder.\n \n     Example:\n \n@@ -979,7 +979,7 @@ class Qwen2_5OmniConfig(PretrainedConfig):\n         thinker_config (`dict`, *optional*): Configuration of the underlying thinker sub-model.\n         talker_config (`dict`, *optional*): Configuration of the underlying talker sub-model.\n         token2wav_config (`dict`, *optional*): Configuration of the underlying codec sub-model.\n-        enable_audio_output (`bool`, *optional*, defaults to `True`): Whether enabel audio output and load talker and token2wav module.\n+        enable_audio_output (`bool`, *optional*, defaults to `True`): Whether enable audio output and load talker and token2wav module.\n \n     Example:\n \n@@ -1054,7 +1054,7 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n             decoder (`Optional[bool]`, *optional*, defaults to `False`):\n                 If set to `True`, then only search for decoder config names.\n         \"\"\"\n-        # Overriden for deeply nested config like Qwen2-Omni. We don't have any omni model\n+        # Overridden for deeply nested config like Qwen2-Omni. We don't have any omni model\n         # except for Qwen yet. This has to be generalized if more deeply nested configs are\n         # added. NOTE: currently method used only by vLLM\n         return self.thinker_config.get_text_config()"
        },
        {
            "sha": "17551dc5aebe3d42266be82428dc78bdd46ad9fe",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -180,7 +180,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             min_dtype (`float`):\n                 The minimum value representable with the dtype `dtype`.\n             cache_position (`torch.Tensor`):\n@@ -678,7 +678,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -4492,10 +4492,10 @@ def generate(\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n         if speaker not in self.speaker_map:\n-            raise ValueError(f\"{speaker} is not availible, availible speakers: {self.speaker_map.keys()}\")\n+            raise ValueError(f\"{speaker} is not available, available speakers: {self.speaker_map.keys()}\")\n         if return_audio and not self.has_talker:\n             raise ValueError(\n-                \"Cannot use talker when talker module not initalized. Use `enable_talker` method or set enable_talker in config to enable talker.\"\n+                \"Cannot use talker when talker module not initialized. Use `enable_talker` method or set enable_talker in config to enable talker.\"\n             )\n         if return_audio is None:\n             return_audio = self.has_talker"
        },
        {
            "sha": "9d3f2d9ec6a4c6678b2d8c5aac0bbc88bc50f23e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -195,7 +195,7 @@ class Qwen2_5OmniAudioEncoderConfig(Qwen2AudioEncoderConfig):\n         n_window (`int`, *optional*, defaults to 100):\n             The chunk for conv and flash attn in AudioEncoder.\n         output_dim (`int`, *optional*, defaults to 3584):\n-            The output dimention of AudioEncoder.\n+            The output dimension of AudioEncoder.\n \n     Example:\n \n@@ -964,7 +964,7 @@ class Qwen2_5OmniConfig(PretrainedConfig):\n         thinker_config (`dict`, *optional*): Configuration of the underlying thinker sub-model.\n         talker_config (`dict`, *optional*): Configuration of the underlying talker sub-model.\n         token2wav_config (`dict`, *optional*): Configuration of the underlying codec sub-model.\n-        enable_audio_output (`bool`, *optional*, defaults to `True`): Whether enabel audio output and load talker and token2wav module.\n+        enable_audio_output (`bool`, *optional*, defaults to `True`): Whether enable audio output and load talker and token2wav module.\n \n     Example:\n \n@@ -1039,7 +1039,7 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n             decoder (`Optional[bool]`, *optional*, defaults to `False`):\n                 If set to `True`, then only search for decoder config names.\n         \"\"\"\n-        # Overriden for deeply nested config like Qwen2-Omni. We don't have any omni model\n+        # Overridden for deeply nested config like Qwen2-Omni. We don't have any omni model\n         # except for Qwen yet. This has to be generalized if more deeply nested configs are\n         # added. NOTE: currently method used only by vLLM\n         return self.thinker_config.get_text_config()\n@@ -1095,7 +1095,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             min_dtype (`float`):\n                 The minimum value representable with the dtype `dtype`.\n             cache_position (`torch.Tensor`):\n@@ -1593,7 +1593,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -4179,10 +4179,10 @@ def generate(\n                 - **Audio waveform** (`torch.Tensor`): Generated audio waveform.\n         \"\"\"\n         if speaker not in self.speaker_map:\n-            raise ValueError(f\"{speaker} is not availible, availible speakers: {self.speaker_map.keys()}\")\n+            raise ValueError(f\"{speaker} is not available, available speakers: {self.speaker_map.keys()}\")\n         if return_audio and not self.has_talker:\n             raise ValueError(\n-                \"Cannot use talker when talker module not initalized. Use `enable_talker` method or set enable_talker in config to enable talker.\"\n+                \"Cannot use talker when talker module not initialized. Use `enable_talker` method or set enable_talker in config to enable talker.\"\n             )\n         if return_audio is None:\n             return_audio = self.has_talker"
        },
        {
            "sha": "c90c90c72b7ba21ed02a9c89a77dc74d88616a36",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -209,7 +209,7 @@ def _preprocess(\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n             patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spacial patch size of the vision encoder.\n+                The spatial patch size of the vision encoder.\n             temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n                 The temporal patch size of the vision encoder.\n             merge_size (`int`, *optional*, defaults to `self.merge_size`):\n@@ -352,7 +352,7 @@ def preprocess(\n             max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n                 The max pixels of the image to resize the image.\n             patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spacial patch size of the vision encoder.\n+                The spatial patch size of the vision encoder.\n             temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n                 The temporal patch size of the vision encoder.\n             merge_size (`int`, *optional*, defaults to `self.merge_size`):"
        },
        {
            "sha": "d44ea279cb70643c96d25784c3a3e7a6697087fd",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -169,7 +169,7 @@ def _preprocess(\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n             patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spacial patch size of the vision encoder.\n+                The spatial patch size of the vision encoder.\n             temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n                 The temporal patch size of the vision encoder.\n             merge_size (`int`, *optional*, defaults to `self.merge_size`):\n@@ -306,7 +306,7 @@ def preprocess(\n             max_pixels (`int`, *optional*, defaults to `self.max_pixels`):\n                 The max pixels of the image to resize the image.\n             patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spacial patch size of the vision encoder.\n+                The spatial patch size of the vision encoder.\n             temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n                 The temporal patch size of the vision encoder.\n             merge_size (`int`, *optional*, defaults to `self.merge_size`):"
        },
        {
            "sha": "bac47bb85fa51c7f11229b4458a74157d202321c",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -124,7 +124,7 @@ class Qwen3MoeConfig(PretrainedConfig):\n         norm_topk_prob (`bool`, *optional*, defaults to `False`):\n             Whether to normalize the topk probabilities.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "dca4eb04d3f0ad42432ee23d0471471ccb78dfc0",
            "filename": "src/transformers/models/rag/configuration_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -126,7 +126,7 @@ def __init__(\n         )\n         if \"question_encoder\" not in kwargs or \"generator\" not in kwargs:\n             raise ValueError(\n-                f\"A configuraton of type {self.model_type} cannot be instantiated because \"\n+                f\"A configuration of type {self.model_type} cannot be instantiated because \"\n                 f\"both `question_encoder` and `generator` sub-configurations were not passed, only {kwargs}\"\n             )\n         question_encoder_config = kwargs.pop(\"question_encoder\")"
        },
        {
            "sha": "79fa08c6c53ba45f51598c5a74fcd260c48916fa",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1186,7 +1186,7 @@ def extend_enc_output(tensor, num_beams=None):\n                 raise ValueError(\n                     \"Beam search decoding cannot return more sequences than it has beams. Please set num_beams >=\"\n                     f\" num_return_sequences, got {generation_config.num_beams} and\"\n-                    f\" {generation_config.num_return_sequences} (respectivelly)\"\n+                    f\" {generation_config.num_return_sequences} (respectively)\"\n                 )\n \n             def unflatten_beam_dim(tensor):"
        },
        {
            "sha": "ab8b34500dee9d158239a0ac1cb414109521d3f1",
            "filename": "src/transformers/models/regnet/convert_regnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -90,7 +90,7 @@ def __call__(self, x: Tensor):\n         for dest_m, src_m in zip(dest_traced, src_traced):\n             dest_m.load_state_dict(src_m.state_dict())\n             if self.verbose == 1:\n-                print(f\"Transfered from={src_m} to={dest_m}\")\n+                print(f\"Transferred from={src_m} to={dest_m}\")\n \n \n class FakeRegNetVisslWrapper(nn.Module):"
        },
        {
            "sha": "4a4e0a424a40767137d377ea199adaeb37a92f80",
            "filename": "src/transformers/models/regnet/modeling_flax_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fmodeling_flax_regnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -623,7 +623,7 @@ def __call__(\n             output_hidden_states,\n             return_dict,\n             rngs=rngs,\n-            mutable=[\"batch_stats\"] if train else False,  # Returing tuple with batch_stats only when train is True\n+            mutable=[\"batch_stats\"] if train else False,  # Returning tuple with batch_stats only when train is True\n         )\n \n "
        },
        {
            "sha": "7552dbaefb917a7a22fc841d2f46ecef45bbf6cf",
            "filename": "src/transformers/models/resnet/convert_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -87,7 +87,7 @@ def __call__(self, x: Tensor):\n         for dest_m, src_m in zip(dest_traced, src_traced):\n             dest_m.load_state_dict(src_m.state_dict())\n             if self.verbose == 1:\n-                print(f\"Transfered from={src_m} to={dest_m}\")\n+                print(f\"Transferred from={src_m} to={dest_m}\")\n \n \n def convert_weight_and_push(name: str, config: ResNetConfig, save_directory: Path, push_to_hub: bool = True):"
        },
        {
            "sha": "aa6c84c0fdbca94861538383198d81ab78caf8e1",
            "filename": "src/transformers/models/resnet/modeling_flax_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_flax_resnet.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -514,7 +514,7 @@ def __call__(\n             output_hidden_states,\n             return_dict,\n             rngs=rngs,\n-            mutable=[\"batch_stats\"] if train else False,  # Returing tuple with batch_stats only when train is True\n+            mutable=[\"batch_stats\"] if train else False,  # Returning tuple with batch_stats only when train is True\n         )\n \n "
        },
        {
            "sha": "7f046a340fd1e67461e52805677e603827779d20",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1075,7 +1075,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "6e9748c70b2d458b143df485ca9a4dd269dc395c",
            "filename": "src/transformers/models/roformer/tokenization_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -226,7 +226,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "f7e2071a34ce510c60f8c6092568af1ca552d2a5",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1039,7 +1039,7 @@ class RTDetrPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [r\"RTDetrHybridEncoder\", r\"RTDetrDecoderLayer\"]\n \n     def _init_weights(self, module):\n-        \"\"\"Initalize the weights\"\"\"\n+        \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (RTDetrForObjectDetection, RTDetrDecoder)):\n             if module.class_embed is not None:\n                 for layer in module.class_embed:"
        },
        {
            "sha": "1ab37c0e9424283e94a446fa2b769629bff8c5e5",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1313,7 +1313,7 @@ class RTDetrV2PreTrainedModel(PreTrainedModel):\n     _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n \n     def _init_weights(self, module):\n-        \"\"\"Initalize the weights\"\"\"\n+        \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, (RTDetrV2ForObjectDetection, RTDetrV2Decoder)):\n             if module.class_embed is not None:\n                 for layer in module.class_embed:"
        },
        {
            "sha": "be90607c55f4699b437600264a57c9927d6b95a0",
            "filename": "src/transformers/models/rwkv/configuration_rwkv.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frwkv%2Fconfiguration_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Frwkv%2Fconfiguration_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fconfiguration_rwkv.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -57,7 +57,7 @@ class RwkvConfig(PretrainedConfig):\n             The id of the end of sentence token in the vocabulary. Defaults to 0 as RWKV uses the same tokenizer as\n             GPTNeoX.\n         rescale_every (`int`, *optional*, defaults to 6):\n-            At inference, the hidden states (and weights of the correponding output layers) are divided by 2 every\n+            At inference, the hidden states (and weights of the corresponding output layers) are divided by 2 every\n             `rescale_every` layer. If set to 0 or a negative number, no rescale is done.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether or not to tie the word embeddings with the input token embeddings."
        },
        {
            "sha": "9288cc1485c0d12413a29cbc11b9170332551717",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -827,7 +827,7 @@ def filter_masks(\n             iou_scores (`Union[torch.Tensor, tf.Tensor]`):\n                 List of IoU scores.\n             original_size (`Tuple[int,int]`):\n-                Size of the orginal image.\n+                Size of the original image.\n             cropped_box_image (`np.array`):\n                 The cropped image.\n             pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n@@ -887,7 +887,7 @@ def _filter_masks_pt(\n             iou_scores (`torch.Tensor`):\n                 List of IoU scores.\n             original_size (`Tuple[int,int]`):\n-                Size of the orginal image.\n+                Size of the original image.\n             cropped_box_image (`np.array`):\n                 The cropped image.\n             pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n@@ -939,7 +939,7 @@ def _filter_masks_pt(\n         converted_boxes = converted_boxes[keep_mask]\n \n         masks = _pad_masks(masks, cropped_box_image, original_height, original_width)\n-        # conversion to rle is necessary to run non-maximum suppresion\n+        # conversion to rle is necessary to run non-maximum suppression\n         masks = _mask_to_rle_pytorch(masks)\n \n         return masks, scores, converted_boxes\n@@ -967,7 +967,7 @@ def _filter_masks_tf(\n             iou_scores (`tf.Tensor`):\n                 List of IoU scores.\n             original_size (`Tuple[int,int]`):\n-                Size of the orginal image.\n+                Size of the original image.\n             cropped_box_image (`np.array`):\n                 The cropped image.\n             pred_iou_thresh (`float`, *optional*, defaults to 0.88):\n@@ -1016,15 +1016,15 @@ def _filter_masks_tf(\n         converted_boxes = converted_boxes[keep_mask]\n \n         masks = _pad_masks_tf(masks, cropped_box_image, original_height, original_width)\n-        # conversion to rle is necessary to run non-maximum suppresion\n+        # conversion to rle is necessary to run non-maximum suppression\n         masks = _mask_to_rle_tf(masks)\n \n         return masks, scores, converted_boxes\n \n \n def _compute_stability_score_pt(masks: \"torch.Tensor\", mask_threshold: float, stability_score_offset: int):\n     # One mask is always contained inside the other.\n-    # Save memory by preventing unnecesary cast to torch.int64\n+    # Save memory by preventing unnecessary cast to torch.int64\n     intersections = (\n         (masks > (mask_threshold + stability_score_offset)).sum(-1, dtype=torch.int16).sum(-1, dtype=torch.int32)\n     )"
        },
        {
            "sha": "c81b351c8c20612114d1c2fe8479ded62c212b39",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -690,8 +690,8 @@ def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -\n         # torch.where and expanding the labels tensor is required by the ONNX export\n         point_embedding = torch.where(labels[..., None] == -1, self.not_a_point_embed.weight, point_embedding)\n \n-        # This is required for the ONNX export. The dtype, device need to be explicitely\n-        # specificed as otherwise torch.onnx.export interprets as double\n+        # This is required for the ONNX export. The dtype, device need to be explicitly\n+        # specified as otherwise torch.onnx.export interprets as double\n         point_embedding = torch.where(\n             labels[..., None] != -10,\n             point_embedding,\n@@ -1250,7 +1250,7 @@ def _init_weights(self, module):\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `torch` tensor, with each dimension corresponding respectively to the image batch\n-            size, the number of boxes per image and the coordinates of the top left and botton right point of the box.\n+            size, the number of boxes per image and the coordinates of the top left and bottom right point of the box.\n             In the order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "fcfb7c21843978496f33f8c00ac19fdd386ee956",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1374,7 +1374,7 @@ class TFSamPreTrainedModel(TFPreTrainedModel):\n             Input boxes for the points, this is used by the prompt encoder to encode the prompt. Generally yields to\n             much better generated masks. The boxes can be obtained by passing a list of list of list to the processor,\n             that will generate a `tf` tensor, with each dimension corresponding respectively to the image batch size,\n-            the number of boxes per image and the coordinates of the top left and botton right point of the box. In the\n+            the number of boxes per image and the coordinates of the top left and bottom right point of the box. In the\n             order (`x1`, `y1`, `x2`, `y2`):\n \n             - `x1`: the x coordinate of the top left point of the input box"
        },
        {
            "sha": "ff6a4cd7633d0e4170b26d85b9c3623ddb239e7b",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -113,7 +113,7 @@ def __call__(\n             **output_kwargs[\"images_kwargs\"],\n         )\n \n-        # pop arguments that are not used in the foward but used nevertheless\n+        # pop arguments that are not used in the forward but used nevertheless\n         original_sizes = encoding_image_processor[\"original_sizes\"]\n \n         if hasattr(original_sizes, \"numpy\"):  # Checks if Torch or TF tensor"
        },
        {
            "sha": "8e179463d104243935eb3caf5c4d72d5e39b7bd3",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -100,7 +100,7 @@ class SeamlessM4TGenerationOutput(ModelOutput):\n \n             [What are input IDs?](../glossary#input-ids)\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+            Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n             [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n     \"\"\"\n \n@@ -118,7 +118,7 @@ class SeamlessM4TGenerationOutput(ModelOutput):\n SEAMLESS_M4T_INPUTS_DOCSTRING_SPEECH_PART = r\"\"\"\n     Args:\n         input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-            Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+            Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n             [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n         \"\"\"\n \n@@ -960,7 +960,7 @@ def forward(\n \n         # The rest of the computation is identical to a vanilla Transformer\n         # encoder layer.\n-        hidden_states, attn_weigths = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n@@ -3084,7 +3084,7 @@ def generate(\n \n         Parameters:\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n \n             tgt_lang (`str`, *optional*):\n@@ -3694,7 +3694,7 @@ def generate(\n \n         Args:\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n             return_intermediate_token_ids (`bool`, *optional*):\n                 If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want\n@@ -4103,7 +4103,7 @@ def generate(\n \n                 [What are input IDs?](../glossary#input-ids)\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`, *optional*):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n             return_intermediate_token_ids (`bool`, *optional*):\n                 If `True`, also returns the intermediate generated text and unit tokens. Set to `True` if you also want"
        },
        {
            "sha": "9f7482b8a22c9c5a689a7ef48c1b2b1d5d2bac43",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -906,7 +906,7 @@ def forward(\n \n         # The rest of the computation is identical to a vanilla Transformer\n         # encoder layer.\n-        hidden_states, attn_weigths = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,\n@@ -3374,7 +3374,7 @@ def generate(\n \n         Parameters:\n             input_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_banks)`):\n-                Input audio features. This should be returnes by the [`SeamlessM4TFeatureExtractor`] class or the\n+                Input audio features. This should be returned by the [`SeamlessM4TFeatureExtractor`] class or the\n                 [`SeamlessM4TProcessor`] class. See [`SeamlessM4TFeatureExtractor.__call__`] for details.\n \n             tgt_lang (`str`, *optional*):"
        },
        {
            "sha": "aff4e3754263a8b7b1c7fcd3a1af34fa753645f5",
            "filename": "src/transformers/models/sew/configuration_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fconfiguration_sew.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -99,7 +99,7 @@ class SEWConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -111,7 +111,7 @@ class SEWConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "b965c856629c85bf2862ee3d4008696c0b23d842",
            "filename": "src/transformers/models/sew_d/configuration_sew_d.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fconfiguration_sew_d.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -112,7 +112,7 @@ class SEWDConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -124,7 +124,7 @@ class SEWDConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "87fbcb66c148a70d724d3342bc52245b9184a05b",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -105,7 +105,7 @@ def __call__(\n             text: Not supported.\n             videos: Not supported.\n             audio: Not supported.\n-            kwargs: An optional dictionary of keyword arguments to configre the\n+            kwargs: An optional dictionary of keyword arguments to configure the\n                 processor. Possible values include:\n \n                 *   `custom_policies`: Additional policy definitions that augment the `self.policy_definitions` passed\n@@ -116,7 +116,7 @@ def __call__(\n                     generated for every key in the joint dictionary.\n \n         Returns:\n-            A `BatchFeature` continaing `input_ids`, `pixel_values`, etc. where each Tensor is of shape\n+            A `BatchFeature` containing `input_ids`, `pixel_values`, etc. where each Tensor is of shape\n             `(len(images) * len(policies), )`, and the order within the batch will be\n             img1_policy1, ... img1_policyN, ... imgM_policyN.\n         \"\"\""
        },
        {
            "sha": "63e161fcbe74775fcfd8ab6dee18f664ca694194",
            "filename": "src/transformers/models/speech_encoder_decoder/configuration_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fconfiguration_speech_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -78,7 +78,7 @@ def __init__(self, **kwargs):\n         super().__init__(**kwargs)\n         if \"encoder\" not in kwargs or \"decoder\" not in kwargs:\n             raise ValueError(\n-                f\"A configuraton of type {self.model_type} cannot be instantiated because not both `encoder` and\"\n+                f\"A configuration of type {self.model_type} cannot be instantiated because not both `encoder` and\"\n                 f\" `decoder` sub-configurations are passed, but only {kwargs}\"\n             )\n "
        },
        {
            "sha": "772b1d23bd551ce04807c267d9855b437d6905ac",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -807,7 +807,7 @@ def from_encoder_decoder_pretrained(\n                       [`~FlaxPreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "9cab18104e1cdd8922795ecfbc4787f71b9fcb07",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -317,7 +317,7 @@ def from_encoder_decoder_pretrained(\n                       PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "b1a4d2065dfc1634aac6966f847c252e9f1154f8",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -547,7 +547,7 @@ def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n \n     def _get_feature_vector_attention_mask(self, feature_vector_length, attention_mask):\n         # generate creates 3D attention mask, because of the shape of input_features\n-        # convert it to 2D if thats the case\n+        # convert it to 2D if that's the case\n         if len(attention_mask.shape) > 2:\n             attention_mask = attention_mask[:, :, -1]\n "
        },
        {
            "sha": "858079f5b607bc9403577b9632c8bcfd54662aba",
            "filename": "src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -815,7 +815,7 @@ def _get_feat_extract_output_lengths(self, input_lengths: tf.Tensor):\n \n     def _get_feature_vector_attention_mask(self, feature_vector_length, attention_mask):\n         # generate creates 3D attention mask, because of the shape of input_features\n-        # convert it to 2D if thats the case\n+        # convert it to 2D if that's the case\n         if len(attention_mask.shape) > 2:\n             attention_mask = attention_mask[:, :, -1]\n "
        },
        {
            "sha": "7af0dc222efbbadcec87d95adf49bd2c44459689",
            "filename": "src/transformers/models/speecht5/configuration_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fconfiguration_speecht5.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -108,7 +108,7 @@ class SpeechT5Config(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -120,7 +120,7 @@ class SpeechT5Config(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "67243d4ee434e13d452404100c5477f08cd1c50c",
            "filename": "src/transformers/models/splinter/tokenization_splinter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -460,7 +460,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n           text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "e3a73917f35ec2decc59ca825d62a858108ef91f",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -464,7 +464,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "3c25697c394de5b66f0dca7f42e69ddaaa92714b",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -141,7 +141,7 @@ class TableTransformerObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~TableTransformerImageProcessor.post_process_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -1223,8 +1223,8 @@ def forward(\n         flattened_mask = mask.flatten(1)\n \n         # Fourth, sent flattened_features + flattened_mask + object queries through encoder\n-        # flattened_features is a Tensor of shape (batch_size, heigth*width, hidden_size)\n-        # flattened_mask is a Tensor of shape (batch_size, heigth*width)\n+        # flattened_features is a Tensor of shape (batch_size, height*width, hidden_size)\n+        # flattened_mask is a Tensor of shape (batch_size, height*width)\n         if encoder_outputs is None:\n             encoder_outputs = self.encoder(\n                 inputs_embeds=flattened_features,"
        },
        {
            "sha": "c6a9fac40ca9a77aba375bf90d0a5de26e7c570c",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -2162,7 +2162,7 @@ def tokenize(self, text):\n         Tokenizes a piece of text into its word pieces. This uses a greedy longest-match-first algorithm to perform\n         tokenization using the given vocabulary.\n \n-        For example, `input = \"unaffable\"` wil return as output `[\"un\", \"##aff\", \"##able\"]`.\n+        For example, `input = \"unaffable\"` will return as output `[\"un\", \"##aff\", \"##able\"]`.\n \n         Args:\n             text: A single token or whitespace separated tokens. This should have"
        },
        {
            "sha": "91a0fc10b1a430479df1b6a36b832d718bce47f3",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -251,7 +251,7 @@ def add_2d_positional_embeddings(self, grid, interpolate_pos_encoding: bool = Fa\n         # (batch_size, height, width, hidden_dim)\n         positional_embeddings = row_position_embeddings + col_position_embeddings\n \n-        # This interpolation gets triggered ONLY when the input image dim is larger in any dimenstion than the original position embeddings\n+        # This interpolation gets triggered ONLY when the input image dim is larger in any dimension than the original position embeddings\n         if interpolate_pos_encoding and (\n             height > self.max_grid_row_position_embeddings or width > self.max_grid_col_position_embeddings\n         ):"
        },
        {
            "sha": "6a5d2ea40139433c774099be1999f9dca1607b9e",
            "filename": "src/transformers/models/unispeech/configuration_unispeech.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fconfiguration_unispeech.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -105,7 +105,7 @@ class UniSpeechConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -117,7 +117,7 @@ class UniSpeechConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "de81639d629ac36b252a60c879b891c8ad364b1c",
            "filename": "src/transformers/models/unispeech_sat/configuration_unispeech_sat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fconfiguration_unispeech_sat.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -106,7 +106,7 @@ class UniSpeechSatConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -118,7 +118,7 @@ class UniSpeechSatConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "50b0304eb542c3c9142cbfc7dfc4141cce2b129d",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -46,7 +46,7 @@ class VideoLlavaProcessor(ProcessorMixin):\n             Patch size from the vision tower.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n-            Shoudl be same as in model's config\n+            Should be same as in model's config\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n             Special token used to denote image location.\n         video_token (`str`, *optional*, defaults to `\"<video>\"`):"
        },
        {
            "sha": "659afb976c392b63f6c508ef4e022646b4c71034",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -749,7 +749,7 @@ def from_encoder_decoder_pretrained(\n                       [`~FlaxPreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "29d855728a10a297965c5f1c1736db481633bf32",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -340,7 +340,7 @@ def from_encoder_decoder_pretrained(\n                       `decoder_from_pt` should be set to `True`.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "69f95ff05697b59fdfa8b44d7670d9ed9efee485",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -410,7 +410,7 @@ def from_encoder_decoder_pretrained(\n                       PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "57afc18988d279cf6828eabc23e29d0576679d7a",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -41,7 +41,7 @@\n     should be fine-tuned on a downstream task, like contrastive image-text modeling.\n \n     In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\n-    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment\n+    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement\n     on new zero-shot vision tasks such as image classification or retrieval.\n \n     After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other\n@@ -444,7 +444,7 @@ def from_vision_text_pretrained(\n                       conversion scripts and loading the Flax model afterwards.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "6ce382a74b09c74cef0faf7e13dca49838441993",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -48,7 +48,7 @@\n     should be fine-tuned on a downstream task, like contrastive image-text modeling.\n \n     In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\n-    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment\n+    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement\n     on new zero-shot vision tasks such as image classification or retrieval.\n \n     After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other\n@@ -491,7 +491,7 @@ def from_vision_text_pretrained(\n                       should be set to `True` and a configuration object should be provided as `config` argument.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "7b2e262e872e4cf3eea5e155d1454ddfff0ed405",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -38,7 +38,7 @@\n     should be fine-tuned on a downstream task, like contrastive image-text modeling.\n \n     In [LiT: Zero-Shot Transfer with Locked-image Text Tuning](https://arxiv.org/abs/2111.07991) it is shown how\n-    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvment\n+    leveraging pre-trained (locked/frozen) image and text model for contrastive learning yields significant improvement\n     on new zero-shot vision tasks such as image classification or retrieval.\n \n     After such a Vision-Text-Dual-Encoder model has been trained/fine-tuned, it can be saved/loaded just like any other\n@@ -440,7 +440,7 @@ def from_vision_text_pretrained(\n                       conversion scripts and loading the Flax model afterwards.\n \n             model_args (remaining positional arguments, *optional*):\n-                All remaning positional arguments will be passed to the underlying model's `__init__` method.\n+                All remaining positional arguments will be passed to the underlying model's `__init__` method.\n \n             kwargs (remaining dictionary of keyword arguments, *optional*):\n                 Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,"
        },
        {
            "sha": "822f0c4041ba877531bc6e2c54566909815228aa",
            "filename": "src/transformers/models/vit_mae/modeling_tf_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_tf_vit_mae.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1055,7 +1055,7 @@ def build(self, input_shape=None):\n \n     def interpolate_pos_encoding(self, embeddings) -> tf.Tensor:\n         \"\"\"\n-        This method is a modified version of the interpolation function for ViT-mae model at the deocder, that\n+        This method is a modified version of the interpolation function for ViT-mae model at the decoder, that\n         allows to interpolate the pre-trained decoder position encodings, to be able to use the model on higher\n         resolution images.\n "
        },
        {
            "sha": "b1702ae22e4431294f53657aa95dfc88fdd284b6",
            "filename": "src/transformers/models/wav2vec2/configuration_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconfiguration_wav2vec2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -105,7 +105,7 @@ class Wav2Vec2Config(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -117,7 +117,7 @@ class Wav2Vec2Config(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "46c5dd790a33b812c373d0e607e973de34822ccf",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1993,7 +1993,7 @@ def forward(\n         extract_features = self.dropout_features(outputs[1])\n \n         if attention_mask is not None:\n-            # compute reduced attention_mask correponding to feature vectors\n+            # compute reduced attention_mask corresponding to feature vectors\n             attention_mask = self._get_feature_vector_attention_mask(\n                 extract_features.shape[1], attention_mask, add_adapter=False\n             )"
        },
        {
            "sha": "c4af932213683f0e8d3c147cf8e0600323765ffb",
            "filename": "src/transformers/models/wav2vec2_bert/configuration_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconfiguration_wav2vec2_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -75,7 +75,7 @@ class Wav2Vec2BertConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates `mask_time_prob*len(time_axis)/mask_time_length ``independent masks over the axis. If\n+            procedure generates `mask_time_prob*len(time_axis)/mask_time_length ``independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -87,7 +87,7 @@ class Wav2Vec2BertConfig(PretrainedConfig):\n             mask_time_min_masks`.\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n+            masking procedure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "780dedd8ac2718f4ecb24f8ad9c3c1838a55043a",
            "filename": "src/transformers/models/wav2vec2_bert/convert_wav2vec2_seamless_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -104,7 +104,7 @@ def _convert_model(\n             state_dict[new_key] = state_dict.pop(k)\n \n     extra_keys = set(state_dict.keys()) - set(hf_model.state_dict().keys())\n-    extra_keys = set({k for k in extra_keys if \"num_updates\" not in k})  # filter unecessary param\n+    extra_keys = set({k for k in extra_keys if \"num_updates\" not in k})  # filter unnecessary param\n     missing_keys = set(hf_model.state_dict().keys()) - set(state_dict.keys())\n     if len(extra_keys) != 0:\n         raise ValueError(f\"extra keys found: {extra_keys}\")"
        },
        {
            "sha": "27c809ac5a422b4dc4cd586735e651a45b67d588",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -707,7 +707,7 @@ def forward(\n \n         # The rest of the computation is identical to a vanilla Transformer\n         # encoder layer.\n-        hidden_states, attn_weigths = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "8447bde849e6057304168ef061015829819e7792",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -582,7 +582,7 @@ def forward(\n \n         # The rest of the computation is identical to a vanilla Transformer\n         # encoder layer.\n-        hidden_states, attn_weigths = self.self_attn(\n+        hidden_states, attn_weights = self.self_attn(\n             hidden_states,\n             attention_mask=attention_mask,\n             output_attentions=output_attentions,"
        },
        {
            "sha": "d33adb90d643314e7a27ebb935f53e30811542d7",
            "filename": "src/transformers/models/wav2vec2_conformer/configuration_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fconfiguration_wav2vec2_conformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -102,7 +102,7 @@ class Wav2Vec2ConformerConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n+            procedure generates ''mask_time_prob*len(time_axis)/mask_time_length'' independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment is True`.\n@@ -114,7 +114,7 @@ class Wav2Vec2ConformerConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n+            masking procedure generates ''mask_feature_prob*len(feature_axis)/mask_time_length'' independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "e5a0f2d87b09777750a4c1c3741b0435d187a09d",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1437,7 +1437,7 @@ def forward(\n         extract_features = self.dropout_features(outputs[1])\n \n         if attention_mask is not None:\n-            # compute reduced attention_mask correponding to feature vectors\n+            # compute reduced attention_mask corresponding to feature vectors\n             attention_mask = self._get_feature_vector_attention_mask(\n                 extract_features.shape[1], attention_mask, add_adapter=False\n             )"
        },
        {
            "sha": "9f60bf346a31ffd85093592f8c4983e75e186656",
            "filename": "src/transformers/models/wav2vec2_phoneme/tokenization_wav2vec2_phoneme.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_phoneme%2Ftokenization_wav2vec2_phoneme.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -69,7 +69,7 @@ class Wav2Vec2PhonemeCTCTokenizerOutput(ModelOutput):\n             Decoded logits in text from. Usually the speech transcription.\n         char_offsets (list of `List[Dict[str, Union[int, str]]]` or `List[Dict[str, Union[int, str]]]`):\n             Offsets of the decoded characters. In combination with sampling rate and model downsampling rate char\n-            offsets can be used to compute time stamps for each charater. Total logit score of the beam associated with\n+            offsets can be used to compute time stamps for each character. Total logit score of the beam associated with\n             produced text.\n     \"\"\"\n "
        },
        {
            "sha": "8ad742558e1d6c228f33ab4a9f433169bd67349f",
            "filename": "src/transformers/models/whisper/configuration_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fconfiguration_whisper.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -131,7 +131,7 @@ class WhisperConfig(PretrainedConfig):\n             function. NON_SPEECH_TOKENS and NON_SPEECH_TOKENS_MULTI each correspond to the `english-only` and the\n             `multilingual` model.\n         begin_suppress_tokens (`List[int]`, *optional*, defaults to `[220,50256]`):\n-            A list containing tokens that will be supressed at the beginning of the sampling process. Initialized as\n+            A list containing tokens that will be suppressed at the beginning of the sampling process. Initialized as\n             the token for `\" \"` (`blank_token_id`) and the `eos_token_id`\n         use_weighted_layer_sum (`bool`, *optional*, defaults to `False`):\n             Whether to use a weighted average of layer outputs with learned weights. Only relevant when using an\n@@ -145,7 +145,7 @@ class WhisperConfig(PretrainedConfig):\n             Recognition](https://arxiv.org/abs/1904.08779).\n         mask_time_prob (`float`, *optional*, defaults to 0.05):\n             Percentage (between 0 and 1) of all feature vectors along the time axis which will be masked. The masking\n-            procecure generates `mask_time_prob*len(time_axis)/mask_time_length` independent masks over the axis. If\n+            procedure generates `mask_time_prob*len(time_axis)/mask_time_length` independent masks over the axis. If\n             reasoning from the probability of each feature vector to be chosen as the start of the vector span to be\n             masked, *mask_time_prob* should be `prob_vector_start*mask_time_length`. Note that overlap may decrease the\n             actual percentage of masked vectors. This is only relevant if `apply_spec_augment == True`.\n@@ -157,7 +157,7 @@ class WhisperConfig(PretrainedConfig):\n             mask_time_min_masks''\n         mask_feature_prob (`float`, *optional*, defaults to 0.0):\n             Percentage (between 0 and 1) of all feature vectors along the feature axis which will be masked. The\n-            masking procecure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n+            masking procedure generates `mask_feature_prob*len(feature_axis)/mask_time_length` independent masks over\n             the axis. If reasoning from the probability of each feature vector to be chosen as the start of the vector\n             span to be masked, *mask_feature_prob* should be `prob_vector_start*mask_feature_length`. Note that overlap\n             may decrease the actual percentage of masked vectors. This is only relevant if `apply_spec_augment is"
        },
        {
            "sha": "575bfb1c54f178a363d5dcc8cdc8c056bdcffdd6",
            "filename": "src/transformers/models/whisper/feature_extraction_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ffeature_extraction_whisper.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -49,9 +49,9 @@ class WhisperFeatureExtractor(SequenceFeatureExtractor):\n         sampling_rate (`int`, *optional*, defaults to 16000):\n             The sampling rate at which the audio files should be digitalized expressed in hertz (Hz).\n         hop_length (`int`, *optional*, defaults to 160):\n-            Length of the overlaping windows for the STFT used to obtain the Mel Frequency coefficients.\n+            Length of the overlapping windows for the STFT used to obtain the Mel Frequency coefficients.\n         chunk_length (`int`, *optional*, defaults to 30):\n-            The maximum number of chuncks of `sampling_rate` samples used to trim and pad longer or shorter audio\n+            The maximum number of chunks of `sampling_rate` samples used to trim and pad longer or shorter audio\n             sequences.\n         n_fft (`int`, *optional*, defaults to 400):\n             Size of the Fourier transform."
        },
        {
            "sha": "da1d83b2a8b08b970372dc140b4729b881457b59",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -253,7 +253,7 @@ def _extract_token_timestamps(\n             # beam search takes `decoder_input_ids` into account in the `beam_indices` length\n             # but forgot to shift the beam_indices by the number of `decoder_input_ids`\n             beam_indices = torch.zeros_like(generate_outputs.beam_indices[:, :weight_length])\n-            # we actually shif the beam indices here\n+            # we actually shift the beam indices here\n             beam_indices[:, num_input_ids:] = generate_outputs.beam_indices[:, : weight_length - num_input_ids]\n \n             weights = weights[:, :, :weight_length]\n@@ -294,7 +294,7 @@ def _extract_token_timestamps(\n                 weights = weights[..., : num_frames[0] // 2]\n \n             else:\n-                # num_frames is of shape (batch_size,) whereas batch_size is truely batch_size*num_return_sequences\n+                # num_frames is of shape (batch_size,) whereas batch_size is truly batch_size*num_return_sequences\n                 repeat_time = batch_size if isinstance(num_frames, int) else batch_size // len(num_frames)\n                 num_frames = num_frames.cpu() if isinstance(num_frames, (torch.Tensor)) else num_frames\n                 num_frames = np.repeat(num_frames, repeat_time)\n@@ -865,7 +865,7 @@ def generate(\n \n         if return_dict_in_generate and generation_config.return_dict_in_generate:\n             logger.warning_once(\n-                \"You have passed `return_dict_in_generate=True` and `return_timestamps=True`, this automatically sets `return_segments=True` to access the resuls of the underlying calls to GenerationMixin's generate in the returned `segments`.\"\n+                \"You have passed `return_dict_in_generate=True` and `return_timestamps=True`, this automatically sets `return_segments=True` to access the results of the underlying calls to GenerationMixin's generate in the returned `segments`.\"\n             )\n             return_segments = True\n         elif not return_segments and not return_token_timestamps:\n@@ -1466,7 +1466,7 @@ def language_to_id(language: str) -> int:\n         if language is not None:\n             lang_ids = [language_to_id(l) for l in languages]\n         elif hasattr(generation_config, \"lang_to_id\") and is_lang_id_undefined:\n-            # language is not defined or intentially set to `None` to trigger language detection\n+            # language is not defined or intentionally set to `None` to trigger language detection\n             lang_ids = self.detect_language(\n                 input_features=input_features,\n                 encoder_outputs=kwargs.get(\"encoder_outputs\", None),\n@@ -1554,7 +1554,7 @@ def detect_language(\n         if input_features is None and encoder_outputs is None:\n             raise ValueError(\"You have to specify either `input_features` or `encoder_outputs`\")\n         elif input_features is not None and encoder_outputs is not None:\n-            raise ValueError(\"Make sure to specificy only one of `input_features` or `encoder_outputs` - not both!\")\n+            raise ValueError(\"Make sure to specify only one of `input_features` or `encoder_outputs` - not both!\")\n         elif input_features is not None:\n             inputs = {\"input_features\": input_features[:, :, :num_segment_frames]}\n             batch_size = input_features.shape[0]"
        },
        {
            "sha": "b713566bfcd29646058f458aafe0b957beb2f285",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -515,7 +515,7 @@ def _basic_normalize(self, text, remove_diacritics=False):\n \n     def normalize(self, text):\n         \"\"\"\n-        Normalize a given string using the `EnglishTextNormalizer` class, which preforms commons transformation on\n+        Normalize a given string using the `EnglishTextNormalizer` class, which performs commons transformation on\n         english text.\n         \"\"\"\n         normalizer = EnglishTextNormalizer(self.english_spelling_normalizer)\n@@ -524,7 +524,7 @@ def normalize(self, text):\n     @staticmethod\n     def basic_normalize(text, remove_diacritics=False):\n         \"\"\"\n-        Normalize a given string using the `BasicTextNormalizer` class, which preforms commons transformation on\n+        Normalize a given string using the `BasicTextNormalizer` class, which performs commons transformation on\n         multilingual text.\n         \"\"\"\n         normalizer = BasicTextNormalizer(remove_diacritics=remove_diacritics)\n@@ -1045,7 +1045,7 @@ def new_chunk():\n                         # as a stop where it should be a start.\n                         # This is an issue in the underlying model output\n                         # Let's just skip it so it becomes de-factor\n-                        # a start agin\n+                        # a start again\n                         pass\n                     else:\n                         chunk[\"timestamp\"][1] = time"
        },
        {
            "sha": "59abecaa269f75f7540053db7291b72639daffdb",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -420,7 +420,7 @@ def _basic_normalize(self, text, remove_diacritics=False):\n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.normalize\n     def normalize(self, text):\n         \"\"\"\n-        Normalize a given string using the `EnglishTextNormalizer` class, which preforms commons transformation on\n+        Normalize a given string using the `EnglishTextNormalizer` class, which performs commons transformation on\n         english text.\n         \"\"\"\n         normalizer = EnglishTextNormalizer(self.english_spelling_normalizer)\n@@ -430,7 +430,7 @@ def normalize(self, text):\n     # Copied from transformers.models.whisper.tokenization_whisper.WhisperTokenizer.basic_normalize\n     def basic_normalize(text, remove_diacritics=False):\n         \"\"\"\n-        Normalize a given string using the `BasicTextNormalizer` class, which preforms commons transformation on\n+        Normalize a given string using the `BasicTextNormalizer` class, which performs commons transformation on\n         multilingual text.\n         \"\"\"\n         normalizer = BasicTextNormalizer(remove_diacritics=remove_diacritics)"
        },
        {
            "sha": "310ca960c31d8de8192610ffa8717270caa2418b",
            "filename": "src/transformers/models/x_clip/configuration_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -252,7 +252,7 @@ class XCLIPConfig(PretrainedConfig):\n         prompt_projection_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout probability for the projection layers in the video specific prompt generator.\n         logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n-            The inital value of the *logit_scale* parameter. Default is used as per the original XCLIP implementation.\n+            The initial value of the *logit_scale* parameter. Default is used as per the original XCLIP implementation.\n         kwargs (*optional*):\n             Dictionary of keyword arguments.\n     \"\"\""
        },
        {
            "sha": "2705a412b7928ef6b0cf88a83fb85855ad481955",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -67,7 +67,7 @@ class YolosObjectDetectionOutput(ModelOutput):\n             possible padding). You can use [`~YolosImageProcessor.post_process`] to retrieve the unnormalized bounding\n             boxes.\n         auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n             and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n             `pred_boxes`) for each decoder layer.\n         last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n@@ -155,9 +155,9 @@ def forward(self, pos_embed, img_size=(800, 1344)) -> torch.Tensor:\n         patch_pos_embed = patch_pos_embed.view(batch_size, hidden_size, patch_height, patch_width)\n \n         height, width = img_size\n-        new_patch_heigth, new_patch_width = height // self.config.patch_size, width // self.config.patch_size\n+        new_patch_height, new_patch_width = height // self.config.patch_size, width // self.config.patch_size\n         patch_pos_embed = nn.functional.interpolate(\n-            patch_pos_embed, size=(new_patch_heigth, new_patch_width), mode=\"bicubic\", align_corners=False\n+            patch_pos_embed, size=(new_patch_height, new_patch_width), mode=\"bicubic\", align_corners=False\n         )\n         patch_pos_embed = patch_pos_embed.flatten(2).transpose(1, 2)\n         scale_pos_embed = torch.cat((cls_pos_embed, patch_pos_embed, det_pos_embed), dim=1)"
        },
        {
            "sha": "07652ed02115f025eafe82d36d9c497ebc050212",
            "filename": "src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -85,7 +85,7 @@ def convert_yoso_checkpoint(checkpoint_path, yoso_config_file, pytorch_dump_path\n     model.eval()\n     model.save_pretrained(pytorch_dump_path)\n \n-    print(f\"Checkpoint successfuly converted. Model saved at {pytorch_dump_path}\")\n+    print(f\"Checkpoint successfully converted. Model saved at {pytorch_dump_path}\")\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "31949cf6a08e9178dec90c63d05b90a961f322fb",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1310,7 +1310,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `ZambaHybridDynamicCache`\n+        # Overwritten -- has a unique cache type, `ZambaHybridDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "19fc05b217e035383e5b629ce9cef64fe0e163d8",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -571,7 +571,7 @@ def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n             projection_size,\n             bias=config.add_bias_linear,\n         )\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n \n         # time step projection (discretization)\n         # instantiate once and copy inv_dt in init_weights of PretrainedModel\n@@ -1719,7 +1719,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `Zamba2HybridDynamicCache`\n+        # Overwritten -- has a unique cache type, `Zamba2HybridDynamicCache`\n \n         empty_past_kv = past_key_values is None\n "
        },
        {
            "sha": "be4f38044c43c87200df80f502617a45ecb3f94e",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -341,7 +341,7 @@ def __init__(self, config: Zamba2Config, layer_idx: Optional[int] = None):\n             projection_size,\n             bias=config.add_bias_linear,\n         )\n-        # selective projection used to make dt, B and C input dependant\n+        # selective projection used to make dt, B and C input dependent\n \n         # time step projection (discretization)\n         # instantiate once and copy inv_dt in init_weights of PretrainedModel"
        },
        {
            "sha": "9d87898235bc5449001b110c551cb9aa605a826f",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -107,10 +107,10 @@ class ZoeDepthImageProcessor(BaseImageProcessor):\n         do_pad (`bool`, *optional*, defaults to `True`):\n             Whether to apply pad the input.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overidden by `do_rescale` in\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n             `preprocess`.\n         rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overidden by `rescale_factor` in `preprocess`.\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in `preprocess`.\n         do_normalize (`bool`, *optional*, defaults to `True`):\n             Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n             method.\n@@ -121,25 +121,25 @@ class ZoeDepthImageProcessor(BaseImageProcessor):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions. Can be overidden by `do_resize` in `preprocess`.\n+            Whether to resize the image's (height, width) dimensions. Can be overridden by `do_resize` in `preprocess`.\n         size (`Dict[str, int]` *optional*, defaults to `{\"height\": 384, \"width\": 512}`):\n             Size of the image after resizing. Size of the image after resizing. If `keep_aspect_ratio` is `True`,\n             the image is resized by choosing the smaller of the height and width scaling factors and using it for both dimensions.\n             If `ensure_multiple_of` is also set, the image is further resized to a size that is a multiple of this value.\n-            Can be overidden by `size` in `preprocess`.\n+            Can be overridden by `size` in `preprocess`.\n         resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n-            Defines the resampling filter to use if resizing the image. Can be overidden by `resample` in `preprocess`.\n+            Defines the resampling filter to use if resizing the image. Can be overridden by `resample` in `preprocess`.\n         keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n             If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n             for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n             within the desired output size. In case `ensure_multiple_of` is also set, the image is further resized to a\n             size that is a multiple of this value by flooring the height and width to the nearest multiple of this value.\n-            Can be overidden by `keep_aspect_ratio` in `preprocess`.\n+            Can be overridden by `keep_aspect_ratio` in `preprocess`.\n         ensure_multiple_of (`int`, *optional*, defaults to 32):\n             If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by flooring\n             the height and width to the nearest multiple of this value.\n \n-            Works both with and without `keep_aspect_ratio` being set to `True`. Can be overidden by `ensure_multiple_of`\n+            Works both with and without `keep_aspect_ratio` being set to `True`. Can be overridden by `ensure_multiple_of`\n             in `preprocess`.\n     \"\"\"\n \n@@ -348,7 +348,7 @@ def preprocess(\n                 If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Works by\n                 flooring the height and width to the nearest multiple of this value.\n \n-                Works both with and without `keep_aspect_ratio` being set to `True`. Can be overidden by\n+                Works both with and without `keep_aspect_ratio` being set to `True`. Can be overridden by\n                 `ensure_multiple_of` in `preprocess`.\n             resample (`int`, *optional*, defaults to `self.resample`):\n                 Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`, Only"
        },
        {
            "sha": "bfca21e38d8c8b3ac493af1bc0be3bbe8a9f1597",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -426,7 +426,7 @@ def preprocess(self, inputs, chunk_length_s=0, stride_length_s=None):\n             if isinstance(stride_length_s, (int, float)):\n                 stride_length_s = [stride_length_s, stride_length_s]\n \n-            # XXX: Carefuly, this variable will not exist in `seq2seq` setting.\n+            # XXX: Carefully, this variable will not exist in `seq2seq` setting.\n             # Currently chunking is not possible at this level for `seq2seq` so\n             # it's ok.\n             align_to = getattr(self.model.config, \"inputs_to_logits_ratio\", 1)"
        },
        {
            "sha": "25bd61121571bafd3e770e05dee8b53bd034a1e1",
            "filename": "src/transformers/pipelines/mask_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fpipelines%2Fmask_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fmask_generation.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -33,7 +33,7 @@\n class MaskGenerationPipeline(ChunkPipeline):\n     \"\"\"\n     Automatic mask generation for images using `SamForMaskGeneration`. This pipeline predicts binary masks for an\n-    image, given an image. It is a `ChunkPipeline` because you can seperate the points in a mini-batch in order to\n+    image, given an image. It is a `ChunkPipeline` because you can separate the points in a mini-batch in order to\n     avoid OOM issues. Use the `points_per_batch` argument to control the number of points that will be processed at the\n     same time. Default is `64`.\n "
        },
        {
            "sha": "2818b9d26e4558aa0407bf4af42b370d2095fbbb",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -239,7 +239,7 @@ def postprocess_model(self, model: \"PreTrainedModel\", **kwargs):\n \n     def dequantize(self, model):\n         \"\"\"\n-        Potentially dequantize the model to retrive the original model, with some loss in accuracy / performance.\n+        Potentially dequantize the model to retrieve the original model, with some loss in accuracy / performance.\n         Note not all quantization schemes support this.\n         \"\"\"\n         model = self._dequantize(model)"
        },
        {
            "sha": "21dc2498d9ad78af785905a99ff8343fa3554897",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -220,7 +220,7 @@ def create_quantized_param(\n         unexpected_keys: List[str],\n     ):\n         \"\"\"\n-        Each nn.Linear layer that needs to be quantized is processsed here.\n+        Each nn.Linear layer that needs to be quantized is processed here.\n         First, we set the value the weight tensor, then we move it to the target device. Finally, we quantize the module.\n         \"\"\"\n         if self.quantization_config.quant_type == \"autoquant\":"
        },
        {
            "sha": "fc82d0b2557efa295eede1ff4afc86276ae31068",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -359,7 +359,7 @@ class Trainer:\n             Processing class used to process the data. If provided, will be used to automatically process the inputs\n             for the model, and it will be saved along the model to make it easier to rerun an interrupted training or\n             reuse the fine-tuned model.\n-            This supercedes the `tokenizer` argument, which is now deprecated.\n+            This supersedes the `tokenizer` argument, which is now deprecated.\n         model_init (`Callable[[], PreTrainedModel]`, *optional*):\n             A function that instantiates the model to be used. If provided, each call to [`~Trainer.train`] will start\n             from a new instance of the model as given by this function."
        },
        {
            "sha": "7291b8f98a0a157a1a2f83731b9803c99745d092",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1887,7 +1887,7 @@ def __init__(\n                 #\n                 # dict_keys(['models.nllb_moe.configuration_nllb_moe', 'models.sew_d.configuration_sew_d'])\n                 #\n-                # with this, we don't only want to be able to import these explicitely, we want to be able to import\n+                # with this, we don't only want to be able to import these explicitly, we want to be able to import\n                 # every intermediate module as well. Therefore, this is what is returned:\n                 #\n                 # {"
        },
        {
            "sha": "155fcd76819a5ae24130183b014768a6768b41af",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -283,7 +283,7 @@ class HqqConfig(QuantizationConfigMixin):\n         nbits (`int`, *optional*, defaults to 4):\n             Number of bits. Supported values are (8, 4, 3, 2, 1).\n         group_size (`int`, *optional*, defaults to 64):\n-            Group-size value. Supported values are any value that is divisble by weight.shape[axis]).\n+            Group-size value. Supported values are any value that is divisible by weight.shape[axis]).\n         view_as_float (`bool`, *optional*, defaults to `False`):\n             View the quantized weight as float (used in distributed training) if set to `True`.\n         axis (`Optional[int]`, *optional*):\n@@ -661,7 +661,7 @@ class GPTQConfig(QuantizationConfigMixin):\n             Whether to quantize columns in order of decreasing activation size. Setting it to False can significantly\n             speed up inference but the perplexity may become slightly worse. Also known as act-order.\n         sym (`bool`, *optional*, defaults to `True`):\n-            Whether to use symetric quantization.\n+            Whether to use symmetric quantization.\n         true_sequential (`bool`, *optional*, defaults to `True`):\n             Whether to perform sequential quantization even within a single Transformer block. Instead of quantizing\n             the entire block at once, we perform layer-wise quantization. As a result, each layer undergoes\n@@ -1729,7 +1729,7 @@ def to_dict(self):\n             # Handle AOBaseConfig serialization\n             from torchao.core.config import config_to_dict\n \n-            # For now we assume there is 1 config per Transfomer, however in the future\n+            # For now we assume there is 1 config per Transformer, however in the future\n             # We may want to support a config per fqn.\n             d[\"quant_type\"] = {\"default\": config_to_dict(self.quant_type)}\n "
        },
        {
            "sha": "b9fbdc9d437f615618fb92c0ccb3925f50dff007",
            "filename": "tests/models/bert/test_modeling_tf_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -650,15 +650,15 @@ def test_model(self):\n     def test_causal_lm_base_model(self):\n         \"\"\"Test the base model of the causal LM model\n \n-        is_deocder=True, no cross_attention, no encoder outputs\n+        is_decoder=True, no cross_attention, no encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n \n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "de9e61ea5408caa57e220b92f3b615f0e17da0bc",
            "filename": "tests/models/electra/test_modeling_tf_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -533,15 +533,15 @@ def test_model(self):\n     def test_causal_lm_base_model(self):\n         \"\"\"Test the base model of the causal LM model\n \n-        is_deocder=True, no cross_attention, no encoder outputs\n+        is_decoder=True, no cross_attention, no encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n \n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "c7478ab3c0b7c69bddba2d47b0321b3ba8b7460a",
            "filename": "tests/models/esm/test_modeling_tf_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -236,7 +236,7 @@ def test_model(self):\n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "b07d84d6074ff3883e9b4a544e643261fd038e5a",
            "filename": "tests/models/rembert/test_modeling_tf_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -618,15 +618,15 @@ def test_model(self):\n     def test_causal_lm_base_model(self):\n         \"\"\"Test the base model of the causal LM model\n \n-        is_deocder=True, no cross_attention, no encoder outputs\n+        is_decoder=True, no cross_attention, no encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n \n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "d2dbc30928104292243da7a0679d70042960bf99",
            "filename": "tests/models/roberta/test_modeling_tf_roberta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -592,15 +592,15 @@ def test_model(self):\n     def test_causal_lm_base_model(self):\n         \"\"\"Test the base model of the causal LM model\n \n-        is_deocder=True, no cross_attention, no encoder outputs\n+        is_decoder=True, no cross_attention, no encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n \n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "835a6d3e3a94592ce9ea14032a149869cbdc5665",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_tf_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -594,15 +594,15 @@ def test_model(self):\n     def test_causal_lm_base_model(self):\n         \"\"\"Test the base model of the causal LM model\n \n-        is_deocder=True, no cross_attention, no encoder outputs\n+        is_decoder=True, no cross_attention, no encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n \n     def test_model_as_decoder(self):\n         \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n \n-        is_deocder=True + cross_attention + pass encoder outputs\n+        is_decoder=True + cross_attention + pass encoder outputs\n         \"\"\"\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)"
        },
        {
            "sha": "fb7c66930c4df9e30bc64709b1f7d4baf2626b44",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214062201e85276720d86a858e6f3b745e64c6ec/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=214062201e85276720d86a858e6f3b745e64c6ec",
            "patch": "@@ -1103,7 +1103,7 @@ def test_chat_template_audio_from_video(self):\n             signature.parameters.get(\"videos\") is not None\n             and signature.parameters[\"videos\"].annotation == inspect._empty\n         ):\n-            self.skipTest(f\"{self.processor_class} does not suport video inputs\")\n+            self.skipTest(f\"{self.processor_class} does not support video inputs\")\n \n         if \"feature_extractor\" not in self.processor_class.attributes:\n             self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")"
        }
    ],
    "stats": {
        "total": 856,
        "additions": 431,
        "deletions": 425
    }
}