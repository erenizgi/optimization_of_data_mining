{
    "author": "MekkCyber",
    "message": "Add Example for Custom quantization (#36286)\n\n* add example\n\n* rename",
    "sha": "e5cea20743755240bee86eca7eb69d646ab8db0e",
    "files": [
        {
            "sha": "e43b2e0fc219938cc4b34de12671e4dfa3828e9a",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "added",
            "additions": 257,
            "deletions": 0,
            "changes": 257,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5cea20743755240bee86eca7eb69d646ab8db0e/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5cea20743755240bee86eca7eb69d646ab8db0e/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=e5cea20743755240bee86eca7eb69d646ab8db0e",
            "patch": "@@ -0,0 +1,257 @@\n+import json\n+from typing import Any, Dict, List, Optional\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from accelerate import init_empty_weights\n+from huggingface_hub import HfApi\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers.quantizers import HfQuantizer, get_module_from_name, register_quantization_config, register_quantizer\n+from transformers.utils.quantization_config import QuantizationConfigMixin\n+\n+\n+# Implement INT8 Symmetric Linear layer\n+class Int8SymmetricLinear(torch.nn.Module):\n+    def __init__(self, in_features, out_features, bias, dtype=torch.float32):\n+        super().__init__()\n+        self.in_features = in_features\n+        self.out_features = out_features\n+\n+        self.register_buffer(\"weight\", torch.zeros((out_features, in_features), dtype=torch.int8))\n+        self.register_buffer(\"weight_scale\", torch.zeros((out_features, 1), dtype=dtype))\n+\n+        if bias:\n+            self.register_buffer(\"bias\", torch.zeros((self.out_features), dtype=dtype))\n+        else:\n+            self.bias = None\n+\n+    def forward(self, x):\n+        dequant_weight = self.weight * self.weight_scale\n+        output = F.linear(x, dequant_weight)\n+        if self.bias is not None:\n+            output = output + self.bias\n+        return output\n+\n+\n+# Function to replace standard linear layers with INT8 symmetric quantized layers\n+def _replace_with_int8_symmetric_linear(\n+    model,\n+    modules_to_not_convert=None,\n+    current_key_name=None,\n+    quantization_config=None,\n+    has_been_replaced=False,\n+    pre_quantized=False,\n+):\n+    \"\"\"\n+    Recursively replaces nn.Linear modules with Int8SymmetricLinear modules.\n+    \"\"\"\n+    if current_key_name is None:\n+        current_key_name = []\n+\n+    for name, module in model.named_children():\n+        current_key_name.append(name)\n+\n+        if (isinstance(module, nn.Linear)) and name not in modules_to_not_convert:\n+            # Check if the current key is not in the `modules_to_not_convert`\n+            current_key_name_str = \".\".join(current_key_name)\n+            if not any(\n+                (key + \".\" in current_key_name_str) or (key == current_key_name_str) for key in modules_to_not_convert\n+            ):\n+                with init_empty_weights(include_buffers=True):\n+                    in_features = module.in_features\n+                    out_features = module.out_features\n+                    model._modules[name] = Int8SymmetricLinear(\n+                        in_features, out_features, module.bias is not None, dtype=module.weight.dtype\n+                    )\n+                    has_been_replaced = True\n+                    model._modules[name].requires_grad_(False)\n+\n+        if len(list(module.children())) > 0:\n+            _, has_been_replaced = _replace_with_int8_symmetric_linear(\n+                module,\n+                modules_to_not_convert,\n+                current_key_name,\n+                quantization_config,\n+                has_been_replaced=has_been_replaced,\n+                pre_quantized=pre_quantized,\n+            )\n+        # Remove the last key for recursion\n+        current_key_name.pop(-1)\n+    return model, has_been_replaced\n+\n+\n+def replace_with_int8_symmetric_linear(\n+    model, modules_to_not_convert=None, current_key_name=None, quantization_config=None, pre_quantized=False\n+):\n+    \"\"\"\n+    Main function to replace model layers with INT8 symmetric quantized versions.\n+    \"\"\"\n+    modules_to_not_convert = [\"lm_head\"] if modules_to_not_convert is None else modules_to_not_convert\n+\n+    if quantization_config.modules_to_not_convert is not None:\n+        modules_to_not_convert.extend(quantization_config.modules_to_not_convert)\n+    modules_to_not_convert = list(set(modules_to_not_convert))\n+\n+    model, has_been_replaced = _replace_with_int8_symmetric_linear(\n+        model, modules_to_not_convert, current_key_name, quantization_config, pre_quantized=pre_quantized\n+    )\n+\n+    if not has_been_replaced:\n+        raise ValueError(\n+            \"You are loading your model using INT8 symmetric quantization but no linear modules were found in your model.\"\n+        )\n+\n+    return model\n+\n+\n+@register_quantization_config(\"int8_symmetric\")\n+class Int8SymmetricConfig(QuantizationConfigMixin):\n+    \"\"\"\n+    Configuration for INT8 symmetric quantization.\n+    \"\"\"\n+\n+    def __init__(self, modules_to_not_convert: Optional[List[str]] = None, **kwargs):\n+        self.quant_method = \"int8_symmetric\"\n+        self.modules_to_not_convert = modules_to_not_convert\n+\n+    def __repr__(self):\n+        config_dict = self.to_dict()\n+        return f\"{self.__class__.__name__} {json.dumps(config_dict, indent=2, sort_keys=True)}\\n\"\n+\n+    def to_diff_dict(self) -> Dict[str, Any]:\n+        config_dict = self.to_dict()\n+        default_config_dict = Int8SymmetricConfig().to_dict()\n+\n+        serializable_config_dict = {}\n+        for key, value in config_dict.items():\n+            if value != default_config_dict[key]:\n+                serializable_config_dict[key] = value\n+\n+        return serializable_config_dict\n+\n+\n+@register_quantizer(\"int8_symmetric\")\n+class Int8SymmetricQuantizer(HfQuantizer):\n+    \"\"\"\n+    Implementation of INT8 symmetric quantization.\n+\n+    \"\"\"\n+\n+    requires_calibration = False\n+    requires_parameters_quantization = True\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+        self.quantization_config = quantization_config\n+\n+    def _process_model_before_weight_loading(self, model, **kwargs):\n+        \"\"\"\n+        Replace model's linear layers with quantized versions before loading weights.\n+        \"\"\"\n+        self.modules_to_not_convert = self.quantization_config.modules_to_not_convert\n+\n+        model = replace_with_int8_symmetric_linear(\n+            model,\n+            modules_to_not_convert=self.modules_to_not_convert,\n+            quantization_config=self.quantization_config,\n+            pre_quantized=self.pre_quantized,\n+        )\n+\n+    def check_quantized_param(\n+        self,\n+        model,\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        state_dict: Dict[str, Any],\n+        **kwargs,\n+    ):\n+        module, tensor_name = get_module_from_name(model, param_name)\n+\n+        if isinstance(module, Int8SymmetricLinear):\n+            if self.pre_quantized or tensor_name == \"bias\":\n+                if tensor_name == \"weight\" and param_value.dtype != torch.int8:\n+                    raise ValueError(\"Expect quantized weights but got an unquantized weight\")\n+                return False\n+            else:\n+                if tensor_name == \"weight_scale\":\n+                    raise ValueError(\"Expect unquantized weights but got a quantized weight_scale\")\n+                return True\n+        return False\n+\n+    def create_quantized_param(\n+        self,\n+        model,\n+        param_value: \"torch.Tensor\",\n+        param_name: str,\n+        target_device: \"torch.device\",\n+        state_dict: Dict[str, Any],\n+        unexpected_keys: Optional[List[str]] = None,\n+    ):\n+        \"\"\"\n+        Quantizes weights to INT8 symmetric format.\n+        \"\"\"\n+        abs_max_per_row = torch.max(torch.abs(param_value), dim=1, keepdim=True)[0].clamp(min=1e-5)\n+\n+        weight_scale = abs_max_per_row / 127.0\n+\n+        weight_quantized = torch.round(param_value / weight_scale).clamp(-128, 127).to(torch.int8)\n+\n+        module, tensor_name = get_module_from_name(model, param_name)\n+        module._buffers[tensor_name] = weight_quantized.to(target_device)\n+        module._buffers[\"weight_scale\"] = weight_scale.to(target_device)\n+\n+    def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n+        not_missing_keys = []\n+        for name, module in model.named_modules():\n+            if isinstance(module, Int8SymmetricLinear):\n+                for missing in missing_keys:\n+                    if (\n+                        (name in missing or name in f\"{prefix}.{missing}\")\n+                        and not missing.endswith(\".weight\")\n+                        and not missing.endswith(\".bias\")\n+                    ):\n+                        not_missing_keys.append(missing)\n+        return [k for k in missing_keys if k not in not_missing_keys]\n+\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n+        \"\"\"\n+        Post-processing after weights are loaded.\n+        \"\"\"\n+        return True\n+\n+    def is_serializable(self, safe_serialization=None):\n+        return True\n+\n+    @property\n+    def is_trainable(self) -> bool:\n+        return False\n+\n+\n+# Example usage\n+if __name__ == \"__main__\":\n+    model_int8 = AutoModelForCausalLM.from_pretrained(\n+        \"meta-llama/Llama-3.2-1B\", quantization_config=Int8SymmetricConfig(), torch_dtype=torch.float, device_map=\"cpu\"\n+    )\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n+    input_text = \"once there is\"\n+    inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cpu\")\n+    output = model_int8.generate(\n+        **inputs,\n+        max_length=100,\n+        num_return_sequences=1,\n+        no_repeat_ngram_size=2,\n+    )\n+    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n+    print(generated_text)\n+\n+    # Save and upload to HUB\n+    output_model_dir = \"Llama-3.2-1B-INT8-CUSTOM\"\n+    model_int8.save_pretrained(output_model_dir)\n+    tokenizer.save_pretrained(output_model_dir)\n+    api = HfApi()\n+    repo_id = \"medmekk/Llama-3.2-1B-INT8-CUSTOM\"\n+    api.create_repo(repo_id, private=False)\n+    api.upload_folder(folder_path=output_model_dir, repo_id=repo_id, repo_type=\"model\")"
        },
        {
            "sha": "7117bc2b5d8045380f582ac09338af3f3f3e3f61",
            "filename": "src/transformers/quantizers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5cea20743755240bee86eca7eb69d646ab8db0e/src%2Ftransformers%2Fquantizers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5cea20743755240bee86eca7eb69d646ab8db0e/src%2Ftransformers%2Fquantizers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2F__init__.py?ref=e5cea20743755240bee86eca7eb69d646ab8db0e",
            "patch": "@@ -13,3 +13,4 @@\n # limitations under the License.\n from .auto import AutoHfQuantizer, AutoQuantizationConfig, register_quantization_config, register_quantizer\n from .base import HfQuantizer\n+from .quantizers_utils import get_module_from_name"
        }
    ],
    "stats": {
        "total": 258,
        "additions": 258,
        "deletions": 0
    }
}