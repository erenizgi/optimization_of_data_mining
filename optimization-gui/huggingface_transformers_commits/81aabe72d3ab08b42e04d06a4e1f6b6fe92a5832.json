{
    "author": "Cyrilvallez",
    "message": "Align tie weights in Idefics (#42551)\n\nfix",
    "sha": "81aabe72d3ab08b42e04d06a4e1f6b6fe92a5832",
    "files": [
        {
            "sha": "03fc2c3dc12dfae2a37fdaeaedc389bce0dac0c6",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 5,
            "deletions": 21,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/81aabe72d3ab08b42e04d06a4e1f6b6fe92a5832/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/81aabe72d3ab08b42e04d06a4e1f6b6fe92a5832/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=81aabe72d3ab08b42e04d06a4e1f6b6fe92a5832",
            "patch": "@@ -1107,31 +1107,15 @@ def __init__(self, config, vision_model=None):\n             bias=False,\n             partially_freeze=config.freeze_lm_head,\n         )\n+        if config.additional_vocab_size > 0:\n+            self._tied_weights_keys = {\n+                \"lm_head.weight\": \"model.embed_tokens.weight\",\n+                \"lm_head.additional_fc.weight\": \"model.embed_tokens.additional_embedding.weight\",\n+            }\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def tie_weights(self, **kwargs):\n-        \"\"\"\n-        Overwrite `transformers.modeling_utils.PreTrainedModel.tie_weights` to handle the case of\n-        IdeficsDecoupledLinear and IdeficsDecoupledEmbedding.\n-        \"\"\"\n-        output_embeddings = self.get_output_embeddings()\n-        input_embeddings = self.get_input_embeddings()\n-\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n-            output_embeddings.weight = input_embeddings.weight\n-            if input_embeddings.num_additional_embeddings > 0:\n-                assert output_embeddings.out_additional_features == input_embeddings.num_additional_embeddings\n-                output_embeddings.additional_fc.weight = input_embeddings.additional_embedding.weight\n-\n-        if hasattr(output_embeddings, \"out_features\") and hasattr(input_embeddings, \"num_embeddings\"):\n-            output_embeddings.out_features = input_embeddings.num_embeddings\n-            if hasattr(output_embeddings, \"out_additional_features\") and hasattr(\n-                input_embeddings, \"num_additional_embeddings\"\n-            ):\n-                output_embeddings.out_additional_features = input_embeddings.num_additional_embeddings\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        }
    ],
    "stats": {
        "total": 26,
        "additions": 5,
        "deletions": 21
    }
}