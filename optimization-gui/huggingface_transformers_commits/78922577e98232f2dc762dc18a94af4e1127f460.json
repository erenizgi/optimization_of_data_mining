{
    "author": "zucchini-nlp",
    "message": "FA2 can continue generation from cache (#39843)\n\n* add fa2 support to continue generation from cache\n\n* update q-len",
    "sha": "78922577e98232f2dc762dc18a94af4e1127f460",
    "files": [
        {
            "sha": "af97522a92cccc9163bf2d73f21cb4e6f152019f",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/78922577e98232f2dc762dc18a94af4e1127f460/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78922577e98232f2dc762dc18a94af4e1127f460/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=78922577e98232f2dc762dc18a94af4e1127f460",
            "patch": "@@ -677,24 +677,6 @@ def prepare_inputs_for_generation(\n         if encoder_attention_mask is not None:\n             model_inputs[\"attention_mask\"] = encoder_attention_mask\n \n-        if \"flash\" in self.config._attn_implementation and self._supports_attention_backend:\n-            tensor_kws = {\"dtype\": torch.int32, \"device\": self.device}\n-            pos = model_inputs[\"position_ids\"][:, -1]\n-\n-            cu_seq_lens_k = torch.cat([torch.zeros(1, **tensor_kws), pos.cumsum(0).add(1)], 0)\n-            max_length_k = int(pos.max()) + 1\n-\n-            bs, seq_len = input_ids.size()\n-            q_len = torch.ones(bs, **tensor_kws) if seq_len == 1 else pos.to(torch.int32).add(1)\n-            cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0)], 0)\n-            max_length_q = int(q_len.max())\n-\n-            model_inputs.update(\n-                cu_seq_lens_q=cu_seq_lens_q.to(self.device),\n-                cu_seq_lens_k=cu_seq_lens_k.to(self.device),\n-                max_length_q=max_length_q,\n-                max_length_k=max_length_k,\n-            )\n         # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:"
        },
        {
            "sha": "d9866e40951cbc5dec943972d4be9360531e3e6f",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 22,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/78922577e98232f2dc762dc18a94af4e1127f460/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78922577e98232f2dc762dc18a94af4e1127f460/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=78922577e98232f2dc762dc18a94af4e1127f460",
            "patch": "@@ -190,7 +190,7 @@ def _upad_input(\n     )\n \n \n-def _prepare_from_posids(query, key, value, position_ids):\n+def _prepare_from_posids(query, key, value, position_ids, query_length):\n     \"\"\"\n     This function returns necessary arguments to call `flash_attn_varlen_func`.\n     All three query, key, value states will be flattened.\n@@ -205,43 +205,66 @@ def _prepare_from_posids(query, key, value, position_ids):\n             Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n+        query_length (`int`):\n+            Sequence length of the input queries.\n     Return:\n         query (`torch.Tensor`):\n             Query state without padding. Shape: (total_target_length, num_heads, head_dim).\n         key (`torch.Tensor`):\n             Key state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n         value (`torch.Tensor`):\n             Value state with padding. Shape: (total_source_length, num_key_value_heads, head_dim).\n-        indices_q (`torch.Tensor`):\n-            The indices of non-masked tokens from the flattened input target sequence.\n         (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n             The cumulative sequence lengths for the target (query) and source (key, value), used to index into ragged (unpadded) tensors. `cu_seqlens` shape is (batch_size + 1,).\n         (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n+    kv_length = key.shape[1]\n     query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n     key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n     value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n \n-    position_ids = position_ids.flatten()\n-    indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n+    # If the lengths are not equal, most probably we are in decoding stage with cache\n+    # In that case the position ids will not always start with `0` and we need a better way to infer\n+    # cumulative seq lengths.\n+    if query_length != kv_length:\n+        indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n \n-    cu_seq_lens = torch.cat(\n-        (\n-            indices_q[position_ids == 0],\n-            torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n+        tensor_kws = {\"dtype\": torch.int32, \"device\": position_ids.device}\n+        last_position_ids = position_ids[:, -1]\n+\n+        cu_seq_lens_k = torch.cat(\n+            [torch.zeros(1, **tensor_kws), last_position_ids.cumsum(0).add(1).to(torch.int32)], 0\n         )\n-    )\n-    # NOTE: With torch compile, this will cause a graph break if you don't set\n-    # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n-    # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n-    # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n-    # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n-    # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n-    # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n-    # for some models (e.g. qwen2-vl).\n-    max_length = cu_seq_lens.diff().max().item()\n-    return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n+        max_length_k = int(last_position_ids.max()) + 1\n+\n+        batch_size, seq_len = query.shape[:2]\n+        q_len = torch.ones(batch_size, **tensor_kws) if query_length == 1 else last_position_ids.add(1)\n+        cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kws), q_len.cumsum(0).to(torch.int32)], 0)\n+        max_length_q = int(q_len.max())\n+    else:\n+        position_ids = position_ids.flatten()\n+        indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n+\n+        cu_seq_lens_q = torch.cat(\n+            (\n+                indices_q[position_ids == 0],\n+                torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n+            )\n+        )\n+        cu_seq_lens_k = cu_seq_lens_q\n+\n+        # NOTE: With torch compile, this will cause a graph break if you don't set\n+        # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n+        # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n+        # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n+        # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n+        # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n+        # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n+        # for some models (e.g. qwen2-vl).\n+        max_length_q = cu_seq_lens_q.diff().max().item()\n+        max_length_k = max_length_q\n+    return (query, key, value, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k))\n \n \n def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n@@ -430,8 +453,8 @@ def _flash_attention_forward(\n                 raise ValueError(\n                     \"Position ids should be passed if the attention mask is not passed and the cu_seq-lens are not passed.\"\n                 )\n-            q, k, v, idx, (cu_q, cu_k), (mq, mk) = _prepare_from_posids(\n-                query_states, key_states, value_states, position_ids\n+            q, k, v, (cu_q, cu_k), (mq, mk) = _prepare_from_posids(\n+                query_states, key_states, value_states, position_ids, query_length=query_length\n             )\n         else:\n             q = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))"
        },
        {
            "sha": "8fb4832799fb4efb328c1c1f726d8678dcc0e3d4",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/78922577e98232f2dc762dc18a94af4e1127f460/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/78922577e98232f2dc762dc18a94af4e1127f460/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=78922577e98232f2dc762dc18a94af4e1127f460",
            "patch": "@@ -4280,6 +4280,93 @@ def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa\n             attn_implementation=\"flash_attention_3\", fa_kwargs=True\n         )\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attention_2_continue_generate_with_position_ids(self):\n+        \"\"\"\n+        Tests that the given attention implementation can work with packed sequences and infers the mask\n+        from position ids. This test requires the model to use new attention mask API which handles packing.\n+        \"\"\"\n+\n+        max_new_tokens = 2\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention.\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if config.is_encoder_decoder:\n+                self.skipTest(\"Model is an encoder-decoder\")\n+\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(f\"{model_class.__name__} doesn't support caching\")\n+\n+            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n+                self.skipTest(\"Model dummy inputs should contain text input ids\")\n+\n+            # make sure that all models have enough positions for generation\n+            dummy_input_ids = inputs_dict[\"input_ids\"]\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.bfloat16,\n+                        attn_implementation=\"flash_attention_2\",\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # Drop all keys except for `input_ids`. Hard to manipulate with multimodals/head_mask/etc\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+                dummy_position_ids = torch.arange(dummy_input_ids.shape[1], device=torch_device)\n+                dummy_position_ids = dummy_position_ids.unsqueeze(0).repeat(dummy_input_ids.shape[0], 1)\n+\n+                # Store cache for the input prompt\n+                output = model(dummy_input_ids, position_ids=dummy_position_ids, use_cache=True)\n+                if \"past_key_values\" not in output:\n+                    self.skipTest(\"This model doesn't return `past_key_values`\")\n+\n+                # create new input_ids and position_ids to continue generation re-using the cache\n+                new_input_ids = output.logits[:, -1, :].float().argmax(-1)[:, None]\n+                past_length = dummy_input_ids.shape[1]\n+                position_ids = torch.arange(past_length, past_length + new_input_ids.shape[1], device=torch_device)\n+                position_ids = position_ids.unsqueeze(0).repeat(new_input_ids.shape[0], 1)\n+\n+                output = model(\n+                    input_ids=new_input_ids,\n+                    past_key_values=output.past_key_values,\n+                    position_ids=position_ids,\n+                    use_cache=True,\n+                )\n+                next_token_logits = output.logits[:, -1, :].float()\n+\n+                generate_kwargs = {\n+                    \"pad_token_id\": -1,\n+                    \"eos_token_id\": -1,\n+                    \"forced_eos_token_id\": None,\n+                    \"use_cache\": True,\n+                    \"do_sample\": False,\n+                    \"return_dict_in_generate\": True,\n+                    \"output_logits\": True,\n+                    \"max_new_tokens\": max_new_tokens,\n+                }\n+                generation_out = model.generate(dummy_input_ids, **generate_kwargs)\n+                next_token_logits_from_generate = generation_out.logits[-1]\n+\n+                # acceptable numerical instability\n+                # print(next_token_logits_from_generate, next_token_logits)\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(next_token_logits_from_generate, next_token_logits, rtol=tol, atol=tol)\n+\n     def flash_attn_from_config(self, attn_implementation: str):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the"
        }
    ],
    "stats": {
        "total": 172,
        "additions": 132,
        "deletions": 40
    }
}