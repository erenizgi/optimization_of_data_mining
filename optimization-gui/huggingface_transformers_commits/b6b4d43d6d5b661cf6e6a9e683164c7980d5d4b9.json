{
    "author": "yonigozlan",
    "message": "Add support for auto_docstring with model outputs (#38242)\n\n* experiment auto_docstring model outputs\n\n* Fix PatchTSMixer\n\n* Add check model output docstring to check_auto_docstring and fix all model outputs docstring\n\n* add reordering of docstring in check_docstrings\n\n* add check for redundant docstring in check_docstrings, remove redundant docstrings\n\n* refactor check_auto_docstring\n\n* make style\n\n* fix copies\n\n* remove commented code\n\n* change List-> list Tuple-> tuple in docstrings\n\n* fix modular\n\n* make style\n\n* Fix modular vipllava\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
    "files": [
        {
            "sha": "7285c8ba569a8a93be593d334e2a7ffecdec952b",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -570,30 +570,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class AlbertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`AlbertForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        sop_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class AlbertForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    sop_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f608eab3de3638026c82590a2164381c50b98654",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 34,
            "deletions": 50,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -40,20 +40,15 @@\n \n \n @dataclass\n-class AlignVisionModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    \"\"\"\n+)\n+class AlignVisionModelOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -62,26 +57,15 @@ class AlignVisionModelOutput(ModelOutput):\n \n \n @dataclass\n-class AlignTextModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for text model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The text embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class AlignTextModelOutput(ModelOutput):\n+    r\"\"\"\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n@@ -91,25 +75,25 @@ class AlignTextModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class AlignOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`AlignTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The output of [`AlignVisionModel`].\n-        text_model_output(`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            The output of the [`AlignTextModel`].\n-        vision_model_output(`BaseModelOutputWithPoolingAndNoAttention`):\n-            The output of the [`AlignVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`AlignTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The output of [`AlignVisionModel`].\n+    text_model_output (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        The output of the [`AlignTextModel`].\n+    vision_model_output (`BaseModelOutputWithPoolingAndNoAttention`):\n+        The output of the [`AlignVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a8c319f5ec2752a77e030ed99ec05583eb9a8dd7",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -53,26 +53,26 @@ def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring\n # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->AltCLIP\n class AltCLIPOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPVisionModel`].\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`AltCLIPTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`AltCLIPVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`AltCLIPVisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`AltCLIPTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`AltCLIPVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f62069a09f4cff90742e3582215f680b506481d1",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -963,35 +963,26 @@ def forward(\n \n \n @dataclass\n-class AriaCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Aria causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class AriaCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1003,33 +994,22 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class AriaModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Aria outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class AriaModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "85f67ab6d16f6d3cf4e1676aef6cd68a48cd2587",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 64,
            "deletions": 101,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -46,44 +46,35 @@\n \n \n @dataclass\n-class AutoFormerDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Trend tensor for each time series.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class AutoFormerDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Trend tensor for each time series.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n+        weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -95,63 +86,35 @@ class AutoFormerDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class AutoformerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Autoformer model output that contains the additional trend output.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Trend tensor for each time series.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        loc (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*):\n-            Shift values of each time series' context window which is used to give the model inputs of the same\n-            magnitude and then used to shift back to the original magnitude.\n-        scale (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*):\n-            Scaling values of each time series' context window which is used to give the model inputs of the same\n-            magnitude and then used to rescale back to the original magnitude.\n-        static_features: (`torch.FloatTensor` of shape `(batch_size, feature size)`, *optional*):\n-            Static features of each time series' in a batch which are copied to the covariates at inference time.\n+    \"\"\"\n+)\n+class AutoformerModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    trend (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Trend tensor for each time series.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+        `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+        blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+    loc (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*):\n+        Shift values of each time series' context window which is used to give the model inputs of the same\n+        magnitude and then used to shift back to the original magnitude.\n+    scale (`torch.FloatTensor` of shape `(batch_size,)` or `(batch_size, input_size)`, *optional*):\n+        Scaling values of each time series' context window which is used to give the model inputs of the same\n+        magnitude and then used to rescale back to the original magnitude.\n+    static_features: (`torch.FloatTensor` of shape `(batch_size, feature size)`, *optional*):\n+        Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -1795,6 +1758,14 @@ def forward(\n             Transformer requires to provide additional features.\n \n             The Autoformer only learns additional embeddings for `static_categorical_features`.\n+        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n+            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n+            in `[0, 1]`:\n+\n+            - 1 for values that are **observed**,\n+            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n+\n+            This mask is used to filter out missing values for the final loss calculation.\n         cross_attn_head_mask (`torch.Tensor` of shape `(decoder_layers, decoder_attention_heads)`, *optional*):\n             Mask to nullify selected heads of the cross-attention modules. Mask values selected in `[0, 1]`:\n \n@@ -1804,14 +1775,6 @@ def forward(\n             Tuple consists of `last_hidden_state`, `hidden_states` (*optional*) and `attentions` (*optional*)\n             `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)` (*optional*) is a sequence of\n             hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        future_observed_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, input_size)`, *optional*):\n-            Boolean mask to indicate which `future_values` were observed and which were missing. Mask values selected\n-            in `[0, 1]`:\n-\n-            - 1 for values that are **observed**,\n-            - 0 for values that are **missing** (i.e. NaNs that were replaced by zeros).\n-\n-            This mask is used to filter out missing values for the final loss calculation.\n \n         Examples:\n "
        },
        {
            "sha": "a55986b16316d7fec61ca47e2d8c12e2c1fba2f6",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -117,35 +117,26 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class AyaVisionCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for AyaVision causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class AyaVisionCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -157,33 +148,22 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for AyaVision outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "2b5fb795eaea4230e0775e3d47dd9c76492656d1",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 30,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -44,39 +44,19 @@\n \n logger = logging.get_logger(__name__)\n \n-# General docstring\n-\n-# Base docstring\n-_EXPECTED_OUTPUT_SHAPE = [1, 197, 768]\n-\n-# Image classification docstring\n-_IMAGE_CLASS_CHECKPOINT = \"microsoft/beit-base-patch16-224\"\n-_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n-\n \n @dataclass\n-class BeitModelOutputWithPooling(BaseModelOutputWithPooling):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`BeitModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n-            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n-            will be returned.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BeitModelOutputWithPooling(BaseModelOutputWithPooling):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+        *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+        will be returned.\n     \"\"\"\n \n "
        },
        {
            "sha": "c9ed5e3f6b4697da7bf526a3e9cdad41c7d39e15",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -805,30 +805,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class BertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`BertForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BertForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "1ac24c1bc2385a906bfcd2cbf992ea1598cb3365",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 24,
            "deletions": 46,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1744,30 +1744,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class BigBirdForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`BigBirdForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BigBirdForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1778,30 +1769,17 @@ class BigBirdForPreTrainingOutput(ModelOutput):\n \n \n @dataclass\n-class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of question answering models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, 1)`):\n-            pooler output from BigBigModel\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, 1)`):\n+        pooler output from BigBigModel\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "c799c2bf9609739ee84999c32d53d5448ac8e884",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 67,
            "deletions": 89,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,31 +49,31 @@ def blip_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n-class BlipForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Adapted from the base class for vision model's outputs that also contains image embeddings of the pooling of the\n     last hidden states. This class also adds the loss term from the text decoder.\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Language modeling loss from the text decoder.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n-            Prediction scores of the language modeling head of the text decoder model.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*):\n-            The image embeddings obtained after applying the Vision Transformer model to the input image.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BlipForConditionalGenerationModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Language modeling loss from the text decoder.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n+        Prediction scores of the language modeling head of the text decoder model.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*):\n+        The image embeddings obtained after applying the Vision Transformer model to the input image.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n     \"\"\"\n \n     loss: Optional[tuple[torch.FloatTensor]] = None\n@@ -94,29 +94,18 @@ def decoder_logits(self):\n \n \n @dataclass\n-class BlipTextVisionModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Adapted from the base class for vision model's outputs that also contains image embeddings of the pooling of the\n     last hidden states. This class also adds the loss term from the text decoder.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss from the text decoder.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BlipTextVisionModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss from the text decoder.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -127,36 +116,25 @@ class BlipTextVisionModelOutput(ModelOutput):\n \n \n @dataclass\n-class BlipImageTextMatchingModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Adapted from the base class for vision model's outputs that also contains image embeddings of the pooling of the\n     last hidden states. This class also adds the loss term from the text decoder as well as the image-text similarity\n     scores.\n-\n-    Args:\n-        itm_score (`torch.FloatTensor`):\n-            The image-text similarity scores.\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss from the text decoder.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        vision_pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*):\n-            Last layer hidden-state of the vision of the vision-only branch of the model.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        question_embeds (`torch.FloatTensor`):\n-            The question embeddings obtained by the text projection layer.\n+    \"\"\"\n+)\n+class BlipImageTextMatchingModelOutput(ModelOutput):\n+    r\"\"\"\n+    itm_score (`torch.FloatTensor`):\n+        The image-text similarity scores.\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss from the text decoder.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n+    vision_pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*):\n+        Last layer hidden-state of the vision of the vision-only branch of the model.\n+    question_embeds (`torch.FloatTensor`):\n+        The question embeddings obtained by the text projection layer.\n     \"\"\"\n \n     itm_score: Optional[torch.FloatTensor] = None\n@@ -170,25 +148,25 @@ class BlipImageTextMatchingModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class BlipOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`BlipTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of [`BlipVisionModel`].\n-        text_model_output(`BaseModelOutputWithPooling`):\n-            The output of the [`BlipTextModel`].\n-        vision_model_output(`BaseModelOutputWithPooling`):\n-            The output of the [`BlipVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`BlipTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`BlipVisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`BlipTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`BlipVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8d26aee4e56bf7a10378b4fc69212aa7e930573e",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 50,
            "deletions": 70,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -45,21 +45,23 @@\n \n \n @dataclass\n-class Blip2ForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class defining the outputs of [`Blip2ForConditionalGeneration`].\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Language modeling loss from the language model.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head of the language model.\n-        vision_outputs (`BaseModelOutputWithPooling`):\n-            Outputs of the vision encoder.\n-        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            Outputs of the Q-Former (Querying Transformer).\n-        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n-            Outputs of the language model.\n+    \"\"\"\n+)\n+class Blip2ForConditionalGenerationModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Language modeling loss from the language model.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head of the language model.\n+    vision_outputs (`BaseModelOutputWithPooling`):\n+        Outputs of the vision encoder.\n+    qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        Outputs of the Q-Former (Querying Transformer).\n+    language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n+        Outputs of the language model.\n     \"\"\"\n \n     loss: Optional[tuple[torch.FloatTensor]] = None\n@@ -78,25 +80,25 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n+@auto_docstring\n class Blip2ImageTextMatchingModelOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output.\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`Blip2QFormerModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`Blip2VisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output.\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Blip2QFormerModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Blip2VisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -115,27 +117,16 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for text model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n # Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Blip2\n class Blip2TextModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for text model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The text embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n@@ -145,27 +136,16 @@ class Blip2TextModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n+    \"\"\"\n+)\n # Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Blip2\n class Blip2VisionModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "9b4225655f53a66d5a727003fa1f4794998c3d47",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 32,
            "deletions": 42,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -45,28 +45,20 @@\n \n \n @dataclass\n-class BridgeTowerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`BridgeTowerModel`].\n-\n-    Args:\n-        text_features (`torch.FloatTensor` of shape `(batch_size, text_sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the text output of the last layer of the model.\n-        image_features (`torch.FloatTensor` of shape `(batch_size, image_sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the image output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size x 2)`):\n-            Concatenation of last layer hidden-state of the first token of the text and image sequence (classification\n-            token), respectively, after further processing through layers used for auxiliary pretraining tasks.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of\n-            the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BridgeTowerModelOutput(ModelOutput):\n+    r\"\"\"\n+    text_features (`torch.FloatTensor` of shape `(batch_size, text_sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the text output of the last layer of the model.\n+    image_features (`torch.FloatTensor` of shape `(batch_size, image_sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the image output of the last layer of the model.\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size x 2)`):\n+        Concatenation of last layer hidden-state of the first token of the text and image sequence (classification\n+        token), respectively, after further processing through layers used for auxiliary pretraining tasks.\n     \"\"\"\n \n     text_features: Optional[torch.FloatTensor] = None\n@@ -77,28 +69,26 @@ class BridgeTowerModelOutput(ModelOutput):\n \n \n @dataclass\n-class BridgeTowerContrastiveOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of ['BridgeTowerForContrastiveLearning']\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`:\n-            Image-text contrastive loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        text_embeds (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n-            The text embeddings obtained by applying the projection layer to the pooler_output.\n-        image_embeds (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        cross_embeds  (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n-            The text-image cross-modal embeddings obtained by applying the projection layer to the pooler_output.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of\n-            the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n+    \"\"\"\n+)\n+class BridgeTowerContrastiveOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Image-text contrastive loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    text_embeds (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n+    image_embeds (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n+    cross_embeds (`torch.FloatTensor)`, *optional*, returned when model is initialized with `with_projection=True`):\n+        The text-image cross-modal embeddings obtained by applying the projection layer to the pooler_output.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "e65f0166d7faef61e959478ee4b91c8defca25e4",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 12,
            "deletions": 21,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -40,28 +40,19 @@\n \n \n @dataclass\n-class BrosSpadeOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of token classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n-            Classification loss.\n-        initial_token_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Classification scores for entity initial tokens (before SoftMax).\n-        subsequent_token_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length+1)`):\n-            Classification scores for entity sequence tokens (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BrosSpadeOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    initial_token_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Classification scores for entity initial tokens (before SoftMax).\n+    subsequent_token_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, sequence_length+1)`):\n+        Classification scores for entity sequence tokens (before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "e19fe2534e1c5967ee1bbafe561f942913a1f7b6",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 25,
            "deletions": 23,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,32 +49,34 @@\n \n \n @dataclass\n-class CanineModelOutputWithPooling(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`CanineModel`]. Based on [`~modeling_outputs.BaseModelOutputWithPooling`], but with slightly\n     different `hidden_states` and `attentions`, as these also include the hidden states and attentions of the shallow\n     Transformer encoders.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model (i.e. the output of the final\n-            shallow Transformer encoder).\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Hidden-state of the first token of the sequence (classification token) at the last layer of the deep\n-            Transformer encoder, further processed by a Linear layer and a Tanh activation function. The Linear layer\n-            weights are trained from the next sentence prediction (classification) objective during pretraining.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the input to each encoder + one for the output of each layer of each\n-            encoder) of shape `(batch_size, sequence_length, hidden_size)` and `(batch_size, sequence_length //\n-            config.downsampling_rate, hidden_size)`. Hidden-states of the model at the output of each layer plus the\n-            initial input to each Transformer encoder. The hidden states of the shallow encoders have length\n-            `sequence_length`, but the hidden states of the deep encoder have length `sequence_length` //\n-            `config.downsampling_rate`.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of the 3 Transformer encoders of shape `(batch_size,\n-            num_heads, sequence_length, sequence_length)` and `(batch_size, num_heads, sequence_length //\n-            config.downsampling_rate, sequence_length // config.downsampling_rate)`. Attentions weights after the\n-            attention softmax, used to compute the weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class CanineModelOutputWithPooling(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model (i.e. the output of the final\n+        shallow Transformer encoder).\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Hidden-state of the first token of the sequence (classification token) at the last layer of the deep\n+        Transformer encoder, further processed by a Linear layer and a Tanh activation function. The Linear layer\n+        weights are trained from the next sentence prediction (classification) objective during pretraining.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the input to each encoder + one for the output of each layer of each\n+        encoder) of shape `(batch_size, sequence_length, hidden_size)` and `(batch_size, sequence_length //\n+        config.downsampling_rate, hidden_size)`. Hidden-states of the model at the output of each layer plus the\n+        initial input to each Transformer encoder. The hidden states of the shallow encoders have length\n+        `sequence_length`, but the hidden states of the deep encoder have length `sequence_length` //\n+        `config.downsampling_rate`.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of the 3 Transformer encoders of shape `(batch_size,\n+        num_heads, sequence_length, sequence_length)` and `(batch_size, num_heads, sequence_length //\n+        config.downsampling_rate, sequence_length // config.downsampling_rate)`. Attentions weights after the\n+        attention softmax, used to compute the weighted average in the self-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "2ea9225b5528d10d1f2434f5edc4f9e2acc7d6d4",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -52,27 +52,27 @@ def chinese_clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring\n class ChineseCLIPOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image:(`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text:(`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of\n-            [`ChineseCLIPTextModel`].\n-        image_embeds(`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of\n-            [`ChineseCLIPVisionModel`].\n-        text_model_output(`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            The output of the [`ChineseCLIPTextModel`].\n-        vision_model_output(`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            The output of the [`ChineseCLIPVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of\n+        [`ChineseCLIPTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of\n+        [`ChineseCLIPVisionModel`].\n+    text_model_output (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        The output of the [`ChineseCLIPTextModel`].\n+    vision_model_output (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        The output of the [`ChineseCLIPVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "737dc6abad799254835f54b7ba197a4f1f2cf004",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 36,
            "deletions": 58,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -122,27 +122,16 @@ def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for text model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n # Copied from transformers.models.clip.modeling_clip.CLIPTextModelOutput with CLIP->Clap\n class ClapTextModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for text model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The text embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n@@ -152,26 +141,15 @@ class ClapTextModelOutput(ModelOutput):\n \n \n @dataclass\n-class ClapAudioModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     ClapAudio model output to mimic the output of the original implementation.\n-\n-    Args:\n-        audio_embeds (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            The Audio embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    \"\"\"\n+)\n+class ClapAudioModelOutput(ModelOutput):\n+    r\"\"\"\n+    audio_embeds (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        The Audio embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     audio_embeds: Optional[torch.FloatTensor] = None\n@@ -181,26 +159,26 @@ class ClapAudioModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->Clap, vision->audio, Vision->Audio, image->audio\n class ClapOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for audio-text similarity.\n-        logits_per_audio (`torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `audio_embeds` and `text_embeds`. This represents the audio-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `audio_embeds`. This represents the text-audio\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`ClapTextModel`].\n-        audio_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The audio embeddings obtained by applying the projection layer to the pooled output of [`ClapAudioModel`].\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`ClapTextModel`].\n-        audio_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`ClapAudioModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for audio-text similarity.\n+    logits_per_audio (`torch.FloatTensor` of shape `(audio_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `audio_embeds` and `text_embeds`. This represents the audio-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, audio_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `audio_embeds`. This represents the text-audio\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`ClapTextModel`].\n+    audio_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The audio embeddings obtained by applying the projection layer to the pooled output of [`ClapAudioModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`ClapTextModel`].\n+    audio_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`ClapAudioModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1931,11 +1909,11 @@ def forward(\n         input_features (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n             Input audio features. This should be returned by the [`ClapFeatureExtractor`] class that you can also\n             retrieve from [`AutoFeatureExtractor`]. See [`ClapFeatureExtractor.__call__`] for details.\n-        return_loss (`bool`, *optional*):\n-            Whether or not to return the contrastive loss.\n         is_longer (`torch.FloatTensor`, of shape `(batch_size, 1)`, *optional*):\n             Whether the audio clip is longer than `max_length`. If `True`, a feature fusion will be enabled to enhance\n             the features.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n \n         Examples:\n "
        },
        {
            "sha": "a4147200a532afa0e88f1023f3a790874787532f",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 34,
            "deletions": 56,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -57,26 +57,15 @@ def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n-class CLIPVisionModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class CLIPVisionModelOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -86,26 +75,15 @@ class CLIPVisionModelOutput(ModelOutput):\n \n \n @dataclass\n-class CLIPTextModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for text model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The text embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class CLIPTextModelOutput(ModelOutput):\n+    r\"\"\"\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n@@ -115,25 +93,25 @@ class CLIPTextModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class CLIPOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPVisionModel`].\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`CLIPTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`CLIPVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPVisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`CLIPTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`CLIPVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "c30d92fcdbfc7554969fe716d6f69db71c379878",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,26 +49,26 @@ def clipseg_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring\n # Copied from transformers.models.clip.modeling_clip.CLIPOutput with CLIP->CLIPSeg\n class CLIPSegOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegVisionModel`].\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`CLIPSegTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`CLIPSegVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`CLIPSegVisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`CLIPSegTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`CLIPSegVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -87,18 +87,11 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n+@auto_docstring\n class CLIPSegDecoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, height, width)`):\n-            Classification scores for each pixel.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, height, width)`):\n+        Classification scores for each pixel.\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None\n@@ -107,14 +100,21 @@ class CLIPSegDecoderOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class CLIPSegImageSegmentationOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        ...\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`CLIPSegVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Binary cross entropy loss for segmentation.\n+    logits (`torch.FloatTensor` of shape `(batch_size, height, width)`):\n+        Classification scores for each pixel.\n+    conditional_embeddings (`torch.FloatTensor` of shape `(batch_size, projection_dim)`):\n+        Conditional embeddings used for segmentation.\n+    pooled_output (`torch.FloatTensor` of shape `(batch_size, embed_dim)`):\n+        Pooled output of the [`CLIPSegVisionModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`CLIPSegVisionModel`].\n+    decoder_output (`CLIPSegDecoderOutput`):\n+        The output of the [`CLIPSegDecoder`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1260,15 +1260,15 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, CLIPSegOutput]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n         conditional_pixel_values (`torch.FloatTensor`, *optional*):\n             The pixel values of the conditional images.\n         conditional_embeddings (`torch.FloatTensor` of shape `(batch_size, config.projection_dim)`, *optional*):\n             The conditional embeddings for the query images. If provided, the model will use this instead of computing\n             the embeddings from the conditional_pixel_values.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n \n         Examples:\n "
        },
        {
            "sha": "9eb8140103b65eb702d751646fde34f50abb939f",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 40,
            "deletions": 46,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -144,26 +144,20 @@ def _pad_extra_bos_eos_tokens(\n \n \n @dataclass\n-class ClvpEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for CLVP encoder's outputs that contains a pooling of the last hidden states as well as a projection\n     output (a linear layer on top of the pooled output).\n-\n-    Args:\n-        embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when model is initialized with `with_projection=True`):\n-            The embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            The hidden state of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Pooled output of the `last_hidden_state`.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of\n-            the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class ClvpEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when model is initialized with `with_projection=True`):\n+        The embeddings obtained by applying the projection layer to the pooler_output.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        The hidden state of the last layer of the model.\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Pooled output of the `last_hidden_state`.\n     \"\"\"\n \n     embeds: Optional[torch.FloatTensor] = None\n@@ -174,35 +168,35 @@ class ClvpEncoderOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class ClvpOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for speech-text similarity.\n-        speech_ids (`torch.LongTensor`, *optional*):\n-            speech_ids (or speech candidates) generated by the `ClvpForCausalLM` model.\n-        logits_per_speech (`torch.FloatTensor` of shape `(speech_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `speech_embeds` and `text_embeds`. This represents the speech-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, speech_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `speech_embeds`. This represents the text-speech\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of the text encoder\n-            model.\n-        speech_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The speech embeddings obtained by applying the projection layer to the pooled output of the speech encoder\n-            model.\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The pooled output of the `last_hidden_state` of the text encoder Model.\n-        speech_model_output (`BaseModelOutputWithPooling`):\n-            The pooled output of the `last_hidden_state` of the speech encoder Model.\n-        decoder_hidden_states (`torch.FloatTensor`, *optional*):\n-            The hidden states of the decoder model.\n-        text_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n-            The hidden states of the text encoder model.\n-        speech_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n-            The hidden states of the speech encoder model.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for speech-text similarity.\n+    speech_ids (`torch.LongTensor`, *optional*):\n+        speech_ids (or speech candidates) generated by the `ClvpForCausalLM` model.\n+    logits_per_speech (`torch.FloatTensor` of shape `(speech_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `speech_embeds` and `text_embeds`. This represents the speech-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, speech_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `speech_embeds`. This represents the text-speech\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of the text encoder\n+        model.\n+    speech_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The speech embeddings obtained by applying the projection layer to the pooled output of the speech encoder\n+        model.\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The pooled output of the `last_hidden_state` of the text encoder Model.\n+    speech_model_output (`BaseModelOutputWithPooling`):\n+        The pooled output of the `last_hidden_state` of the speech encoder Model.\n+    decoder_hidden_states (`torch.FloatTensor`, *optional*):\n+        The hidden states of the decoder model.\n+    text_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n+        The hidden states of the text encoder model.\n+    speech_encoder_hidden_states (`torch.FloatTensor`, *optional*):\n+        The hidden states of the speech encoder model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a5c8f8dd6dafef76b6db259d734273ebd57f6e86",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 20,
            "deletions": 28,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -28,6 +28,7 @@\n from .configuration_colpali import ColPaliConfig\n \n \n+@auto_docstring\n class ColPaliPreTrainedModel(PreTrainedModel):\n     config_class = ColPaliConfig\n     base_model_prefix = \"model\"\n@@ -51,35 +52,26 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class ColPaliForRetrievalOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for ColPali embeddings output.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            The embeddings of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n+    \"\"\"\n+)\n+class ColPaliForRetrievalOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        The embeddings of the model.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "5a7633482996a5034a5199b9d5e16db5d33b61af",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 25,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -36,6 +36,7 @@\n     import torch\n \n \n+@auto_docstring\n class ColQwen2PreTrainedModel(PreTrainedModel):\n     config_class = ColQwen2Config\n     base_model_prefix = \"model\"\n@@ -62,32 +63,23 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class ColQwen2ForRetrievalOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for ColQwen2 embeddings output.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            The embeddings of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class ColQwen2ForRetrievalOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        The embeddings of the model.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "b895f7f83fb453047971cd5c692564274a961a61",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 25,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -231,32 +231,23 @@ class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n \n \n @dataclass\n-class ColQwen2ForRetrievalOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for ColQwen2 embeddings output.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            The embeddings of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class ColQwen2ForRetrievalOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        The embeddings of the model.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "bd9bd8a51466a3356a8157037704757622fcccde",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 84,
            "deletions": 153,
            "changes": 237,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -39,131 +39,83 @@\n \n \n @dataclass\n-class ConditionalDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Conditional DETR decoder. This class adds one attribute to\n     BaseModelOutputWithCrossAttentions, namely an optional stack of intermediate decoder activations, i.e. the output\n     of each decoder layer, each of them gone through a layernorm. This is useful when training the model with auxiliary\n     decoding losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n-        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n-            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+)\n+class ConditionalDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    r\"\"\"\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n+    reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+        Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     reference_points: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n-class ConditionalDetrModelOutput(Seq2SeqModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Conditional DETR encoder-decoder model. This class adds one attribute to\n     Seq2SeqModelOutput, namely an optional stack of intermediate decoder activations, i.e. the output of each decoder\n     layer, each of them gone through a layernorm. This is useful when training the model with auxiliary decoding\n     losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n-        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n-            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+)\n+class ConditionalDetrModelOutput(Seq2SeqModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n+    reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+        Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     reference_points: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`ConditionalDetrForObjectDetection`].\n+    \"\"\"\n+)\n # Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->ConditionalDetr\n class ConditionalDetrObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`ConditionalDetrForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -181,59 +133,39 @@ class ConditionalDetrObjectDetectionOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`ConditionalDetrForSegmentation`].\n+    \"\"\"\n+)\n # Copied from transformers.models.detr.modeling_detr.DetrSegmentationOutput with Detr->ConditionalDetr\n class ConditionalDetrSegmentationOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`ConditionalDetrForSegmentation`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        pred_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4, width/4)`):\n-            Segmentation masks logits for all queries. See also\n-            [`~ConditionalDetrImageProcessor.post_process_semantic_segmentation`] or\n-            [`~ConditionalDetrImageProcessor.post_process_instance_segmentation`]\n-            [`~ConditionalDetrImageProcessor.post_process_panoptic_segmentation`] to evaluate semantic, instance and panoptic\n-            segmentation masks respectively.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~ConditionalDetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4, width/4)`):\n+        Segmentation masks logits for all queries. See also\n+        [`~ConditionalDetrImageProcessor.post_process_semantic_segmentation`] or\n+        [`~ConditionalDetrImageProcessor.post_process_instance_segmentation`]\n+        [`~ConditionalDetrImageProcessor.post_process_panoptic_segmentation`] to evaluate semantic, instance and panoptic\n+        segmentation masks respectively.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1022,7 +954,6 @@ def forward(self, x):\n \n \n @auto_docstring\n-\n # Copied from transformers.models.detr.modeling_detr.DetrPreTrainedModel with Detr->ConditionalDetr\n class ConditionalDetrPreTrainedModel(PreTrainedModel):\n     config_class = ConditionalDetrConfig"
        },
        {
            "sha": "19850c0dd8b7784ca0898313e00742c9b4bb0ac4",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 33,
            "deletions": 42,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -46,49 +46,40 @@\n \n \n @dataclass\n-class CsmOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for the model autoregressive outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction) of the depth decoder model.\n-        depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n-        depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-        depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-        backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction) of the backbone model.\n+    \"\"\"\n+)\n+class CsmOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction) of the depth decoder model.\n+    depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n+    depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+    backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction) of the backbone model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "c1a930aa702d731e90cc7ba46b8a694b9c952dd3",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 33,
            "deletions": 42,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -46,49 +46,40 @@\n \n \n @dataclass\n-class CsmOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for the model autoregressive outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction) of the depth decoder model.\n-        depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n-        depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-        depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-        backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction) of the backbone model.\n+    \"\"\"\n+)\n+class CsmOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    depth_decoder_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction) of the depth decoder model.\n+    depth_decoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the depth decoder (scores for each vocabulary token before SoftMax).\n+    depth_decoder_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+    depth_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    depth_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+    backbone_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction) of the backbone model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "aec8ec8123a5b57b7cb27dd400f410017a57b4b1",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -33,19 +33,15 @@\n \n \n @dataclass\n-class BaseModelOutputWithCLSToken(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        cls_token_value (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`):\n-            Classification token at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n+    \"\"\"\n+)\n+class BaseModelOutputWithCLSToken(ModelOutput):\n+    r\"\"\"\n+    cls_token_value (`torch.FloatTensor` of shape `(batch_size, 1, hidden_size)`):\n+        Classification token at the output of the last layer of the model.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "ab4025824972bef80d392b39f68f82c8a282e49c",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 103,
            "deletions": 147,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -433,57 +433,41 @@ def forward(\n \n \n @dataclass\n-class DFineModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the RT-DETR encoder-decoder model.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n-            Logits of predicted bounding boxes coordinates in the encoder stage.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class DFineModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points used for the first decoder layer.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n+        Logits of predicted bounding boxes coordinates in the encoder stage.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -507,76 +491,56 @@ class DFineModelOutput(ModelOutput):\n \n \n @dataclass\n-class DFineObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`DFineForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~DFineImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized (absolute) bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class DFineObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DFineImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized (absolute) bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1008,38 +972,30 @@ def forward(self, pred_corners: torch.Tensor, project: torch.Tensor) -> torch.Te\n \n \n @dataclass\n-class DFineDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the DFineDecoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class DFineDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "0f88a06fe64f43edd5af2c084f4b99e564459db7",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 55,
            "deletions": 103,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -39,133 +39,85 @@\n \n \n @dataclass\n-# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n-class DabDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Conditional DETR decoder. This class adds one attribute to\n     BaseModelOutputWithCrossAttentions, namely an optional stack of intermediate decoder activations, i.e. the output\n     of each decoder layer, each of them gone through a layernorm. This is useful when training the model with auxiliary\n     decoding losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n-        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n-            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+)\n+# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrDecoderOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n+class DabDetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    r\"\"\"\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n+    reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+        Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     reference_points: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n-# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n-class DabDetrModelOutput(Seq2SeqModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Conditional DETR encoder-decoder model. This class adds one attribute to\n     Seq2SeqModelOutput, namely an optional stack of intermediate decoder activations, i.e. the output of each decoder\n     layer, each of them gone through a layernorm. This is useful when training the model with auxiliary decoding\n     losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n-        reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n-            Reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+)\n+# Copied from transformers.models.conditional_detr.modeling_conditional_detr.ConditionalDetrModelOutput with ConditionalDetr->DabDetr,Conditional DETR->DAB-DETR,2 (anchor points)->4 (anchor points)\n+class DabDetrModelOutput(Seq2SeqModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n+    reference_points (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, 2 (anchor points))`):\n+        Reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     reference_points: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`DabDetrForObjectDetection`].\n+    \"\"\"\n+)\n # Copied from transformers.models.detr.modeling_detr.DetrObjectDetectionOutput with Detr->DabDetr\n class DabDetrObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`DabDetrForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~DabDetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DabDetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "b3bca5b63ee985adfe2c9d3c90ec02959b14b25c",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -29,19 +29,19 @@\n \n \n @dataclass\n+@auto_docstring\n class DacOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.Tensor`):\n-            Loss from the encoder model, comprising the weighted combination of the commitment and codebook losses.\n-        audio_values (`torch.Tensor` of shape `(batch_size, input_length)`):\n-            Reconstructed audio data.\n-        quantized_representation (`torch.Tensor` of shape `(batch_size, dimension, time_steps)`):\n-            Quantized continuous representation of input.\n-        audio_codes (`torch.LongTensor` of shape `(batch_size, num_codebooks, time_steps)`):\n-            Codebook indices for each codebook (quantized discrete representation of input).\n-        projected_latents (`torch.Tensor` of shape `(batch_size, num_codebooks * dimension, time_steps)`):\n-            Projected latents (continuous representation of input before quantization).\n+    r\"\"\"\n+    loss (`torch.Tensor`):\n+        Loss from the encoder model, comprising the weighted combination of the commitment and codebook losses.\n+    audio_values (`torch.Tensor` of shape `(batch_size, input_length)`):\n+        Reconstructed audio data.\n+    quantized_representation (`torch.Tensor` of shape `(batch_size, dimension, time_steps)`):\n+        Quantized continuous representation of input.\n+    audio_codes (`torch.LongTensor` of shape `(batch_size, num_codebooks, time_steps)`):\n+        Codebook indices for each codebook (quantized discrete representation of input).\n+    projected_latents (`torch.Tensor` of shape `(batch_size, num_codebooks * dimension, time_steps)`):\n+        Projected latents (continuous representation of input before quantization).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -52,17 +52,17 @@ class DacOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class DacEncoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.Tensor`):\n-            Loss from the encoder model, comprising the weighted combination of the commitment and codebook losses.\n-        quantized_representation (`torch.Tensor` of shape `(batch_size, dimension, time_steps)`, *optional*):\n-            Quantized continuous representation of input.\n-        audio_codes (`torch.Tensor` of shape `(batch_size, num_codebooks, time_steps)`, *optional*):\n-            Codebook indices for each codebook (quantized discrete representation of input).\n-        projected_latents (`torch.Tensor` of shape `(batch_size, num_codebooks * dimension, time_steps)`, *optional*):\n-            Projected latents (continuous representation of input before quantization).\n+    r\"\"\"\n+    loss (`torch.Tensor`):\n+        Loss from the encoder model, comprising the weighted combination of the commitment and codebook losses.\n+    quantized_representation (`torch.Tensor` of shape `(batch_size, dimension, time_steps)`, *optional*):\n+        Quantized continuous representation of input.\n+    audio_codes (`torch.Tensor` of shape `(batch_size, num_codebooks, time_steps)`, *optional*):\n+        Codebook indices for each codebook (quantized discrete representation of input).\n+    projected_latents (`torch.Tensor` of shape `(batch_size, num_codebooks * dimension, time_steps)`, *optional*):\n+        Projected latents (continuous representation of input before quantization).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -72,12 +72,12 @@ class DacEncoderOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n # Copied from transformers.models.encodec.modeling_encodec.EncodecDecoderOutput with Encodec->Dac, segment_length->input_length\n class DacDecoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_values (`torch.FloatTensor`  of shape `(batch_size, input_length)`, *optional*):\n-            Decoded audio values, obtained using the decoder part of Dac.\n+    r\"\"\"\n+    audio_values (`torch.FloatTensor`  of shape `(batch_size, input_length)`, *optional*):\n+        Decoded audio values, obtained using the decoder part of Dac.\n     \"\"\"\n \n     audio_values: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "ede5404571abb39b163ff19fb6850e453169d90f",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 10,
            "deletions": 21,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -43,29 +43,18 @@\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class for outputs of [`Data2VecVisionModel`].\n+    \"\"\"\n+)\n # Copied from transformers.models.beit.modeling_beit.BeitModelOutputWithPooling with Beit->Data2VecVision\n class Data2VecVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n-    \"\"\"\n-    Class for outputs of [`Data2VecVisionModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n-            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n-            will be returned.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+        *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+        will be returned.\n     \"\"\"\n \n "
        },
        {
            "sha": "c2c0a2a64545c73f1eec746d824cb5c26bd5027a",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 12,
            "deletions": 23,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -711,30 +711,19 @@ def forward(\n \n \n @dataclass\n-class DecisionTransformerOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        state_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, state_dim)`):\n-            Environment state predictions\n-        action_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, action_dim)`):\n-            Model action predictions\n-        return_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, 1)`):\n-            Predicted returns for each state\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DecisionTransformerOutput(ModelOutput):\n+    r\"\"\"\n+    state_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, state_dim)`):\n+        Environment state predictions\n+    action_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, action_dim)`):\n+        Model action predictions\n+    return_preds (`torch.FloatTensor` of shape `(batch_size, sequence_length, 1)`):\n+        Predicted returns for each state\n     \"\"\"\n \n     state_preds: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8c26e2ffaf83682a23893994f1b2f4c162eae749",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 71,
            "deletions": 119,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -108,32 +108,24 @@ def forward(\n \n \n @dataclass\n-class DeformableDetrDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the DeformableDetrDecoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class DeformableDetrDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -145,47 +137,27 @@ class DeformableDetrDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class DeformableDetrModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Deformable DETR encoder-decoder model.\n-\n-    Args:\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n+    \"\"\"\n+)\n+class DeformableDetrModelOutput(ModelOutput):\n+    r\"\"\"\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n     \"\"\"\n \n     init_reference_points: Optional[torch.FloatTensor] = None\n@@ -203,64 +175,44 @@ class DeformableDetrModelOutput(ModelOutput):\n \n \n @dataclass\n-class DeformableDetrObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`DeformableDetrForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~DeformableDetrProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, sequence_length, num_heads, 4,\n-            4)`. Attentions weights of the encoder, after the attention softmax, used to compute the weighted average\n-            in the self-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n+    \"\"\"\n+)\n+class DeformableDetrObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DeformableDetrProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "4a63af0ea72182bc64c3ff3722310e23b1e73f7b",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 20,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -807,27 +807,21 @@ def forward(\n \n \n @dataclass\n-class DeiTForImageClassificationWithTeacherOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`DeiTForImageClassificationWithTeacher`].\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores as the average of the cls_logits and distillation logits.\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the\n-            class token).\n-        distillation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the\n-            distillation token).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class DeiTForImageClassificationWithTeacherOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores as the average of the cls_logits and distillation logits.\n+    cls_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the\n+        class token).\n+    distillation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the\n+        distillation token).\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "cd96a3c1b8efed44567907802644b0a7f3b1bdf8",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 20,
            "deletions": 40,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -32,26 +32,17 @@\n \n \n @dataclass\n-class DepthProOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for DepthPro's outputs.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        features (`Union[torch.FloatTensor, list[torch.FloatTensor]]`, *optional*):\n-            Features from encoders. Can be a single feature or a list of features.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer and the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, n_patches_per_batch, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DepthProOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    features (`Union[torch.FloatTensor, List[torch.FloatTensor]]`, *optional*):\n+        Features from encoders. Can be a single feature or a list of features.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -61,28 +52,17 @@ class DepthProOutput(ModelOutput):\n \n \n @dataclass\n-class DepthProDepthEstimatorOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for DepthProForDepthEstimation's output.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        predicted_depth (`torch.FloatTensor` of shape `(batch_size, height, width)`):\n-            Predicted depth for each pixel.\n-        field_of_view (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned when `use_fov_model` is provided):\n-            Field of View Scaler.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer and the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, n_patches_per_batch, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DepthProDepthEstimatorOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    field_of_view (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned when `use_fov_model` is provided):\n+        Field of View Scaler.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "e485fba776061b856b4f06b747df3d9d8f2c606e",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 80,
            "deletions": 148,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -45,122 +45,74 @@\n \n \n @dataclass\n-class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the DETR decoder. This class adds one attribute to BaseModelOutputWithCrossAttentions,\n     namely an optional stack of intermediate decoder activations, i.e. the output of each decoder layer, each of them\n     gone through a layernorm. This is useful when training the model with auxiliary decoding losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n+    \"\"\"\n+)\n+class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    r\"\"\"\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class DetrModelOutput(Seq2SeqModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the DETR encoder-decoder model. This class adds one attribute to Seq2SeqModelOutput,\n     namely an optional stack of intermediate decoder activations, i.e. the output of each decoder layer, each of them\n     gone through a layernorm. This is useful when training the model with auxiliary decoding losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n+    \"\"\"\n+)\n+class DetrModelOutput(Seq2SeqModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, sequence_length, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class DetrObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`DetrForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~DetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class DetrObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -178,58 +130,38 @@ class DetrObjectDetectionOutput(ModelOutput):\n \n \n @dataclass\n-class DetrSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`DetrForSegmentation`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~DetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        pred_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4, width/4)`):\n-            Segmentation masks logits for all queries. See also\n-            [`~DetrImageProcessor.post_process_semantic_segmentation`] or\n-            [`~DetrImageProcessor.post_process_instance_segmentation`]\n-            [`~DetrImageProcessor.post_process_panoptic_segmentation`] to evaluate semantic, instance and panoptic\n-            segmentation masks respectively.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the decoder at the output of each\n-            layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the encoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class DetrSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height/4, width/4)`):\n+        Segmentation masks logits for all queries. See also\n+        [`~DetrImageProcessor.post_process_semantic_segmentation`] or\n+        [`~DetrImageProcessor.post_process_instance_segmentation`]\n+        [`~DetrImageProcessor.post_process_panoptic_segmentation`] to evaluate semantic, instance and panoptic\n+        segmentation masks respectively.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "0618e0d2ded8da72ddfc5d9318d0ed2df9b8ef80",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 41,
            "deletions": 72,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -57,30 +57,19 @@ def natten2dav(*args, **kwargs):\n \n \n @dataclass\n-class DinatEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Dinat encoder's outputs, with potential hidden states and attentions.\n+    \"\"\"\n+)\n+class DinatEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -90,32 +79,21 @@ class DinatEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class DinatModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Dinat model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n-            Average pooling of the last layer hidden-state.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class DinatModelOutput(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n+        Average pooling of the last layer hidden-state.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -126,32 +104,23 @@ class DinatModelOutput(ModelOutput):\n \n \n @dataclass\n-class DinatImageClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Dinat outputs for image classification.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class DinatImageClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a2dafed7405806b2378f499b14a67c1277c98513",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 41,
            "deletions": 72,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -38,31 +38,20 @@\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    DonutSwin encoder's outputs, with potential hidden states and attentions.\n+    \"\"\"\n+)\n # Copied from transformers.models.swin.modeling_swin.SwinEncoderOutput with Swin->DonutSwin\n class DonutSwinEncoderOutput(ModelOutput):\n-    \"\"\"\n-    DonutSwin encoder's outputs, with potential hidden states and attentions.\n+    r\"\"\"\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -72,33 +61,22 @@ class DonutSwinEncoderOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    DonutSwin model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n # Copied from transformers.models.swin.modeling_swin.SwinModelOutput with Swin->DonutSwin\n class DonutSwinModelOutput(ModelOutput):\n-    \"\"\"\n-    DonutSwin model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n-            Average pooling of the last layer hidden-state.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n+        Average pooling of the last layer hidden-state.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -109,33 +87,24 @@ class DonutSwinModelOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    DonutSwin outputs for image classification.\n+    \"\"\"\n+)\n # Copied from transformers.models.swin.modeling_swin.SwinImageClassifierOutput with Swin->DonutSwin\n class DonutSwinImageClassifierOutput(ModelOutput):\n-    \"\"\"\n-    DonutSwin outputs for image classification.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "3e18b3e732f3edd56212374843724c953784e857",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 33,
            "deletions": 60,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -40,26 +40,17 @@\n \n \n @dataclass\n-class DPRContextEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`DPRQuestionEncoder`].\n-\n-    Args:\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`):\n-            The DPR encoder outputs the *pooler_output* that corresponds to the context representation. Last layer\n-            hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.\n-            This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DPRContextEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`):\n+        The DPR encoder outputs the *pooler_output* that corresponds to the context representation. Last layer\n+        hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.\n+        This output is to be used to embed contexts for nearest neighbors queries with questions embeddings.\n     \"\"\"\n \n     pooler_output: torch.FloatTensor\n@@ -68,26 +59,17 @@ class DPRContextEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class DPRQuestionEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`DPRQuestionEncoder`].\n-\n-    Args:\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`):\n-            The DPR encoder outputs the *pooler_output* that corresponds to the question representation. Last layer\n-            hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.\n-            This output is to be used to embed questions for nearest neighbors queries with context embeddings.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DPRQuestionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, embeddings_size)`):\n+        The DPR encoder outputs the *pooler_output* that corresponds to the question representation. Last layer\n+        hidden-state of the first token of the sequence (classification token) further processed by a Linear layer.\n+        This output is to be used to embed questions for nearest neighbors queries with context embeddings.\n     \"\"\"\n \n     pooler_output: torch.FloatTensor\n@@ -96,29 +78,20 @@ class DPRQuestionEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class DPRReaderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`DPRQuestionEncoder`].\n-\n-    Args:\n-        start_logits (`torch.FloatTensor` of shape `(n_passages, sequence_length)`):\n-            Logits of the start index of the span for each passage.\n-        end_logits (`torch.FloatTensor` of shape `(n_passages, sequence_length)`):\n-            Logits of the end index of the span for each passage.\n-        relevance_logits (`torch.FloatTensor` of shape `(n_passages, )`):\n-            Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the\n-            question, compared to all the other passages.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class DPRReaderOutput(ModelOutput):\n+    r\"\"\"\n+    start_logits (`torch.FloatTensor` of shape `(n_passages, sequence_length)`):\n+        Logits of the start index of the span for each passage.\n+    end_logits (`torch.FloatTensor` of shape `(n_passages, sequence_length)`):\n+        Logits of the end index of the span for each passage.\n+    relevance_logits (`torch.FloatTensor` of shape `(n_passages, )`):\n+        Outputs of the QA classifier of the DPRReader that corresponds to the scores of each passage to answer the\n+        question, compared to all the other passages.\n     \"\"\"\n \n     start_logits: torch.FloatTensor"
        },
        {
            "sha": "8ec8be7794c8d105e9ee0e73fba6c8bd2931cbfa",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 23,
            "deletions": 32,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -42,49 +42,40 @@\n \n \n @dataclass\n-class BaseModelOutputWithIntermediateActivations(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that also contains intermediate activations that can be used at later stages. Useful\n     in the context of Vision models.:\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_activations (`tuple(torch.FloatTensor)`, *optional*):\n-            Intermediate activations that can be used to compute hidden states of the model at various layers.\n+    \"\"\"\n+)\n+class BaseModelOutputWithIntermediateActivations(ModelOutput):\n+    r\"\"\"\n+    last_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    intermediate_activations (`tuple(torch.FloatTensor)`, *optional*):\n+        Intermediate activations that can be used to compute hidden states of the model at various layers.\n     \"\"\"\n \n     last_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_activations: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n-class BaseModelOutputWithPoolingAndIntermediateActivations(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that also contains a pooling of the last hidden states as well as intermediate\n     activations that can be used by the model at later stages.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) after further processing\n-            through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n-            the classification token after processing through a linear layer and a tanh activation function. The linear\n-            layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        intermediate_activations (`tuple(torch.FloatTensor)`, *optional*):\n-            Intermediate activations that can be used to compute hidden states of the model at various layers.\n+    \"\"\"\n+)\n+class BaseModelOutputWithPoolingAndIntermediateActivations(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Last layer hidden-state of the first token of the sequence (classification token) after further processing\n+        through the layers used for the auxiliary pretraining task. E.g. for BERT-family of models, this returns\n+        the classification token after processing through a linear layer and a tanh activation function. The linear\n+        layer weights are trained from the next sentence prediction (classification) objective during pretraining.\n+    intermediate_activations (`tuple(torch.FloatTensor)`, *optional*):\n+        Intermediate activations that can be used to compute hidden states of the model at various layers.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "613dea9473b14296dec8fa5c910d69cf7b9f2b82",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 10,
            "deletions": 19,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -667,26 +667,17 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class ElectraForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`ElectraForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss of the ELECTRA objective.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Prediction scores of the head (scores for each token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class ElectraForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss of the ELECTRA objective.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+        Prediction scores of the head (scores for each token before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a74315ab4cccfe23348023c28001c86735af5fe8",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -38,39 +38,39 @@\n \n \n @dataclass\n+@auto_docstring\n class EncodecOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_codes (`torch.LongTensor`  of shape `(batch_size, nb_chunks, chunk_length)`, *optional*):\n-            Discret code embeddings computed using `model.encode`.\n-        audio_values (`torch.FlaotTensor` of shape `(batch_size, sequence_length)`, *optional*)\n-            Decoded audio values, obtained using the decoder part of Encodec.\n+    r\"\"\"\n+    audio_codes (`torch.LongTensor`  of shape `(batch_size, nb_chunks, chunk_length)`, *optional*):\n+        Discret code embeddings computed using `model.encode`.\n+    audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):\n+        Decoded audio values, obtained using the decoder part of Encodec.\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None\n     audio_values: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n+@auto_docstring\n class EncodecEncoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_codes (`torch.LongTensor`  of shape `(batch_size, nb_chunks, chunk_length)`, *optional*):\n-            Discret code embeddings computed using `model.encode`.\n-        audio_scales (`torch.Tensor` of shape `(batch_size, nb_chunks)`, *optional*):\n-            Scaling factor for each `audio_codes` input. This is used to unscale each chunk of audio when decoding.\n+    r\"\"\"\n+    audio_codes (`torch.LongTensor`  of shape `(batch_size, nb_chunks, chunk_length)`, *optional*):\n+        Discret code embeddings computed using `model.encode`.\n+    audio_scales (`torch.Tensor` of shape `(batch_size, nb_chunks)`, *optional*):\n+        Scaling factor for each `audio_codes` input. This is used to unscale each chunk of audio when decoding.\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None\n     audio_scales: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n+@auto_docstring\n class EncodecDecoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):\n-            Decoded audio values, obtained using the decoder part of Encodec.\n+    r\"\"\"\n+    audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):\n+        Decoded audio values, obtained using the decoder part of Encodec.\n     \"\"\"\n \n     audio_values: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "79898516126d3efcaadc8dad13015d4e11420587",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -647,31 +647,22 @@ def _init_weights(self, module):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`ErnieForPreTraining`].\n+    \"\"\"\n+)\n # Copied from transformers.models.bert.modeling_bert.BertForPreTrainingOutput with Bert->Ernie\n class ErnieForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`ErnieForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "81f4fcc4fece500d9eba93a94d2696edb3259cd5",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 54,
            "deletions": 52,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -53,59 +53,61 @@\n \n \n @dataclass\n-class EsmForProteinFoldingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`EsmForProteinFoldingOutput`].\n-\n-    Args:\n-        frames (`torch.FloatTensor`):\n-            Output frames.\n-        sidechain_frames (`torch.FloatTensor`):\n-            Output sidechain frames.\n-        unnormalized_angles (`torch.FloatTensor`):\n-            Predicted unnormalized backbone and side chain torsion angles.\n-        angles (`torch.FloatTensor`):\n-            Predicted backbone and side chain torsion angles.\n-        positions (`torch.FloatTensor`):\n-            Predicted positions of the backbone and side chain atoms.\n-        states (`torch.FloatTensor`):\n-            Hidden states from the protein folding trunk.\n-        s_s (`torch.FloatTensor`):\n-            Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.\n-        s_z (`torch.FloatTensor`):\n-            Pairwise residue embeddings.\n-        distogram_logits (`torch.FloatTensor`):\n-            Input logits to the distogram used to compute residue distances.\n-        lm_logits (`torch.FloatTensor`):\n-            Logits output by the ESM-2 protein language model stem.\n-        aatype (`torch.FloatTensor`):\n-            Input amino acids (AlphaFold2 indices).\n-        atom14_atom_exists (`torch.FloatTensor`):\n-            Whether each atom exists in the atom14 representation.\n-        residx_atom14_to_atom37 (`torch.FloatTensor`):\n-            Mapping between atoms in the atom14 and atom37 representations.\n-        residx_atom37_to_atom14 (`torch.FloatTensor`):\n-            Mapping between atoms in the atom37 and atom14 representations.\n-        atom37_atom_exists (`torch.FloatTensor`):\n-            Whether each atom exists in the atom37 representation.\n-        residue_index (`torch.FloatTensor`):\n-            The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be\n-            a sequence of integers from 0 to `sequence_length`.\n-        lddt_head (`torch.FloatTensor`):\n-            Raw outputs from the lddt head used to compute plddt.\n-        plddt (`torch.FloatTensor`):\n-            Per-residue confidence scores. Regions of low confidence may indicate areas where the model's prediction is\n-            uncertain, or where the protein structure is disordered.\n-        ptm_logits (`torch.FloatTensor`):\n-            Raw logits used for computing ptm.\n-        ptm (`torch.FloatTensor`):\n-            TM-score output representing the model's high-level confidence in the overall structure.\n-        aligned_confidence_probs (`torch.FloatTensor`):\n-            Per-residue confidence scores for the aligned structure.\n-        predicted_aligned_error (`torch.FloatTensor`):\n-            Predicted error between the model's prediction and the ground truth.\n-        max_predicted_aligned_error (`torch.FloatTensor`):\n-            Per-sample maximum predicted error.\n+    \"\"\"\n+)\n+class EsmForProteinFoldingOutput(ModelOutput):\n+    r\"\"\"\n+    frames (`torch.FloatTensor`):\n+        Output frames.\n+    sidechain_frames (`torch.FloatTensor`):\n+        Output sidechain frames.\n+    unnormalized_angles (`torch.FloatTensor`):\n+        Predicted unnormalized backbone and side chain torsion angles.\n+    angles (`torch.FloatTensor`):\n+        Predicted backbone and side chain torsion angles.\n+    positions (`torch.FloatTensor`):\n+        Predicted positions of the backbone and side chain atoms.\n+    states (`torch.FloatTensor`):\n+        Hidden states from the protein folding trunk.\n+    s_s (`torch.FloatTensor`):\n+        Per-residue embeddings derived by concatenating the hidden states of each layer of the ESM-2 LM stem.\n+    s_z (`torch.FloatTensor`):\n+        Pairwise residue embeddings.\n+    distogram_logits (`torch.FloatTensor`):\n+        Input logits to the distogram used to compute residue distances.\n+    lm_logits (`torch.FloatTensor`):\n+        Logits output by the ESM-2 protein language model stem.\n+    aatype (`torch.FloatTensor`):\n+        Input amino acids (AlphaFold2 indices).\n+    atom14_atom_exists (`torch.FloatTensor`):\n+        Whether each atom exists in the atom14 representation.\n+    residx_atom14_to_atom37 (`torch.FloatTensor`):\n+        Mapping between atoms in the atom14 and atom37 representations.\n+    residx_atom37_to_atom14 (`torch.FloatTensor`):\n+        Mapping between atoms in the atom37 and atom14 representations.\n+    atom37_atom_exists (`torch.FloatTensor`):\n+        Whether each atom exists in the atom37 representation.\n+    residue_index (`torch.FloatTensor`):\n+        The index of each residue in the protein chain. Unless internal padding tokens are used, this will just be\n+        a sequence of integers from 0 to `sequence_length`.\n+    lddt_head (`torch.FloatTensor`):\n+        Raw outputs from the lddt head used to compute plddt.\n+    plddt (`torch.FloatTensor`):\n+        Per-residue confidence scores. Regions of low confidence may indicate areas where the model's prediction is\n+        uncertain, or where the protein structure is disordered.\n+    ptm_logits (`torch.FloatTensor`):\n+        Raw logits used for computing ptm.\n+    ptm (`torch.FloatTensor`):\n+        TM-score output representing the model's high-level confidence in the overall structure.\n+    aligned_confidence_probs (`torch.FloatTensor`):\n+        Per-residue confidence scores for the aligned structure.\n+    predicted_aligned_error (`torch.FloatTensor`):\n+        Predicted error between the model's prediction and the ground truth.\n+    max_predicted_aligned_error (`torch.FloatTensor`):\n+        Per-sample maximum predicted error.\n     \"\"\"\n \n     frames: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "426e557d9d3cb203980182e88496f18999b1fa1d",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 25,
            "deletions": 33,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -492,24 +492,19 @@ def _init_weights(self, module):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class for the FALCONMAMBA model outputs.\n+    \"\"\"\n+)\n # Copied from transformers.models.mamba.modeling_mamba.MambaOutput with MAMBA->FALCONMAMBA,Mamba->FalconMamba,FalconMambaCache->MambaCache\n class FalconMambaOutput(ModelOutput):\n-    \"\"\"\n-    Class for the FALCONMAMBA model outputs.\n+    r\"\"\"\n+    cache_params (`MambaCache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        cache_params (`MambaCache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -518,26 +513,23 @@ class FalconMambaOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n # Copied from transformers.models.mamba.modeling_mamba.MambaCausalLMOutput with Mamba->FalconMamba,FalconMambaCache->MambaCache\n class FalconMambaCausalLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        cache_params (`MambaCache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    cache_params (`MambaCache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n+\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f19ea88c176d48dc61c1ec7d9546b639ec3b2c0d",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 30,
            "deletions": 79,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -35,46 +35,21 @@\n \n \n @dataclass\n-class FastSpeech2ConformerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`FastSpeech2ConformerModel`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Spectrogram generation loss.\n-        spectrogram (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_bins)`):\n-            The predicted spectrogram.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        duration_outputs (`torch.LongTensor` of shape `(batch_size, max_text_length + 1)`, *optional*):\n-            Outputs of the duration predictor.\n-        pitch_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n-            Outputs of the pitch predictor.\n-        energy_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n-            Outputs of the energy predictor.\n-\n+    \"\"\"\n+)\n+class FastSpeech2ConformerModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Spectrogram generation loss.\n+    duration_outputs (`torch.LongTensor` of shape `(batch_size, max_text_length + 1)`, *optional*):\n+        Outputs of the duration predictor.\n+    pitch_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n+        Outputs of the pitch predictor.\n+    energy_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n+        Outputs of the energy predictor.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -90,47 +65,23 @@ class FastSpeech2ConformerModelOutput(ModelOutput):\n \n \n @dataclass\n-class FastSpeech2ConformerWithHifiGanOutput(FastSpeech2ConformerModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`FastSpeech2ConformerWithHifiGan`].\n-\n-    Args:\n-        waveform (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n-            Speech output as a result of passing the predicted mel spectrogram through the vocoder.\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Spectrogram generation loss.\n-        spectrogram (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_bins)`):\n-            The predicted spectrogram.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        duration_outputs (`torch.LongTensor` of shape `(batch_size, max_text_length + 1)`, *optional*):\n-            Outputs of the duration predictor.\n-        pitch_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n-            Outputs of the pitch predictor.\n-        energy_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n-            Outputs of the energy predictor.\n+    \"\"\"\n+)\n+class FastSpeech2ConformerWithHifiGanOutput(FastSpeech2ConformerModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Spectrogram generation loss.\n+    duration_outputs (`torch.LongTensor` of shape `(batch_size, max_text_length + 1)`, *optional*):\n+        Outputs of the duration predictor.\n+    pitch_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n+        Outputs of the pitch predictor.\n+    energy_outputs (`torch.FloatTensor` of shape `(batch_size, max_text_length + 1, 1)`, *optional*):\n+        Outputs of the energy predictor.\n+    waveform (`torch.FloatTensor` of shape `(batch_size, audio_length)`):\n+        Speech output as a result of passing the predicted mel spectrogram through the vocoder.\n     \"\"\"\n \n     waveform: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f9a549d20559d69c541f3c7623d447e140084efa",
            "filename": "src/transformers/models/flaubert/modeling_flaubert.py",
            "status": "modified",
            "additions": 56,
            "deletions": 64,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_flaubert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -246,27 +246,28 @@ def forward(self, x, y=None):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for outputs of question answering models using a [`~modeling_utils.FlaubertSQuADHead`].\n+    \"\"\"\n+)\n # Copied from transformers.models.xlm.modeling_xlm.XLMSquadHeadOutput with XLM->Flaubert\n class FlaubertSquadHeadOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of question answering models using a [`~modeling_utils.FlaubertSQuADHead`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n-            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n-            losses.\n-        start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n-        start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top config.start_n_top start token possibilities (beam-search).\n-        end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n-            (beam-search).\n-        end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the `is_impossible` label of the answers.\n-\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n+        Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n+        losses.\n+    start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n+    start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Indices for the top config.start_n_top start token possibilities (beam-search).\n+    end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n+        (beam-search).\n+    end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n+    cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the `is_impossible` label of the answers.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -815,6 +816,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n+        langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            A parallel sequence of tokens to be used to indicate the language of each token in the input. Indices are\n+            languages ids which can be obtained from the language names by using two conversion mappings provided in\n+            the configuration of the model (only provided for multilingual models). More precisely, the *language name\n+            to language id* mapping is in `model.config.lang2id` (which is a dictionary string to int) and the\n+            *language id to language name* mapping is in `model.config.id2lang` (dictionary int to string).\n+\n+            See usage examples detailed in the [multilingual documentation](../multilingual).\n         lengths (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Length of each sentence that can be used to avoid performing attention on padding token indices. You can\n             also use `attention_mask` for the same result (see above), kept here for compatibility. Indices selected in\n@@ -824,14 +833,6 @@ def forward(\n             attention blocks) as computed by the model (see `cache` output below). Can be used to speed up sequential\n             decoding. The dictionary object will be modified in-place during the forward pass to add newly computed\n             hidden-states.\n-        langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            A parallel sequence of tokens to be used to indicate the language of each token in the input. Indices are\n-            languages ids which can be obtained from the language names by using two conversion mappings provided in\n-            the configuration of the model (only provided for multilingual models). More precisely, the *language name\n-            to language id* mapping is in `model.config.lang2id` (which is a dictionary string to int) and the\n-            *language id to language name* mapping is in `model.config.id2lang` (dictionary int to string).\n-\n-            See usage examples detailed in the [multilingual documentation](../multilingual).\n         \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1040,6 +1041,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, MaskedLMOutput]:\n         r\"\"\"\n+        langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            A parallel sequence of tokens to be used to indicate the language of each token in the input. Indices are\n+            languages ids which can be obtained from the language names by using two conversion mappings provided in\n+            the configuration of the model (only provided for multilingual models). More precisely, the *language name\n+            to language id* mapping is in `model.config.lang2id` (which is a dictionary string to int) and the\n+            *language id to language name* mapping is in `model.config.id2lang` (dictionary int to string).\n+\n+            See usage examples detailed in the [multilingual documentation](../multilingual).\n         lengths (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Length of each sentence that can be used to avoid performing attention on padding token indices. You can\n             also use `attention_mask` for the same result (see above), kept here for compatibility. Indices selected in\n@@ -1053,14 +1062,6 @@ def forward(\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n-        langs (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            A parallel sequence of tokens to be used to indicate the language of each token in the input. Indices are\n-            languages ids which can be obtained from the language names by using two conversion mappings provided in\n-            the configuration of the model (only provided for multilingual models). More precisely, the *language name\n-            to language id* mapping is in `model.config.lang2id` (which is a dictionary string to int) and the\n-            *language id to language name* mapping is in `model.config.id2lang` (dictionary int to string).\n-\n-            See usage examples detailed in the [multilingual documentation](../multilingual).\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1413,37 +1414,28 @@ def forward(\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for outputs of question answering models using a `SquadHead`.\n+    \"\"\"\n+)\n # Copied from transformer.models.xlm.modeling_xlm.XLMForQuestionAnsweringOutput with XLM->Flaubert\n class FlaubertForQuestionAnsweringOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of question answering models using a `SquadHead`.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n-            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n-            losses.\n-        start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n-        start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top config.start_n_top start token possibilities (beam-search).\n-        end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n-            (beam-search).\n-        end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the `is_impossible` label of the answers.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n+        Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n+        losses.\n+    start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n+    start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Indices for the top config.start_n_top start token possibilities (beam-search).\n+    end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n+        (beam-search).\n+    end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n+    cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n+        Log probabilities for the `is_impossible` label of the answers.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a1a30f369a101de89cc478bba52f5ffe2c3d74aa",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 100,
            "deletions": 95,
            "changes": 195,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,27 +49,29 @@\n \n \n @dataclass\n-class FlavaModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output from FlavaModel containing embeddings and outputs from individual encoders.\n \n     Note that `image_embeddings` and `text_embeddigns` returned are similar to pooled output returned from a\n     transformer. If you want embeddings for contrastive loss or retrieval use a FLAVA model's `image_projection` and\n     `text_projection` layers on `image_embeddings` and `text_embeddings` respectively.\n-\n-    Args:\n-        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n-            The image embeddings which are basically the pooled output of [`FlavaImageModel`].\n-        image_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n-            The output of the [`FlavaImageModel`].\n-        text_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` are present):\n-            The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        text_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids` are present):\n-            The output of the [`FlavaTextModel`].\n-        multimodal_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present and `skip_multimodal_encoder` is `None` or `False`):\n-            The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        multimodal_output (`BaseModelOutputWithPooling`, returned when `input_ids` and `pixel_values` are present and `skip_multimodal_encoder` is `None` or `False`):\n-            The output of the [`FlavaMultimodalModel`].\n+    \"\"\"\n+)\n+class FlavaModelOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n+        The image embeddings which are basically the pooled output of [`FlavaImageModel`].\n+    image_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n+        The output of the [`FlavaImageModel`].\n+    text_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` are present):\n+        The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    text_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids` are present):\n+        The output of the [`FlavaTextModel`].\n+    multimodal_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present and `skip_multimodal_encoder` is `None` or `False`):\n+        The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    multimodal_output (`BaseModelOutputWithPooling`, returned when `input_ids` and `pixel_values` are present and `skip_multimodal_encoder` is `None` or `False`):\n+        The output of the [`FlavaMultimodalModel`].\n     \"\"\"\n \n     image_embeddings: Optional[torch.FloatTensor] = None\n@@ -87,24 +89,27 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class representing pretraining losses from FLAVA model\n+    \"\"\"\n+)\n class FlavaLosses(ModelOutput):\n-    \"\"\"Class representing pretraining losses from FLAVA model\n-\n-    Args:\n-        mim (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mim_labels` and `pixel_values` are present, `input_ids_masked` is absent and `mim_weight` > 0.:\n-            Masked Image Modeling loss as used in BeIT calculated only for unimodal image data.\n-        mlm (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mlm_labels` and `input_ids_masked` are present, `pixel_values` is absent and `mlm_weight` > 0.:\n-            Masked Language Modeling loss as used in BERT calculated only for unimodal text data.\n-        itm (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `itm_labels`, `input_ids_masked`, `pixel_values` are present and `itm_weight` > 0.:\n-            Image Text Matching (ITM) loss calculated for paired image-text data. Note that ITM loss is calculated on\n-            masked pairs in FLAVA.\n-        global_contrastive (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `input_ids` and `pixel_values` are present and `global_contrastive_weight` > 0.:\n-            Contrastive loss for image-text similarity similar to CLIP but calculated globally for paired image-text\n-            data. This is calculated on unmasked images and texts.\n-        mmm_image (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mim_labels`, `pixel_values` and `input_ids_masked` are present and `mmm_image_weight` > 0.:\n-            Masked Multimodal Modeling loss's image component calculated on paired image-text data.\n-        mmm_text (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mlm_labels`, `pixel_values` and `input_ids_masked` are present and `mmm_text_weight` > 0.:\n-            Masked Multimodal Modeling loss's text component calculated on paired image-text data.\n+    r\"\"\"\n+    mim (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mim_labels` and `pixel_values` are present, `input_ids_masked` is absent and `mim_weight` > 0.):\n+        Masked Image Modeling loss as used in BeIT calculated only for unimodal image data.\n+    mlm (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mlm_labels` and `input_ids_masked` are present, `pixel_values` is absent and `mlm_weight` > 0.):\n+        Masked Language Modeling loss as used in BERT calculated only for unimodal text data.\n+    itm (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `itm_labels`, `input_ids_masked`, `pixel_values` are present and `itm_weight` > 0.):\n+        Image Text Matching (ITM) loss calculated for paired image-text data. Note that ITM loss is calculated on\n+        masked pairs in FLAVA.\n+    global_contrastive (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `input_ids` and `pixel_values` are present and `global_contrastive_weight` > 0.):\n+        Contrastive loss for image-text similarity similar to CLIP but calculated globally for paired image-text\n+        data. This is calculated on unmasked images and texts.\n+    mmm_image (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mim_labels`, `pixel_values` and `input_ids_masked` are present and `mmm_image_weight` > 0.):\n+        Masked Multimodal Modeling loss's image component calculated on paired image-text data.\n+    mmm_text (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mlm_labels`, `pixel_values` and `input_ids_masked` are present and `mmm_text_weight` > 0.):\n+        Masked Multimodal Modeling loss's text component calculated on paired image-text data.\n     \"\"\"\n \n     mim: Optional[torch.FloatTensor] = None\n@@ -124,69 +129,69 @@ def all_none(self) -> bool:\n \n \n @dataclass\n-class FlavaForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output from FlavaForPreTraining containing embeddings, and outputs from individual encoders.\n \n     Note that `image_embeddings` and `text_embeddings` returned are similar to pooled output returned from a\n     transformer. If you want embeddings for contrastive loss or retrieval use a FLAVA model's `image_projection` and\n     `text_projection` layers on `image_embeddings` and `text_embeddings` respectively.\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `return_loss` is True):\n-            Total loss calculated for this model.\n-        loss_info (`FlavaLosses`):\n-            Detailed info for FLAVA Pretraining losses. Check `FlavaLosses` class description for the information on\n-            the keys.\n-        image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n-            The image embeddings which are basically the pooled output of [`FlavaImageModel`].\n-        image_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n-            The output of the [`FlavaImageModel`].\n-        text_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` are present):\n-            The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        text_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids` are present):\n-            The output of the [`FlavaTextModel`].\n-        multimodal_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present and `skip_unmasked_multimodal_encoder` is `None` or `False`):\n-            The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        multimodal_output (`BaseModelOutputWithPooling`, returned when `input_ids` and `pixel_values` are present and `skip_unmasked_multimodal_encoder` is `None` or `False`):\n-            The output of the [`FlavaMultimodalModel`].\n-\n-        image_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n-            The image embeddings which are basically the pooled output of [`FlavaImageModel`]. Uses `bool_masked_pos`\n-            to create masked images.\n-        image_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n-            The output of the [`FlavaImageModel`]. Uses `bool_masked_pos` to create masked images.\n-        text_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids_masked` are present):\n-            The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        text_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids_masked` are present):\n-            The output of the [`FlavaTextModel`].\n-        multimodal_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present):\n-            The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n-        multimodal_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids_masked` and `pixel_values` are present):\n-            The output of the [`FlavaMultimodalModel`].\n-\n-        mim_logits (`torch.FloatTensor` of shape `(batch_size, num_image_patches, image_vocab_size)` or of shape `(total_masked_patches, image_vocab_size)` , *optional*, returned when `pixel_values` are present and `input_ids_masked` are not):\n-                The logits for MIM unimodal loss. Uses `book_masked_pos` to get masked patches. The flattened output is\n-                returned when `bool_masked_pos` has some of the patches masked.\n-        mlm_logits (`torch.FloatTensor` of shape `(batch_size, text_seq_length, text_vocab_size)` or of shape `(total_masked_seq_length, text_vocab_size)`, *optional*, returned when `input_ids_masked` are present and `pixel_values` are not):\n-                The logits for MLM unimodal loss. The flattened output is returned when `input_ids_masked` has some of\n-                the tokens masked.\n-        itm_logits (`torch.FloatTensor` of shape `(batch_size, 2)`, *optional*, returned when `input_ids_masked` and `pixel_values` are present):\n-                The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.\n-        mmm_image_logits (`torch.FloatTensor` of shape `(batch_size, num_image_patches, image_vocab_size)` or of shape`(total_masked_patches, image_vocab_size)`, *optional*, returned when `pixel_values` and `input_ids_masked` are present):\n-                The logits for MMM image multimodal loss. Uses `book_masked_pos` to get masked patches. The flattened\n-                output is returned when `bool_masked_pos` has some of the patches masked.\n-        mmm_text_logits (`torch.FloatTensor` of shape `(batch_size, text_seq_length, text_vocab_size)` or of shape `(`(total_masked_seq_length, text_vocab_size)`), *optional*, returned when `pixel_values` and `input_ids_masked` are present):\n-                The logits for MMM text multimodal loss. The flattened output is returned when `input_ids_masked` has\n-                some of the tokens masked.\n-        contrastive_logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeddings` and `text_embeddings` but passed through FLAVA's\n-            `image_projection` and `text_projection` layers respectively. This represents the image-text similarity\n-            scores. This is calculated on unmasked images and texts.\n-        contrastive_logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeddings` and `image_embeddings` but passed through FLAVA's\n-            `text_projection` and `image_projection` layers respectively. This is calculated on unmasked images and\n-            texts.\n+    \"\"\"\n+)\n+class FlavaForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*, returned when `return_loss` is True):\n+        Total loss calculated for this model.\n+    loss_info (`FlavaLosses`):\n+        Detailed info for FLAVA Pretraining losses. Check `FlavaLosses` class description for the information on\n+        the keys.\n+    image_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n+        The image embeddings which are basically the pooled output of [`FlavaImageModel`].\n+    image_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n+        The output of the [`FlavaImageModel`].\n+    text_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` are present):\n+        The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    text_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids` are present):\n+        The output of the [`FlavaTextModel`].\n+    multimodal_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present and `skip_unmasked_multimodal_encoder` is `None` or `False`):\n+        The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    multimodal_output (`BaseModelOutputWithPooling`, returned when `input_ids` and `pixel_values` are present and `skip_unmasked_multimodal_encoder` is `None` or `False`):\n+        The output of the [`FlavaMultimodalModel`].\n+    image_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `pixel_values` are present):\n+        The image embeddings which are basically the pooled output of [`FlavaImageModel`]. Uses `bool_masked_pos`\n+        to create masked images.\n+    image_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `pixel_values` are present):\n+        The output of the [`FlavaImageModel`]. Uses `bool_masked_pos` to create masked images.\n+    text_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids_masked` are present):\n+        The text embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    text_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids_masked` are present):\n+        The output of the [`FlavaTextModel`].\n+    multimodal_masked_embeddings (`torch.FloatTensor` of shape `(batch_size, output_dim)`, *optional*, returned when `input_ids` and `pixel_values` are present):\n+        The multimodal embeddings which are basically the pooled output of [`FlavaTextModel`].\n+    multimodal_masked_output (`BaseModelOutputWithPooling`, *optional*, returned when `input_ids_masked` and `pixel_values` are present):\n+        The output of the [`FlavaMultimodalModel`].\n+    mim_logits (`torch.FloatTensor` of shape `(batch_size, num_image_patches, image_vocab_size)` or of shape `(total_masked_patches, image_vocab_size)` , *optional*, returned when `pixel_values` are present and `input_ids_masked` are not):\n+        The logits for MIM unimodal loss. Uses `book_masked_pos` to get masked patches. The flattened output is\n+            returned when `bool_masked_pos` has some of the patches masked.\n+    mlm_logits (`torch.FloatTensor` of shape `(batch_size, text_seq_length, text_vocab_size)` or of shape `(total_masked_seq_length, text_vocab_size)`, *optional*, returned when `input_ids_masked` are present and `pixel_values` are not):\n+        The logits for MLM unimodal loss. The flattened output is returned when `input_ids_masked` has some of\n+            the tokens masked.\n+    itm_logits (`torch.FloatTensor` of shape `(batch_size, 2)`, *optional*, returned when `input_ids_masked` and `pixel_values` are present):\n+        The logits for ITM loss. Note that ITM loss is calculated on masked pairs in FLAVA.\n+    contrastive_logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeddings` and `text_embeddings` but passed through FLAVA's\n+        `image_projection` and `text_projection` layers respectively. This represents the image-text similarity\n+        scores. This is calculated on unmasked images and texts.\n+    contrastive_logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeddings` and `image_embeddings` but passed through FLAVA's\n+        `text_projection` and `image_projection` layers respectively. This is calculated on unmasked images and\n+        texts.\n+    mmm_image_logits (`torch.FloatTensor` of shape `(batch_size, num_image_patches, image_vocab_size)` or of shape`(total_masked_patches, image_vocab_size)`, *optional*, returned when `pixel_values` and `input_ids_masked` are present):\n+        The logits for MMM image multimodal loss. Uses `book_masked_pos` to get masked patches. The flattened\n+            output is returned when `bool_masked_pos` has some of the patches masked.\n+    mmm_text_logits (`torch.FloatTensor` of shape `(batch_size, text_seq_length, text_vocab_size)` or of shape `(`(total_masked_seq_length, text_vocab_size)`), *optional*, returned when `pixel_values` and `input_ids_masked` are present):\n+        The logits for MMM text multimodal loss. The flattened output is returned when `input_ids_masked` has\n+            some of the tokens masked.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1207,12 +1212,12 @@ def forward(\n             [What are token type IDs?](../glossary#token-type-ids)\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, image_num_patches)`):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n-        skip_multimodal_encoder (*bool*, *optional*):\n-            Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.\n         image_attention_mask (`torch.Tensor` of shape `(batch_size, image_num_patches)`, *optional*):\n             Mask to avoid performing attention on padding pixel values for image inputs. Mask values selected in `[0, 1]`:\n             - 1 for pixel values that are real (i.e., **not masked**),\n             - 0 for pixel values that are padding (i.e., **masked**).\n+        skip_multimodal_encoder (*bool*, *optional*):\n+            Skip any calculations for multimodal encoder. Useful if multimodal encoding is not going to be used.\n \n         Examples:\n \n@@ -1681,6 +1686,8 @@ def forward(\n             to be used with MLM. Indices can be obtained using [`AutoTokenizer`] along with\n             [`DataCollatorForMaskedLanguageModeling`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details. [What are input IDs?](../glossary#input-ids)\n+        codebook_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_image_patches, patch_size, patch_size, 3)`, *optional*):\n+            Pixel values for image patches that are used to compute the image codebook labels for masked image modeling.\n         token_type_ids (`torch.LongTensor` of shape `(batch_size, text_seq_len)`, *optional*):\n             Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n             1]`:\n@@ -1714,8 +1721,6 @@ def forward(\n             The pairs with 0 will be skipped for calculation of MMM and global contrastive losses as well.\n         return_loss (`bool`, *optional*, default to None):\n             Whether to return calculated loss or not.\n-        codebook_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_image_patches, patch_size, patch_size, 3)`, *optional*):\n-            Pixel values for image patches that are used to compute the image codebook labels for masked image modeling.\n \n         Examples:\n         ```python"
        },
        {
            "sha": "e35031ebc3b20a1181b9df480d6bcef201ad42d3",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -409,23 +409,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class FNetForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`FNetForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n+    \"\"\"\n+)\n+class FNetForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "091424fb41f3c8c7113d4fd7410fc86b365e436b",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 57,
            "deletions": 74,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -37,25 +37,19 @@\n \n \n @dataclass\n-class FocalNetEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     FocalNet encoder's outputs, with potential hidden states.\n+    \"\"\"\n+)\n+class FocalNetEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -64,26 +58,21 @@ class FocalNetEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class FocalNetModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     FocalNet model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n-            Average pooling of the last layer hidden-state.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class FocalNetModelOutput(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n+        Average pooling of the last layer hidden-state.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -93,26 +82,23 @@ class FocalNetModelOutput(ModelOutput):\n \n \n @dataclass\n-class FocalNetMaskedImageModelingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     FocalNet masked image model outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided):\n-            Masked image modeling (MLM) loss.\n-        reconstruction (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Reconstructed pixel values.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class FocalNetMaskedImageModelingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `bool_masked_pos` is provided):\n+        Masked image modeling (MLM) loss.\n+    reconstruction (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Reconstructed pixel values.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -122,26 +108,23 @@ class FocalNetMaskedImageModelingOutput(ModelOutput):\n \n \n @dataclass\n-class FocalNetImageClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     FocalNet outputs for image classification.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, hidden_size, height, width)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class FocalNetImageClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, hidden_size, height, width)`.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "9f17bf08a0a87917dd9d23cb4510a5ddc217eb5d",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 10,
            "deletions": 19,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -804,26 +804,17 @@ def forward(self, hidden: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n-class FunnelForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`FunnelForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss of the ELECTRA-style objective.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Prediction scores of the head (scores for each token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class FunnelForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss of the ELECTRA-style objective.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+        Prediction scores of the head (scores for each token before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "ed826b20d8fd815515546ee7587c901203e958c3",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -290,12 +290,12 @@ def forward(\n         image_patches (`torch.FloatTensor` of shape `(batch_size, num_total_patches, patch_size_ x patch_size x num_channels)`, *optional*):\n             Image patches to be used as continuous embeddings. The patches are flattened and then projected to the\n             hidden size of the model.\n+        image_patches_indices (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Tensor of indices of the image patches in the input_ids tensor.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n-        image_patches_indices (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Tensor of indices of the image patches in the input_ids tensor.\n \n         Examples:\n "
        },
        {
            "sha": "db15678c25c9d4b958f3ce6077b36c87fbcc12c6",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -48,68 +48,48 @@\n \n \n @dataclass\n-class Gemma3ModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Gemma3 outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class Gemma3ModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class Gemma3CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Gemma3 causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n+    \"\"\"\n+)\n+class Gemma3CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "1069edf1e4cc07580c8bd02800c4c9596b3ab525",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -15,7 +15,6 @@\n # limitations under the License.\n import copy\n from collections.abc import Callable\n-from dataclasses import dataclass\n from typing import Any, Optional, Union\n \n import torch\n@@ -346,12 +345,10 @@ def __init__(\n         super().__init__(**kwargs)\n \n \n-@dataclass\n class Gemma3ModelOutputWithPast(PaligemmaModelOutputWithPast):\n     pass\n \n \n-@dataclass\n class Gemma3CausalLMOutputWithPast(PaligemmaCausalLMOutputWithPast):\n     pass\n "
        },
        {
            "sha": "a116ecb55170193c92192b4642672d7ea6c431b2",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 8,
            "deletions": 19,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,27 +49,16 @@\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n+    \"\"\"\n+)\n # Copied from transformers.models.clip.modeling_clip.CLIPVisionModelOutput with CLIP->Git\n class GitVisionModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "1b5cd63e7fd76decb0d636715792c5452a1228ac",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 42,
            "deletions": 73,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -289,27 +289,16 @@ def forward(\n \n \n @dataclass\n-class GotOcr2VisionEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for got_ocr2 vision model's outputs that also contains image embeddings obtained by applying the projection\n     layer to the pooler_output.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class GotOcr2VisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -505,35 +494,26 @@ def forward(self, vision_embeddings: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n-class GotOcr2CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for GotOcr2 causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class GotOcr2CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -545,33 +525,22 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for GotOcr2 outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "b5569dc989062eed87141ed1556d5493ca02b7c7",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 29,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -597,36 +597,27 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class GPT2DoubleHeadsModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of models predicting if two sentences are consecutive or not.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        mc_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels` is provided):\n-            Multiple choice classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n-            Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n-        past_key_values (`tuple[tuple[torch.Tensor]]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of length `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,\n-            sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            GPT2Attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n+    \"\"\"\n+)\n+class GPT2DoubleHeadsModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    mc_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels` is provided):\n+        Multiple choice classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n+        Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n+    past_key_values (`tuple[tuple[torch.Tensor]]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of length `config.n_layers`, containing tuples of tensors of shape `(batch_size, num_heads,\n+        sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f0779cb03324e1e0272d8d405d19b8de35ed8626",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 18,
            "deletions": 27,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -33,32 +33,23 @@\n \n \n @dataclass\n-class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for LlavaNext causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -382,12 +373,12 @@ def forward(\n             The tensors corresponding to the input audios. input features can be obtained using\n             [`AutoFeatureExtractor`]. See [`GraniteSpeechFeatureExtractor.__call__`] for details.\n             [`GraniteSpeechProcessor`] uses [`GraniteSpeechFeatureExtractor`] for processing audio.\n+        input_features_mask (`torch.Tensor`, *optional*):\n+            Mask to be applied to audio features prior to scattering into the language embeddings.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n             config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-        input_features_mask (`torch.Tensor`, *optional*):\n-            Mask to be applied to audio features prior to scattering into the language embeddings.\n         \"\"\"\n         # TODO (@alex-jw-brooks) add an example to this docstring once models are released\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions"
        },
        {
            "sha": "31ccb4becd157e80d098c03aca37b6a1a69666e2",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 125,
            "deletions": 153,
            "changes": 278,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -102,28 +102,20 @@ def forward(\n \n \n @dataclass\n-class GroundingDinoDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the GroundingDinoDecoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    \"\"\"\n+)\n+class GroundingDinoDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -134,30 +126,27 @@ class GroundingDinoDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class GroundingDinoEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the GroundingDinoEncoder. This class extends BaseModelOutput, due to:\n     - vision and text last hidden states\n     - vision and text intermediate hidden states\n-\n-    Args:\n-        last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the vision encoder.\n-        last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the text encoder.\n-        vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n-            layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n-            output of each layer plus the initial embedding outputs.\n-        text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n-            of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n-            each layer plus the initial embedding outputs.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the text-vision attention, vision-text attention, text-enhancer (self-attention) and\n-            multi-scale deformable attention heads.\n+    \"\"\"\n+)\n+class GroundingDinoEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the vision encoder.\n+    last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the text encoder.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n+        layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n+        output of each layer plus the initial embedding outputs.\n+    text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n+        of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n+        each layer plus the initial embedding outputs.\n     \"\"\"\n \n     last_hidden_state_vision: Optional[torch.FloatTensor] = None\n@@ -168,55 +157,49 @@ class GroundingDinoEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class GroundingDinoModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Grounding DINO encoder-decoder model.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n-        encoder_last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n-            layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n-            output of each layer plus the initial embedding outputs.\n-        encoder_text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n-            of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n-            each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the text-vision attention, vision-text attention, text-enhancer (self-attention) and\n-            multi-scale deformable attention heads. attention softmax, used to compute the weighted average in the\n-            bi-attention heads.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.num_queries` scoring bounding boxes are picked as\n-            region proposals in the first stage. Output of bounding box binary classification (i.e. foreground and\n-            background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n-            Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n-        encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n-            Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n+    \"\"\"\n+)\n+class GroundingDinoModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    encoder_last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+    encoder_last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+    encoder_vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n+        layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n+        output of each layer plus the initial embedding outputs.\n+    encoder_text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n+        of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n+        each layer plus the initial embedding outputs.\n+    encoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+        sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+        weighted average in the text-vision attention, vision-text attention, text-enhancer (self-attention) and\n+        multi-scale deformable attention heads. attention softmax, used to compute the weighted average in the\n+        bi-attention heads.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.num_queries` scoring bounding boxes are picked as\n+        region proposals in the first stage. Output of bounding box binary classification (i.e. foreground and\n+        background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+        Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n+    encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+        Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -237,73 +220,62 @@ class GroundingDinoModelOutput(ModelOutput):\n \n \n @dataclass\n-class GroundingDinoObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`GroundingDinoForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~GroundingDinoProcessor.post_process_grounded_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n-        encoder_last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n-            layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n-            output of each layer plus the initial embedding outputs.\n-        encoder_text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n-            of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n-            each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the text-vision attention, vision-text attention, text-enhancer (self-attention) and\n-            multi-scale deformable attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.num_queries` scoring bounding boxes are picked as\n-            region proposals in the first stage. Output of bounding box binary classification (i.e. foreground and\n-            background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n-            Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n-        encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n-            Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Encoded candidate labels sequence. Used in processor to post process object detection result.\n+    \"\"\"\n+)\n+class GroundingDinoObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~GroundingDinoProcessor.post_process_grounded_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    encoder_last_hidden_state_vision (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+    encoder_last_hidden_state_text (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n+    encoder_vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the vision embeddings + one for the output of each\n+        layer) of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the vision encoder at the\n+        output of each layer plus the initial embedding outputs.\n+    encoder_text_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the text embeddings + one for the output of each layer)\n+        of shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the text encoder at the output of\n+        each layer plus the initial embedding outputs.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.num_queries` scoring bounding boxes are picked as\n+        region proposals in the first stage. Output of bounding box binary classification (i.e. foreground and\n+        background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+        Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n+    encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+        Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n+    input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        Encoded candidate labels sequence. Used in processor to post process object detection result.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a33e455753b06513ae8d486dc490daed481e3b50",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 30,
            "deletions": 31,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -259,38 +259,37 @@ def forward(self, image_tokens, group_tokens):\n \n \n @dataclass\n+@auto_docstring\n class GroupViTModelOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n-            Classification scores for each pixel.\n-\n-            <Tip warning={true}>\n-\n-            The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n-            to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n-            original image size as post-processing. You should always check your logits shape and resize as needed.\n-\n-            </Tip>\n-\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of\n-            [`GroupViTTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of\n-            [`GroupViTVisionModel`].\n-        text_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`GroupViTTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`GroupViTVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    segmentation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels, logits_height, logits_width)`):\n+        Classification scores for each pixel.\n+\n+        <Tip warning={true}>\n+\n+        The logits returned do not necessarily have the same size as the `pixel_values` passed as inputs. This is\n+        to avoid doing two interpolations and lose some quality when a user needs to resize the logits to the\n+        original image size as post-processing. You should always check your logits shape and resize as needed.\n+\n+        </Tip>\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of\n+        [`GroupViTTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of\n+        [`GroupViTVisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`GroupViTTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`GroupViTVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "2efa7c2f37c0091ffdabe06d878abfba6c4e8cdf",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 74,
            "deletions": 100,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -42,30 +42,19 @@\n \n \n @dataclass\n-class HieraEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Hiera encoder's outputs, with potential hidden states and attentions.\n+    \"\"\"\n+)\n+class HieraEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Thesre are the unrolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -75,36 +64,25 @@ class HieraEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class HieraModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Hiera model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n-            Average pooling of the last layer hidden-state.\n-        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n-            Tensor indicating which patches are masked (0) and which are not (1).\n-        ids_restore (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Tensor containing the original index of the (shuffled) masked patches.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. These are the unrolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class HieraModelOutput(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*, returned when `add_pooling_layer=True` is passed):\n+        Average pooling of the last layer hidden-state.\n+    bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+        Tensor indicating which patches are masked (0) and which are not (1).\n+    ids_restore (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+        Tensor containing the original index of the (shuffled) masked patches.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -117,32 +95,34 @@ class HieraModelOutput(ModelOutput):\n \n \n @dataclass\n-class HieraForImageClassificationOutput(ImageClassifierOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Hiera image classification outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, `optional`):\n-            Loss value for the training task.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n-            Prediction scores of the classification head (logits of the output layer).\n-        hidden_states (`tuple(torch.FloatTensor)`, `optional`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. These are the unrolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, `optional`):\n-            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, `optional`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n-            include the spatial dimensions.\n+    \"\"\"\n+)\n+class HieraForImageClassificationOutput(ImageClassifierOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, `optional`):\n+        Loss value for the training task.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n+        Prediction scores of the classification head (logits of the output layer).\n+    hidden_states (`tuple(torch.FloatTensor)`, `optional`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. These are the unrolled hidden states of the model.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+    attentions (`tuple(torch.FloatTensor)`, `optional`):\n+        Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, `optional`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, height, width, hidden_size)`. These are the reshaped and re-rolled hidden states of the model.\n+\n+        Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+        include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -153,31 +133,25 @@ class HieraForImageClassificationOutput(ImageClassifierOutput):\n \n \n @dataclass\n-class HieraForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for HieraForPreTraining's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`):\n-            Pixel reconstruction loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, patch_size ** 2 * num_channels)`):\n-            Pixel reconstruction logits.\n-        bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n-            Tensor indicating which patches are masked (0) and which are not (1).\n-        ids_restore (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Tensor containing the original index of the (shuffled) masked patches.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, height, width, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs reshaped to include the spatial dimensions.\n+    \"\"\"\n+)\n+class HieraForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`):\n+        Pixel reconstruction loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, patch_size ** 2 * num_channels)`):\n+        Pixel reconstruction logits.\n+    bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, sequence_length)`):\n+        Tensor indicating which patches are masked (0) and which are not (1).\n+    ids_restore (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+        Tensor containing the original index of the (shuffled) masked patches.\n+    reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, height, width, hidden_size)`. Hidden-states of the model at the output of each layer\n+        plus the initial embedding outputs reshaped to include the spatial dimensions.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "957960ef80763ebfccbfb7424a5311fa1161c094",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 50,
            "deletions": 68,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -52,41 +52,32 @@\n \n \n @dataclass\n-class IdeficsBaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Idefics model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+)\n+class IdeficsBaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -97,37 +88,28 @@ class IdeficsBaseModelOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class IdeficsCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Idefics causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+)\n+class IdeficsCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1445,16 +1427,16 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[tuple, IdeficsCausalLMOutputWithPast]:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         image_encoder_embeddings (`torch.FloatTensor`, *optional*):\n             The output of the image encoder.\n         perceiver_embeddings (`torch.FloatTensor`, *optional*):\n             The output of the perceiver resampler.\n         image_attention_mask (`torch.LongTensor`, *optional*):\n             The attention mask for the image encoder.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "792d5fe3f464c99d5568a8f217d1d67e3e167961",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 41,
            "deletions": 53,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -39,35 +39,29 @@\n \n \n @dataclass\n-class Idefics2BaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Idefics2 model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+)\n+class Idefics2BaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -78,33 +72,27 @@ class Idefics2BaseModelOutputWithPast(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for Idefics2 causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n # Copied from transformers.models.idefics.modeling_idefics.IdeficsCausalLMOutputWithPast with Idefics->Idefics2\n class Idefics2CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for Idefics2 causal language model (or autoregressive) outputs.\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "bb57db4222946412a1a77fce4ac96e29f7549070",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 41,
            "deletions": 54,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -39,35 +39,29 @@\n \n \n @dataclass\n-class Idefics3BaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Idefics3 model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder\n+    \"\"\"\n+)\n+class Idefics3BaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -78,33 +72,26 @@ class Idefics3BaseModelOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class Idefics3CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Idefics causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder\n+    \"\"\"\n+)\n+class Idefics3CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "fa0649309939086a5d2343695f474d7d4657075d",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -44,22 +44,24 @@\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class defining the outputs of [`InstructBlipForConditionalGeneration`].\n+    \"\"\"\n+)\n # Copied from transformers.models.blip_2.modeling_blip_2.Blip2ForConditionalGenerationModelOutput with Blip2->InstructBlip\n class InstructBlipForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n-    Class defining the outputs of [`InstructBlipForConditionalGeneration`].\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Language modeling loss from the language model.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head of the language model.\n-        vision_outputs (`BaseModelOutputWithPooling`):\n-            Outputs of the vision encoder.\n-        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            Outputs of the Q-Former (Querying Transformer).\n-        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n-            Outputs of the language model.\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Language modeling loss from the language model.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head of the language model.\n+    vision_outputs (`BaseModelOutputWithPooling`):\n+        Outputs of the vision encoder.\n+    qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        Outputs of the Q-Former (Querying Transformer).\n+    language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n+        Outputs of the language model.\n     \"\"\"\n \n     loss: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "ea3d19bd3f54fcb7ea0344e9a49ca77a3c6ddf0b",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 16,
            "deletions": 14,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1147,21 +1147,23 @@ def forward(\n \n \n @dataclass\n-class InstructBlipVideoForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class defining the outputs of [`InstructBlipVideoForConditionalGeneration`].\n-\n-    Args:\n-        loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Language modeling loss from the language model.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head of the language model.\n-        vision_outputs (`BaseModelOutputWithPooling`):\n-            Outputs of the vision encoder.\n-        qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n-            Outputs of the Q-Former (Querying Transformer).\n-        language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n-            Outputs of the language model.\n+    \"\"\"\n+)\n+class InstructBlipVideoForConditionalGenerationModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Language modeling loss from the language model.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head of the language model.\n+    vision_outputs (`BaseModelOutputWithPooling`):\n+        Outputs of the vision encoder.\n+    qformer_outputs (`BaseModelOutputWithPoolingAndCrossAttentions`):\n+        Outputs of the Q-Former (Querying Transformer).\n+    language_model_outputs (`CausalLMOutputWithPast` or `Seq2SeqLMOutput`):\n+        Outputs of the language model.\n     \"\"\"\n \n     loss: Optional[tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "e2e6496ed6dd43f1f618ff2ce4dc2da5be44da76",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from dataclasses import dataclass\n from typing import Optional, Union\n \n import torch\n@@ -189,7 +188,6 @@ class InstructBlipVideoQFormerModel(InstructBlipQFormerModel):\n     pass\n \n \n-@dataclass\n class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForConditionalGenerationModelOutput):\n     pass\n "
        },
        {
            "sha": "e634a281c5c778d8ed21d800a861c8e128af4632",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 44,
            "deletions": 75,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -209,28 +209,17 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`InternVLVisionModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n-            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n-            will be returned.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+        *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+        will be returned.\n     \"\"\"\n \n \n@@ -569,33 +558,22 @@ def forward(self, image_features):\n \n \n @dataclass\n-class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for InternVL outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -805,35 +783,26 @@ def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5\n \n \n @dataclass\n-class InternVLCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for InternVL causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class InternVLCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "17fac99080501de45f851acd5360e7a4bedc0d62",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 21,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -171,28 +171,17 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`InternVLVisionModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n-            *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n-            will be returned.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class InternVLVisionModelOutputWithPooling(BaseModelOutputWithPooling):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Average of the last layer hidden states of the patch tokens (excluding the *[CLS]* token) if\n+        *config.use_mean_pooling* is set to True. If set to False, then the final hidden state of the *[CLS]* token\n+        will be returned.\n     \"\"\"\n \n "
        },
        {
            "sha": "abdfea032f825b315daedff1ace9e4579ae50e9b",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 56,
            "deletions": 71,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -87,56 +87,50 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class JanusVQVAEOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Janus VQ-VAE mode model outputs.\n-    Args:\n-        decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n-            Reconstructed pixel values after encoding and decoding the input.\n-        embedding_loss (`torch.FloatTensor`):\n-            Embedding loss.\n+    \"\"\"\n+)\n+class JanusVQVAEOutput(ModelOutput):\n+    r\"\"\"\n+    decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+        Reconstructed pixel values after encoding and decoding the input.\n+    embedding_loss (`torch.FloatTensor`):\n+        Embedding loss.\n     \"\"\"\n \n     decoded_pixel_values: Optional[torch.FloatTensor] = None\n     embedding_loss: torch.FloatTensor = None\n \n \n @dataclass\n-class JanusBaseModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Janus model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+)\n+class JanusBaseModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -147,37 +141,28 @@ class JanusBaseModelOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class JanusCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Janus causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n-            sequence_length, hidden_size)`.\n-\n-            image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n+    \"\"\"\n+)\n+class JanusCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the image embeddings, `(batch_size, num_images,\n+        sequence_length, hidden_size)`.\n+\n+        image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "d485074df39273039640dfbcaba7b01de77ba42c",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 10,
            "deletions": 9,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -408,26 +408,27 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class JanusVQVAEOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Janus VQ-VAE mode model outputs.\n-    Args:\n-        decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n-            Reconstructed pixel values after encoding and decoding the input.\n-        embedding_loss (`torch.FloatTensor`):\n-            Embedding loss.\n+    \"\"\"\n+)\n+class JanusVQVAEOutput(ModelOutput):\n+    r\"\"\"\n+    decoded_pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n+        Reconstructed pixel values after encoding and decoding the input.\n+    embedding_loss (`torch.FloatTensor`):\n+        Embedding loss.\n     \"\"\"\n \n     decoded_pixel_values: Optional[torch.FloatTensor] = None\n     embedding_loss: torch.FloatTensor = None\n \n \n-@dataclass\n class JanusBaseModelOutputWithPast(IdeficsBaseModelOutputWithPast):\n     pass\n \n \n-@dataclass\n class JanusCausalLMOutputWithPast(IdeficsCausalLMOutputWithPast):\n     pass\n "
        },
        {
            "sha": "0926d17b318cb382b2c64d3e4039e71f62eb985f",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 60,
            "deletions": 80,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -90,43 +90,32 @@ def create_position_ids_from_input_ids(input_ids, padding_idx, past_key_values_l\n \n \n @dataclass\n-class Kosmos2ModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for text model's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n-        projection_attentions (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights given by `Kosmos2ImageToTextProjection`, after the attention softmax, used to compute\n-            the weighted average in the self-attention heads.\n-        vision_model_output(`BaseModelOutputWithPooling`, *optional*):\n-            The output of the [`Kosmos2VisionModel`].\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n+    \"\"\"\n+)\n+class Kosmos2ModelOutput(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n+    projection_attentions (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights given by `Kosmos2ImageToTextProjection`, after the attention softmax, used to compute\n+        the weighted average in the self-attention heads.\n+    vision_model_output (`BaseModelOutputWithPooling`, *optional*):\n+        The output of the [`Kosmos2VisionModel`].\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -145,45 +134,36 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n-class Kosmos2ForConditionalGenerationModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Model output class for `Kosmos2ForConditionalGeneration`.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n-        projection_attentions (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights given by `Kosmos2ImageToTextProjection`, after the attention softmax, used to compute\n-            the weighted average in the self-attention heads.\n-        vision_model_output(`BaseModelOutputWithPooling`, *optional*):\n-            The output of the [`Kosmos2VisionModel`].\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n-            `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n-            encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n-            `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n-            input) to speed up sequential decoding.\n+    \"\"\"\n+)\n+class Kosmos2ForConditionalGenerationModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n+        `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n+        encoder_sequence_length, embed_size_per_head)`.\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks and optionally if\n+        `config.is_encoder_decoder=True` in the cross-attention blocks) that can be used (see `past_key_values`\n+        input) to speed up sequential decoding.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n+    projection_attentions (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights given by `Kosmos2ImageToTextProjection`, after the attention softmax, used to compute\n+        the weighted average in the self-attention heads.\n+    vision_model_output (`BaseModelOutputWithPooling`, *optional*):\n+        The output of the [`Kosmos2VisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1332,6 +1312,8 @@ def forward(\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n         image_embeds_position_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to indicate the location in a sequence to insert the image features . Mask values selected in `[0,\n             1]`:\n@@ -1343,8 +1325,6 @@ def forward(\n \n             - 1 indicates the head is **not masked**,\n             - 0 indicates the head is **masked**.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n         \"\"\"\n         return self.model(\n             input_ids=input_ids,\n@@ -1423,6 +1403,8 @@ def forward(\n         **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[tuple, CausalLMOutputWithCrossAttentions]:\n         r\"\"\"\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n         image_embeds_position_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to indicate the location in a sequence to insert the image features . Mask values selected in `[0,\n             1]`:\n@@ -1438,8 +1420,6 @@ def forward(\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -1794,12 +1774,12 @@ def forward(\n \n             - 1 for places where to put the image features,\n             - 0 for places that are not for image features (i.e. for text tokens).\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n+            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, latent_query_num, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of `Kosmos2ImageToTextProjection`.\n \n         Examples:\n "
        },
        {
            "sha": "4055d9148570ed50fc8f47a938aca1a498b18f98",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 119,
            "deletions": 240,
            "changes": 359,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1132,41 +1132,36 @@ def dummy_inputs(self):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for LEDEncoder's outputs, with potential hidden states, local and global attentions.\n+    \"\"\"\n+)\n # Copied from transformers.models.longformer.modeling_longformer.LongformerBaseModelOutput with Longformer->LEDEncoder\n class LEDEncoderBaseModelOutput(ModelOutput):\n-    \"\"\"\n-    Base class for LEDEncoder's outputs, with potential hidden states, local and global attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    r\"\"\"\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -1176,60 +1171,32 @@ class LEDEncoderBaseModelOutput(ModelOutput):\n \n \n @dataclass\n-class LEDSeq2SeqModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n     decoding.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LEDSeq2SeqModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -1244,58 +1211,30 @@ class LEDSeq2SeqModelOutput(ModelOutput):\n \n \n @dataclass\n-class LEDSeq2SeqLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for sequence-to-sequence language models outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LEDSeq2SeqLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1311,58 +1250,30 @@ class LEDSeq2SeqLMOutput(ModelOutput):\n \n \n @dataclass\n-class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of sequence-to-sequence sentence classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `label` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1378,60 +1289,28 @@ class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n \n \n @dataclass\n-class LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of sequence-to-sequence question answering models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    encoder_global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "623c8e2278a2cc9f5d72dfd8920f3043ea2bcf39",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -38,23 +38,21 @@\n \n \n @dataclass\n-class LevitForImageClassificationWithTeacherOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`LevitForImageClassificationWithTeacher`].\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores as the average of the `cls_logits` and `distillation_logits`.\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the\n-            class token).\n-        distillation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the\n-            distillation token).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n+    \"\"\"\n+)\n+class LevitForImageClassificationWithTeacherOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores as the average of the `cls_logits` and `distillation_logits`.\n+    cls_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores of the classification head (i.e. the linear layer on top of the final hidden state of the\n+        class token).\n+    distillation_logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Prediction scores of the distillation head (i.e. the linear layer on top of the final hidden state of the\n+        distillation token).\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "3f1c59836d0a96871b3aff0b397c8f953a3b24a1",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 27,
            "deletions": 25,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -36,36 +36,38 @@\n \n \n @dataclass\n-class LightGlueKeypointMatchingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of LightGlue keypoint matching models. Due to the nature of keypoint detection and matching,\n     the number of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the\n     batch of images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask\n     tensor is used to indicate which values in the keypoints, matches, matching_scores and prune tensors are keypoint\n     matching information.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n-            Loss computed during training.\n-        matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n-            Index of keypoint matched in the other image.\n-        matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n-            Scores of predicted matches.\n-        keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n-            Absolute (x, y) coordinates of predicted keypoints in a given image.\n-        prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n-            Pruning mask indicating which keypoints are removed and at which layer.\n-        mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n-            Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n-            information.\n-        hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n-            num_keypoints)` returned when `output_hidden_states=True` is passed or when\n-            `config.output_hidden_states=True`\n-        attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n-            num_keypoints)` returned when `output_attentions=True` is passed or when\n-            `config.output_attentions=True`\n+    \"\"\"\n+)\n+class LightGlueKeypointMatchingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+        Loss computed during training.\n+    matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Index of keypoint matched in the other image.\n+    matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Scores of predicted matches.\n+    keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+        Absolute (x, y) coordinates of predicted keypoints in a given image.\n+    prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n+        Pruning mask indicating which keypoints are removed and at which layer.\n+    mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n+        Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n+        information.\n+    hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+        num_keypoints)` returned when `output_hidden_states=True` is passed or when\n+        `config.output_hidden_states=True`\n+    attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+        num_keypoints)` returned when `output_attentions=True` is passed or when\n+        `config.output_attentions=True`\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "fbe6037fbf570bd1038fcca98eb474611420fed5",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 27,
            "deletions": 28,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -37,9 +37,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC_ = \"LightGlueConfig\"\n-_CHECKPOINT_FOR_DOC_ = \"ETH-CVG/lightglue_superpoint\"\n-\n \n class LightGlueConfig(PretrainedConfig):\n     r\"\"\"\n@@ -158,36 +155,38 @@ def __init__(\n \n \n @dataclass\n-class LightGlueKeypointMatchingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of LightGlue keypoint matching models. Due to the nature of keypoint detection and matching,\n     the number of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the\n     batch of images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask\n     tensor is used to indicate which values in the keypoints, matches, matching_scores and prune tensors are keypoint\n     matching information.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n-            Loss computed during training.\n-        matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n-            Index of keypoint matched in the other image.\n-        matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n-            Scores of predicted matches.\n-        keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n-            Absolute (x, y) coordinates of predicted keypoints in a given image.\n-        prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n-            Pruning mask indicating which keypoints are removed and at which layer.\n-        mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n-            Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n-            information.\n-        hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n-            num_keypoints)` returned when `output_hidden_states=True` is passed or when\n-            `config.output_hidden_states=True`\n-        attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n-            num_keypoints)` returned when `output_attentions=True` is passed or when\n-            `config.output_attentions=True`\n+    \"\"\"\n+)\n+class LightGlueKeypointMatchingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+        Loss computed during training.\n+    matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Index of keypoint matched in the other image.\n+    matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n+        Scores of predicted matches.\n+    keypoints (`torch.FloatTensor` of shape `(batch_size, num_keypoints, 2)`):\n+        Absolute (x, y) coordinates of predicted keypoints in a given image.\n+    prune (`torch.IntTensor` of shape `(batch_size, num_keypoints)`):\n+        Pruning mask indicating which keypoints are removed and at which layer.\n+    mask (`torch.BoolTensor` of shape `(batch_size, num_keypoints)`):\n+        Mask indicating which values in matches, matching_scores, keypoints and prune are keypoint matching\n+        information.\n+    hidden_states (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of each stage) of shape `(batch_size, 2, num_channels,\n+        num_keypoints)` returned when `output_hidden_states=True` is passed or when\n+        `config.output_hidden_states=True`\n+    attentions (`Tuple[torch.FloatTensor, ...]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, 2, num_heads, num_keypoints,\n+        num_keypoints)` returned when `output_attentions=True` is passed or when\n+        `config.output_attentions=True`\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "53d2c7fc9b2836733f03a748cb8d889c45a98bc4",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -719,35 +719,26 @@ def forward(\n \n \n @dataclass\n-class Llama4CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class Llama4CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "a72965382710e075504cbe85a3325201590234d7",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -36,68 +36,48 @@\n \n \n @dataclass\n-class LlavaModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class LlavaCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "08dfb36d47ecb71b1f4e2150697f6092b7de2551",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -145,68 +145,48 @@ def unpad_image(tensor, original_size):\n \n \n @dataclass\n-class LlavaNextModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaNextModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class LlavaNextCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for LlavaNext causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaNextCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "5a01c3502104afb8043112f0c4274690d931c357",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 40,
            "deletions": 62,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -43,37 +43,25 @@\n \n \n @dataclass\n-class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-\n-        video_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n-            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -82,39 +70,29 @@ class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n \n \n @dataclass\n-class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for LlavaNextVideo causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-\n-        video_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n-            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "badaa23a71bd7ed2dc921f72eee6f44519704ebe",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 24,
            "deletions": 5,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n import math\n-from dataclasses import dataclass\n from typing import Optional, Union\n \n import torch\n@@ -182,9 +181,17 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n-@dataclass\n class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):\n-    \"\"\"\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     video_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n         video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n@@ -193,9 +200,21 @@ class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-@dataclass\n class LlavaNextVideoCausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n-    \"\"\"\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     video_hidden_states (`torch.FloatTensor`, *optional*):\n         A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n         video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state."
        },
        {
            "sha": "a06a9750baae9d65349610fa67171c9d3234526c",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 40,
            "deletions": 62,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -49,37 +49,25 @@\n \n \n @dataclass\n-class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-\n-        video_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n-            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n@@ -88,39 +76,29 @@ class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n \n \n @dataclass\n-class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for LlavaOnevision causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-\n-        video_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n-            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "98390cc6f819e7ca7f1db0124eb8821f08145797",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 220,
            "deletions": 249,
            "changes": 469,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -35,40 +35,35 @@\n \n \n @dataclass\n-class LongformerBaseModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Longformer's outputs, with potential hidden states, local and global attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerBaseModelOutput(ModelOutput):\n+    r\"\"\"\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -78,44 +73,39 @@ class LongformerBaseModelOutput(ModelOutput):\n \n \n @dataclass\n-class LongformerBaseModelOutputWithPooling(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Longformer's outputs that also contains a pooling of the last hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n-            Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n-            prediction (classification) objective during pretraining.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerBaseModelOutputWithPooling(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n+        Linear layer and a Tanh activation function. The Linear layer weights are trained from the next sentence\n+        prediction (classification) objective during pretraining.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -126,42 +116,39 @@ class LongformerBaseModelOutputWithPooling(ModelOutput):\n \n \n @dataclass\n-class LongformerMaskedLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for masked language models outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Masked language modeling (MLM) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerMaskedLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Masked language modeling (MLM) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -172,44 +159,37 @@ class LongformerMaskedLMOutput(ModelOutput):\n \n \n @dataclass\n-class LongformerQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of question answering Longformer models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerQuestionAnsweringModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -221,42 +201,39 @@ class LongformerQuestionAnsweringModelOutput(ModelOutput):\n \n \n @dataclass\n-class LongformerSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of sentence classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerSequenceClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -267,44 +244,41 @@ class LongformerSequenceClassifierOutput(ModelOutput):\n \n \n @dataclass\n-class LongformerMultipleChoiceModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of multiple choice Longformer models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n-            *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n-\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerMultipleChoiceModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n+        *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n+\n+        Classification scores (before SoftMax).\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -315,42 +289,39 @@ class LongformerMultipleChoiceModelOutput(ModelOutput):\n \n \n @dataclass\n-class LongformerTokenClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of token classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n-            attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n-\n-            Local attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token in the sequence to every token with\n-            global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n-            + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n-            remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n-            token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n-            (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n-            If the attention window contains a token with global attention, the attention weight at the corresponding\n-            index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n-            attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n-            accessed from `global_attentions`.\n-        global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n-            where `x` is the number of tokens with global attention mask.\n-\n-            Global attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads. Those are the attention weights from every token with global attention to every token\n-            in the sequence.\n+    \"\"\"\n+)\n+class LongformerTokenClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Classification scores (before SoftMax).\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x +\n+        attention_window + 1)`, where `x` is the number of tokens with global attention mask.\n+\n+        Local attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token in the sequence to every token with\n+        global attention (first `x` values) and to every token in the attention window (remaining `attention_window\n+        + 1` values). Note that the first `x` values refer to tokens with fixed positions in the text, but the\n+        remaining `attention_window + 1` values refer to tokens with relative positions: the attention weight of a\n+        token to itself is located at index `x + attention_window / 2` and the `attention_window / 2` preceding\n+        (succeeding) values are the attention weights to the `attention_window / 2` preceding (succeeding) tokens.\n+        If the attention window contains a token with global attention, the attention weight at the corresponding\n+        index is set to 0; the value should be accessed from the first `x` attention weights. If a token has global\n+        attention, the attention weights to all other tokens in `attentions` is set to 0, the values should be\n+        accessed from `global_attentions`.\n+    global_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length, x)`,\n+        where `x` is the number of tokens with global attention mask.\n+\n+        Global attentions weights after the attention softmax, used to compute the weighted average in the\n+        self-attention heads. Those are the attention weights from every token with global attention to every token\n+        in the sequence.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "65d998625df5a157d24cc7af6da4fce10097c89b",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 145,
            "deletions": 231,
            "changes": 376,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -36,98 +36,70 @@\n \n \n @dataclass\n-class BaseLukeModelOutputWithPooling(BaseModelOutputWithPooling):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the LUKE model.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        entity_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, entity_length, hidden_size)`):\n-            Sequence of entity hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n-            Linear layer and a Tanh activation function.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length +\n-            entity_length, sequence_length + entity_length)`. Attentions weights after the attention softmax, used to\n-            compute the weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class BaseLukeModelOutputWithPooling(BaseModelOutputWithPooling):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Last layer hidden-state of the first token of the sequence (classification token) further processed by a\n+        Linear layer and a Tanh activation function.\n+    entity_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, entity_length, hidden_size)`):\n+        Sequence of entity hidden-states at the output of the last layer of the model.\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     entity_last_hidden_state: Optional[torch.FloatTensor] = None\n     entity_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n-class BaseLukeModelOutput(BaseModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        entity_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, entity_length, hidden_size)`):\n-            Sequence of entity hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class BaseLukeModelOutput(BaseModelOutput):\n+    r\"\"\"\n+    entity_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, entity_length, hidden_size)`):\n+        Sequence of entity hidden-states at the output of the last layer of the model.\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     entity_last_hidden_state: Optional[torch.FloatTensor] = None\n     entity_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n @dataclass\n-class LukeMaskedLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs, with potential hidden states and attentions.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            The sum of masked language modeling (MLM) loss and entity prediction loss.\n-        mlm_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Masked language modeling (MLM) loss.\n-        mep_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Masked entity prediction (MEP) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        entity_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the entity prediction head (scores for each entity vocabulary token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class LukeMaskedLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        The sum of masked language modeling (MLM) loss and entity prediction loss.\n+    mlm_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Masked language modeling (MLM) loss.\n+    mep_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Masked entity prediction (MEP) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    entity_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the entity prediction head (scores for each entity vocabulary token before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -141,27 +113,21 @@ class LukeMaskedLMOutput(ModelOutput):\n \n \n @dataclass\n-class EntityClassificationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of entity classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class EntityClassificationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -172,27 +138,21 @@ class EntityClassificationOutput(ModelOutput):\n \n \n @dataclass\n-class EntityPairClassificationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of entity pair classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class EntityPairClassificationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -203,27 +163,21 @@ class EntityPairClassificationOutput(ModelOutput):\n \n \n @dataclass\n-class EntitySpanClassificationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of entity span classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, entity_length, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class EntitySpanClassificationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, entity_length, config.num_labels)`):\n+        Classification scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -234,30 +188,21 @@ class EntitySpanClassificationOutput(ModelOutput):\n \n \n @dataclass\n-class LukeSequenceClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of sentence classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class LukeSequenceClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -268,30 +213,21 @@ class LukeSequenceClassifierOutput(ModelOutput):\n \n \n @dataclass\n-class LukeTokenClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of token classification models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided) :\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class LukeTokenClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Classification scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -302,32 +238,19 @@ class LukeTokenClassifierOutput(ModelOutput):\n \n \n @dataclass\n-class LukeQuestionAnsweringModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of question answering models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n-        start_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-start scores (before SoftMax).\n-        end_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            Span-end scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class LukeQuestionAnsweringModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Total span extraction loss is the sum of a Cross-Entropy for the start and end positions.\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -339,32 +262,23 @@ class LukeQuestionAnsweringModelOutput(ModelOutput):\n \n \n @dataclass\n-class LukeMultipleChoiceModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of multiple choice models.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided):\n-            Classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n-            *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n-\n-            Classification scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n-            layer plus the initial entity embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class LukeMultipleChoiceModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided):\n+        Classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n+        *num_choices* is the second dimension of the input tensors. (see *input_ids* above).\n+\n+        Classification scores (before SoftMax).\n+    entity_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, entity_length, hidden_size)`. Entity hidden-states of the model at the output of each\n+        layer plus the initial entity embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "bc8a84d17e01e3d6a131929ee09f5dada5c32cbb",
            "filename": "src/transformers/models/lxmert/modeling_lxmert.py",
            "status": "modified",
            "additions": 94,
            "deletions": 90,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Fmodeling_lxmert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -42,39 +42,40 @@ def forward(self, x):\n \n \n @dataclass\n-class LxmertModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Lxmert's outputs that contain the last hidden states, pooled outputs, and attention probabilities for the language,\n     visual, and, cross-modality encoders. (note: the visual encoder in Lxmert is referred to as the \"relation-ship\"\n     encoder\")\n-\n-\n-    Args:\n-        language_output (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the language encoder.\n-        vision_output (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the visual encoder.\n-        pooled_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state of the first token of the sequence (classification, CLS, token) further processed\n-            by a Linear layer and a Tanh activation function. The Linear\n-        language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class LxmertModelOutput(ModelOutput):\n+    r\"\"\"\n+    language_output (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the language encoder.\n+    vision_output (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the visual encoder.\n+    pooled_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Last layer hidden-state of the first token of the sequence (classification, CLS, token) further processed\n+        by a Linear layer and a Tanh activation function. The Linear\n+    language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n     \"\"\"\n \n     language_output: Optional[torch.FloatTensor] = None\n@@ -88,34 +89,36 @@ class LxmertModelOutput(ModelOutput):\n \n \n @dataclass\n-class LxmertForQuestionAnsweringOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`LxmertForQuestionAnswering`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.k.\n-        question_answering_score (`torch.FloatTensor` of shape `(batch_size, n_qa_answers)`, *optional*):\n-            Prediction scores of question answering objective (classification).\n-        language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n+    \"\"\"\n+)\n+class LxmertForQuestionAnsweringOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.k.\n+    question_answering_score (`torch.FloatTensor` of shape `(batch_size, n_qa_answers)`, *optional*):\n+        Prediction scores of question answering objective (classification).\n+    language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -128,40 +131,41 @@ class LxmertForQuestionAnsweringOutput(ModelOutput):\n \n \n @dataclass\n-class LxmertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`LxmertForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        cross_relationship_score (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the textual matching objective (classification) head (scores of True/False\n-            continuation before SoftMax).\n-        question_answering_score (`torch.FloatTensor` of shape `(batch_size, n_qa_answers)`):\n-            Prediction scores of question answering objective (classification).\n-        language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-        language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-\n+    \"\"\"\n+)\n+class LxmertForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    cross_relationship_score (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the textual matching objective (classification) head (scores of True/False\n+        continuation before SoftMax).\n+    question_answering_score (`torch.FloatTensor` of shape `(batch_size, n_qa_answers)`):\n+        Prediction scores of question answering objective (classification).\n+    language_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for input features + one for the output of each cross-modality layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+    language_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n+    cross_encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f2347833db6c45b598cb93ec605db592238851f8",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 25,
            "deletions": 33,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -429,23 +429,18 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class MambaOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for the MAMBA model outputs.\n+    \"\"\"\n+)\n+class MambaOutput(ModelOutput):\n+    r\"\"\"\n+    cache_params (`MambaCache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n \n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        cache_params (`MambaCache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -454,25 +449,22 @@ class MambaOutput(ModelOutput):\n \n \n @dataclass\n-class MambaCausalLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        cache_params (`MambaCache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    \"\"\"\n+)\n+class MambaCausalLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    cache_params (`MambaCache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n+\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "1f663462d5e17a772593e2c2023ee00a60b2143f",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 33,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -763,24 +763,19 @@ def _init_weights(self, module):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class for the MAMBA2 model outputs.\n+    \"\"\"\n+)\n # Copied from transformers.models.mamba.modeling_mamba.MambaOutput with MAMBA->MAMBA2,Mamba->Mamba2\n class Mamba2Output(ModelOutput):\n-    \"\"\"\n-    Class for the MAMBA2 model outputs.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        cache_params (`Mamba2Cache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+    r\"\"\"\n+    cache_params (`Mamba2Cache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n \n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -789,26 +784,23 @@ class Mamba2Output(ModelOutput):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n # Copied from transformers.models.mamba.modeling_mamba.MambaCausalLMOutput with Mamba->Mamba2\n class Mamba2CausalLMOutput(ModelOutput):\n-    \"\"\"\n-    Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        cache_params (`Mamba2Cache`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-\n-            Includes both the State space model state matrices after the selective scan, and the Convolutional states\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    cache_params (`Mamba2Cache`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n+\n+        Includes both the State space model state matrices after the selective scan, and the Convolutional states\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "e7f8fc41100a9ac84655c883710b118c778bca56",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 123,
            "deletions": 115,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -44,22 +44,24 @@\n \n \n @dataclass\n-class Mask2FormerPixelDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Mask2Former's pixel decoder module output, practically a Multi-Scale Deformable Attention based decoder. It returns\n     the mask features and the multiscale features.\n-\n-    Args:\n-        multi_scale_features (`tuple(torch.FloatTensor)`):\n-            Tuple of multi-scale features of scales [1/8, 1/16, 1/32] and shape `(batch_size, num_channels, height,\n-            width)`from the Multi-Scale Deformable Attenntion based Pixel Decoder.\n-        mask_features (`torch.FloatTensor`):\n-            Tensor of shape `(batch_size, num_channels, height, width)`, 1/4 scale features from the last Pixel Decoder\n-            Layer.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights from pixel decoder. Returned when `output_attentions=True` is passed\n-            or when `config.output_attentions=True`\n+    \"\"\"\n+)\n+class Mask2FormerPixelDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    multi_scale_features (`tuple(torch.FloatTensor)`):\n+        Tuple of multi-scale features of scales [1/8, 1/16, 1/32] and shape `(batch_size, num_channels, height,\n+        width)`from the Multi-Scale Deformable Attenntion based Pixel Decoder.\n+    mask_features (`torch.FloatTensor`):\n+        Tensor of shape `(batch_size, num_channels, height, width)`, 1/4 scale features from the last Pixel Decoder\n+        Layer.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights from pixel decoder. Returned when `output_attentions=True` is passed\n+        or when `config.output_attentions=True`\n     \"\"\"\n \n     multi_scale_features: tuple[torch.FloatTensor] = None\n@@ -68,28 +70,28 @@ class Mask2FormerPixelDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Transformer decoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions for mask predictions logits and a tuple of intermediate decoder activations,\n     i.e. the output of each decoder layer, each of them gone through a layernorm.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs. Returned when `output_hidden_states=True`.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads. Returned when `output_attentions=True`.\n-        masks_queries_logits (`tuple(torch.FloatTensor)` of shape `(batch_size, num_queries, height, width)`):\n-            Tuple of mask predictions from all layers of the transformer decoder.\n-        intermediate_hidden_states (`tuple(torch.FloatTensor)` of shape `(num_queries, 1, hidden_size)`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n+    \"\"\"\n+)\n+class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    r\"\"\"\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+        plus the initial embedding outputs. Returned when `output_hidden_states=True`.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n+        the self-attention heads. Returned when `output_attentions=True`.\n+    masks_queries_logits (`tuple(torch.FloatTensor)` of shape `(batch_size, num_queries, height, width)`):\n+        Tuple of mask predictions from all layers of the transformer decoder.\n+    intermediate_hidden_states (`tuple(torch.FloatTensor)` of shape `(num_queries, 1, hidden_size)`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -100,28 +102,30 @@ class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions\n \n \n @dataclass\n-class Mask2FormerPixelLevelModuleOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Mask2Former's pixel level module output. It returns the output of the encoder (optional) and all hidden states\n     (multi-scale features) from the `decoder`. By default, the `encoder` is a Swin Backbone and the `decoder` is a\n     Multi-Scale Deformable Attention based decoder.\n \n     The `decoder_last_hidden_state` are the **per-pixel embeddings** while `decoder_hidden_states` refer to multi-scale\n     feature maps produced using **multi-scaling strategy** defined in the paper.\n-\n-    Args:\n-        encoder_last_hidden_state (`torch.FloatTensor`):\n-            Last hidden states (final feature map of shape `(batch_size, num_channels, height, width)`) of the last\n-            stage of the encoder.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also\n-            called feature maps) of the model at the output of each stage. Returned if output_hidden_states is set to\n-            True.\n-        decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)):\n-            1/4 scale features from the last Pixel Decoder Layer.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`):\n-            Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also\n-            called feature maps) of the model at the output of each stage.\n+    \"\"\"\n+)\n+class Mask2FormerPixelLevelModuleOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_last_hidden_state (`torch.FloatTensor`):\n+        Last hidden states (final feature map of shape `(batch_size, num_channels, height, width)`) of the last\n+        stage of the encoder.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also\n+        called feature maps) of the model at the output of each stage. Returned if output_hidden_states is set to\n+        True.\n+    decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)):\n+        1/4 scale features from the last Pixel Decoder Layer.\n+    decoder_hidden_states (`tuple(torch.FloatTensor)`):\n+        Tuple of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden states (also\n+        called feature maps) of the model at the output of each stage.\n     \"\"\"\n \n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -131,38 +135,40 @@ class Mask2FormerPixelLevelModuleOutput(ModelOutput):\n \n \n @dataclass\n-class Mask2FormerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`Mask2FormerModel`]. This class returns all the needed hidden states to compute the logits.\n-\n-    Args:\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`, *optional*):\n-            Last hidden states (final feature map) of the last stage of the encoder model (backbone). Returned when\n-            `output_hidden_states=True` is passed.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage. Returned when `output_hidden_states=True` is passed.\n-        pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`, *optional*):\n-            Last hidden states (final feature map) of the last stage of the pixel decoder model.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, , *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage. Returned when `output_hidden_states=True` is passed.\n-        transformer_decoder_last_hidden_state (`tuple(torch.FloatTensor)`):\n-            Final output of the transformer decoder `(batch_size, sequence_length, hidden_size)`.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n-            transformer decoder at the output of each stage. Returned when `output_hidden_states=True` is passed.\n-        transformer_decoder_intermediate_states (`tuple(torch.FloatTensor)` of shape `(num_queries, 1, hidden_size)`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n-        masks_queries_logits (`tuple(torch.FloatTensor)` of shape `(batch_size, num_queries, height, width)`)\n-            Mask Predictions from each layer in the transformer decoder.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed):\n-            Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Self attentions weights from transformer decoder.\n+    \"\"\"\n+)\n+class Mask2FormerModelOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`, *optional*):\n+        Last hidden states (final feature map) of the last stage of the encoder model (backbone). Returned when\n+        `output_hidden_states=True` is passed.\n+    pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`, *optional*):\n+        Last hidden states (final feature map) of the last stage of the pixel decoder model.\n+    transformer_decoder_last_hidden_state (`tuple(torch.FloatTensor)`):\n+        Final output of the transformer decoder `(batch_size, sequence_length, hidden_size)`.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage. Returned when `output_hidden_states=True` is passed.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, , *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage. Returned when `output_hidden_states=True` is passed.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n+        transformer decoder at the output of each stage. Returned when `output_hidden_states=True` is passed.\n+    transformer_decoder_intermediate_states (`tuple(torch.FloatTensor)` of shape `(num_queries, 1, hidden_size)`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n+    masks_queries_logits (`tuple(torch.FloatTensor)` of shape `(batch_size, num_queries, height, width)`)\n+        Mask Predictions from each layer in the transformer decoder.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self attentions weights from transformer decoder.\n     \"\"\"\n \n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -177,47 +183,49 @@ class Mask2FormerModelOutput(ModelOutput):\n \n \n @dataclass\n-class Mask2FormerForUniversalSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`Mask2FormerForUniversalSegmentationOutput`].\n \n     This output can be directly passed to [`~Mask2FormerImageProcessor.post_process_semantic_segmentation`] or\n     [`~Mask2FormerImageProcessor.post_process_instance_segmentation`] or\n     [`~Mask2FormerImageProcessor.post_process_panoptic_segmentation`] to compute final segmentation maps. Please, see\n     [`~Mask2FormerImageProcessor] for details regarding usage.\n-\n-    Args:\n-        loss (`torch.Tensor`, *optional*):\n-            The computed loss, returned when labels are present.\n-        class_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n-            query. Note the `+ 1` is needed because we incorporate the null class.\n-        masks_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n-            query.\n-        auxiliary_logits (`list[Dict(str, torch.FloatTensor)]`, *optional*):\n-            List of class and mask predictions from each layer of the transformer decoder.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage.\n-        pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the pixel decoder model.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage.\n-        transformer_decoder_last_hidden_state (`tuple(torch.FloatTensor)`):\n-            Final output of the transformer decoder `(batch_size, sequence_length, hidden_size)`.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n-            transformer decoder at the output of each stage.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    \"\"\"\n+)\n+class Mask2FormerForUniversalSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.Tensor`, *optional*):\n+        The computed loss, returned when labels are present.\n+    class_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n+        query. Note the `+ 1` is needed because we incorporate the null class.\n+    masks_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n+        query.\n+    auxiliary_logits (`list[Dict(str, torch.FloatTensor)]`, *optional*):\n+        List of class and mask predictions from each layer of the transformer decoder.\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n+    pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the pixel decoder model.\n+    transformer_decoder_last_hidden_state (`tuple(torch.FloatTensor)`):\n+        Final output of the transformer decoder `(batch_size, sequence_length, hidden_size)`.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n+        transformer decoder at the output of each stage.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8f1febefc8e9c52c8e075382b0181639a9863e1e",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 104,
            "deletions": 118,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -53,59 +53,53 @@\n \n \n @dataclass\n-# Copied from transformers.models.detr.modeling_detr.DetrDecoderOutput\n-class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the DETR decoder. This class adds one attribute to BaseModelOutputWithCrossAttentions,\n     namely an optional stack of intermediate decoder activations, i.e. the output of each decoder layer, each of them\n     gone through a layernorm. This is useful when training the model with auxiliary decoding losses.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n-            Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n-            layernorm.\n+    \"\"\"\n+)\n+# Copied from transformers.models.detr.modeling_detr.DetrDecoderOutput\n+class DetrDecoderOutput(BaseModelOutputWithCrossAttentions):\n+    r\"\"\"\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(config.decoder_layers, batch_size, num_queries, hidden_size)`, *optional*, returned when `config.auxiliary_loss=True`):\n+        Intermediate decoder activations, i.e. the output of each decoder layer, each of them gone through a\n+        layernorm.\n     \"\"\"\n \n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class MaskFormerPixelLevelModuleOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     MaskFormer's pixel level module output. It returns both the last and (optionally) the hidden states from the\n     `encoder` and `decoder`. By default, the `encoder` is a MaskFormerSwin Transformer and the `decoder` is a Feature\n     Pyramid Network (FPN).\n \n     The `encoder_last_hidden_state` are referred on the paper as **images features**, while `decoder_last_hidden_state`\n     as **pixel embeddings**\n-\n-    Args:\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape`(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the encoder.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the model at\n-            the output of each stage.\n-        decoder_last_hidden_state (`torch.FloatTensor` of shape`(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the decoder.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the model at\n-            the output of each stage.\n+    \"\"\"\n+)\n+class MaskFormerPixelLevelModuleOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape`(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the encoder.\n+    decoder_last_hidden_state (`torch.FloatTensor` of shape`(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the decoder.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the model at\n+        the output of each stage.\n+    decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the model at\n+        the output of each stage.\n     \"\"\"\n \n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -115,22 +109,16 @@ class MaskFormerPixelLevelModuleOutput(ModelOutput):\n \n \n @dataclass\n-class MaskFormerPixelDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     MaskFormer's pixel decoder module output, practically a Feature Pyramid Network. It returns the last hidden state\n     and (optionally) the hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights from Detr's decoder after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class MaskFormerPixelDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the model.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -139,36 +127,34 @@ class MaskFormerPixelDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class MaskFormerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`MaskFormerModel`]. This class returns all the needed hidden states to compute the logits.\n-\n-    Args:\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n-        pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the pixel decoder model (FPN).\n-        transformer_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Last hidden states (final feature map) of the last stage of the transformer decoder model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n-            transformer decoder at the output of each stage.\n-        hidden_states `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and\n-            `decoder_hidden_states`\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights from Detr's decoder after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class MaskFormerModelOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n+    pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the pixel decoder model (FPN).\n+    transformer_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Last hidden states (final feature map) of the last stage of the transformer decoder model.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n+        transformer decoder at the output of each stage.\n+    hidden_states `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and\n+        `decoder_hidden_states`\n     \"\"\"\n \n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -182,49 +168,49 @@ class MaskFormerModelOutput(ModelOutput):\n \n \n @dataclass\n-class MaskFormerForInstanceSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`MaskFormerForInstanceSegmentation`].\n \n     This output can be directly passed to [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or or\n     [`~MaskFormerImageProcessor.post_process_instance_segmentation`] or\n     [`~MaskFormerImageProcessor.post_process_panoptic_segmentation`] depending on the task. Please, see\n     [`~MaskFormerImageProcessor] for details regarding usage.\n-\n-    Args:\n-        loss (`torch.Tensor`, *optional*):\n-            The computed loss, returned when labels are present.\n-        class_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n-            query. Note the `+ 1` is needed because we incorporate the null class.\n-        masks_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n-            query.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n-        pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Last hidden states (final feature map) of the last stage of the pixel decoder model (FPN).\n-        transformer_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Last hidden states (final feature map) of the last stage of the transformer decoder model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the transformer decoder at the output\n-            of each stage.\n-        hidden_states `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and\n-            `decoder_hidden_states`.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights from Detr's decoder after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class MaskFormerForInstanceSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.Tensor`, *optional*):\n+        The computed loss, returned when labels are present.\n+    class_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n+        query. Note the `+ 1` is needed because we incorporate the null class.\n+    masks_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n+        query.\n+    auxiliary_logits (`Dict[str, torch.FloatTensor]`, *optional*, returned when `output_auxiliary_logits=True`):\n+        Dictionary containing auxiliary predictions for each decoder layer when auxiliary losses are enabled.\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the encoder model (backbone).\n+    pixel_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last stage of the pixel decoder model (FPN).\n+    transformer_decoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Last hidden states (final feature map) of the last stage of the transformer decoder model.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the transformer decoder at the output\n+        of each stage.\n+    hidden_states `tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` containing `encoder_hidden_states`, `pixel_decoder_hidden_states` and\n+        `decoder_hidden_states`.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "935ffcb67bfc0d62218c1635b5cd6c94aa62ea70",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 24,
            "deletions": 50,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -30,36 +30,25 @@\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n-from ...utils import torch_int\n+from ...utils import auto_docstring, torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_maskformer_swin import MaskFormerSwinConfig\n \n \n @dataclass\n-class MaskFormerSwinModelOutputWithPooling(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for MaskFormerSwinModel's outputs that also contains the spatial dimensions of the hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n-            Last layer hidden-state after a mean pooling operation.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        hidden_states_spatial_dimensions (`tuple(tuple(int, int))`, *optional*):\n-            A tuple containing the spatial dimension of each `hidden_state` needed to reshape the `hidden_states` to\n-            `batch, channels, height, width`. Due to padding, their spatial size cannot be inferred before the\n-            `forward` method.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MaskFormerSwinModelOutputWithPooling(ModelOutput):\n+    r\"\"\"\n+    pooler_output (`torch.FloatTensor` of shape `(batch_size, hidden_size)`):\n+        Last layer hidden-state after a mean pooling operation.\n+    hidden_states_spatial_dimensions (`tuple(tuple(int, int))`, *optional*):\n+        A tuple containing the spatial dimension of each `hidden_state` needed to reshape the `hidden_states` to\n+        `batch, channels, height, width`. Due to padding, their spatial size cannot be inferred before the\n+        `forward` method.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -70,28 +59,17 @@ class MaskFormerSwinModelOutputWithPooling(ModelOutput):\n \n \n @dataclass\n-class MaskFormerSwinBaseModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for SwinEncoder's outputs.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        hidden_states_spatial_dimensions (`tuple(tuple(int, int))`, *optional*):\n-            A tuple containing the spatial dimension of each `hidden_state` needed to reshape the `hidden_states` to\n-            `batch, channels, height, width`. Due to padding, their spatial size cannot inferred before the `forward`\n-            method.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MaskFormerSwinBaseModelOutput(ModelOutput):\n+    r\"\"\"\n+    hidden_states_spatial_dimensions (`tuple(tuple(int, int))`, *optional*):\n+        A tuple containing the spatial dimension of each `hidden_state` needed to reshape the `hidden_states` to\n+        `batch, channels, height, width`. Due to padding, their spatial size cannot inferred before the `forward`\n+        method.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -759,12 +737,8 @@ def forward(\n         )\n \n \n+@auto_docstring\n class MaskFormerSwinPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = MaskFormerSwinConfig\n     base_model_prefix = \"model\"\n     main_input_name = \"pixel_values\""
        },
        {
            "sha": "5410f6bf0ee107a13372469dc3aa55aeb812ca9d",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -700,31 +700,22 @@ def _init_weights(self, module):\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`MegatronBertForPreTraining`].\n+    \"\"\"\n+)\n # Copied from transformers.models.bert.modeling_bert.BertForPreTrainingOutput with Bert->MegatronBert\n class MegatronBertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`MegatronBertForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "ef05462f875ed51b7305136b72543c230f9e6850",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -69,35 +69,26 @@ def extra_repr(self) -> str:\n \n \n @dataclass\n-class MgpstrModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n-\n-    Args:\n-        logits (`tuple(torch.FloatTensor)` of shape `(batch_size, config.num_character_labels)`):\n-            Tuple of `torch.FloatTensor` (one for the output of character of shape `(batch_size,\n-            config.max_token_length, config.num_character_labels)`, + one for the output of bpe of shape `(batch_size,\n-            config.max_token_length, config.num_bpe_labels)`, + one for the output of wordpiece of shape `(batch_size,\n-            config.max_token_length, config.num_wordpiece_labels)`) .\n-\n-            Classification scores (before SoftMax) of character, bpe and wordpiece.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, config.max_token_length,\n-            sequence_length, sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        a3_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_a3_attentions=True` is passed or when `config.output_a3_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for the attention of character, + one for the attention of bpe`, + one\n-            for the attention of wordpiece) of shape `(batch_size, config.max_token_length, sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MgpstrModelOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`tuple(torch.FloatTensor)` of shape `(batch_size, config.num_character_labels)`):\n+        Tuple of `torch.FloatTensor` (one for the output of character of shape `(batch_size,\n+        config.max_token_length, config.num_character_labels)`, + one for the output of bpe of shape `(batch_size,\n+        config.max_token_length, config.num_bpe_labels)`, + one for the output of wordpiece of shape `(batch_size,\n+        config.max_token_length, config.num_wordpiece_labels)`) .\n+\n+        Classification scores (before SoftMax) of character, bpe and wordpiece.\n+    a3_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_a3_attentions=True` is passed or when `config.output_a3_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for the attention of character, + one for the attention of bpe`, + one\n+        for the attention of wordpiece) of shape `(batch_size, config.max_token_length, sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n     \"\"\"\n \n     logits: tuple[torch.FloatTensor] = None"
        },
        {
            "sha": "f1363f789768802783a85f7a789f9696b19222ee",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 42,
            "deletions": 42,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -47,29 +47,29 @@\n \n \n @dataclass\n+@auto_docstring\n class MimiOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n-            Discret code embeddings computed using `model.encode`.\n-        audio_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*)\n-            Decoded audio values, obtained using the decoder part of Mimi.\n-        encoder_past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the encoder transformer.\n-            This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            The model will output the same cache format that is fed as input.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n-            have their past key value states given to this model).\n-        decoder_past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the decoder transformer.\n-            This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            The model will output the same cache format that is fed as input.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n-            have their past key value states given to this model).\n+    r\"\"\"\n+    audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n+        Discret code embeddings computed using `model.encode`.\n+    audio_values (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        Decoded audio values, obtained using the decoder part of Mimi.\n+    encoder_past_key_values (`Cache`, *optional*):\n+        Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the encoder transformer.\n+        This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+        The model will output the same cache format that is fed as input.\n+\n+        If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n+        have their past key value states given to this model).\n+    decoder_past_key_values (`Cache`, *optional*):\n+        Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the decoder transformer.\n+        This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+\n+        The model will output the same cache format that is fed as input.\n+\n+        If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n+        have their past key value states given to this model).\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None\n@@ -79,39 +79,39 @@ class MimiOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class MimiEncoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n-            Discret code embeddings computed using `model.encode`.\n-        encoder_past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the encoder transformer.\n-            This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+    r\"\"\"\n+    audio_codes (`torch.LongTensor`  of shape `(batch_size, num_quantizers, codes_length)`, *optional*):\n+        Discret code embeddings computed using `model.encode`.\n+    encoder_past_key_values (`Cache`, *optional*):\n+        Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the encoder transformer.\n+        This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            The model will output the same cache format that is fed as input.\n+        The model will output the same cache format that is fed as input.\n \n-            If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n-            have their past key value states given to this model).\n+        If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n+        have their past key value states given to this model).\n     \"\"\"\n \n     audio_codes: Optional[torch.LongTensor] = None\n     encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n \n \n @dataclass\n+@auto_docstring\n class MimiDecoderOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):\n-            Decoded audio values, obtained using the decoder part of Mimi.\n-        decoder_past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the decoder transformer.\n-            This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n+    r\"\"\"\n+    audio_values (`torch.FloatTensor`  of shape `(batch_size, segment_length)`, *optional*):\n+        Decoded audio values, obtained using the decoder part of Mimi.\n+    decoder_past_key_values (`Cache`, *optional*):\n+        Pre-computed hidden-states (key and values in the self-attention blocks) that can be used to speed up sequential decoding of the decoder transformer.\n+        This typically consists in the `past_key_values` returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            The model will output the same cache format that is fed as input.\n+        The model will output the same cache format that is fed as input.\n \n-            If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n-            have their past key value states given to this model).\n+        If `past_key_values` are used, the user can optionally input only the last `audio_values` or `audio_codes (those that don't\n+        have their past key value states given to this model).\n     \"\"\"\n \n     audio_values: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "af4f8fb3b2344ec969ae53ad5fde9153127294b1",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1209,17 +1209,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "2576c85a785a51efecf82305faa427965c044100",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -760,17 +760,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "d78cc22ca24fc82c1d6a8d2273729120bc560bbe",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -29,8 +29,6 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n-\n \n class MistralMLP(LlamaMLP):\n     def __init__(self, config):\n@@ -247,17 +245,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "9b885c3c385915d3c1f3c509a7a8736af8e476bd",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -123,35 +123,26 @@ def forward(self, image_features: torch.Tensor, image_sizes: torch.Tensor):\n \n \n @dataclass\n-class Mistral3CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Mistral3 causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class Mistral3CausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -163,33 +154,22 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Mistral3 outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "50d9189427d2fbc107131ca026f255c169576226",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -992,17 +992,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "d33edcb3dd006400af1e86b07db42fe03847a621",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1531,10 +1531,6 @@ def forward(\n             For each text token (in seq_length):\n             - 1 indicates the token **should attend** to the corresponding image tile\n             - 0 indicates the token **should not attend** to the corresponding image tile\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n         full_text_row_masked_out_mask (`tuple[torch.Tensor, torch.Tensor]`, *optional*):\n             A tuple containing two tensors that mask out rows in the cross-attention mechanism:\n             - The first tensor has shape `(batch_size, 1, seq_length, 1)` and contains values of 0 or 1.\n@@ -1544,6 +1540,10 @@ def forward(\n               the forward pass of cross-attention layers.\n             This mask is derived from the cross_attention_mask and is used to handle cases where a text token\n             should not attend to any image token.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n \n         Example:\n "
        },
        {
            "sha": "b1c267c959bc1b19bb24261c3340ce4fe6bf15ed",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -678,30 +678,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class MobileBertForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`MobileBertForPreTraining`].\n-\n-    Args:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n-            Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n-            before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MobileBertForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    seq_relationship_logits (`torch.FloatTensor` of shape `(batch_size, 2)`):\n+        Prediction scores of the next sequence prediction (classification) head (scores of True/False continuation\n+        before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "669d2a8e806b52946e0d47a5b71a08146bebaad7",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 90,
            "deletions": 110,
            "changes": 200,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -50,41 +50,43 @@\n \n \n @dataclass\n-class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Outputs of [`MoshiForConditionalConditionalGeneration.generate`].\n-\n-    Args:\n-        audio_sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, 1, sequence_length)`, *optional*):\n-            The generated audio waveforms.\n-        sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n-            The generated text sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n-            if all batches finished early due to the `eos_token_id`.\n-        sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True`):\n-            Final beam scores of the generated `sequences`.\n-        scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n-            Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n-            of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n-            Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n-            with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n-        logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n-            Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n-            at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n-            each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n-        beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True`):\n-            Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n-            `(batch_size*num_return_sequences, sequence_length)`.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n-            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n-            `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n-        hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n-            Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n-            `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n-        past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n-            Returns the model cache, used to speed up decoding. Different models have a different cache format, check\n-            the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n-        audio_codes (`torch.LongTensor` of shape `(batch_size*num_return_sequences, num_codeooks, sequence_length)`, *optional*):\n-            The generated audio codes. Returned if `return_audio_codes=True`. Intermediate audio \"tokens\" which transforms to `audio_sequences` once passed through the audio decoder.\n+    \"\"\"\n+)\n+class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n+    r\"\"\"\n+    audio_sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, 1, sequence_length)`, *optional*):\n+        The generated audio waveforms.\n+    sequences (`torch.LongTensor` of shape `(batch_size*num_return_sequences, sequence_length)`):\n+        The generated text sequences. The second dimension (sequence_length) is either equal to `max_length` or shorter\n+        if all batches finished early due to the `eos_token_id`.\n+    sequences_scores (`torch.FloatTensor` of shape `(batch_size*num_return_sequences)`, *optional*, returned when `output_scores=True`):\n+        Final beam scores of the generated `sequences`.\n+    scores (`tuple(torch.FloatTensor)` *optional*, returned when `output_scores=True`):\n+        Beam transition scores for each vocabulary token at each generation step. Beam transition scores consisting\n+        of log probabilities of tokens conditioned on log softmax of previously generated tokens in this beam.\n+        Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for each generated token),\n+        with each tensor of shape `(batch_size*num_beams, config.vocab_size)`.\n+    logits (`tuple(torch.FloatTensor)` *optional*, returned when `output_logits=True`):\n+        Unprocessed prediction scores of the language modeling head (scores for each vocabulary token before SoftMax)\n+        at each generation step. Tuple of `torch.FloatTensor` with up to `max_new_tokens` elements (one element for\n+        each generated token), with each tensor of shape `(batch_size, config.vocab_size)`.\n+    beam_indices (`torch.LongTensor`, *optional*, returned when `output_scores=True`):\n+        Beam indices of generated token id at each generation step. `torch.LongTensor` of shape\n+        `(batch_size*num_return_sequences, sequence_length)`.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True`):\n+        Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+        `torch.FloatTensor` of shape `(batch_size*num_beams, num_heads, generated_length, sequence_length)`.\n+    hidden_states (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_hidden_states=True`):\n+        Tuple (one element for each generated token) of tuples (one element for each layer of the decoder) of\n+        `torch.FloatTensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n+    past_key_values (`tuple(tuple(torch.FloatTensor)))`, *optional*, returned when `use_cache=True`):\n+        Contains the model cache, used to speed up decoding. Different models have a different cache format, check\n+        the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n+    audio_codes (`torch.LongTensor` of shape `(batch_size*num_return_sequences, num_codeooks, sequence_length)`, *optional*):\n+        The generated audio codes. Returned if `return_audio_codes=True`. Intermediate audio \"tokens\" which transforms to `audio_sequences` once passed through the audio decoder.\n     \"\"\"\n \n     audio_sequences: Optional[torch.Tensor] = None\n@@ -100,34 +102,23 @@ class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n \n \n @dataclass\n-class MoshiCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     `MoshiForCausalLM` outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MoshiCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -139,45 +130,34 @@ class MoshiCausalLMOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     `MoshiForConditionalGeneration` outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `text_labels` is provided):\n-            Text language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the text language modeling head (scores for each vocabulary token before SoftMax).\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        depth_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `audio_labels` is provided):\n-            Audio language modeling loss (for next-token prediction).\n-        audio_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the audio language modeling heads.\n-        depth_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Past key-values of the depth decoder.\n-        depth_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Hidden states of the depth decoder\n-        depth_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Depth decoder's Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `text_labels` is provided):\n+        Text language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the text language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    depth_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `audio_labels` is provided):\n+        Audio language modeling loss (for next-token prediction).\n+    audio_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the audio language modeling heads.\n+    depth_past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Past key-values of the depth decoder.\n+    depth_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Hidden states of the depth decoder\n+    depth_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Depth decoder's Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -194,18 +174,18 @@ class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class MoshiUnconditionalInput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        input_ids (`torch.Tensor `of shape `(batch_size, sequence_length), *optional*):\n-            The sequence used as a text prompt for the generation.\n-        user_audio_codes (`torch.Tensor `of shape `(batch_size, num_codebooks, sequence_length), *optional*):\n-            The audio codes used as audio user prompt for the generation. Has priority over `user_input_values` and represents the audio \"tokens\" of `user_input_values` once passed through the audio encoder.\n-        moshi_audio_codes (`torch.Tensor `of shape `(batch_size, num_codebooks, sequence_length), *optional*):\n-            The audio codes used as audio Moshi prompt for the generation. Has priority over `moshi_input_values` and represents the audio \"tokens\" of `moshi_input_values` once passed through the audio encoder.\n-        attention_mask (`torch.LongTensor`)  of shape `(batch_size, sequence_length)`, *optional*):\n-            Attention mask to avoid performing attention on padding token indices. Mask values selected in `[0,\n-            1]`: 1 for tokens that are **not masked**, 0 for tokens that are **masked**.\n+    r\"\"\"\n+    input_ids (`torch.Tensor `of shape `(batch_size, sequence_length), *optional*):\n+        The sequence used as a text prompt for the generation.\n+    user_audio_codes (`torch.Tensor `of shape `(batch_size, num_codebooks, sequence_length), *optional*):\n+        The audio codes used as audio user prompt for the generation. Has priority over `user_input_values` and represents the audio \"tokens\" of `user_input_values` once passed through the audio encoder.\n+    moshi_audio_codes (`torch.Tensor `of shape `(batch_size, num_codebooks, sequence_length), *optional*):\n+        The audio codes used as audio Moshi prompt for the generation. Has priority over `moshi_input_values` and represents the audio \"tokens\" of `moshi_input_values` once passed through the audio encoder.\n+    attention_mask (`torch.LongTensor`)  of shape `(batch_size, sequence_length)`, *optional*):\n+        Attention mask to avoid performing attention on padding token indices. Mask values selected in `[0,\n+        1]`: 1 for tokens that are **not masked**, 0 for tokens that are **masked**.\n     \"\"\"\n \n     input_ids: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "bc21773655172a065ecfc0bd20fbc33a710c6e0b",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -70,17 +70,17 @@\n \n \n @dataclass\n+@auto_docstring\n class MusicgenUnconditionalInput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        encoder_outputs  (`tuple[torch.FloatTensor]` of length 1, with tensor shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the text encoder model.\n-        attention_mask (`torch.LongTensor`)  of shape `(batch_size, sequence_length)`, *optional*):\n-            Encoder attention mask to avoid performing attention on padding token indices. Mask values selected in `[0,\n-            1]`: 1 for tokens that are **not masked**, 0 for tokens that are **masked**.\n-        guidance_scale (`float`, *optional*):\n-            Guidance scale for classifier free guidance, setting the balance between the conditional logits (predicted\n-            from the prompts) and the unconditional logits (predicted without prompts).\n+    r\"\"\"\n+    encoder_outputs (`tuple[torch.FloatTensor]` of length 1, with tensor shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the text encoder model.\n+    attention_mask (`torch.LongTensor`)  of shape `(batch_size, sequence_length)`, *optional*):\n+        Encoder attention mask to avoid performing attention on padding token indices. Mask values selected in `[0,\n+        1]`: 1 for tokens that are **not masked**, 0 for tokens that are **masked**.\n+    guidance_scale (`float`, *optional*):\n+        Guidance scale for classifier free guidance, setting the balance between the conditional logits (predicted\n+        from the prompts) and the unconditional logits (predicted without prompts).\n     \"\"\"\n \n     encoder_outputs: tuple[torch.FloatTensor] = None\n@@ -1704,6 +1704,13 @@ def forward(\n         **kwargs,\n     ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n+        padding_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n         decoder_input_ids (`torch.LongTensor` of shape `(batch_size * num_codebooks, target_sequence_length)`, *optional*):\n             Indices of decoder input sequence tokens in the vocabulary, corresponding to the sequence of audio codes.\n \n@@ -1729,13 +1736,6 @@ def forward(\n             Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\n             `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\n             are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\n-        padding_mask (`torch.BoolTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n \n         Examples:\n         ```python"
        },
        {
            "sha": "b3a2322e4aa1239ef3924ef1cb279f0b1fbeb8ab",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -62,35 +62,26 @@\n \n \n @dataclass\n-class MusicgenMelodyOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Musicgen Melody autoregressive outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-            Sequence of conditional hidden-states representing the concatenation of the projected text encoder output and the projected audio encoder output.\n-            Used as a conditional signal.\n+    \"\"\"\n+)\n+class MusicgenMelodyOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n+        Sequence of conditional hidden-states representing the concatenation of the projected text encoder output and the projected audio encoder output.\n+        Used as a conditional signal.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "0dfbf83332499c764b9ab790851987420387ffb1",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 70,
            "deletions": 83,
            "changes": 153,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -43,26 +43,17 @@\n \n \n @dataclass\n-class OmDetTurboEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the OmDetTurboHybridEncoder.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor`):\n-            Last hidden states of the encoder.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        extracted_states (`tuple[torch.FloatTensor]`):\n-            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n+    \"\"\"\n+)\n+class OmDetTurboEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor`):\n+        Last hidden states of the encoder.\n+    extracted_states (`tuple[torch.FloatTensor]`):\n+        The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -72,33 +63,27 @@ class OmDetTurboEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class OmDetTurboDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the OmDetTurboDecoder.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder.\n-        decoder_coords (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The predicted coordinates of the objects.\n-        decoder_classes (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n-            The predicted classes of the objects.\n-        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The predicted coordinates of the objects from the encoder.\n-        encoder_class_logits (`tuple[torch.FloatTensor]`) of shape `(batch_size, num_queries, num_classes)`:\n-            The predicted class of the objects from the encoder.\n-        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The initial reference points.\n-        intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):\n-            The intermediate reference points.\n-        hidden_states (`Optional[tuple[torch.FloatTensor]]`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`Optional[tuple[tuple[torch.FloatTensor]]]`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    \"\"\"\n+)\n+class OmDetTurboDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder.\n+    decoder_coords (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The predicted coordinates of the objects.\n+    decoder_classes (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n+        The predicted classes of the objects.\n+    encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The predicted coordinates of the objects from the encoder.\n+    encoder_class_logits (`tuple[torch.FloatTensor]` of shape `(batch_size, num_queries, num_classes)`):\n+        The predicted class of the objects from the encoder.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The initial reference points.\n+    intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):\n+        The intermediate reference points.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -113,45 +98,47 @@ class OmDetTurboDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class OmDetTurboObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`OmDetTurboObjectDetectionOutput`].\n-\n-    Args:\n-        loss (`torch.FloatTensor`):\n-            The loss value.\n-        decoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The predicted coordinates logits of the objects.\n-        decoder_class_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n-            The predicted class of the objects.\n-        init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The initial reference points.\n-        intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):\n-            The intermediate reference points.\n-        encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            The predicted coordinates of the objects from the encoder.\n-        encoder_class_logits (`tuple[torch.FloatTensor]`):\n-            The predicted class of the objects from the encoder.\n-        encoder_extracted_states (`torch.FloatTensor`):\n-            The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n-        decoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n-        encoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n-            `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        encoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):\n-            Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n-            sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n-            weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n-        classes_structure (`torch.LongTensor`, *optional*):\n-            The number of queried classes for each image.\n+    \"\"\"\n+)\n+class OmDetTurboObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`):\n+        The loss value.\n+    decoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The predicted coordinates logits of the objects.\n+    decoder_class_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes)`):\n+        The predicted class of the objects.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The initial reference points.\n+    intermediate_reference_points (`tuple[tuple[torch.FloatTensor]]`):\n+        The intermediate reference points.\n+    encoder_coord_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        The predicted coordinates of the objects from the encoder.\n+    encoder_class_logits (`tuple[torch.FloatTensor]`):\n+        The predicted class of the objects from the encoder.\n+    encoder_extracted_states (`torch.FloatTensor`):\n+        The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n+    decoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n+        `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+        plus the initial embedding outputs.\n+    decoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):\n+        Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+        sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+        weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    encoder_hidden_states (`tuple[torch.FloatTensor]`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n+        `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n+        plus the initial embedding outputs.\n+    encoder_attentions (`tuple[tuple[torch.FloatTensor]]`, *optional*):\n+        Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n+        sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n+        weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+    classes_structure (`torch.LongTensor`, *optional*):\n+        The number of queried classes for each image.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "05e22056a514135928a746720690465fe18b820c",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 127,
            "deletions": 117,
            "changes": 244,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -731,22 +731,24 @@ def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> tor\n \n \n @dataclass\n-class OneFormerTransformerDecoderOutput(BaseModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Transformer decoder. This class adds attributes for class predictions, mask\n     predictions and contrastive logits to BaseModelOutputWithCrossAttentions.\n-\n-    Args:\n-        object_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n-            Queries representation for the region proposals.\n-        contrastive_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n-            Queries representation for the contrastive loss.\n-        prediction_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`):\n-            Mask predictions from last layer of the transformer decoder.\n-        prediction_class (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n-            Class predictions from last layer of the transformer decoder.\n-        auxiliary_predictions (Tuple of Dict of `str, torch.FloatTensor`, *optional*):\n-            Tuple of class and mask predictions from each layer of the transformer decoder.\n+    \"\"\"\n+)\n+class OneFormerTransformerDecoderOutput(BaseModelOutput):\n+    r\"\"\"\n+    object_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Queries representation for the region proposals.\n+    contrastive_logits (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Queries representation for the contrastive loss.\n+    prediction_masks (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`):\n+        Mask predictions from last layer of the transformer decoder.\n+    prediction_class (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n+        Class predictions from last layer of the transformer decoder.\n+    auxiliary_predictions (Tuple of Dict of `str, torch.FloatTensor`, *optional*):\n+        Tuple of class and mask predictions from each layer of the transformer decoder.\n     \"\"\"\n \n     object_queries: Optional[torch.FloatTensor] = None\n@@ -757,23 +759,25 @@ class OneFormerTransformerDecoderOutput(BaseModelOutput):\n \n \n @dataclass\n-# Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoderOutput with Mask2->One\n-class OneFormerPixelDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     OneFormer's pixel decoder module output, practically a Multi-Scale Deformable Attention based decoder. It returns\n     the mask features and the multiscale features.\n-\n-    Args:\n-        multi_scale_features (`tuple(torch.FloatTensor)`):\n-            Tuple of multi-scale features of scales [1/8, 1/16, 1/32] and shape `(batch_size, num_channels, height,\n-            width)`from the Multi-Scale Deformable Attenntion based Pixel Decoder.\n-        mask_features (`torch.FloatTensor`):\n-            Tensor of shape `(batch_size, num_channels, height, width)`, 1/4 scale features from the last Pixel Decoder\n-            Layer.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights from pixel decoder. Returned when `output_attentions=True` is passed\n-            or when `config.output_attentions=True`\n+    \"\"\"\n+)\n+# Copied from transformers.models.mask2former.modeling_mask2former.Mask2FormerPixelDecoderOutput with Mask2->One\n+class OneFormerPixelDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    multi_scale_features (`tuple(torch.FloatTensor)`):\n+        Tuple of multi-scale features of scales [1/8, 1/16, 1/32] and shape `(batch_size, num_channels, height,\n+        width)`from the Multi-Scale Deformable Attenntion based Pixel Decoder.\n+    mask_features (`torch.FloatTensor`):\n+        Tensor of shape `(batch_size, num_channels, height, width)`, 1/4 scale features from the last Pixel Decoder\n+        Layer.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights from pixel decoder. Returned when `output_attentions=True` is passed\n+        or when `config.output_attentions=True`\n     \"\"\"\n \n     multi_scale_features: tuple[torch.FloatTensor] = None\n@@ -782,21 +786,23 @@ class OneFormerPixelDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class OneFormerPixelLevelModuleOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     OneFormer's pixel level module output. It returns both the last and (optionally) the hidden states from the\n     `encoder` and `decoder`. By default, the `encoder` is a Swin/Dinat Backbone and the `decoder` is a Multi-Scale\n     Deformable Attention based decoder.\n-\n-    Args:\n-        encoder_features (List of `(torch.FloatTensor)`):\n-            List of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden-states (also\n-            called feature maps) of the model at the output of each stage.\n-        decoder_features (List of `(torch.FloatTensor)`):\n-            List of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden-states (also\n-            called feature maps) of the model at the output of each stage.\n-        decoder_last_feature (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)):\n-            1/4 scale features from the last Pixel Decoder Layer.\n+    \"\"\"\n+)\n+class OneFormerPixelLevelModuleOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_features (List of `(torch.FloatTensor)`):\n+        List of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden-states (also\n+        called feature maps) of the model at the output of each stage.\n+    decoder_features (List of `(torch.FloatTensor)`):\n+        List of `torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`. Hidden-states (also\n+        called feature maps) of the model at the output of each stage.\n+    decoder_last_feature (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)):\n+        1/4 scale features from the last Pixel Decoder Layer.\n     \"\"\"\n \n     encoder_features: list[torch.FloatTensor] = None\n@@ -805,40 +811,42 @@ class OneFormerPixelLevelModuleOutput(ModelOutput):\n \n \n @dataclass\n-class OneFormerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`OneFormerModel`]. This class returns all the needed hidden states to compute the logits.\n-\n-    Args:\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n-            transformer decoder at the output of each stage.\n-        transformer_decoder_object_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`)\n-            Output object queries from the last layer in the transformer decoder.\n-        transformer_decoder_contrastive_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`)\n-            Contrastive queries from the transformer decoder.\n-        transformer_decoder_mask_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`)\n-            Mask Predictions from the last layer in the transformer decoder.\n-        transformer_decoder_class_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n-            Class Predictions from the last layer in the transformer decoder.\n-        transformer_decoder_auxiliary_predictions (Tuple of Dict of `str, torch.FloatTensor`, *optional*):\n-            Tuple of class and mask predictions from each layer of the transformer decoder.\n-        text_queries (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries, hidden_dim)`)\n-            Text queries derived from the input text list used for calculating contrastive loss during training.\n-        task_token (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`)\n-            1D task token to condition the queries.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    \"\"\"\n+)\n+class OneFormerModelOutput(ModelOutput):\n+    r\"\"\"\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n+        transformer decoder at the output of each stage.\n+    transformer_decoder_object_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Output object queries from the last layer in the transformer decoder.\n+    transformer_decoder_contrastive_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Contrastive queries from the transformer decoder.\n+    transformer_decoder_mask_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`):\n+        Mask Predictions from the last layer in the transformer decoder.\n+    transformer_decoder_class_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n+        Class Predictions from the last layer in the transformer decoder.\n+    transformer_decoder_auxiliary_predictions (Tuple of Dict of `str, torch.FloatTensor`, *optional*):\n+        Tuple of class and mask predictions from each layer of the transformer decoder.\n+    text_queries (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries, hidden_dim)`):\n+        Text queries derived from the input text list used for calculating contrastive loss during training.\n+    task_token (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`):\n+        1D task token to condition the queries.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n     \"\"\"\n \n     encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n@@ -855,55 +863,57 @@ class OneFormerModelOutput(ModelOutput):\n \n \n @dataclass\n-class OneFormerForUniversalSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for outputs of [`OneFormerForUniversalSegmentationOutput`].\n \n     This output can be directly passed to [`~OneFormerImageProcessor.post_process_semantic_segmentation`] or\n     [`~OneFormerImageProcessor.post_process_instance_segmentation`] or\n     [`~OneFormerImageProcessor.post_process_panoptic_segmentation`] depending on the task. Please, see\n     [`~OneFormerImageProcessor] for details regarding usage.\n-\n-    Args:\n-        loss (`torch.Tensor`, *optional*):\n-            The computed loss, returned when labels are present.\n-        class_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n-            query. Note the `+ 1` is needed because we incorporate the null class.\n-        masks_queries_logits (`torch.FloatTensor`):\n-            A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n-            query.\n-        auxiliary_predictions (List of Dict of `str, torch.FloatTensor`, *optional*):\n-            List of class and mask predictions from each layer of the transformer decoder.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n-            model at the output of each stage.\n-        pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n-            decoder model at the output of each stage.\n-        transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n-            transformer decoder at the output of each stage.\n-        transformer_decoder_object_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`)\n-            Output object queries from the last layer in the transformer decoder.\n-        transformer_decoder_contrastive_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`)\n-            Contrastive queries from the transformer decoder.\n-        transformer_decoder_mask_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`)\n-            Mask Predictions from the last layer in the transformer decoder.\n-        transformer_decoder_class_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n-            Class Predictions from the last layer in the transformer decoder.\n-        transformer_decoder_auxiliary_predictions (List of Dict of `str, torch.FloatTensor`, *optional*):\n-            List of class and mask predictions from each layer of the transformer decoder.\n-        text_queries (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries, hidden_dim)`)\n-            Text queries derived from the input text list used for calculating contrastive loss during training.\n-        task_token (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`)\n-            1D task token to condition the queries.\n-        attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    \"\"\"\n+)\n+class OneFormerForUniversalSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.Tensor`, *optional*):\n+        The computed loss, returned when labels are present.\n+    class_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n+        query. Note the `+ 1` is needed because we incorporate the null class.\n+    masks_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n+        query.\n+    auxiliary_predictions (List of Dict of `str, torch.FloatTensor`, *optional*):\n+        List of class and mask predictions from each layer of the transformer decoder.\n+    encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the encoder\n+        model at the output of each stage.\n+    pixel_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, num_channels, height, width)`. Hidden-states (also called feature maps) of the pixel\n+        decoder model at the output of each stage.\n+    transformer_decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states (also called feature maps) of the\n+        transformer decoder at the output of each stage.\n+    transformer_decoder_object_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Output object queries from the last layer in the transformer decoder.\n+    transformer_decoder_contrastive_queries (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_dim)`):\n+        Contrastive queries from the transformer decoder.\n+    transformer_decoder_mask_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, height, width)`):\n+        Mask Predictions from the last layer in the transformer decoder.\n+    transformer_decoder_class_predictions (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes+1)`):\n+        Class Predictions from the last layer in the transformer decoder.\n+    transformer_decoder_auxiliary_predictions (List of Dict of `str, torch.FloatTensor`, *optional*):\n+        List of class and mask predictions from each layer of the transformer decoder.\n+    text_queries (`torch.FloatTensor`, *optional* of shape `(batch_size, num_queries, hidden_dim)`):\n+        Text queries derived from the input text list used for calculating contrastive loss during training.\n+    task_token (`torch.FloatTensor` of shape `(batch_size, hidden_dim)`):\n+        1D task token to condition the queries.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "fd83b02b90e2d7a7c958b481792242f8aa43cdfe",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -380,30 +380,21 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class OpenAIGPTDoubleHeadsModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of models predicting if two sentences are consecutive or not.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        mc_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels` is provided):\n-            Multiple choice classification loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n-            Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class OpenAIGPTDoubleHeadsModelOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    mc_loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `mc_labels` is provided):\n+        Multiple choice classification loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_choices, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    mc_logits (`torch.FloatTensor` of shape `(batch_size, num_choices)`):\n+        Prediction scores of the multiple choice classification head (scores for each choice before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "8940943851d517dd7db8394b437528a756fca51b",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 84,
            "deletions": 80,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -54,26 +54,26 @@ def owlv2_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring\n class Owlv2Output(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size * num_max_text_queries, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`Owlv2TextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of\n-            [`Owlv2VisionModel`].\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`Owlv2TextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`Owlv2VisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size * num_max_text_queries, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`Owlv2TextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of\n+        [`Owlv2VisionModel`].\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`Owlv2TextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Owlv2VisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -160,39 +160,41 @@ def generalized_box_iou(boxes1, boxes2):\n \n \n @dataclass\n-class Owlv2ObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`Owlv2ForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n-            Classification logits (including no-object) for all queries.\n-        objectness_logits (`torch.FloatTensor` of shape `(batch_size, num_patches, 1)`):\n-            The objectness logits of all image patches. OWL-ViT represents images as a set of image patches where the\n-            total number of patches is (image_size / patch_size)**2.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, num_max_text_queries, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`Owlv2TextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes image\n-            embeddings for each patch.\n-        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n-            Class embeddings of all image patches. OWLv2 represents images as a set of image patches where the total\n-            number of patches is (image_size / patch_size)**2.\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`Owlv2TextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`Owlv2VisionModel`].\n+    \"\"\"\n+)\n+class Owlv2ObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n+        Classification logits (including no-object) for all queries.\n+    objectness_logits (`torch.FloatTensor` of shape `(batch_size, num_patches, 1)`):\n+        The objectness logits of all image patches. OWL-ViT represents images as a set of image patches where the\n+        total number of patches is (image_size / patch_size)**2.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, num_max_text_queries, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`Owlv2TextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes image\n+        embeddings for each patch.\n+    class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n+        Class embeddings of all image patches. OWLv2 represents images as a set of image patches where the total\n+        number of patches is (image_size / patch_size)**2.\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`Owlv2TextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Owlv2VisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -214,37 +216,39 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`Owlv2ForObjectDetection.image_guided_detection`].\n+    \"\"\"\n+)\n # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTImageGuidedObjectDetectionOutput with OwlViT->Owlv2,OWL-ViT->OWLv2\n class Owlv2ImageGuidedObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n-    Output type of [`Owlv2ForObjectDetection.image_guided_detection`].\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n-            Classification logits (including no-object) for all queries.\n-        target_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual target image in the batch\n-            (disregarding possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to\n-            retrieve the unnormalized bounding boxes.\n-        query_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual query image in the batch\n-            (disregarding possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to\n-            retrieve the unnormalized bounding boxes.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes\n-            image embeddings for each patch.\n-        query_image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes\n-            image embeddings for each patch.\n-        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n-            Class embeddings of all image patches. OWLv2 represents images as a set of image patches where the total\n-            number of patches is (image_size / patch_size)**2.\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`Owlv2TextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`Owlv2VisionModel`].\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n+        Classification logits (including no-object) for all queries.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes\n+        image embeddings for each patch.\n+    query_image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`Owlv2VisionModel`]. OWLv2 represents images as a set of image patches and computes\n+        image embeddings for each patch.\n+    target_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual target image in the batch\n+        (disregarding possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to\n+        retrieve the unnormalized bounding boxes.\n+    query_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual query image in the batch\n+        (disregarding possible padding). You can use [`~Owlv2ImageProcessor.post_process_object_detection`] to\n+        retrieve the unnormalized bounding boxes.\n+    class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n+        Class embeddings of all image patches. OWLv2 represents images as a set of image patches where the total\n+        number of patches is (image_size / patch_size)**2.\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`Owlv2TextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`Owlv2VisionModel`].\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "f93a6836f3cb6044225986b3168183b320cae730",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 81,
            "deletions": 77,
            "changes": 158,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -54,26 +54,26 @@ def owlvit_loss(similarity: torch.Tensor) -> torch.Tensor:\n \n \n @dataclass\n+@auto_docstring\n class OwlViTOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n-            Contrastive loss for image-text similarity.\n-        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n-            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n-            similarity scores.\n-        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n-            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n-            similarity scores.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size * num_max_text_queries, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n-            The image embeddings obtained by applying the projection layer to the pooled output of\n-            [`OwlViTVisionModel`].\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`OwlViTTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`OwlViTVisionModel`].\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size * num_max_text_queries, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of\n+        [`OwlViTVisionModel`].\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`OwlViTTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`OwlViTVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -160,36 +160,38 @@ def generalized_box_iou(boxes1, boxes2):\n \n \n @dataclass\n-class OwlViTObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`OwlViTForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized bounding boxes.\n-        text_embeds (`torch.FloatTensor` of shape `(batch_size, num_max_text_queries, output_dim`):\n-            The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n-            image embeddings for each patch.\n-        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n-            Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total\n-            number of patches is (image_size / patch_size)**2.\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`OwlViTTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`OwlViTVisionModel`].\n+    \"\"\"\n+)\n+class OwlViTObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized bounding boxes.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, num_max_text_queries, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`OwlViTTextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n+        image embeddings for each patch.\n+    class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n+        Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total\n+        number of patches is (image_size / patch_size)**2.\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`OwlViTTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`OwlViTVisionModel`].\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -210,36 +212,38 @@ def to_tuple(self) -> tuple[Any]:\n \n \n @dataclass\n-class OwlViTImageGuidedObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`OwlViTForObjectDetection.image_guided_detection`].\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n-            Classification logits (including no-object) for all queries.\n-        target_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual target image in the batch\n-            (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to\n-            retrieve the unnormalized bounding boxes.\n-        query_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual query image in the batch\n-            (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to\n-            retrieve the unnormalized bounding boxes.\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n-            image embeddings for each patch.\n-        query_image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n-            Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n-            image embeddings for each patch.\n-        class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n-            Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total\n-            number of patches is (image_size / patch_size)**2.\n-        text_model_output (tuple[`BaseModelOutputWithPooling`]):\n-            The output of the [`OwlViTTextModel`].\n-        vision_model_output (`BaseModelOutputWithPooling`):\n-            The output of the [`OwlViTVisionModel`].\n+    \"\"\"\n+)\n+class OwlViTImageGuidedObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_patches, num_queries)`):\n+        Classification logits (including no-object) for all queries.\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n+        image embeddings for each patch.\n+    query_image_embeds (`torch.FloatTensor` of shape `(batch_size, patch_size, patch_size, output_dim`):\n+        Pooled output of [`OwlViTVisionModel`]. OWL-ViT represents images as a set of image patches and computes\n+        image embeddings for each patch.\n+    target_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual target image in the batch\n+        (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to\n+        retrieve the unnormalized bounding boxes.\n+    query_pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_patches, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual query image in the batch\n+        (disregarding possible padding). You can use [`~OwlViTImageProcessor.post_process_object_detection`] to\n+        retrieve the unnormalized bounding boxes.\n+    class_embeds (`torch.FloatTensor` of shape `(batch_size, num_patches, hidden_size)`):\n+        Class embeddings of all image patches. OWL-ViT represents images as a set of image patches where the total\n+        number of patches is (image_size / patch_size)**2.\n+    text_model_output (tuple[`BaseModelOutputWithPooling`]):\n+        The output of the [`OwlViTTextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`OwlViTVisionModel`].\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "05c16d868e2a4b462c8962fd1565e75ab2d446de",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -36,68 +36,48 @@\n \n \n @dataclass\n-class PaligemmaModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Paligemma outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class PaligemmaModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for PaliGemma causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n+    \"\"\"\n+)\n+class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "6e31efa7f8d016d4cb2abce899649668b4be90a6",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 114,
            "deletions": 111,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1126,15 +1126,17 @@ def forward(\n \n \n @dataclass\n-class PatchTSMixerEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for `PatchTSMixerEncoderOutput`, with potential hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, d_model)`):\n-            Hidden-state at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer.\n+    \"\"\"\n+)\n+class PatchTSMixerEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, d_model)`):\n+        Hidden-state at the output of the last layer of the model.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -1211,25 +1213,27 @@ def forward(\n \n \n @dataclass\n-class PatchTSMixerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs, with potential hidden states.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor`  of shape `(batch_size, num_channels, num_patches, d_model)`):\n-            Hidden-state at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer.\n-        patch_input (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n-            Patched input data to the model.\n-        mask: (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches)`,*optional*):\n-            Bool Tensor indicating True in masked patches and False otherwise.\n-        loc: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*):\n-            Gives the mean of the context window per channel. Used for revin denorm outside the model, if revin\n-            enabled.\n-        scale: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`,*optional*):\n-            Gives the std dev of the context window per channel. Used for revin denorm outside the model, if revin\n-            enabled.\n+    \"\"\"\n+)\n+class PatchTSMixerModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor`  of shape `(batch_size, num_channels, num_patches, d_model)`):\n+        Hidden-state at the output of the last layer of the model.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer.\n+    patch_input (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n+        Patched input data to the model.\n+    mask (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches)`, *optional*):\n+        Bool Tensor indicating True in masked patches and False otherwise.\n+    loc (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*):\n+        Gives the mean of the context window per channel. Used for revin denorm outside the model, if revin\n+        enabled.\n+    scale (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*):\n+        Gives the std dev of the context window per channel. Used for revin denorm outside the model, if revin\n+        enabled.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -1343,19 +1347,21 @@ def forward(\n \n \n @dataclass\n-class PatchTSMixerForPreTrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSMixerForPreTrainingOutput`].\n-\n-    Args:\n-        prediction_outputs (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, patch_length)`):\n-            Prediction output from the pretrain head.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n-            Backbone embeddings before passing through the head.\n-        loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n-            Total loss\n+    \"\"\"\n+)\n+class PatchTSMixerForPreTrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n+        Total loss\n+    prediction_outputs (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, patch_length)`):\n+        Prediction output from the pretrain head.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n+        Backbone embeddings before passing through the head.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1364,18 +1370,12 @@ class PatchTSMixerForPreTrainingOutput(ModelOutput):\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n \n \n-class PatchTSMixerForPretraining(PatchTSMixerPreTrainedModel):\n-    r\"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     `PatchTSMixer` for mask pretraining.\n-\n-    Args:\n-        config (`PatchTSMixerConfig`):\n-            Configuration.\n-\n-    Returns:\n-        `None`.\n     \"\"\"\n-\n+)\n+class PatchTSMixerForPretraining(PatchTSMixerPreTrainedModel):\n     def __init__(self, config: PatchTSMixerConfig):\n         super().__init__(config)\n         self.model = PatchTSMixerModel(config, mask_input=True)\n@@ -1460,24 +1460,25 @@ def forward(\n \n \n @dataclass\n-class PatchTSMixerForPredictionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSMixerForPredictionOutput`].\n-\n-    Args:\n-        prediction_outputs (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_input_channels)`):\n-            Prediction output from the forecast head.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n-            Backbone embeddings before passing through the head.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n-            Total loss.\n-        loc (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`):\n-            Input mean\n-        scale (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`):\n-            Input std dev\n-\n+    \"\"\"\n+)\n+class PatchTSMixerForPredictionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n+        Total loss.\n+    prediction_outputs (`torch.FloatTensor` of shape `(batch_size, prediction_length, num_input_channels)`):\n+        Prediction output from the forecast head.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n+        Backbone embeddings before passing through the head.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+    loc (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`):\n+        Input mean\n+    scale (`torch.FloatTensor`, *optional* of shape `(batch_size, 1, num_input_channels)`):\n+        Input std dev\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1489,28 +1490,32 @@ class PatchTSMixerForPredictionOutput(ModelOutput):\n \n \n @dataclass\n-class SamplePatchTSMixerPredictionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for time series model's predictions outputs that contains the sampled values from the chosen\n     distribution.\n-\n-    Args:\n-        sequences (`torch.FloatTensor` of shape `(batch_size, num_samples, prediction_length, number_channels)`):\n-            Sampled values from the chosen distribution.\n+    \"\"\"\n+)\n+class SamplePatchTSMixerPredictionOutput(ModelOutput):\n+    r\"\"\"\n+    sequences (`torch.FloatTensor` of shape `(batch_size, num_samples, prediction_length, number_channels)`):\n+        Sampled values from the chosen distribution.\n     \"\"\"\n \n     sequences: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class SamplePatchTSMixerRegressionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for time series model's predictions outputs that contains the sampled values from the chosen\n     distribution.\n-\n-    Args:\n-        sequences (`torch.FloatTensor` of shape `(batch_size, num_samples, num_targets)`\n-                Sampled values from the chosen distribution.\n+    \"\"\"\n+)\n+class SamplePatchTSMixerRegressionOutput(ModelOutput):\n+    r\"\"\"\n+    sequences (`torch.FloatTensor` of shape `(batch_size, num_samples, prediction_length, number_channels)`):\n+        Sampled values from the chosen distribution.\n     \"\"\"\n \n     sequences: Optional[torch.FloatTensor] = None\n@@ -1764,19 +1769,21 @@ def generate(\n \n \n @dataclass\n-class PatchTSMixerForTimeSeriesClassificationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSMixerForTimeSeriesClassificationOutput`].\n-\n-    Args:\n-        prediction_outputs (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n-            Prediction output from the classification head.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n-            Backbone embeddings before passing through the head.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n-            Total loss.\n+    \"\"\"\n+)\n+class PatchTSMixerForTimeSeriesClassificationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n+        Total loss.\n+    prediction_outputs (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n+        Prediction output from the classification head.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n+        Backbone embeddings before passing through the head.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1896,19 +1903,21 @@ def forward(\n \n \n @dataclass\n-class PatchTSMixerForRegressionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSMixerForRegressionOutput`].\n-\n-    Args:\n-        regression_outputs (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n-            Prediction output from the regression head.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n-            Backbone embeddings before passing through the head.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n-            Total loss.\n+    \"\"\"\n+)\n+class PatchTSMixerForRegressionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `y` is provided, `torch.FloatTensor` of shape `()`):\n+        Total loss.\n+    regression_outputs (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n+        Prediction output from the regression head.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_input_channels, num_patches, d_model)`):\n+        Backbone embeddings before passing through the head.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*):\n+        Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1957,18 +1966,12 @@ def forward(self, inputs: torch.Tensor, loc: torch.Tensor, scale: torch.Tensor):\n         return inputs\n \n \n-class PatchTSMixerForRegression(PatchTSMixerPreTrainedModel):\n-    r\"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     `PatchTSMixer` for regression application.\n-\n-    Args:\n-        config (`PatchTSMixerConfig`):\n-            Configuration.\n-\n-    Returns:\n-        `None`.\n     \"\"\"\n-\n+)\n+class PatchTSMixerForRegression(PatchTSMixerPreTrainedModel):\n     def __init__(self, config: PatchTSMixerConfig):\n         super().__init__(config)\n "
        },
        {
            "sha": "60f877e8a700e9e6f228f73a1407ed30dacc7d07",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 79,
            "deletions": 105,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -772,25 +772,27 @@ def forward(\n \n \n @dataclass\n-class PatchTSTModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs, with potential hidden states.\n-\n-    Parameters:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, num_channels, height, width)`. Hidden-states of\n-            the model at the output of each layer plus the optional initial embedding outputs.\n-        mask: (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches)`, *optional*)\n-            Bool masked tensor indicating which patches are masked\n-        loc: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n-            Mean of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n-        scale: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n-            Std of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n-        patch_input (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n-            Patched input to the Transformer\n+    \"\"\"\n+)\n+class PatchTSTModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n+        Sequence of hidden-states at the output of the last layer of the model.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, num_channels, height, width)`. Hidden-states of\n+        the model at the output of each layer plus the optional initial embedding outputs.\n+    mask (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches)`, *optional*):\n+        Bool masked tensor indicating which patches are masked\n+    loc (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*):\n+        Mean of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n+    scale (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*):\n+        Std of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n+    patch_input (`torch.FloatTensor` of shape `(batch_size, num_channels, num_patches, patch_length)`):\n+        Patched input to the Transformer\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -803,26 +805,17 @@ class PatchTSTModelOutput(ModelOutput):\n \n \n @dataclass\n-class PatchTSTForPretrainingOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSTForPretraining`].\n-\n-    Parameters:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            MSE loss.\n-        prediction_outputs (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction outputs of the time series modeling heads.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class PatchTSTForPretrainingOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        MSE loss.\n+    prediction_output (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction outputs of the time series modeling heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -832,26 +825,17 @@ class PatchTSTForPretrainingOutput(ModelOutput):\n \n \n @dataclass\n-class PatchTSTForRegressionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSTForRegression`].\n-\n-    Parameters:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            MSE loss.\n-        regression_outputs (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n-            Regression outputs of the time series modeling heads.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class PatchTSTForRegressionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        MSE loss.\n+    regression_outputs (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n+        Regression outputs of the time series modeling heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -861,30 +845,27 @@ class PatchTSTForRegressionOutput(ModelOutput):\n \n \n @dataclass\n-class PatchTSTForPredictionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSTForPrediction`].\n-\n-    Parameters:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            MSE loss.\n-        prediction_outputs (`torch.FloatTensor` of shape `(batch_size, prediction_length, -1)`):\n-            Prediction outputs of the time series modeling heads.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        loc: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n-            Mean of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n-        scale: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n-            Std of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n+    \"\"\"\n+)\n+class PatchTSTForPredictionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        MSE loss.\n+    prediction_outputs (`torch.FloatTensor` of shape `(batch_size, prediction_length, -1)`):\n+        Prediction outputs of the time series modeling heads.\n+    attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n+    loc: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n+        Mean of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n+    scale: (`torch.FloatTensor` of shape `(batch_size, 1, num_channels)`, *optional*)\n+        Std of the input data (batch_size, sequence_length, num_channels) over the sequence_length\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -896,27 +877,18 @@ class PatchTSTForPredictionOutput(ModelOutput):\n \n \n @dataclass\n-class PatchTSTForClassificationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`PatchTSTForClassification`].\n-\n-    Parameters:\n-        loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n-            Total loss as the sum of the masked language modeling loss and the next sequence prediction\n-            (classification) loss.\n-        prediction_logits (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n-            Prediction scores of the PatchTST modeling head (scores before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class PatchTSTForClassificationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (*optional*, returned when `labels` is provided, `torch.FloatTensor` of shape `(1,)`):\n+        Total loss as the sum of the masked language modeling loss and the next sequence prediction\n+        (classification) loss.\n+    prediction_logits (`torch.FloatTensor` of shape `(batch_size, num_targets)`):\n+        Prediction scores of the PatchTST modeling head (scores before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -926,14 +898,16 @@ class PatchTSTForClassificationOutput(ModelOutput):\n \n \n @dataclass\n-class SamplePatchTSTOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for time series model's predictions outputs that contains the sampled values from the chosen\n     distribution.\n-\n-    Parameters:\n-        sequences `(batch_size, num_samples, prediction_length, num_targets)`):\n-                Sampled values from the chosen distribution.\n+    \"\"\"\n+)\n+class SamplePatchTSTOutput(ModelOutput):\n+    r\"\"\"\n+    sequences (`torch.FloatTensor` of shape `(batch_size, num_samples, prediction_length, num_targets)`):\n+        Sampled values from the chosen distribution.\n     \"\"\"\n \n     sequences: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "13f0ea27a6e47b764ef04178776bdc6257542f68",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -14,8 +14,8 @@\n # limitations under the License.\n \"\"\"PyTorch PEGASUS-X model.\"\"\"\n \n-import dataclasses\n import math\n+from dataclasses import dataclass\n from typing import Callable, Optional, Union\n \n import numpy as np\n@@ -42,12 +42,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    auto_docstring,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n from .configuration_pegasus_x import PegasusXConfig\n \n \n@@ -58,7 +53,7 @@\n logger = logging.get_logger(__name__)\n \n \n-@dataclasses.dataclass\n+@dataclass\n class DimensionInfo:\n     \"\"\"Wrapper for dimension info.\"\"\"\n "
        },
        {
            "sha": "bfa1fd456f52c80ccbb81f29df2b3504f851f2ab",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 39,
            "deletions": 73,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -45,27 +45,15 @@\n \n \n @dataclass\n-class PerceiverModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Perceiver base model's outputs, with potential hidden states, attentions and cross-attentions.\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class PerceiverModelOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None\n@@ -76,45 +64,33 @@ class PerceiverModelOutput(ModelOutput):\n \n \n @dataclass\n-class PerceiverDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Perceiver decoder outputs, with potential cross-attentions.\n-\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n-            Output of the basic decoder.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class PerceiverDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_labels)`):\n+        Output of the basic decoder.\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n @dataclass\n-class PerceiverMaskedLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Perceiver's masked language model outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Masked language modeling (MLM) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_latents,\n-            num_latents)`. Attentions weights after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class PerceiverMaskedLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Masked language modeling (MLM) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -125,28 +101,18 @@ class PerceiverMaskedLMOutput(ModelOutput):\n \n \n @dataclass\n-class PerceiverClassifierOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Perceiver's outputs of sequence/image classification models, optical flow and multimodal\n     autoencoding.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Classification (or regression if config.num_labels==1) loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n-            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class PerceiverClassifierOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Classification (or regression if config.num_labels==1) loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+        Classification (or regression if config.num_labels==1) scores (before SoftMax).\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1852,13 +1818,13 @@ def forward(\n         r\"\"\"\n         inputs (`torch.FloatTensor`):\n             Inputs to the perceiver. Can be anything: images, text, audio, video, etc.\n+        subsampled_output_points (`dict[str, torch.Tensor]`, *optional*):\n+            Dictionary of tensors used as queries for the decoder. The decoder maps these queries to the latent\n+            representation of the model. Used for subsampled decoding, e.g. when only decoding certain image patches.\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n             config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n             `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        subsampled_output_points (`dict[str, torch.Tensor]`, *optional*):\n-            Dictionary of tensors used as queries for the decoder. The decoder maps these queries to the latent\n-            representation of the model. Used for subsampled decoding, e.g. when only decoding certain image patches.\n \n         Examples:\n "
        },
        {
            "sha": "fdc248287a086b03bc4a0da8743cb50773aedf94",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 136,
            "deletions": 210,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -112,65 +112,41 @@ def compute_all_stream_relative_buckets(num_buckets, max_distance, position_ids)\n \n \n @dataclass\n-class ProphetNetSeq2SeqLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for sequence-to-sequence language models outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, config.vocab_size)`):\n-            Prediction scores of the main stream language modeling head (scores for each vocabulary token before\n-            SoftMax).\n-        logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n-            Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n-            SoftMax).\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n-            outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        decoder_ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n-            weighted average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to\n-            compute the weighted average in the\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, encoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, encoder_sequence_length)`. Attentions weights of the encoder, after the attention\n-            softmax, used to compute the weighted average in the self-attention heads.\n+    \"\"\"\n+)\n+class ProphetNetSeq2SeqLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, config.vocab_size)`):\n+        Prediction scores of the main stream language modeling head (scores for each vocabulary token before\n+        SoftMax).\n+    logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n+        Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n+        SoftMax).\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    decoder_ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n+\n+        Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n+        outputs.\n+    decoder_ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n+        decoder_sequence_length, decoder_sequence_length)`.\n+\n+        Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n+        weighted average in the self-attention heads.\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -197,67 +173,41 @@ def decoder_cross_attentions(self):\n \n \n @dataclass\n-class ProphetNetSeq2SeqModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model encoder's outputs that also contains : pre-computed hidden states that can speed up sequential\n     decoding.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, hidden_size)`):\n-            Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size,ngram * decoder_sequence_length, config.vocab_size)`, *optional*):\n-            Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.\n-        decoder_ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n-            outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        decoder_ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n-            weighted average in the\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to\n-            compute the weighted average in the\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, encoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, encoder_sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n+    \"\"\"\n+)\n+class ProphetNetSeq2SeqModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, hidden_size)`):\n+        Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size,ngram * decoder_sequence_length, config.vocab_size)`, *optional*):\n+        Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    decoder_ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n+\n+        Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n+        outputs.\n+    decoder_ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n+        decoder_sequence_length, decoder_sequence_length)`.\n+\n+        Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n+        weighted average in the\n+    encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, encoder_sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the encoder of the model.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -283,53 +233,38 @@ def decoder_cross_attentions(self):\n \n \n @dataclass\n-class ProphetNetDecoderModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, hidden_size)`):\n-            Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n-            Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.\n-        ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n-            outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n-            weighted average in the\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to\n-            compute the weighted average in the\n+    \"\"\"\n+)\n+class ProphetNetDecoderModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, hidden_size)`):\n+        Sequence of main stream hidden-states at the output of the last layer of the decoder of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    last_hidden_state_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n+        Sequence of predict stream hidden-states at the output of the last layer of the decoder of the model.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    hidden_states_ngram (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n+\n+        Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n+        outputs.\n+    ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n+        decoder_sequence_length, decoder_sequence_length)`.\n+\n+        Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n+        weighted average in the\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -343,54 +278,45 @@ class ProphetNetDecoderModelOutput(ModelOutput):\n \n \n @dataclass\n-class ProphetNetDecoderLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for model's outputs that may also contain a past key/values (to speed up sequential decoding).\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, config.vocab_size)`):\n-            Prediction scores of the main stream language modeling head (scores for each vocabulary token before\n-            SoftMax).\n-        logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n-            Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n-            SoftMax).\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n-\n-            Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n-            used (see `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of main stream of the decoder at the output of each layer plus the initial embedding outputs.\n-        ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n-\n-            Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n-            outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            decoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n-            weighted average in the\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n-            encoder_sequence_length, decoder_sequence_length)`.\n-\n-            Attentions weights of the cross-attention layer of the decoder, after the attention softmax, used to\n-            compute the weighted average in the\n+    \"\"\"\n+)\n+class ProphetNetDecoderLMOutput(ModelOutput):\n+    r\"\"\"\n+    ngram_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n+\n+        Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n+        outputs.\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, decoder_sequence_length, config.vocab_size)`):\n+        Prediction scores of the main stream language modeling head (scores for each vocabulary token before\n+        SoftMax).\n+    logits_ngram (`torch.FloatTensor` of shape `(batch_size, ngram * decoder_sequence_length, config.vocab_size)`):\n+        Prediction scores of the predict stream language modeling head (scores for each vocabulary token before\n+        SoftMax).\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_attn_heads, decoder_sequence_length, embed_size_per_head)`).\n+\n+        Contains pre-computed hidden-states (key and values in the attention blocks) of the decoder that can be\n+        used (see `past_key_values` input) to speed up sequential decoding.\n+    hidden_states_ngram (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n+        shape `(batch_size, ngram * decoder_sequence_length, hidden_size)`.\n+\n+        Hidden-states of the predict stream of the decoder at the output of each layer plus the initial embedding\n+        outputs.\n+    ngram_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_attn_heads,\n+        decoder_sequence_length, decoder_sequence_length)`.\n+\n+        Attentions weights of the predict stream of the decoder, after the attention softmax, used to compute the\n+        weighted average in the\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "880f209cc21ab86726492e1f4a95edbff0f37b32",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 45,
            "deletions": 60,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -530,34 +530,25 @@ def get_rope_index(\n \n \n @dataclass\n-class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2.5OmniThinker causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -2151,34 +2142,28 @@ def prepare_inputs_for_generation(\n \n \n @dataclass\n-class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2.5OmniTalker causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Hidden states from the thinker model that are used as input for the talker model. These represent the encoded\n+        response that the talker model will use to generate speech tokens.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -2396,20 +2381,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, Qwen2_5OmniTalkerCausalLMOutputWithPast]:\n         r\"\"\"\n+        thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Hidden states from the thinker model's output that represent the text reply part to be processed.\n         rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n             The rope index difference between sequence length and multimodal rope.\n+        input_text_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Input token IDs for text-only content, used for position calculation in multimodal contexts.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n-        audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n-            The length of feature shape of each audio in LLM.\n-        thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Hidden states from the thinker model's output that represent the text reply part to be processed.\n-        input_text_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Input token IDs for text-only content, used for position calculation in multimodal contexts.\n         use_audio_in_video (`bool`, *optional*):\n             Whether or not use audio track in video, should same as the parameter in `process_audio_info`.\n+        audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n+            The length of feature shape of each audio in LLM.\n         video_second_per_grid (`torch.LongTensor` of shape `(num_videos)`, *optional*):\n             Number of seconds per grid for each video, used for temporal feature mapping.\n "
        },
        {
            "sha": "a6e330845cc193c83cd500a47b252c1ecfacbea2",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 45,
            "deletions": 60,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1560,34 +1560,25 @@ def get_rope_index(\n \n \n @dataclass\n-class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2.5OmniThinker causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -2586,34 +2577,28 @@ def prepare_inputs_for_generation(\n \n \n @dataclass\n-class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2.5OmniTalker causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n+    thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Hidden states from the thinker model that are used as input for the talker model. These represent the encoded\n+        response that the talker model will use to generate speech tokens.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -2690,20 +2675,20 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple, Qwen2_5OmniTalkerCausalLMOutputWithPast]:\n         r\"\"\"\n+        thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Hidden states from the thinker model's output that represent the text reply part to be processed.\n         rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n             The rope index difference between sequence length and multimodal rope.\n+        input_text_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Input token IDs for text-only content, used for position calculation in multimodal contexts.\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         video_grid_thw (`torch.LongTensor` of shape `(num_videos, 3)`, *optional*):\n             The temporal, height and width of feature shape of each video in LLM.\n-        audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n-            The length of feature shape of each audio in LLM.\n-        thinker_reply_part (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Hidden states from the thinker model's output that represent the text reply part to be processed.\n-        input_text_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Input token IDs for text-only content, used for position calculation in multimodal contexts.\n         use_audio_in_video (`bool`, *optional*):\n             Whether or not use audio track in video, should same as the parameter in `process_audio_info`.\n+        audio_feature_lengths (`torch.LongTensor` of shape `(num_audios)`, *optional*):\n+            The length of feature shape of each audio in LLM.\n         video_second_per_grid (`torch.LongTensor` of shape `(num_videos)`, *optional*):\n             Number of seconds per grid for each video, used for temporal feature mapping.\n "
        },
        {
            "sha": "0bcbf1cb5061d722acc254e4a52217183c680d3f",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 52,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -526,32 +526,21 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n \n \n @dataclass\n-class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor = None\n@@ -1421,34 +1410,25 @@ def forward(\n \n \n @dataclass\n-class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2_5_VL causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "81f3dddf4ac842ae2932e8f8f103ab9649d4a9c8",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -19,7 +19,6 @@\n # limitations under the License.\n \"\"\"PyTorch Qwen2.5-VL model.\"\"\"\n \n-from dataclasses import dataclass\n from typing import Optional, Union\n \n import numpy as np\n@@ -405,7 +404,6 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         return hidden_states\n \n \n-@dataclass\n class Qwen2_5_VLModelOutputWithPast(Qwen2VLModelOutputWithPast):\n     pass\n \n@@ -749,7 +747,6 @@ def forward(\n         return output if return_dict else output.to_tuple()\n \n \n-@dataclass\n class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast):\n     pass\n "
        },
        {
            "sha": "f2923ba06976f6c450e61939c55cb3c2e85678e7",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -23,6 +23,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\n from typing import Optional, Union\n \n import numpy as np"
        },
        {
            "sha": "45fcbe804957137aab29003ac95904e017790668",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 21,
            "deletions": 30,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -37,37 +37,28 @@\n \n \n @dataclass\n-class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2Audio causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are\n-            two sets of pre-computed hidden-states: key and values states in the self-attention blocks.\n-            The `past_key_values` are returned when `use_cache=True` is passed or when `config.use_cache=True`.\n-            It is a [`~cache_utils.Cache`] instance.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those\n-            that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n-            all `input_ids` of shape `(batch_size, sequence_length)`.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        attention_mask (`torch.FloatTensor`, *optional*):\n-            Attentions mask, used to update attention mask and position_ids.\n+    \"\"\"\n+)\n+class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Pre-computed hidden-states that can be used to speed up auto-regressive (sequential) decoding. There are\n+        two sets of pre-computed hidden-states: key and values states in the self-attention blocks.\n+        The `past_key_values` are returned when `use_cache=True` is passed or when `config.use_cache=True`.\n+        It is a [`~cache_utils.Cache`] instance.\n+\n+        If `past_key_values` are used, the user can optionally input only the last `input_ids` (those\n+        that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n+        all `input_ids` of shape `(batch_size, sequence_length)`.\n+    attention_mask (`torch.FloatTensor`, *optional*):\n+        Attentions mask, used to update attention mask and position_ids.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "7f3ad466344740bdeb979d19b44770210755975d",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1401,17 +1401,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         **kwargs,\n     ) -> QuestionAnsweringModelOutput:\n-        r\"\"\"\n-        start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the start of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for position (index) of the end of the labelled span for computing the token classification loss.\n-            Positions are clamped to the length of the sequence (`sequence_length`). Position outside of the sequence\n-            are not taken into account for computing the loss.\n-        \"\"\"\n-\n         outputs: MoeModelOutputWithPast = self.model(\n             input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "f6d43308145bebbc8c3af011eea10e3f483bf195",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 32,
            "deletions": 52,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -57,32 +57,21 @@\n \n \n @dataclass\n-class Qwen2VLModelOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Llava outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2VLModelOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor = None\n@@ -93,34 +82,25 @@ class Qwen2VLModelOutputWithPast(ModelOutput):\n \n \n @dataclass\n-class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Qwen2VL causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+)\n+class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+        The rope index difference between sequence length and multimodal rope.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "5d13990f4830ac314dbff6895f8705eedfe3a1d0",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 143,
            "deletions": 140,
            "changes": 283,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -34,79 +34,81 @@\n \n \n @dataclass\n-class RetrievAugLMMarginOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for retriever augmented marginalized models outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head. The score is possibly marginalized over all documents for\n-            each vocabulary token.\n-        doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n-            Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n-            `question_encoder_last_hidden_state`.\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n-            (see `past_key_values` input) to speed up sequential decoding.\n-        retrieved_doc_embeds (`torch.FloatTensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, returned when *output_retrieved=True*):\n-            Embedded documents retrieved by the retriever. Is used with `question_encoder_last_hidden_state` to compute\n-            the `doc_scores`.\n-        retrieved_doc_ids (`torch.LongTensor` of shape `(batch_size, config.n_docs)`, *optional*, returned when *output_retrieved=True*):\n-            The indexes of the embedded documents retrieved by the retriever.\n-        context_input_ids (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n-            Input ids post-processed from the retrieved documents and the question encoder input_ids by the retriever.\n-        context_attention_mask (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n-            Attention mask post-processed from the retrieved documents and the question encoder `input_ids` by the\n-            retriever.\n-        question_encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\n-            model.\n-        question_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the question encoder at the output of each layer plus the initial embedding outputs.\n-        question_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the question encoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_enc_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\n-        generator_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the generator encoder at the output of each layer plus the initial embedding outputs.\n-        generator_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the generator encoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_dec_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the generator decoder at the output of each layer plus the initial embedding outputs.\n-        generator_dec_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the generator decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Cross-attentions weights of the generator decoder, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class RetrievAugLMMarginOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head. The score is possibly marginalized over all documents for\n+        each vocabulary token.\n+    doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n+        Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n+        `question_encoder_last_hidden_state`.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n+        (see `past_key_values` input) to speed up sequential decoding.\n+    retrieved_doc_embeds (`torch.FloatTensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, returned when *output_retrieved=True*):\n+        Embedded documents retrieved by the retriever. Is used with `question_encoder_last_hidden_state` to compute\n+        the `doc_scores`.\n+    retrieved_doc_ids (`torch.LongTensor` of shape `(batch_size, config.n_docs)`, *optional*, returned when *output_retrieved=True*):\n+        The indexes of the embedded documents retrieved by the retriever.\n+    context_input_ids (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n+        Input ids post-processed from the retrieved documents and the question encoder input_ids by the retriever.\n+    context_attention_mask (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n+        Attention mask post-processed from the retrieved documents and the question encoder `input_ids` by the\n+        retriever.\n+    question_encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\n+        model.\n+    question_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the question encoder at the output of each layer plus the initial embedding outputs.\n+    question_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the question encoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_enc_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\n+    generator_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the generator encoder at the output of each layer plus the initial embedding outputs.\n+    generator_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the generator encoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_dec_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the generator decoder at the output of each layer plus the initial embedding outputs.\n+    generator_dec_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the generator decoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Cross-attentions weights of the generator decoder, after the attention softmax, used to compute the\n+        weighted average in the cross-attention heads.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -129,75 +131,75 @@ class RetrievAugLMMarginOutput(ModelOutput):\n \n \n @dataclass\n+@auto_docstring\n class RetrievAugLMOutput(ModelOutput):\n-    \"\"\"\n-    Args:\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head. The score is possibly marginalized over all documents for\n-            each vocabulary token.\n-        doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n-            Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n-            `question_encoder_last_hidden_state`.\n-        past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n-            num_heads, sequence_length, embed_size_per_head)`).\n-\n-            Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n-            (see `past_key_values` input) to speed up sequential decoding.\n-        retrieved_doc_embeds (`torch.FloatTensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, returned when *output_retrieved=True*):\n-            Embedded documents retrieved by the retriever. Is used with `question_encoder_last_hidden_state` to compute\n-            the `doc_scores`.\n-        retrieved_doc_ids (`torch.LongTensor` of shape `(batch_size, config.n_docs)`, *optional*, returned when *output_retrieved=True*):\n-            The indexes of the embedded documents retrieved by the retriever.\n-        context_input_ids (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n-            Input ids post-processed from the retrieved documents and the question encoder input_ids by the retriever.\n-        context_attention_mask (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n-            Attention mask post-processed from the retrieved documents and the question encoder `input_ids` by the\n-            retriever.\n-        question_encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\n-            model.\n-        question_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the question encoder at the output of each layer plus the initial embedding outputs.\n-        question_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the question encoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_enc_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\n-        generator_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the generator encoder at the output of each layer plus the initial embedding outputs.\n-        generator_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the generator encoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_dec_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden states of the generator decoder at the output of each layer plus the initial embedding outputs.\n-        generator_dec_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the generator decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        generator_cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Cross-attentions weights of the generator decoder, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n+    r\"\"\"\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head. The score is possibly marginalized over all documents for\n+        each vocabulary token.\n+    doc_scores (`torch.FloatTensor` of shape `(batch_size, config.n_docs)`):\n+        Score between each retrieved document embeddings (see `retrieved_doc_embeds`) and\n+        `question_encoder_last_hidden_state`.\n+    past_key_values (`list[torch.FloatTensor]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `torch.FloatTensor` of length `config.n_layers`, with each tensor of shape `(2, batch_size,\n+        num_heads, sequence_length, embed_size_per_head)`).\n+\n+        Contains precomputed hidden-states (key and values in the attention blocks) of the decoder that can be used\n+        (see `past_key_values` input) to speed up sequential decoding.\n+    retrieved_doc_embeds (`torch.FloatTensor` of shape `(batch_size, config.n_docs, hidden_size)`, *optional*, returned when *output_retrieved=True*):\n+        Embedded documents retrieved by the retriever. Is used with `question_encoder_last_hidden_state` to compute\n+        the `doc_scores`.\n+    retrieved_doc_ids (`torch.LongTensor` of shape `(batch_size, config.n_docs)`, *optional*, returned when *output_retrieved=True*):\n+        The indexes of the embedded documents retrieved by the retriever.\n+    context_input_ids (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n+        Input ids post-processed from the retrieved documents and the question encoder input_ids by the retriever.\n+    context_attention_mask (`torch.LongTensor` of shape `(batch_size * config.n_docs, config.max_combined_length)`, *optional*, returned when *output_retrieved=True*):\n+        Attention mask post-processed from the retrieved documents and the question encoder `input_ids` by the\n+        retriever.\n+    question_encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden states at the output of the last layer of the question encoder pooled output of the\n+        model.\n+    question_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the question encoder at the output of each layer plus the initial embedding outputs.\n+    question_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the question encoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_enc_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+        Sequence of hidden-states at the output of the last layer of the generator encoder of the model.\n+    generator_enc_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the generator encoder at the output of each layer plus the initial embedding outputs.\n+    generator_enc_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the generator encoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_dec_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n+        shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden states of the generator decoder at the output of each layer plus the initial embedding outputs.\n+    generator_dec_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights of the generator decoder, after the attention softmax, used to compute the weighted\n+        average in the self-attention heads.\n+    generator_cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Cross-attentions weights of the generator decoder, after the attention softmax, used to compute the\n+        weighted average in the cross-attention heads.\n     \"\"\"\n \n     logits: Optional[torch.FloatTensor] = None\n@@ -227,6 +229,7 @@ class RetrievAugLMOutput(ModelOutput):\n     generator, the encoder and generator are trainable while the retriever is just an indexed dataset.\n     \"\"\"\n )\n+@auto_docstring\n class RagPreTrainedModel(PreTrainedModel):\n     config_class = RagConfig\n     base_model_prefix = \"rag\""
        },
        {
            "sha": "b9915efd1e53cd88f7a5eb49877182ada6b5cc67",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 42,
            "deletions": 60,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1805,34 +1805,25 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class ReformerModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`ReformerModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_predict, hidden_size)`):\n-            Sequence of hidden-states at the last layer of the model.\n-\n-            `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict`\n-            corresponds to `sequence_length`.\n-        past_buckets_states (`list[Tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n-            being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n-            second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n-\n-            Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states` input) to speed\n-            up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class ReformerModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_predict, hidden_size)`):\n+        Sequence of hidden-states at the last layer of the model.\n+\n+        `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict`\n+        corresponds to `sequence_length`.\n+    past_buckets_states (`list[tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n+        being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n+        second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n+\n+        Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states` input) to speed\n+        up sequential decoding.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n@@ -1842,36 +1833,27 @@ class ReformerModelOutput(ModelOutput):\n \n \n @dataclass\n-class ReformerModelWithLMHeadOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`ReformerModelWithLMHead`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided)\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-\n-            `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict`\n-            corresponds to `sequence_length`.\n-        past_buckets_states (`list[Tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n-            being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n-            second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n-\n-            Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states` input) to speed\n-            up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            TTuple of `torch.FloatTensor` (one for the output of the embeddings and one for the output of each layer)\n-            of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class ReformerModelWithLMHeadOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape *(1,)*, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_predict, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+\n+        `num_predict` corresponds to `target_mapping.shape[1]`. If `target_mapping` is `None`, then `num_predict`\n+        corresponds to `sequence_length`.\n+    past_buckets_states (`list[tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        List of `tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n+        being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n+        second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n+\n+        Contains precomputed buckets and hidden-states that can be used (see `past_buckets_states` input) to speed\n+        up sequential decoding.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -1940,8 +1922,8 @@ def forward(\n             the default defined in `config.num_hashes`.\n \n             For more information, see `num_hashes` in [`ReformerConfig`].\n-        past_buckets_states (`list[Tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*):\n-            List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n+        past_buckets_states (`list[tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*):\n+            List of `tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n             being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n             second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n \n@@ -2176,8 +2158,8 @@ def forward(\n             the default defined in `config.num_hashes`.\n \n             For more information, see `num_hashes` in [`ReformerConfig`].\n-        past_buckets_states (`list[Tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*):\n-            List of `Tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n+        past_buckets_states (`list[tuple(torch.LongTensor, torch.FloatTensor)]`, *optional*):\n+            List of `tuple(torch.LongTensor, torch.FloatTensor` of length `config.n_layers`, with the first element\n             being the previous *buckets* of shape `(batch_size, num_heads, num_hashes, sequence_length)`) and the\n             second being the previous *hidden_states* of shape `(batch_size, sequence_length, hidden_size)`).\n "
        },
        {
            "sha": "0a2e74e579df8809295a76e38a95be5c215621fd",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -1023,6 +1023,12 @@ def forward(\n             attack sample pronunciation ids for computing the contrastive loss. Indices should be in `[-100, 0,\n             ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored\n             (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`\n+        attack_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices for the attack sample. Mask values selected in\n+            `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED tokens.\n+        attack_token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Segment token indices to indicate different portions of the attack inputs. Indices are selected in `[0, 1]`:\n+            `0` corresponds to a sentence A token, `1` corresponds to a sentence B token.\n         labels_input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             target ids for computing the contrastive loss and masked_lm_loss . Indices should be in `[-100, 0, ...,\n             config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are ignored (masked),\n@@ -1036,12 +1042,6 @@ def forward(\n             `[-100, 0, ..., config.vocab_size]` (see `input_ids` docstring) Tokens with indices set to `-100` are\n             ignored (masked), the loss is only computed for the tokens with labels in `[0, ...,\n             config.vocab_size]`\n-        attack_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices for the attack sample. Mask values selected in\n-            `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED tokens.\n-        attack_token_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Segment token indices to indicate different portions of the attack inputs. Indices are selected in `[0, 1]`:\n-            `0` corresponds to a sentence A token, `1` corresponds to a sentence B token.\n         labels_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices for the label sample. Mask values selected in\n             `[0, 1]`: `1` for tokens that are NOT MASKED, `0` for MASKED tokens."
        },
        {
            "sha": "3fdbefe89634b6df5b7ac8ffa56512bf862c5259",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 103,
            "deletions": 147,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -103,38 +103,30 @@ def forward(\n \n \n @dataclass\n-class RTDetrDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the RTDetrDecoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class RTDetrDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -149,57 +141,41 @@ class RTDetrDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class RTDetrModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the RT-DETR encoder-decoder model.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n-            Logits of predicted bounding boxes coordinates in the encoder stage.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class RTDetrModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points used for the first decoder layer.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n+        Logits of predicted bounding boxes coordinates in the encoder stage.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -223,76 +199,56 @@ class RTDetrModelOutput(ModelOutput):\n \n \n @dataclass\n-class RTDetrObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`RTDetrForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~RTDetrImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized (absolute) bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class RTDetrObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~RTDetrImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized (absolute) bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "9d659dd49e8f9bf7778f6814b33f24703581b5df",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 103,
            "deletions": 147,
            "changes": 250,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -446,38 +446,30 @@ def forward(\n \n \n @dataclass\n-class RTDetrV2DecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the RTDetrV2Decoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class RTDetrV2DecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -492,57 +484,41 @@ class RTDetrV2DecoderOutput(ModelOutput):\n \n \n @dataclass\n-class RTDetrV2ModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the RT-DETR encoder-decoder model.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n-            Logits of predicted bounding boxes coordinates in the encoder stage.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class RTDetrV2ModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points used for the first decoder layer.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n+        Logits of predicted bounding boxes coordinates in the encoder stage.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -566,76 +542,56 @@ class RTDetrV2ModelOutput(ModelOutput):\n \n \n @dataclass\n-class RTDetrV2ObjectDetectionOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Output type of [`RTDetrV2ForObjectDetection`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-            Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-            bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-            scale-invariant IoU loss.\n-        loss_dict (`Dict`, *optional*):\n-            A dictionary containing the individual losses. Useful for logging.\n-        logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-            Classification logits (including no-object) for all queries.\n-        pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-            Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-            values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the\n-            unnormalized (absolute) bounding boxes.\n-        auxiliary_outputs (`list[Dict]`, *optional*):\n-            Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-            and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-            `pred_boxes`) for each decoder layer.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n-            Stacked intermediate logits (logits of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-        initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked initial reference points (initial reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the encoder.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n-        denoising_meta_values (`dict`):\n-            Extra dictionary for the denoising related values\n+    \"\"\"\n+)\n+class RTDetrV2ObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized (absolute) bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "81b529175cf66143181f9cf7fab7e056c2f1508a",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 22,
            "deletions": 42,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -447,27 +447,16 @@ def _init_weights(self, module):\n \n \n @dataclass\n-class RwkvOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class for the RWKV model outputs.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class RwkvOutput(ModelOutput):\n+    r\"\"\"\n+    state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -477,29 +466,20 @@ class RwkvOutput(ModelOutput):\n \n \n @dataclass\n-class RwkvCausalLMOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n-            The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n-            avoid providing the old `input_ids`.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class RwkvCausalLMOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    state (list of five `torch.FloatTensor` of shape `(batch_size, hidden_size, num_hidden_layers)`):\n+        The state of the model at the last time step. Can be used in a forward method with the next `input_ids` to\n+        avoid providing the old `input_ids`.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "0aa42eeb99407801133fd3d0a5e0815cbe91600f",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 35,
            "deletions": 44,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -41,27 +41,16 @@\n \n \n @dataclass\n-class SamVisionEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for sam vision model's outputs that also contains image embeddings obtained by applying the projection\n     layer to the pooler_output.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class SamVisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -71,32 +60,34 @@ class SamVisionEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class SamImageSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Segment-Anything model's output\n-\n-    Args:\n-        iou_scores (`torch.FloatTensor` of shape `(batch_size, num_masks)`):\n-            The iou scores of the predicted masks.\n-        pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n-            The predicted low resolutions masks. Needs to be post-processed by the processor\n-        vision_hidden_states  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.\n-        vision_attentions  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class SamImageSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    iou_scores (`torch.FloatTensor` of shape `(batch_size, num_masks)`):\n+        The iou scores of the predicted masks.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n+        The predicted low resolutions masks. Needs to be post-processed by the processor\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n+    mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n     \"\"\"\n \n     iou_scores: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "b5f896b62b6e183b35be48bda0f0a353132f4cd0",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 39,
            "deletions": 49,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -40,32 +40,20 @@\n \n \n @dataclass\n-class SamHQVisionEncoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for sam_hq vision model's outputs that also contains image embeddings obtained by applying the projection\n     layer to the pooler_output.\n-\n-    Args:\n-        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n-            The image embeddings obtained by applying the projection layer to the pooler_output.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-\n-        intermediate_embeddings (`list(torch.FloatTensor)`, *optional*):\n-            A list of intermediate embeddings collected from certain blocks within the model, typically those without\n-            windowed attention. Each element in the list is of shape `(batch_size, sequence_length, hidden_size)`.\n-            This is specific to SAM-HQ and not present in base SAM.\n+    \"\"\"\n+)\n+class SamHQVisionEncoderOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n+    intermediate_embeddings (`list(torch.FloatTensor)`, *optional*):\n+        A list of intermediate embeddings collected from certain blocks within the model, typically those without\n+        windowed attention. Each element in the list is of shape `(batch_size, sequence_length, hidden_size)`.\n+        This is specific to SAM-HQ and not present in base SAM.\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n@@ -77,32 +65,34 @@ class SamHQVisionEncoderOutput(ModelOutput):\n \n \n @dataclass\n-class SamHQImageSegmentationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for Segment-Anything model's output\n-\n-    Args:\n-        iou_scores (`torch.FloatTensor` of shape `(batch_size, num_masks)`):\n-            The iou scores of the predicted masks.\n-        pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n-            The predicted low resolutions masks. Needs to be post-processed by the processor\n-        vision_hidden_states  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.\n-        vision_attentions  (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n+    \"\"\"\n+)\n+class SamHQImageSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    iou_scores (`torch.FloatTensor` of shape `(batch_size, num_masks)`):\n+        The iou scores of the predicted masks.\n+    pred_masks (`torch.FloatTensor` of shape `(batch_size, num_masks, height, width)`):\n+        The predicted low resolutions masks. Needs to be post-processed by the processor\n+    vision_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+        one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+        Hidden-states of the vision model at the output of each layer plus the optional initial embedding outputs.\n+    vision_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n+    mask_decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`.\n+\n+        Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+        heads.\n     \"\"\"\n \n     iou_scores: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "9e844fa9b019c017c568095053ff3543396ad329",
            "filename": "src/transformers/models/sam_hq/modular_sam_hq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodular_sam_hq.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -13,7 +13,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from dataclasses import dataclass\n from typing import Optional, Union\n \n import torch\n@@ -109,9 +108,10 @@ class SamHQConfig(SamConfig):\n     pass\n \n \n-@dataclass\n class SamHQVisionEncoderOutput(SamVisionEncoderOutput):\n-    \"\"\"\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n     intermediate_embeddings (`list(torch.FloatTensor)`, *optional*):\n         A list of intermediate embeddings collected from certain blocks within the model, typically those without\n         windowed attention. Each element in the list is of shape `(batch_size, sequence_length, hidden_size)`.\n@@ -121,7 +121,6 @@ class SamHQVisionEncoderOutput(SamVisionEncoderOutput):\n     intermediate_embeddings: Optional[list[torch.FloatTensor]] = None\n \n \n-@dataclass\n class SamHQImageSegmentationOutput(SamImageSegmentationOutput):\n     pass\n "
        },
        {
            "sha": "21403244ee688bf8e4c6d25b7a59721ab6689311",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 18,
            "deletions": 16,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -80,24 +80,26 @@\n \n \n @dataclass\n-class SeamlessM4TGenerationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class defining the generated outputs from [`SeamlessM4TModel`], [`SeamlessM4TForTextToText`],\n     [`SeamlessM4TForTextToSpeech`], [`SeamlessM4TForSpeechToSpeech`] and [`SeamlessM4TForTextToSpeech`].\n-\n-    Args:\n-        waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            The final audio waveform predicted by the model.\n-        waveform_lengths (`torch.IntTensor` of shape `(batch_size,)`, *optional*):\n-            The length in samples of each element in the `waveform` batch.\n-        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.\n-            The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished\n-            early due to the `eos_token_id`.\n-        unit_sequences (`torch.LongTensor` of shape `(batch_size, unit_sequence_length)`, *optional*):\n-            The generated translated unit sequences. This is the output of the text-to-units model. The second\n-            dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished\n-            early due to the `t2u_eos_token_id`.\n+    \"\"\"\n+)\n+class SeamlessM4TGenerationOutput(ModelOutput):\n+    r\"\"\"\n+    waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+        The final audio waveform predicted by the model.\n+    waveform_lengths (`torch.IntTensor` of shape `(batch_size,)`, *optional*):\n+        The length in samples of each element in the `waveform` batch.\n+    sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.\n+        The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished\n+        early due to the `eos_token_id`.\n+    unit_sequences (`torch.LongTensor` of shape `(batch_size, unit_sequence_length)`, *optional*):\n+        The generated translated unit sequences. This is the output of the text-to-units model. The second\n+        dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished\n+        early due to the `t2u_eos_token_id`.\n     \"\"\"\n \n     waveform: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "2210e1426dd261589de4af7cc3ff73a17009bc06",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 45,
            "deletions": 76,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9",
            "patch": "@@ -77,25 +77,27 @@\n \n \n @dataclass\n-# Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput with SeamlessM4T->SeamlessM4Tv2\n-class SeamlessM4Tv2GenerationOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class defining the generated outputs from [`SeamlessM4Tv2Model`], [`SeamlessM4Tv2ForTextToText`],\n     [`SeamlessM4Tv2ForTextToSpeech`], [`SeamlessM4Tv2ForSpeechToSpeech`] and [`SeamlessM4Tv2ForTextToSpeech`].\n-\n-    Args:\n-        waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n-            The final audio waveform predicted by the model.\n-        waveform_lengths (`torch.IntTensor` of shape `(batch_size,)`, *optional*):\n-            The length in samples of each element in the `waveform` batch.\n-        sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.\n-            The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished\n-            early due to the `eos_token_id`.\n-        unit_sequences (`torch.LongTensor` of shape `(batch_size, unit_sequence_length)`, *optional*):\n-            The generated translated unit sequences. This is the output of the text-to-units model. The second\n-            dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished\n-            early due to the `t2u_eos_token_id`.\n+    \"\"\"\n+)\n+# Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TGenerationOutput with SeamlessM4T->SeamlessM4Tv2\n+class SeamlessM4Tv2GenerationOutput(ModelOutput):\n+    r\"\"\"\n+    waveform (`torch.FloatTensor` of shape `(batch_size, sequence_length)`):\n+        The final audio waveform predicted by the model.\n+    waveform_lengths (`torch.IntTensor` of shape `(batch_size,)`, *optional*):\n+        The length in samples of each element in the `waveform` batch.\n+    sequences (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        The generated translated sequences. This is the output of the text-to-text or the speech-to-text models.\n+        The second dimension (sequence_length) is either equal to `max_length` or shorter if all batches finished\n+        early due to the `eos_token_id`.\n+    unit_sequences (`torch.LongTensor` of shape `(batch_size, unit_sequence_length)`, *optional*):\n+        The generated translated unit sequences. This is the output of the text-to-units model. The second\n+        dimension (unit_sequence_length) is either equal to `t2u_max_length` or shorter if all batches finished\n+        early due to the `t2u_eos_token_id`.\n     \"\"\"\n \n     waveform: Optional[torch.FloatTensor] = None\n@@ -105,27 +107,16 @@ class SeamlessM4Tv2GenerationOutput(ModelOutput):\n \n \n @dataclass\n-class SeamlessM4Tv2TextToUnitDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Class defining the outputs from [`SeamlessM4Tv2TextToUnitDecoder`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        padding_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0\n-            for *masked*\n+    \"\"\"\n+)\n+class SeamlessM4Tv2TextToUnitDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    padding_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0\n+        for *masked*\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -135,46 +126,24 @@ class SeamlessM4Tv2TextToUnitDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class SeamlessM4Tv2TextToUnitOutput(ModelOutput):\n-    \"\"\"\n-        Class defining the outputs from [`SeamlessM4Tv2TextToUnitForConditionalGeneration`] and\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class defining the outputs from [`SeamlessM4Tv2TextToUnitForConditionalGeneration`] and\n         [`SeamlessM4Tv2TextToUnitModel`].\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-\n-            If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n-            hidden_size)` is output.\n-        padding_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0\n-            for *masked*\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the decoder at the output of each layer plus the optional initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the encoder at the output of each layer plus the optional initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss.\n+    \"\"\"\n+)\n+class SeamlessM4Tv2TextToUnitOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+\n+        If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n+        hidden_size)` is output.\n+    padding_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        Indicates which inputs are to be ignored due to padding, where elements are either 1 for *not masked* or 0\n+        for *masked*\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "80a51fb55650f4b7f77bc448b5267692201a833b",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "e56d5bfc89a376754135b87a1e413b908fd457d1",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 34,
            "deletions": 56,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "bb147b1ce2c14a43e9594ecaf410653e45d6e378",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 34,
            "deletions": 56,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "f775c371c3d170b1cd581c0f9a9e4658b6e5b231",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 41,
            "deletions": 54,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "259272f445ebd7d9fe3fc9b994aef787dd59e62f",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 12,
            "deletions": 21,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "7bcf8d98251c3a2901713750b7d4da1f9a0845b3",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 23,
            "deletions": 21,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "b80e5fa02bf938bb8d777bebe26a3356f4f6e2b8",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 21,
            "deletions": 19,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "7ea56890b58b1c37c86689ea4562bf3a0002da83",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 57,
            "deletions": 97,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "4b16bc954dc4db4b40a0b174694a248a9e70210b",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 18,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "d18c126fe4b262e543a21985d5199d3aa4c0637b",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 57,
            "deletions": 97,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "2f0eb777c0098906951527fc38fac11816e67769",
            "filename": "src/transformers/models/table_transformer/modeling_table_transformer.py",
            "status": "modified",
            "additions": 51,
            "deletions": 99,
            "changes": 150,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fmodeling_table_transformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "2629c82d78d14dc3048534146ade9b7961ec7096",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 13,
            "deletions": 19,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "39202068c733882d672f7ab1aed0ddd9e42bdb0e",
            "filename": "src/transformers/models/timesfm/modeling_timesfm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodeling_timesfm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "5c8f4f0d3313f5e37e991ae5e2f35bd94e08502b",
            "filename": "src/transformers/models/timesfm/modular_timesfm.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fmodular_timesfm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "f36eb5382b5a346a809a06d79e29357238593824",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 15,
            "deletions": 17,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "8dfb0851d8f14b8552b2da64dc3c6e0cbeef969c",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "7a5e0bd501819f4a6a2231c578b752743356e982",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 33,
            "deletions": 30,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "373b25b4e1aa2fe8bb690faf9105cc276f0a8f37",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 17,
            "deletions": 24,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "b1e0568c12dbba6fcd3369db50f0020f8cd52855",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 17,
            "deletions": 24,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "0ce8a7c8154dd543911d830f64009f394e936fcf",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 19,
            "deletions": 24,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "4fa9b41e52ab912333ec568e6024faa31ac81f7c",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 19,
            "deletions": 24,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "579ede5aa16c40e0669fd43cd8a07f56d7c3c800",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 10,
            "deletions": 8,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "7402bfbacd76aa4e871e6bde311a9a31c06292bf",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 40,
            "deletions": 60,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "72ead059892ba6a3eee85ed47d55ded240600713",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 18,
            "deletions": 30,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "f54cc65822d8529c6839bd59ed72971cffa9a8c6",
            "filename": "src/transformers/models/vilt/modeling_vilt.py",
            "status": "modified",
            "additions": 14,
            "deletions": 16,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fmodeling_vilt.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "88d87a31bcaed234b857754613e81282ee2c0453",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 37,
            "deletions": 57,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "0c52b0136397fd6da1f4f7d1a79e3f2debd82424",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "305cc68a39ec790482b929576fea5f7db9cb4277",
            "filename": "src/transformers/models/visual_bert/modeling_visual_bert.py",
            "status": "modified",
            "additions": 14,
            "deletions": 23,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvisual_bert%2Fmodeling_visual_bert.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "9ed395b475aecd7159b8775292405c16263a0dec",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 36,
            "deletions": 56,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "deea920d24de6cdbb1a893e45671fa70aab11dfa",
            "filename": "src/transformers/models/vitmatte/modeling_vitmatte.py",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fmodeling_vitmatte.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "5752621990a475b612090121c550883c5dc2542e",
            "filename": "src/transformers/models/vitpose/modeling_vitpose.py",
            "status": "modified",
            "additions": 14,
            "deletions": 18,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fmodeling_vitpose.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "6d66f7d4f57deeb297df4f29a8fa5b34de90f442",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 23,
            "deletions": 43,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "f545ee089a5f38b92e9d858aee4f80aa711e68d3",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 43,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "c7d04dab28f01a530844cc6e02f86efadbe0ada8",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 28,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "ec88628296c45476d9f2b19daa4e09206f7a0686",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 28,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "c239585254d0d53b88e8c9ecd75cb2e77c088aa0",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 28,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "9a2c937e51d1db251940aa6f853b643766ea793e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "7a90c695dc3a40dd33e4cbff897fe161d998d4ea",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "b823c67227d1ca7b409f4d93c96f147ee0a74626",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 40,
            "deletions": 48,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "315d35bbd7f23d6900dd844e6e6697dd189d1026",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 116,
            "deletions": 179,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "6312f98e2017d184d2e677d0287eda7b5fe1243e",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 25,
            "deletions": 31,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "48ff8174186e720cb91edc636c2ba31d55c95ec2",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 10,
            "deletions": 23,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "21a36162810700724725650e40448c784c33f0a5",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "00cf4009fa5591f042fa450fbf1b65056cbe761a",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 367,
            "deletions": 42,
            "changes": 409,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        },
        {
            "sha": "3c27476bdc030e67dbf0c983eb94d4af3f2395c2",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 196,
            "deletions": 78,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=b6b4d43d6d5b661cf6e6a9e683164c7980d5d4b9"
        }
    ],
    "stats": {
        "total": 15965,
        "additions": 6972,
        "deletions": 8993
    }
}