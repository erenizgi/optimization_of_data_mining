{
    "author": "gante",
    "message": "CI: avoid human error, automatically infer generative models (#33212)\n\n* tmp commit\r\n\r\n* move tests to the right class\r\n\r\n* remove ALL all_generative_model_classes = ...\r\n\r\n* skip tf roberta\r\n\r\n* skip InstructBlipForConditionalGenerationDecoderOnlyTest\r\n\r\n* videollava\r\n\r\n* reduce diff\r\n\r\n* reduce diff\r\n\r\n* remove  on vlms\r\n\r\n* fix a few more\r\n\r\n* manual rebase bits\r\n\r\n* more manual rebase\r\n\r\n* remove all manual generative model class test entries\r\n\r\n* fix up to ernie\r\n\r\n* a few more removals\r\n\r\n* handle remaining cases\r\n\r\n* recurrent gemma\r\n\r\n* it's better here\r\n\r\n* make fixup\r\n\r\n* tf idefics is broken\r\n\r\n* tf bert + generate is broken\r\n\r\n* don't touch tf :()\r\n\r\n* don't touch tf :(\r\n\r\n* make fixup\r\n\r\n* better comments for test skips\r\n\r\n* revert tf changes\r\n\r\n* remove empty line removal\r\n\r\n* one more\r\n\r\n* missing one",
    "sha": "62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
    "files": [
        {
            "sha": "8e48263c93007bae247194c8c6e819ac3ce5eee2",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -1507,6 +1507,14 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n \n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n \n+    @classmethod\n+    def can_generate(cls) -> bool:\n+        \"\"\"\n+        Legacy correction: BertForMaskedLM can't call `generate()` from GenerationMixin.\n+        Remove after v4.50, when we stop making `PreTrainedModel` inherit from `GenerationMixin`.\n+        \"\"\"\n+        return False\n+\n \n @add_start_docstrings(\n     \"\"\"Bert Model with a `next sentence prediction (classification)` head on top.\"\"\","
        },
        {
            "sha": "975466f551d2baf21e53c54808ba6c1dac5dd6d3",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -1325,6 +1325,14 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n \n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n \n+    @classmethod\n+    def can_generate(cls) -> bool:\n+        \"\"\"\n+        Legacy correction: ErnieForMaskedLM can't call `generate()` from GenerationMixin.\n+        Remove after v4.50, when we stop making `PreTrainedModel` inherit from `GenerationMixin`.\n+        \"\"\"\n+        return False\n+\n \n @add_start_docstrings(\n     \"\"\"Ernie Model with a `next sentence prediction (classification)` head on top.\"\"\","
        },
        {
            "sha": "302617c6688def1e7a46dc1f9b8478071e701792",
            "filename": "tests/generation/test_flax_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fgeneration%2Ftest_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fgeneration%2Ftest_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_flax_utils.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -66,7 +66,6 @@ def random_attention_mask(shape, rng=None):\n @require_flax\n class FlaxGenerationTesterMixin:\n     model_tester = None\n-    all_generative_model_classes = ()\n \n     def _get_input_ids_and_config(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "ce31cc844f19bbe104c9c82ef7ce80ee7eba5db9",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -28,7 +28,7 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoConfig, is_torch_available, pipeline\n+from transformers import AutoConfig, AutoProcessor, AutoTokenizer, is_torch_available, pipeline\n from transformers.testing_utils import (\n     is_flaky,\n     require_accelerate,\n@@ -61,8 +61,6 @@\n         AutoModelForSeq2SeqLM,\n         AutoModelForSpeechSeq2Seq,\n         AutoModelForVision2Seq,\n-        AutoProcessor,\n-        AutoTokenizer,\n         BartForConditionalGeneration,\n         BartTokenizer,\n         GPT2LMHeadModel,\n@@ -119,7 +117,6 @@\n class GenerationTesterMixin:\n     input_name = \"input_ids\"\n     model_tester = None\n-    all_generative_model_classes = ()\n     max_new_tokens = 3\n \n     def prepare_config_and_inputs_for_generate(self, batch_size=2):"
        },
        {
            "sha": "f12ff24b17f1a6117ece2074a1faff9a4e0deb4b",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -189,7 +189,6 @@ class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMi\n     \"\"\"\n \n     all_model_classes = (AriaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (AriaForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "3a2ed6385151ac901625caa3b33be20b62f528af",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -205,7 +205,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class AutoformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (AutoformerModel, AutoformerForPrediction) if is_torch_available() else ()\n-    all_generative_model_classes = (AutoformerForPrediction,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": AutoformerModel} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "aa5bbbfba6cbdf521c95a66e04c82c6f949b0920",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -257,15 +257,7 @@ def create_and_check_decoder_model_past_large_inputs(\n \n @require_torch\n class BambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            BambaModel,\n-            BambaForCausalLM,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n-    all_generative_model_classes = (BambaForCausalLM,) if is_torch_available() else ()\n+    all_model_classes = (BambaModel, BambaForCausalLM) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BambaModel,"
        },
        {
            "sha": "d94f6d26d6e8762da55349e26e857dc831bb86ca",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -22,6 +22,7 @@\n import pytest\n \n from transformers import (\n+    BarkCausalModel,\n     BarkCoarseConfig,\n     BarkConfig,\n     BarkFineConfig,\n@@ -53,7 +54,6 @@\n     import torch\n \n     from transformers import (\n-        BarkCausalModel,\n         BarkCoarseModel,\n         BarkFineModel,\n         BarkModel,\n@@ -527,6 +527,8 @@ def get_pipeline_config(self):\n @require_torch\n class BarkSemanticModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BarkSemanticModel,) if is_torch_available() else ()\n+    # `BarkSemanticModel` inherits from `BarkCausalModel`, but requires an advanced generation config.\n+    # `BarkCausalModel` does not, so we run generation tests there.\n     all_generative_model_classes = (BarkCausalModel,) if is_torch_available() else ()\n \n     is_encoder_decoder = False\n@@ -614,8 +616,9 @@ def test_generate_fp16(self):\n \n @require_torch\n class BarkCoarseModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n-    # Same tester as BarkSemanticModelTest, except for model_class and config_class\n     all_model_classes = (BarkCoarseModel,) if is_torch_available() else ()\n+    # `BarkCoarseModel` inherits from `BarkCausalModel`, but requires an advanced generation config.\n+    # `BarkCausalModel` does not, so we run generation tests there.\n     all_generative_model_classes = (BarkCausalModel,) if is_torch_available() else ()\n \n     is_encoder_decoder = False"
        },
        {
            "sha": "1cddb898e9c6623863a65ece94311bb19eb972be",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -419,7 +419,6 @@ class BartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (BartForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BartModel,\n@@ -1502,7 +1501,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BartDecoder, BartForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (BartForCausalLM,) if is_torch_available() else ()\n     fx_comptatible = True\n     test_pruning = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "87603ce127b3f3cc86c366f98672e426981d91ad",
            "filename": "tests/models/bart/test_modeling_flax_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -336,7 +336,6 @@ class FlaxBartModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGenerationT\n         if is_flax_available()\n         else ()\n     )\n-    all_generative_model_classes = (FlaxBartForConditionalGeneration,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxBartModelTester(self)"
        },
        {
            "sha": "9c8ee2a69ebb9a0c568859ee022d38c9d4963195",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -451,7 +451,6 @@ class BertModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (BertLMHeadModel,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BertModel,"
        },
        {
            "sha": "a78bd8c41c1cd38939175f06b8373ab415d37da3",
            "filename": "tests/models/bert_generation/test_modeling_bert_generation.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -243,7 +243,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BertGenerationEncoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (BertGenerationEncoder, BertGenerationDecoder) if is_torch_available() else ()\n-    all_generative_model_classes = (BertGenerationDecoder,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": BertGenerationEncoder, \"text-generation\": BertGenerationDecoder}\n         if is_torch_available()"
        },
        {
            "sha": "6aca3cbc410988d12e235c375ab8fb9a3599ef53",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -451,7 +451,6 @@ class BigBirdModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (BigBirdForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BigBirdModel,"
        },
        {
            "sha": "9103c2d52f627329b076d5dd8eb709082d169e4b",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -250,7 +250,6 @@ class BigBirdPegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (BigBirdPegasusForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BigBirdPegasusModel,\n@@ -792,7 +791,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BigBirdPegasusStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BigBirdPegasusDecoder, BigBirdPegasusForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (BigBirdPegasusForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "836f0e8216b6fa783ec3f568a3dd0c4b5a782093",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -284,7 +284,6 @@ class BioGptModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (BioGptForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BioGptModel,"
        },
        {
            "sha": "d12154b128c92c50530690d59366439523f2001e",
            "filename": "tests/models/blenderbot/test_modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_blenderbot.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -226,7 +226,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class BlenderbotModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotModel, BlenderbotForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (BlenderbotForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BlenderbotModel,\n@@ -533,7 +532,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlenderbotStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotDecoder, BlenderbotForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (BlenderbotForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "d5d9c3d7cfb75bf0f60ee0c98bf28f5bca7bf168",
            "filename": "tests/models/blenderbot/test_modeling_flax_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -319,7 +319,6 @@ class FlaxBlenderbotModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGener\n         if is_flax_available()\n         else ()\n     )\n-    all_generative_model_classes = (FlaxBlenderbotForConditionalGeneration,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxBlenderbotModelTester(self)"
        },
        {
            "sha": "c2fcdd852dcadba63e241ba02e3e76d33700881b",
            "filename": "tests/models/blenderbot_small/test_modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_blenderbot_small.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -217,7 +217,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class BlenderbotSmallModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotSmallModel, BlenderbotSmallForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (BlenderbotSmallForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BlenderbotSmallModel,\n@@ -542,7 +541,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class BlenderbotSmallStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (BlenderbotSmallDecoder, BlenderbotSmallForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (BlenderbotSmallForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "f09b7f05ce3a7beb4d2c135c3d3e2a3cd3d34e80",
            "filename": "tests/models/blenderbot_small/test_modeling_flax_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -318,7 +318,6 @@ class FlaxBlenderbotSmallModelTest(FlaxModelTesterMixin, unittest.TestCase, Flax\n         if is_flax_available()\n         else ()\n     )\n-    all_generative_model_classes = (FlaxBlenderbotSmallForConditionalGeneration,) if is_flax_available() else ()\n \n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "e26232e3eb43f06f7b069a1825016c9823908936",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -472,7 +472,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -995,6 +994,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration, Blip2Model) if is_torch_available() else ()\n+    # Doesn't run generation tests. TODO: fix generation tests for Blip2ForConditionalGeneration\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Blip2Model,"
        },
        {
            "sha": "d5de0e92d480c061821a437f1b3661fb29b1d3fe",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -328,7 +328,6 @@ class BloomModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         else ()\n     )\n \n-    all_generative_model_classes = (BloomForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BloomModel,"
        },
        {
            "sha": "dffee6793652816994367386ea13646b9b09f148",
            "filename": "tests/models/bloom/test_modeling_flax_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -171,7 +171,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxBloomModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGenerationTesterMixin):\n     all_model_classes = (FlaxBloomModel, FlaxBloomForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = () if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxBloomModelTester(self)\n@@ -199,7 +198,6 @@ def test_model_from_pretrained(self):\n @require_flax\n class FlaxBloomGenerationTest(unittest.TestCase):\n     all_model_classes = (FlaxBloomForCausalLM,) if is_flax_available() else ()\n-    all_generative_model_classes = () if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_id = \"bigscience/bloom-560m\""
        },
        {
            "sha": "d3c11a6f3696cdc9132f467eaa03afe7bf586832",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -285,7 +285,6 @@ class BrosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = () if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": BrosModel, \"token-classification\": BrosForTokenClassification}\n         if is_torch_available()"
        },
        {
            "sha": "09eec986857a312219ff0c9152091adbd1f48ec5",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -271,7 +271,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ChameleonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ChameleonModel, ChameleonForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (ChameleonForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ChameleonModel,"
        },
        {
            "sha": "839c831eb9f69ba7f8f5056ef133d5bd5e771011",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -281,7 +281,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ClvpDecoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ClvpModel, ClvpForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (ClvpForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": ClvpModelForConditionalGeneration} if is_torch_available() else {}\n \n     test_pruning = False"
        },
        {
            "sha": "fc8ece13a801f2512d3553d14a3b793683598d7e",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -322,7 +322,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class CodeGenModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CodeGenModel, CodeGenForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (CodeGenForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": CodeGenModel, \"text-generation\": CodeGenForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "1d3a435fdb76922a71d632f4a899b64847c94211",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -274,7 +274,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class CohereModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CohereModel, CohereForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (CohereForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": CohereModel,"
        },
        {
            "sha": "ab0af27c00e239bb07cc254c3257b04790d612a0",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -54,7 +54,6 @@ class Cohere2ModelTester(CohereModelTester):\n @require_torch\n class Cohere2ModelTest(CohereModelTest, unittest.TestCase):\n     all_model_classes = (Cohere2Model, Cohere2ForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (Cohere2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Cohere2Model,"
        },
        {
            "sha": "285729382f50f6bebd9b262d319db559319227fb",
            "filename": "tests/models/ctrl/test_modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_ctrl.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -193,7 +193,6 @@ def create_and_check_ctrl_for_sequence_classification(self, config, input_ids, h\n @require_torch\n class CTRLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CTRLModel, CTRLLMHeadModel, CTRLForSequenceClassification) if is_torch_available() else ()\n-    all_generative_model_classes = (CTRLLMHeadModel,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": CTRLModel,"
        },
        {
            "sha": "7bc2d4eb945f9702a27b195cb0a564d333aded46",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -373,7 +373,6 @@ class Data2VecTextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTes\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Data2VecTextForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Data2VecTextModel,"
        },
        {
            "sha": "bc0249d666dce2e081df4a4a03ceb85993153d7c",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -322,7 +322,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class DbrxModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DbrxModel, DbrxForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (DbrxForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-generation\": DbrxForCausalLM} if is_torch_available() else {}\n     test_headmasking = False\n     test_pruning = False"
        },
        {
            "sha": "f22911db9580a704ff83a94af7de55a746da3904",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -127,7 +127,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class DecisionTransformerModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DecisionTransformerModel,) if is_torch_available() else ()\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = {\"feature-extraction\": DecisionTransformerModel} if is_torch_available() else {}\n \n     # Ignoring of a failing test from GenerationTesterMixin, as the model does not use inputs_ids"
        },
        {
            "sha": "81b963cfc47f2ed4a9c135519403d221015f8cd8",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -296,7 +296,6 @@ class DiffLlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (DiffLlamaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": DiffLlamaModel,"
        },
        {
            "sha": "4563cc17dfce49e634e840847b7805c65f8b90f1",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -124,7 +124,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Emu3Text2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Emu3ForCausalLM,) if is_torch_available() else ()\n-    all_generative_model_classes = (Emu3ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"text-generation\": Emu3ForCausalLM,\n@@ -312,7 +311,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Emu3ForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Emu3ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {}\n     test_headmasking = False\n     test_pruning = False"
        },
        {
            "sha": "33ebef8e7b6299621592dd449960eab567474657",
            "filename": "tests/models/ernie/test_modeling_ernie.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie%2Ftest_modeling_ernie.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -442,7 +442,6 @@ class ErnieModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (ErnieForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ErnieModel,"
        },
        {
            "sha": "7504ec2462ebab68aa22a212328f07907cb05437",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -195,7 +195,6 @@ class EsmModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": EsmModel,"
        },
        {
            "sha": "7450f0295f7758b2f652761789fdb1b09dcc298b",
            "filename": "tests/models/esm/test_modeling_esmfold.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -169,7 +169,6 @@ class EsmFoldModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_mismatched_shapes = False\n \n     all_model_classes = (EsmForProteinFolding,) if is_torch_available() else ()\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = {} if is_torch_available() else {}\n     test_sequence_classification_problem_types = False\n "
        },
        {
            "sha": "2838df380745f2ec9ad715644e3ad5c58265616d",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -290,7 +290,6 @@ class FalconModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (FalconForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": FalconModel,"
        },
        {
            "sha": "6ac432766ac13560ccbab2955eb87ef15c138ed9",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -247,7 +247,6 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from transformers.tests.models.mamba.MambaModelTest with Mamba->Falcon,mamba->falcon_mamba,FalconMambaCache->MambaCache\n class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FalconMambaModel, FalconMambaForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (FalconMambaForCausalLM,) if is_torch_available() else ()\n     has_attentions = False  # FalconMamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker"
        },
        {
            "sha": "95789c844aec9f010a1bc1f2937212333e689b92",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -163,7 +163,6 @@ def prepare_fsmt_inputs_dict(\n @require_torch\n class FSMTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FSMTModel, FSMTForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (FSMTForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": FSMTModel,"
        },
        {
            "sha": "1b308973fb141f769d24a778cd794f427f1f0e07",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -268,7 +268,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class FuyuModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n-    all_generative_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"text-generation\": FuyuForCausalLM, \"image-text-to-text\": FuyuForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "3a56cbfb6b54abc45ea6f42b8b189326c1c236ed",
            "filename": "tests/models/gemma/test_modeling_flax_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -176,7 +176,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxGemmaModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxGemmaModel, FlaxGemmaForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxGemmaForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxGemmaModelTester(self)"
        },
        {
            "sha": "ffadf3377e0ab7eb12f430d8dd16953d10c621cb",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -300,7 +300,6 @@ class GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GemmaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GemmaModel,"
        },
        {
            "sha": "c881ecaea559f258c277836a60c8b096a031f839",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -63,7 +63,6 @@ class Gemma2ModelTest(GemmaModelTest, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Gemma2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Gemma2Model,"
        },
        {
            "sha": "9912d9b6fea8c2e77bee3a8f3e9cfbc448ab4bc6",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -399,7 +399,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class GitModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GitModel, GitForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (GitForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GitModel,"
        },
        {
            "sha": "e4ceec2d097861a229a264d3eac1c192ad80c8d3",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -286,7 +286,6 @@ class GlmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GlmForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GlmModel,"
        },
        {
            "sha": "ac044de5ca960ca0317dd2b1387f10059bb5d1d5",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -169,7 +169,6 @@ def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_\n @require_torch\n class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GotOcr2ForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (GotOcr2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"image-to-text\": GotOcr2ForConditionalGeneration,"
        },
        {
            "sha": "2e98930e4c692b95c87667a901207824c81f6441",
            "filename": "tests/models/gpt2/test_modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -211,7 +211,6 @@ def check_bool_attention_mask_in_generation(self, model_class_name, config, inpu\n @require_flax\n class FlaxGPT2ModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxGPT2Model, FlaxGPT2LMHeadModel) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxGPT2LMHeadModel,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxGPT2ModelTester(self)"
        },
        {
            "sha": "b45c84fc6445f4a8c5cc7c07697619f30c5a8a6f",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -492,7 +492,6 @@ class GPT2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GPT2LMHeadModel, GPT2DoubleHeadsModel) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPT2Model,"
        },
        {
            "sha": "18b2e6408888d79e1b16e358cabdf5a33be822b1",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -390,7 +390,6 @@ class GPTBigCodeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GPTBigCodeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPTBigCodeModel,"
        },
        {
            "sha": "490d58c8d112e2e01ad17db9934c9776f960cc64",
            "filename": "tests/models/gpt_neo/test_modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -183,7 +183,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxGPTNeoModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxGPTNeoModel, FlaxGPTNeoForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxGPTNeoForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxGPTNeoModelTester(self)"
        },
        {
            "sha": "213c3ed497f123a851793b8ef2787c0b834563fc",
            "filename": "tests/models/gpt_neo/test_modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_gpt_neo.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -376,7 +376,6 @@ class GPTNeoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GPTNeoForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPTNeoModel,"
        },
        {
            "sha": "34a8e54f700d0daad5c62ea1e7e0727efb6fd672",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -274,7 +274,6 @@ class GPTNeoXModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GPTNeoXForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPTNeoXModel,"
        },
        {
            "sha": "22bd647081314bc793b323d254dbffc425dcc83b",
            "filename": "tests/models/gpt_neox_japanese/test_modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox_japanese%2Ftest_modeling_gpt_neox_japanese.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -198,7 +198,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class GPTNeoXModelJapaneseTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GPTNeoXJapaneseModel, GPTNeoXJapaneseForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (GPTNeoXJapaneseForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": GPTNeoXJapaneseModel, \"text-generation\": GPTNeoXJapaneseForCausalLM}\n         if is_torch_available()"
        },
        {
            "sha": "ece207ee5dbb7f952ed3ba8c453a12a481fb0578",
            "filename": "tests/models/gptj/test_modeling_flax_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -180,7 +180,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxGPTJModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxGPTJModel, FlaxGPTJForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxGPTJForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxGPTJModelTester(self)"
        },
        {
            "sha": "793afc7f5c307a8675fb934be8d322a34279b35a",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -341,7 +341,6 @@ class GPTJModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GPTJForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GPTJModel,"
        },
        {
            "sha": "469e96fd83042ec9a6078577cc6f5c8427bd8e95",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -281,7 +281,6 @@ class GraniteModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GraniteForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GraniteModel,"
        },
        {
            "sha": "0e64d29c9189e420de61cf4f5e50bef0544ecc9a",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -280,7 +280,6 @@ class GraniteMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (GraniteMoeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": GraniteMoeModel,"
        },
        {
            "sha": "fb732110077648075e415d6df0677290915c9fa1",
            "filename": "tests/models/helium/test_modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhelium%2Ftest_modeling_helium.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -55,7 +55,6 @@ class HeliumModelTest(GemmaModelTest, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (HeliumForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": HeliumModel,"
        },
        {
            "sha": "5d19f5b0202579d0d89a6e0f3b1d0df00f9dcaa8",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -593,7 +593,6 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n @require_torch\n class IdeficsForVisionText2TextTest(IdeficsModelTest, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsForVisionText2Text,) if is_torch_available() else ()\n-    all_generative_model_classes = (IdeficsForVisionText2Text,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = IdeficsModelTester("
        },
        {
            "sha": "56df4bb801b674f6118109afc97520d59ea7d9e1",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -369,7 +369,6 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     \"\"\"\n \n     all_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics2ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "dae54afe620804d73d6d9f353577a4eb45c79073",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -327,7 +327,6 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     \"\"\"\n \n     all_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Idefics3ForConditionalGeneration} if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "a2d65f4d4b6dbb01a57db862ede50465ba6bdca0",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -230,7 +230,6 @@ class ImageGPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n     all_model_classes = (\n         (ImageGPTForCausalImageModeling, ImageGPTForImageClassification, ImageGPTModel) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (ImageGPTForCausalImageModeling,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"image-feature-extraction\": ImageGPTModel, \"image-classification\": ImageGPTForImageClassification}\n         if is_torch_available()"
        },
        {
            "sha": "5415717cd4b8625ae959963a41661acc66cf3794",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -190,7 +190,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class InformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (InformerModel, InformerForPrediction) if is_torch_available() else ()\n-    all_generative_model_classes = (InformerForPrediction,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": InformerModel} if is_torch_available() else {}\n     is_encoder_decoder = True\n     test_pruning = False"
        },
        {
            "sha": "e072499ad3f1fc42c25eca56b140b962cb545d17",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -477,7 +477,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     fx_compatible = False\n     test_head_masking = False"
        },
        {
            "sha": "0534b4f5ea739da54cef53c5f1bfd56106ef8e0a",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -496,7 +496,6 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     ModelTesterMixin, GenerationTesterMixin, unittest.TestCase\n ):\n     all_model_classes = (InstructBlipVideoForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (InstructBlipVideoForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "0036c21521cda3f7b2e3091bd1ec86a819b788ca",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -327,7 +327,6 @@ class JambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (JambaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": JambaModel,"
        },
        {
            "sha": "4538ad56108b7992998753fb0e2b3d994f7c3fec",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -280,7 +280,6 @@ class JetMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     all_model_classes = (\n         (JetMoeModel, JetMoeForCausalLM, JetMoeForSequenceClassification) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (JetMoeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": JetMoeModel,"
        },
        {
            "sha": "315130b232cffc63c419790187399efb175ad114",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -259,7 +259,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Kosmos2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Kosmos2Model, Kosmos2ForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (Kosmos2ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Kosmos2Model,"
        },
        {
            "sha": "4b9d2e283fb8d5e54717ad977fce32444defd7ed",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -281,7 +281,6 @@ class LEDModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (LEDForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LEDModel,"
        },
        {
            "sha": "da326e797d614ea435658f10b7b5c52f4ceb10a4",
            "filename": "tests/models/llama/test_modeling_flax_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -176,7 +176,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxLlamaModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxLlamaModel, FlaxLlamaForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxLlamaForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxLlamaModelTester(self)"
        },
        {
            "sha": "01d807fbdba2869625acf0d798e05c339954b5e3",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -289,7 +289,6 @@ class LlamaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (LlamaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LlamaModel,"
        },
        {
            "sha": "25e1a747ce9fed04b11f4ec8639a79de27084ad1",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -181,7 +181,6 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n     \"\"\"\n \n     all_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"image-to-text\": LlavaForConditionalGeneration, \"image-text-to-text\": LlavaForConditionalGeneration}\n         if is_torch_available()"
        },
        {
            "sha": "eaeda3cecb7b877c57d5340d71682078fc788853",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -215,7 +215,6 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     \"\"\"\n \n     all_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": LlavaNextForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "0f4642402644bc6c6eb848028449a1bca4fa1a69",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -231,7 +231,6 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     \"\"\"\n \n     all_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "63be10a774db75041f53cda9c1e37a7445322891",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -215,7 +215,6 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     \"\"\"\n \n     all_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"image-text-to-text\": LlavaOnevisionForConditionalGeneration} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "f779ceefc5bde557c0ce056d2cef6ea8277851b9",
            "filename": "tests/models/longt5/test_modeling_flax_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -237,7 +237,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_flax\n class FlaxLongT5ModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxLongT5Model, FlaxLongT5ForConditionalGeneration) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxLongT5ForConditionalGeneration,) if is_flax_available() else ()\n     is_encoder_decoder = True\n \n     def setUp(self):"
        },
        {
            "sha": "38b159679f04e5d25ff5fff94d03e82b44906048",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -502,7 +502,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class LongT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (LongT5Model, LongT5ForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (LongT5ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LongT5Model,"
        },
        {
            "sha": "015a2c5f3875b16d4f587b1fb364642d4f45ee9a",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -240,7 +240,6 @@ class M2M100ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (M2M100ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": M2M100Model,"
        },
        {
            "sha": "0c1bbef910f9ca901d8015745f46e1c19ff80265",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -239,7 +239,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MambaModel, MambaForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (MambaForCausalLM,) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n     test_torchscript = False  # FIXME let's try to support this @ArthurZucker"
        },
        {
            "sha": "0fae7e830c077d6b0cdddc9314c6370c61ece102",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -214,7 +214,6 @@ def create_and_check_mamba2_slow_vs_fast_forward(self, config, input_ids, *args,\n @require_torch\n class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Mamba2Model, Mamba2ForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (Mamba2ForCausalLM,) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @molbap\n     test_torchscript = False  # FIXME I think this should be doable @molbap @ArthurZucker"
        },
        {
            "sha": "9f15291754de99c61dbe0fa30af36841ddbf5b18",
            "filename": "tests/models/marian/test_modeling_flax_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -231,7 +231,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n class FlaxMarianModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGenerationTesterMixin):\n     is_encoder_decoder = True\n     all_model_classes = (FlaxMarianModel, FlaxMarianMTModel) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxMarianMTModel,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxMarianModelTester(self)"
        },
        {
            "sha": "f27ee53575d5b311aca67fe569b0978ee89d5e01",
            "filename": "tests/models/marian/test_modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_marian.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -237,7 +237,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class MarianModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MarianModel, MarianMTModel) if is_torch_available() else ()\n-    all_generative_model_classes = (MarianMTModel,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MarianModel,\n@@ -871,7 +870,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MarianStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MarianDecoder, MarianForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (MarianForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "6e0230646051860879146369c423b4d80f360557",
            "filename": "tests/models/mbart/test_modeling_flax_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -342,7 +342,6 @@ class FlaxMBartModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGeneration\n         if is_flax_available()\n         else ()\n     )\n-    all_generative_model_classes = (FlaxMBartForConditionalGeneration,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxMBartModelTester(self)"
        },
        {
            "sha": "9ff5ef33bdb25d571f8dc628914253804d2c2bb7",
            "filename": "tests/models/mbart/test_modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_mbart.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -229,7 +229,6 @@ class MBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (MBartForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MBartModel,\n@@ -727,7 +726,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MBartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MBartDecoder, MBartForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (MBartForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "c78a402fa90c2f55f6a4b5bd5107a5903138ae65",
            "filename": "tests/models/mistral/test_modeling_flax_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -187,7 +187,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxMistralModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxMistralModel, FlaxMistralForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxMistralForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxMistralModelTester(self)"
        },
        {
            "sha": "bbd6aee82a787041970d8fb481f9a878847fd350",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -301,7 +301,6 @@ class MistralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (MistralForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MistralModel,"
        },
        {
            "sha": "7ad8792439338c03422a20e9691786ea627d3cf0",
            "filename": "tests/models/mixtral/test_modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmixtral%2Ftest_modeling_mixtral.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -300,7 +300,6 @@ class MixtralModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (MixtralForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MixtralModel,"
        },
        {
            "sha": "9dcc712346a919660224d8e949b5e32454095a15",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -124,7 +124,6 @@ class MllamaForCausalLMModelTest(ModelTesterMixin, GenerationTesterMixin, unitte\n     \"\"\"\n \n     all_model_classes = (MllamaForCausalLM,) if is_torch_available() else ()\n-    all_generative_model_classes = (MllamaForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n \n@@ -264,7 +263,6 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n     \"\"\"\n \n     all_model_classes = (MllamaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (MllamaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": MllamaForConditionalGeneration} if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "5cd26b3523664b7ce4fbfbc60bee66e28c9f8fa2",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -229,7 +229,6 @@ class ModernBertModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ModernBertModel,"
        },
        {
            "sha": "bf30f2c3d5227aedd890622a737f11f14345b9e0",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -170,6 +170,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MoonshineModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MoonshineModel, MoonshineForConditionalGeneration) if is_torch_available() else ()\n+    # Doesn't run generation tests. TODO (eustache): remove this line and then make CI green\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"automatic-speech-recognition\": MoonshineForConditionalGeneration,"
        },
        {
            "sha": "37b4bc46baa6492db4c0161bfdb93c91b422eac8",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -152,9 +152,6 @@ def prepare_config_and_inputs_for_common(self, batch_size=None):\n @require_torch\n class MoshiDecoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MoshiModel, MoshiForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (\n-        (MoshiForCausalLM,) if is_torch_available() else ()\n-    )  # we don't want to run all the generation tests, only a specific subset\n     test_pruning = False\n     test_resize_embeddings = True\n     test_head_masking = False\n@@ -528,7 +525,6 @@ def prepare_config_and_inputs_for_common(self, batch_size=None):\n @require_torch\n class MoshiTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MoshiForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (MoshiForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False  # training is not supported yet for Moshi\n     test_headmasking = False\n     test_resize_embeddings = False"
        },
        {
            "sha": "1f581f875cd1ba413cbf4a3bc616974a6b7aac27",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -354,7 +354,6 @@ class MptModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else ()\n     )\n \n-    all_generative_model_classes = (MptForCausalLM,) if is_torch_available() else ()\n     fx_compatible = False\n     test_missing_keys = False\n     test_pruning = False"
        },
        {
            "sha": "dfeebf28bcf6e8a75de0036b3d301b13c5ad9807",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -298,7 +298,6 @@ class MraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     has_attentions = False\n \n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MraModel,"
        },
        {
            "sha": "3c3256da8b2495982345db012d9395c18d10ab28",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -553,7 +553,6 @@ class MT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (MT5ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MT5Model,"
        },
        {
            "sha": "3852d8c3c4ff49a8f9eb954b4ed5db9c597c0f3d",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -176,6 +176,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MusicgenDecoderTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MusicgenModel, MusicgenForCausalLM) if is_torch_available() else ()\n+    # Doesn't run generation tests. See `greedy_sample_model_classes` below\n+    all_generative_model_classes = ()\n     greedy_sample_model_classes = (\n         (MusicgenForCausalLM,) if is_torch_available() else ()\n     )  # we don't want to run all the generation tests, only a specific subset\n@@ -801,6 +803,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MusicgenTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MusicgenForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. See `greedy_sample_model_classes` below\n+    all_generative_model_classes = ()\n     greedy_sample_model_classes = (MusicgenForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-to-audio\": MusicgenForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False  # training is not supported yet for MusicGen"
        },
        {
            "sha": "7cb31adaedbd8a84de863c58508d7def0436a8bc",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -182,6 +182,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MusicgenMelodyDecoderTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MusicgenMelodyModel, MusicgenMelodyForCausalLM) if is_torch_available() else ()\n+    # Doesn't run generation tests. See `greedy_sample_model_classes` below\n+    all_generative_model_classes = ()\n     greedy_sample_model_classes = (\n         (MusicgenMelodyForCausalLM,) if is_torch_available() else ()\n     )  # the model uses a custom generation method so we only run a specific subset of the generation tests\n@@ -820,6 +822,8 @@ def prepare_config_and_inputs_for_common(self):\n # Copied from tests.models.musicgen.test_modeling_musicgen.MusicgenTest with Musicgen->MusicgenMelody, musicgen->musicgen_melody, EncoderDecoder->DecoderOnly, input_values->input_features\n class MusicgenMelodyTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MusicgenMelodyForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. See `greedy_sample_model_classes` below\n+    all_generative_model_classes = ()\n     greedy_sample_model_classes = (MusicgenMelodyForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-to-audio\": MusicgenMelodyForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False  # training is not supported yet for MusicGen"
        },
        {
            "sha": "52734b2eb8ddfc55ad29d25f84594339fc8eb078",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -418,7 +418,6 @@ class MvpModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (MvpForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MvpModel,\n@@ -803,7 +802,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class MvpStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MvpDecoder, MvpForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (MvpForCausalLM,) if is_torch_available() else ()\n     fx_comptatible = True\n     test_pruning = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "fd905f8e0c592cfa24cc807fdd8e75dd4774e14e",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -75,7 +75,6 @@ class NemotronModelTest(GemmaModelTest):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (NemotronForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": NemotronModel,"
        },
        {
            "sha": "b3ae5779bd273086810a4b65acc2aba407b709ed",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -247,7 +247,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class NllbMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (NllbMoeModel, NllbMoeForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (NllbMoeForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": NllbMoeModel,"
        },
        {
            "sha": "a96eb91113595cbd5f942f9ead610e5fd7411b0d",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -274,7 +274,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class OlmoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (OlmoModel, OlmoForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (OlmoForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": OlmoModel,"
        },
        {
            "sha": "51496188f9fcfd695eab45adcdf98ed71adfc452",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -273,7 +273,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Olmo2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Olmo2Model, Olmo2ForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (Olmo2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Olmo2Model,"
        },
        {
            "sha": "07d904699faa2b4e788d9537f80e24eaaf9af217",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -287,7 +287,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class OlmoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (OlmoeModel, OlmoeForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (OlmoeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": OlmoeModel,"
        },
        {
            "sha": "c47485568496d83ce5614f15efbc2e6615fc1996",
            "filename": "tests/models/openai/test_modeling_openai.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_openai.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -195,9 +195,6 @@ class OpenAIGPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (\n-        (OpenAIGPTLMHeadModel,) if is_torch_available() else ()\n-    )  # TODO (PVP): Add Double HeadsModel when generate() function is changed accordingly\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": OpenAIGPTModel,"
        },
        {
            "sha": "c5c3d10f23c5e2b491d904a0538799c4e672f1fb",
            "filename": "tests/models/opt/test_modeling_flax_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -205,7 +205,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxOPTModelTest(FlaxModelTesterMixin, unittest.TestCase, FlaxGenerationTesterMixin):\n     all_model_classes = (FlaxOPTModel, FlaxOPTForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = () if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxOPTModelTester(self)"
        },
        {
            "sha": "3e3d2159a022c4468d140d6a54fa72080099d306",
            "filename": "tests/models/opt/test_modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_opt.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -211,7 +211,6 @@ class OPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (OPTForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": OPTModel,"
        },
        {
            "sha": "570ccf3742728e08abc458a93fdccb3c5aafc01f",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -179,7 +179,6 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     \"\"\"\n \n     all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "8eb9c9a2d92f20fe51743a4009a49ec99e59d21f",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -175,7 +175,6 @@ class PaliGemma2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     \"\"\"\n \n     all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "939072d825fe01b3975a90b8711c028ceb76d3cc",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -219,9 +219,6 @@ class PatchTSMixerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (\n-        (PatchTSMixerForPrediction, PatchTSMixerForPretraining) if is_torch_available() else ()\n-    )\n     pipeline_model_mapping = {\"feature-extraction\": PatchTSMixerModel} if is_torch_available() else {}\n     is_encoder_decoder = False\n     test_pruning = False"
        },
        {
            "sha": "e70770ccc1cd20535f3956e1c7e9bc890a374b40",
            "filename": "tests/models/pegasus/test_modeling_flax_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -230,7 +230,6 @@ class FlaxPegasusModelTest(FlaxModelTesterMixin, unittest.TestCase):\n         if is_flax_available()\n         else ()\n     )\n-    all_generative_model_classes = (FlaxPegasusForConditionalGeneration,) if is_flax_available() else ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "90e0fcaabc9103bccf58bb52411b81f50c2aee86",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -235,7 +235,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class PegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (PegasusModel, PegasusForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (PegasusForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PegasusModel,\n@@ -562,7 +561,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class PegasusStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (PegasusDecoder, PegasusForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (PegasusForCausalLM,) if is_torch_available() else ()\n     test_resize_position_embeddings = True\n     test_pruning = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "97451ce766a11cddfc0fcef78ebcad9f1d1dcc32",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -203,7 +203,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class PegasusXModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (PegasusXModel, PegasusXForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (PegasusXForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PegasusXModel,\n@@ -850,7 +849,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class PegasusXStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (PegasusXDecoder,) if is_torch_available() else ()\n-    all_generative_model_classes = ()\n     test_pruning = False\n     is_encoder_decoder = False\n     test_head_masking = False"
        },
        {
            "sha": "744788cf6447f6d78ee9527cc4c07b1058fd24a9",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -296,7 +296,6 @@ class PersimmonModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n         else {}\n     )\n \n-    all_generative_model_classes = (PersimmonForCausalLM,) if is_torch_available() else ()\n     test_headmasking = False\n     test_pruning = False\n "
        },
        {
            "sha": "9b7d44ca1cb753587cb4431635a512e8823ebb15",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -278,7 +278,6 @@ class PhiModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (PhiForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PhiModel,"
        },
        {
            "sha": "a6a9ab4e63f25a3146787d709d78a21b69a7f954",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -333,7 +333,6 @@ class Phi3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Phi3ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Phi3Model,"
        },
        {
            "sha": "40448a0a85e8e7449e9dffdbbbc7afc864221639",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -338,7 +338,6 @@ class PhimoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     all_model_classes = (\n         (PhimoeModel, PhimoeForCausalLM, PhimoeForSequenceClassification) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (PhimoeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PhimoeModel,"
        },
        {
            "sha": "cbef43f3fb33db55ccaef7c63c79b126d9fb0a88",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -421,7 +421,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Pix2StructModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Pix2StructForConditionalGeneration,) if is_torch_available() else {}\n     pipeline_model_mapping = (\n         {\"image-to-text\": Pix2StructForConditionalGeneration, \"image-text-to-text\": Pix2StructForConditionalGeneration}\n         if is_torch_available()"
        },
        {
            "sha": "6a4e5df5992131b851c86e3959d8578e7a76ff02",
            "filename": "tests/models/plbart/test_modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fplbart%2Ftest_modeling_plbart.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -224,7 +224,6 @@ class PLBartModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     all_model_classes = (\n         (PLBartModel, PLBartForConditionalGeneration, PLBartForSequenceClassification) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (PLBartForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": PLBartModel,\n@@ -658,7 +657,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class PLBartStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (PLBartDecoder, PLBartForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (PLBartForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False\n "
        },
        {
            "sha": "50d25aaf5cca759e2aec9db81f0d53993aa25f8f",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -507,6 +507,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Pop2PianoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pop2PianoForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. Has custom generation method with a different interface\n     all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\"automatic-speech-recognition\": Pop2PianoForConditionalGeneration} if is_torch_available() else {}"
        },
        {
            "sha": "2687cd67f7fdee00f117f3b84c7163481bffe4b7",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -888,7 +888,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ProphetNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ProphetNetModel, ProphetNetForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (ProphetNetForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ProphetNetModel,\n@@ -1127,7 +1126,6 @@ def test_generate_with_head_masking(self):\n @require_torch\n class ProphetNetStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (ProphetNetDecoder, ProphetNetForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (ProphetNetForCausalLM,) if is_torch_available() else ()\n     test_pruning = False\n \n     test_resize_embeddings = False"
        },
        {
            "sha": "e4b9a299e8f61ee3ef83cbec6cc24347f6b46f66",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -312,7 +312,6 @@ class Qwen2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Qwen2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen2Model,"
        },
        {
            "sha": "cfcfd3a620c9fa4a7aacabd002d280be98520570",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -229,7 +229,6 @@ class Qwen2_5_VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     \"\"\"\n \n     all_model_classes = (Qwen2_5_VLForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Qwen2_5_VLForConditionalGeneration,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n "
        },
        {
            "sha": "570b678ec3ebfd6d6befd028145d678e2e046a6e",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -337,7 +337,6 @@ class Qwen2MoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Qwen2MoeForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Qwen2MoeModel,"
        },
        {
            "sha": "655effb09d74b7a90449c286d393b8bdad3c228f",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -225,7 +225,6 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n     \"\"\"\n \n     all_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": Qwen2VLForConditionalGeneration}\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "3519604c8c0e74a94b0783cbd661af3514195bcf",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -283,7 +283,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class RecurrentGemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (RecurrentGemmaForCausalLM,) if is_torch_available() else ()\n-    # all_generative_model_classes = (RecurrentGemmaForCausalLM,) if is_torch_available() else () #TODO @gante not fully supported\n+    # Doesn't run generation tests. TODO @gante not fully supported\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RecurrentGemmaModel,"
        },
        {
            "sha": "d7e1e3c6915304ceec0c1d155cf431824d97e588",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -603,7 +603,6 @@ class ReformerLocalAttnModelTest(ReformerTesterMixin, GenerationTesterMixin, Mod\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (ReformerModelWithLMHead,) if is_torch_available() else ()\n     test_pruning = False\n     test_headmasking = False\n     test_torchscript = False\n@@ -716,7 +715,6 @@ class ReformerLSHAttnModelTest(\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (ReformerModelWithLMHead,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ReformerModel,"
        },
        {
            "sha": "d5f9e0d5eccf77e89d3eacd11c15044e34e82605",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -373,7 +373,6 @@ class RemBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (RemBertForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RemBertModel,"
        },
        {
            "sha": "5d688075a803b9d3ea83a95a8f6a118db5e80687",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -380,7 +380,6 @@ class RobertaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (RobertaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RobertaModel,"
        },
        {
            "sha": "11fbf6de35a3dcf85a04026bc48be8201ab19669",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -378,7 +378,6 @@ class RobertaPreLayerNormModelTest(ModelTesterMixin, GenerationTesterMixin, Pipe\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (RobertaPreLayerNormForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RobertaPreLayerNormModel,"
        },
        {
            "sha": "2f13664e18b8d479c8d0a8c2f117ee3cd990875d",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -570,7 +570,6 @@ class RoCBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (RoCBertForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RoCBertModel,"
        },
        {
            "sha": "7ad8165c8483d17897cecc9ea89af10ff11eb6d2",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -392,7 +392,6 @@ class RoFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (RoFormerForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RoFormerModel,"
        },
        {
            "sha": "bade708ff9bcf035b8bd4ed0e639a54a4792e7ee",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -234,7 +234,6 @@ class RwkvModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     pipeline_model_mapping = (\n         {\"feature-extraction\": RwkvModel, \"text-generation\": RwkvForCausalLM} if is_torch_available() else {}\n     )\n-    all_generative_model_classes = (RwkvForCausalLM,) if is_torch_available() else ()\n     fx_compatible = False\n     test_missing_keys = False\n     test_model_parallel = False"
        },
        {
            "sha": "a68030c86b17c9edca5b334a4437ae51dcfaf679",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -358,7 +358,6 @@ class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (SeamlessM4TForSpeechToText,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = SeamlessM4TModelTester(self, input_modality=\"speech\")\n@@ -602,7 +601,8 @@ class SeamlessM4TModelWithTextInputTest(\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (SeamlessM4TForTextToText,) if is_torch_available() else ()\n+    # Doesn't run generation tests. Has custom generation method with a different interface\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"automatic-speech-recognition\": SeamlessM4TForSpeechToText,"
        },
        {
            "sha": "c53bc4a8b186376a8ea9419d33f25dac0db058c4",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -374,7 +374,6 @@ class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (SeamlessM4Tv2ForSpeechToText,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = SeamlessM4Tv2ModelTester(self, input_modality=\"speech\")\n@@ -615,7 +614,8 @@ class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, GenerationTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (SeamlessM4Tv2ForTextToText,) if is_torch_available() else ()\n+    # Doesn't run generation tests. Has custom generation method with a different interface\n+    all_generative_model_classes = ()\n \n     def setUp(self):\n         self.model_tester = SeamlessM4Tv2ModelTester(self, input_modality=\"text\")"
        },
        {
            "sha": "7250cc22109879e954a008094e8d80137a8e71c7",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -271,7 +271,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class Speech2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Speech2TextModel, Speech2TextForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (Speech2TextForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"automatic-speech-recognition\": Speech2TextForConditionalGeneration, \"feature-extraction\": Speech2TextModel}\n         if is_torch_available()"
        },
        {
            "sha": "efc384e7051d61cfae8adf4e443b84c0524b12f8",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -362,7 +362,6 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n @require_torch\n class SpeechT5ForSpeechToTextTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SpeechT5ForSpeechToText,) if is_torch_available() else ()\n-    all_generative_model_classes = (SpeechT5ForSpeechToText,) if is_torch_available() else ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_headmasking = False\n@@ -880,7 +879,6 @@ def create_and_check_model_forward(self, config, inputs_dict):\n @require_torch\n class SpeechT5ForTextToSpeechTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SpeechT5ForTextToSpeech,) if is_torch_available() else ()\n-    all_generative_model_classes = (SpeechT5ForTextToSpeech,) if is_torch_available() else ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_headmasking = False\n@@ -1423,7 +1421,6 @@ def create_and_check_model_forward(self, config, inputs_dict):\n @require_torch\n class SpeechT5ForSpeechToSpeechTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SpeechT5ForSpeechToSpeech,) if is_torch_available() else ()\n-    all_generative_model_classes = (SpeechT5ForSpeechToSpeech,) if is_torch_available() else ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_headmasking = False"
        },
        {
            "sha": "946b220e0ea902eaa0ff1bfb90490b6e24ab3a94",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -299,7 +299,6 @@ class StableLmModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         else {}\n     )\n \n-    all_generative_model_classes = (StableLmForCausalLM,) if is_torch_available() else ()\n     test_headmasking = False\n     test_pruning = False\n "
        },
        {
            "sha": "9f06697a1948c3ae8e9dccf9cc16e4cb9fb2ea23",
            "filename": "tests/models/starcoder2/test_modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstarcoder2%2Ftest_modeling_starcoder2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -294,7 +294,6 @@ class Starcoder2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Starcoder2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Starcoder2Model,"
        },
        {
            "sha": "1d6ccc59d6194b6ce584af14832def0d65f46ba4",
            "filename": "tests/models/superglue/test_modeling_superglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_modeling_superglue.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -119,7 +119,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SuperGlueModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SuperGlueForKeypointMatching,) if is_torch_available() else ()\n-    all_generative_model_classes = () if is_torch_available() else ()\n \n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "6fe2df000255abb2b40b7fe4dc545fb2cb619355",
            "filename": "tests/models/superpoint/test_modeling_superpoint.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -113,7 +113,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SuperPointModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SuperPointForKeypointDetection,) if is_torch_available() else ()\n-    all_generative_model_classes = () if is_torch_available() else ()\n \n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "03de2f72d01d0ee0c1cd318c2714c149388f2ec5",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -557,7 +557,6 @@ class SwitchTransformersModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n     all_model_classes = (\n         (SwitchTransformersModel, SwitchTransformersForConditionalGeneration) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (SwitchTransformersForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": SwitchTransformersModel,"
        },
        {
            "sha": "516fb5c6d52849d70631c2b32ed9c614d69d1b7e",
            "filename": "tests/models/t5/test_modeling_flax_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -229,7 +229,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_flax\n class FlaxT5ModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxT5Model, FlaxT5ForConditionalGeneration) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxT5ForConditionalGeneration,) if is_flax_available() else ()\n     is_encoder_decoder = True\n \n     def setUp(self):"
        },
        {
            "sha": "9886684d608826d8b1e44fda002ae584d414f660",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -557,7 +557,6 @@ class T5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (T5ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": T5Model,"
        },
        {
            "sha": "c886bb08856ce731eb670c770cca36dffa1cbb41",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -179,7 +179,6 @@ class TimeSeriesTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unit\n     all_model_classes = (\n         (TimeSeriesTransformerModel, TimeSeriesTransformerForPrediction) if is_torch_available() else ()\n     )\n-    all_generative_model_classes = (TimeSeriesTransformerForPrediction,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": TimeSeriesTransformerModel} if is_torch_available() else {}\n     is_encoder_decoder = True\n     test_pruning = False"
        },
        {
            "sha": "26654546f6483a3a314a503c68b7739c437a8aa9",
            "filename": "tests/models/trocr/test_modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftrocr%2Ftest_modeling_trocr.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -161,7 +161,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class TrOCRStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (TrOCRDecoder, TrOCRForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (TrOCRForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"text-generation\": TrOCRForCausalLM} if is_torch_available() else {}\n     fx_compatible = True\n     test_pruning = False"
        },
        {
            "sha": "e750d50b62d1a7a9b3f3efe081fcf5963c6c71aa",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -274,7 +274,6 @@ class UdopModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (UdopForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": UdopModel, \"image-text-to-text\": UdopForConditionalGeneration}\n         if is_torch_available()"
        },
        {
            "sha": "e9a5d7e64221400d2bd591f94f0c3cc09e65db1a",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -295,7 +295,6 @@ class UMT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (UMT5ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": UMT5Model,"
        },
        {
            "sha": "b8d4d4167e57f8fed89b74fb6c3833cf1a93abad",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -192,7 +192,6 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     \"\"\"\n \n     all_model_classes = (VideoLlavaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (VideoLlavaForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True"
        },
        {
            "sha": "f6a601c8a02d425ee5dbab8a1cc653ab48ee3972",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -167,7 +167,6 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n     \"\"\"\n \n     all_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n-    all_generative_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": VipLlavaForConditionalGeneration} if is_torch_available() else {}\n     fx_compatible = False\n     test_pruning = False"
        },
        {
            "sha": "f018d0d4198cbd9e292f7027e6f5dd4330cd6fb2",
            "filename": "tests/models/whisper/test_modeling_flax_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -190,7 +190,6 @@ def make_partial_class(full_class, *args, **kwargs):\n @require_flax\n class FlaxWhisperModelTest(FlaxModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxWhisperForConditionalGeneration, FlaxWhisperModel) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxWhisperForConditionalGeneration,) if is_flax_available() else ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_head_masking = False"
        },
        {
            "sha": "9a951c9306d31c68b06f5712e05cd158ea9ba36a",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -376,7 +376,6 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n @require_torch\n class WhisperModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (WhisperModel, WhisperForConditionalGeneration) if is_torch_available() else ()\n-    all_generative_model_classes = (WhisperForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"audio-classification\": WhisperForAudioClassification,\n@@ -3966,7 +3965,6 @@ def create_and_check_decoder_model_attention_mask_past(self, config, input_ids):\n @require_torch\n class WhisperStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (WhisperDecoder, WhisperForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (WhisperForCausalLM,) if is_torch_available() else ()\n     fx_comptatible = False\n     test_pruning = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "b34aee8f3fa8a2bfa6d14e505dd59d7fd2eecddd",
            "filename": "tests/models/xglm/test_modeling_flax_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -183,7 +183,6 @@ def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input\n @require_flax\n class FlaxXGLMModelTest(FlaxModelTesterMixin, FlaxGenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (FlaxXGLMModel, FlaxXGLMForCausalLM) if is_flax_available() else ()\n-    all_generative_model_classes = (FlaxXGLMForCausalLM,) if is_flax_available() else ()\n \n     def setUp(self):\n         self.model_tester = FlaxXGLMModelTester(self)"
        },
        {
            "sha": "e321aaf643e83df90da352007c2235975bfc8f54",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -283,7 +283,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class XGLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (XGLMModel, XGLMForCausalLM) if is_torch_available() else ()\n-    all_generative_model_classes = (XGLMForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": XGLMModel, \"text-generation\": XGLMForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "afe0a20a00cc99021021b4d93e6fc13fee59b120",
            "filename": "tests/models/xlm/test_modeling_xlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_xlm.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -374,9 +374,6 @@ class XLMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (\n-        (XLMWithLMHeadModel,) if is_torch_available() else ()\n-    )  # TODO (PVP): Check other models whether language generation is also applicable\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": XLMModel,"
        },
        {
            "sha": "094f977f15fc7cfab08e7abd39ef28a8d98b6e30",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -372,7 +372,6 @@ class XLMRobertaXLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTes\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (XLMRobertaXLForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": XLMRobertaXLModel,"
        },
        {
            "sha": "23a9ee0d89ea5923adce6e06522d95c0ad8c381e",
            "filename": "tests/models/xlnet/test_modeling_xlnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_xlnet.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -523,9 +523,6 @@ class XLNetModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (\n-        (XLNetLMHeadModel,) if is_torch_available() else ()\n-    )  # TODO (PVP): Check other models whether language generation is also applicable\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": XLNetModel,"
        },
        {
            "sha": "fd0be2da7e656335a9f4f692e4b0774993b1dd91",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -369,7 +369,6 @@ class XmodModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (XmodForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": XmodModel,"
        },
        {
            "sha": "ec52c950aac42748868f7794a3b8580ae38bdc74",
            "filename": "tests/models/yoso/test_modeling_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -296,7 +296,6 @@ class YosoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_headmasking = False\n     test_torchscript = False\n \n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": YosoModel,"
        },
        {
            "sha": "fc1c893222451c96acbc51fe2bdae98529ae34cf",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -291,7 +291,6 @@ class ZambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (ZambaForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ZambaModel,"
        },
        {
            "sha": "58f6b8e595e089140b72b130b333cbac6578abf4",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -302,7 +302,6 @@ class Zamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n         if is_torch_available()\n         else ()\n     )\n-    all_generative_model_classes = (Zamba2ForCausalLM,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Zamba2Model,"
        },
        {
            "sha": "9dd5877c8b90ab8ed9562c3d3a2cec80c95e3567",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -212,7 +212,6 @@ def sdpa_kernel(enable_flash, enable_math, enable_mem_efficient):\n class ModelTesterMixin:\n     model_tester = None\n     all_model_classes = ()\n-    all_generative_model_classes = ()\n     fx_compatible = False\n     test_torchscript = True\n     test_pruning = True\n@@ -230,6 +229,10 @@ class ModelTesterMixin:\n     _is_composite = False\n     model_split_percents = [0.5, 0.7, 0.9]\n \n+    @property\n+    def all_generative_model_classes(self):\n+        return tuple(model_class for model_class in self.all_model_classes if model_class.can_generate())\n+\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)\n         if model_class.__name__ in get_values(MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES):"
        },
        {
            "sha": "e6e3a860772df1465cd0f491be82ffc6ac2f7e36",
            "filename": "tests/test_modeling_flax_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Ftest_modeling_flax_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62c7ea0201cf27153c0bbd110d277ff9f39b0b1f/tests%2Ftest_modeling_flax_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_flax_common.py?ref=62c7ea0201cf27153c0bbd110d277ff9f39b0b1f",
            "patch": "@@ -133,6 +133,10 @@ class FlaxModelTesterMixin:\n     test_head_masking = False\n     has_attentions = True\n \n+    @property\n+    def all_generative_model_classes(self):\n+        return tuple(model_class for model_class in self.all_model_classes if model_class.can_generate())\n+\n     def _prepare_for_class(self, inputs_dict, model_class):\n         inputs_dict = copy.deepcopy(inputs_dict)\n "
        }
    ],
    "stats": {
        "total": 248,
        "additions": 50,
        "deletions": 198
    }
}