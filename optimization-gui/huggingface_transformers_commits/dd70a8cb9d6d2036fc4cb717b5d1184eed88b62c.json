{
    "author": "returnL",
    "message": "Fix MXFP4 quantizer validation to allow CPU inference with dequantize option (#39953)\n\n* Fix MXFP4 quantizer validation to enable CPU dequantization\n\nMove dequantize check before CUDA availability check to allow\r\nCPU inference when quantization_config.dequantize is True.\r\nThis enables users to run MXFP4 models on CPU by automatically\r\nconverting them to BF16 format.\n\n* Add tests for MXFP4 quantizer CPU dequantization validation\n\n* fix: format mxfp4 test file with ruff",
    "sha": "dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c",
    "files": [
        {
            "sha": "b3e52e5e1c54241eb4704ef6e5fdda57a6720399",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c",
            "patch": "@@ -56,15 +56,16 @@ def validate_environment(self, *args, **kwargs):\n                 \"Using mxfp4 quantization requires torch\"\n                 \"Please install the latest version of torch ( pip install --upgrade torch )\"\n             )\n+\n+        if self.quantization_config.dequantize:\n+            return\n+\n         if not torch.cuda.is_available():\n             raise RuntimeError(\"Using MXFP4 quantized models requires a GPU\")\n \n         if not is_accelerate_available():\n             raise ImportError(\"Using mxfp4 requires Accelerate: `pip install accelerate`\")\n \n-        if self.quantization_config.dequantize:\n-            return\n-\n         compute_capability = torch.cuda.get_device_capability()\n         major, minor = compute_capability\n "
        },
        {
            "sha": "56268b44df37ebae6880475834069b6fb521f55b",
            "filename": "tests/quantization/mxfp4/test_mxfp4.py",
            "status": "modified",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fmxfp4%2Ftest_mxfp4.py?ref=dd70a8cb9d6d2036fc4cb717b5d1184eed88b62c",
            "patch": "@@ -131,6 +131,52 @@ def test_quantizer_validation_low_compute_capability_with_dequantize(self):\n                 if \"compute capability\" in str(e):\n                     self.fail(\"Should not raise compute capability error when dequantize=True\")\n \n+    def test_quantizer_validation_dequantize_on_cpu(self):\n+        \"\"\"Test quantizer validation with dequantize enabled on CPU-only environment\"\"\"\n+        with patch(\"torch.cuda.is_available\", return_value=False):\n+            from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n+\n+            config = Mxfp4Config(dequantize=True)\n+            quantizer = Mxfp4HfQuantizer(config)\n+\n+            # Should not raise error when dequantize=True even without CUDA\n+            try:\n+                quantizer.validate_environment()\n+            except RuntimeError as e:\n+                if \"requires a GPU\" in str(e):\n+                    self.fail(\"Should not raise GPU requirement error when dequantize=True on CPU\")\n+\n+    def test_quantizer_validation_order_dequantize_before_cuda_check(self):\n+        \"\"\"Test that dequantize check happens before CUDA availability check\"\"\"\n+        # Mock both torch.cuda.is_available and is_accelerate_available to return False\n+        with (\n+            patch(\"torch.cuda.is_available\", return_value=False),\n+            patch(\n+                \"transformers.quantizers.quantizer_mxfp4.is_accelerate_available\",\n+                return_value=False,\n+            ),\n+        ):\n+            from transformers.quantizers.quantizer_mxfp4 import Mxfp4HfQuantizer\n+\n+            # Test with dequantize=True - should pass even without CUDA and accelerate\n+            config = Mxfp4Config(dequantize=True)\n+            quantizer = Mxfp4HfQuantizer(config)\n+\n+            # This should not raise any error because dequantize check comes first\n+            try:\n+                quantizer.validate_environment()\n+            except (RuntimeError, ImportError) as e:\n+                if \"requires a GPU\" in str(e) or \"requires Accelerate\" in str(e):\n+                    self.fail(f\"Should not raise error when dequantize=True: {e}\")\n+\n+            # Test with dequantize=False - should still fail due to missing CUDA\n+            config = Mxfp4Config(dequantize=False)\n+            quantizer = Mxfp4HfQuantizer(config)\n+\n+            with self.assertRaises(RuntimeError) as context:\n+                quantizer.validate_environment()\n+            self.assertIn(\"requires a GPU\", str(context.exception))\n+\n     def test_quantizer_validation_missing_triton(self):\n         \"\"\"Test quantizer validation when triton is not available\"\"\"\n         with ("
        }
    ],
    "stats": {
        "total": 53,
        "additions": 50,
        "deletions": 3
    }
}