{
    "author": "SunMarc",
    "message": "Fix deepspeed + quantization (#43006)\n\n* fix\n\n* style\n\n* fix\n\n* fix\n\n* no need to this class\n\n* rm\n\n* Apply suggestions from code review\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "42512f7956c7cea5810b2981513178553c299407",
    "files": [
        {
            "sha": "1d051b72286cf0377b4ef442d80194304efff6be",
            "filename": "src/transformers/integrations/compressed_tensors.py",
            "status": "removed",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fcompressed_tensors.py?ref=5f1c05cfddcecb8a233e47ba086336ff205135ad",
            "patch": "@@ -1,48 +0,0 @@\n-# coding=utf-8\n-# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from typing import Optional\n-\n-import torch\n-\n-from transformers.utils import logging\n-from transformers.utils.import_utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    from ..core_model_loading import ConversionOps\n-from ..quantizers.quantizers_utils import get_module_from_name\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class CompressedTensorsMarkInitialize(ConversionOps):\n-    def __init__(self, hf_quantizer):\n-        self.hf_quantizer = hf_quantizer\n-\n-    def convert(\n-        self,\n-        input_dict: dict[str, torch.Tensor],\n-        model: Optional[torch.nn.Module] = None,\n-        full_layer_name: str | None = None,\n-        missing_keys=None,\n-        **kwargs,\n-    ) -> dict[str, torch.Tensor]:\n-        _, value = tuple(input_dict.items())[0]\n-        module, tensor_name = get_module_from_name(model, full_layer_name)\n-        module._is_hf_initialized = True\n-\n-        return {full_layer_name: value}"
        },
        {
            "sha": "1f814ab34b389318037fb9cd1c9bc4cf8bfda7fd",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=42512f7956c7cea5810b2981513178553c299407",
            "patch": "@@ -304,7 +304,15 @@ def _load_state_dict_into_zero3_model(model_to_load, state_dict):\n         state_dict._metadata = metadata\n \n     error_msgs = []\n-    missing_keys = set(model_to_load.state_dict().keys())\n+    meta_model_state_dict = model_to_load.state_dict()\n+    missing_keys = set(meta_model_state_dict.keys())\n+\n+    prefix_model = getattr(model_to_load, \"base_model_prefix\", None)\n+    # take care of the case where in the checkpoint we don't have the prefix\n+    state_dict = {\n+        (f\"{prefix_model}.{k}\" if meta_model_state_dict.get(f\"{prefix_model}.{k}\") is not None else k): v\n+        for k, v in state_dict.items()\n+    }\n \n     # PyTorch's `_load_from_state_dict` does not copy parameters in a module's descendants\n     # so we need to apply the function recursively."
        },
        {
            "sha": "9c13a9c7e617f53d8fa261fffe41df76bf1293ab",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=42512f7956c7cea5810b2981513178553c299407",
            "patch": "@@ -43,6 +43,10 @@ def convert(\n \n         _load_parameter_into_model(model, full_layer_name, value)\n         module, _ = get_module_from_name(model, full_layer_name)\n+        # Need to set those to a specific value, otherwise they will remain on meta device ...\n+        module.input_scale = torch.ones(module.input_scale.shape)\n+        module.output_scale = torch.ones(module.output_scale.shape)\n+        # quantize\n         module.freeze()\n         module.weight.requires_grad = False\n         module._is_hf_initialized = True"
        },
        {
            "sha": "85a45ddd1a7767428b7bc65e61f0db62e25c995d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=42512f7956c7cea5810b2981513178553c299407",
            "patch": "@@ -2162,7 +2162,8 @@ def _init_weights(self, module):\n             std = getattr(self.config.get_text_config(), \"initializer_range\", 0.02)\n \n         if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d, nn.Conv3d, nn.ConvTranspose1d, nn.ConvTranspose2d)):\n-            init.normal_(module.weight, mean=0.0, std=std)\n+            if getattr(module, \"weight\", None) is not None:\n+                init.normal_(module.weight, mean=0.0, std=std)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n         elif isinstance(module, nn.Embedding):\n@@ -4373,12 +4374,26 @@ def _move_missing_keys_from_meta_to_cpu(\n         # will be re-initialized for nothing (which can be quite long)\n         for key in missing_keys - self.all_tied_weights_keys.keys():\n             param = self.get_parameter_or_buffer(key)\n-            value = torch.empty_like(param, device=\"cpu\")\n-            _load_parameter_into_model(self, key, value)\n+            if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n+                with deepspeed.zero.GatheredParameters([param], modifier_rank=0):\n+                    # needed for the sharding\n+                    param.data.copy_(torch.empty_like(param, device=param.device))\n+            else:\n+                value = torch.empty_like(param, device=\"cpu\")\n+                _load_parameter_into_model(self, key, value)\n         # We need to move back non-persistent buffers as well, as they are not part of loaded weights anyway\n         for key, buffer in self.named_non_persistent_buffers():\n-            value = torch.empty_like(buffer, device=\"cpu\")\n-            _load_parameter_into_model(self, key, value)\n+            if is_deepspeed_zero3_enabled() and not is_quantized:\n+                import deepspeed\n+\n+                with deepspeed.zero.GatheredParameters([buffer], modifier_rank=0):\n+                    # needed for the sharding\n+                    param.data.copy_(torch.empty_like(param, device=param.device))\n+            else:\n+                value = torch.empty_like(buffer, device=\"cpu\")\n+                _load_parameter_into_model(self, key, value)\n \n     def _initialize_missing_keys(self, is_quantized: bool) -> None:\n         \"\"\""
        },
        {
            "sha": "586f47ac0d1ca8196ecc3f11f68a47078d2b7862",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42512f7956c7cea5810b2981513178553c299407/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=42512f7956c7cea5810b2981513178553c299407",
            "patch": "@@ -109,17 +109,3 @@ def is_qat_trainable(self) -> bool:\n     def is_serializable(self) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n         return True\n-\n-    def get_weight_conversions(self):\n-        from ..core_model_loading import WeightConverter\n-        from ..integrations.compressed_tensors import CompressedTensorsMarkInitialize\n-\n-        return [\n-            WeightConverter(\n-                source_patterns=[\n-                    \"compressed\",\n-                ],\n-                target_patterns=\"compressed\",\n-                operations=[CompressedTensorsMarkInitialize(self)],\n-            ),\n-        ]"
        },
        {
            "sha": "d6b43723ac47e8718c76d08eb8c108dddfea06b8",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/42512f7956c7cea5810b2981513178553c299407/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42512f7956c7cea5810b2981513178553c299407/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=42512f7956c7cea5810b2981513178553c299407",
            "patch": "@@ -446,9 +446,7 @@ def test_compute_module_sizes(self):\n             # testing prequantized = False should be enough, the shape should be the same whether it is pre-quantized or not\n             hf_quantizer = AutoHfQuantizer.from_config(BitsAndBytesConfig(load_in_8bit=True), pre_quantized=False)\n             hf_quantizer.preprocess_model(model=model, config=model.config, device_map=expanded_device_map)\n-            quantized_model_size, _ = compute_module_sizes(\n-                model, hf_quantizer, only_modules=False, device_map=expanded_device_map\n-            )\n+            quantized_model_size, _ = compute_module_sizes(model, hf_quantizer, only_modules=False)\n \n             expected_keys = [name for name, _ in model.named_parameters()] + [\n                 name for name, _ in model.named_buffers()"
        }
    ],
    "stats": {
        "total": 105,
        "additions": 34,
        "deletions": 71
    }
}