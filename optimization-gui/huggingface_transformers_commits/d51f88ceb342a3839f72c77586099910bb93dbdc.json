{
    "author": "LysandreJik",
    "message": "Initial migration guide",
    "sha": "d51f88ceb342a3839f72c77586099910bb93dbdc",
    "files": [
        {
            "sha": "f24211f809caba0010bed20faead3fecdcb76785",
            "filename": "MIGRATION_GUIDE_V5.md",
            "status": "added",
            "additions": 236,
            "deletions": 0,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/d51f88ceb342a3839f72c77586099910bb93dbdc/MIGRATION_GUIDE_V5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d51f88ceb342a3839f72c77586099910bb93dbdc/MIGRATION_GUIDE_V5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/MIGRATION_GUIDE_V5.md?ref=d51f88ceb342a3839f72c77586099910bb93dbdc",
            "patch": "@@ -0,0 +1,236 @@\n+<!---\n+Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Version 5 Migration guide\n+\n+## Library-wide changes with widespread impact\n+\n+### Removal of TensorFlow and Jax\n+\n+We're removing the TensorFlow and Jax parts of the library. This will help us focus fully on `torch` \n+going forward and will greatly reduce the maintenance cost of models. We are working with tools from \n+the Jax ecosystem still (such as MaxText) in order to see how we can remain compatible with their \n+tool while keeping `torch` as the only backend for now.\n+\n+Linked PR: https://github.com/huggingface/transformers/pull/40760\n+\n+### Dynamic weight loading\n+\n+We introduce a new weight loading API in `transformers`, which significantly improves on the previous API. This\n+weight loading API is designed to apply operations to the checkpoints loaded by transformers.\n+\n+Instead of loading the checkpoint exactly as it is serialized within the model, these operations can reshape, merge,\n+and split the layers according to how they're defined in this new API. These operations are often a necessity when\n+working with quantization or parallelism algorithms.\n+\n+This new API is centered around the new `WeightConverter` class:\n+\n+```python\n+class WeightConverter(WeightTransform):\n+    operations: list[ConversionOps]\n+    source_keys: Union[str, list[str]]\n+    target_keys: Union[str, list[str]]\n+```\n+\n+The weight converter is designed to apply a list of operations on the source keys, resulting in target keys. A common\n+operation done on the attention layers is to fuse the query, key, values layers. Doing so with this API would amount\n+to defining the following conversion:\n+\n+```python\n+conversion = WeightConverter(\n+    [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],  # The input layers\n+    \"self_attn.qkv_proj\",  # The single layer as output\n+    operations=[Concatenate(dim=0)],\n+)\n+```\n+\n+In this situation, we apply the `Concatenate` operation, which accepts a list of layers as input and returns a single \n+layer. \n+\n+This allows us to define a mapping from architecture to a list of weight conversions. Applying those weight conversions\n+can apply arbitrary transformations to the layers themselves. This significantly simplified the `from_pretrained` method\n+and helped us remove a lot of technical debt that we accumulated over the past few years.\n+\n+This results in several improvements:\n+- Much cleaner definition of transformations applied to the checkpoint\n+- Reversible transformations, so loading and saving a checkpoint should result in the same checkpoint\n+- Faster model loading thanks to scheduling of tensor materialization\n+- Enables complex mix of transformations that wouldn't otherwise be possible (such as quantization + MoEs, or TP + MoEs)\n+\n+While this is being implemented, expect varying levels of support across different release candidates.\n+\n+Linked PR: https://github.com/huggingface/transformers/pull/41580\n+\n+## Library-wide changes with lesser impact\n+\n+### `use_auth_token`\n+\n+The `use_auth_token` argument/parameter is deprecated in favor of `token` everywhere.\n+You should be able to search and replace `use_auth_token` with `token` and get the same logic.\n+\n+Linked PR: https://github.com/huggingface/transformers/pull/41666\n+\n+We decided to remove some features for the upcoming v5 as they are currently only supported in a few old models and no longer integrated in current model additions. It's recommended to stick to v4.x in case you need them. Following features are affected:\n+- No more head masking, see #41076. This feature allowed to turn off certain heads during the attention calculation and only worked for eager.\n+- No more relative positional biases in Bert-like models, see #41170. This feature was introduced to allow relative position scores within attention calculations (similar to T5). However, this feature is barely used in official models and a lot of complexity instead. It also only worked with eager.\n+- No more head pruning, see #41417 by @gante. As the name suggests, it allowed to prune heads within your attention layers.\n+\n+### Updates to supported torch APIs\n+\n+We dropped support for two torch APIs:\n+- `torchscript` in https://github.com/huggingface/transformers/pull/41688\n+- `torch.fx` in https://github.com/huggingface/transformers/pull/41683\n+\n+Those APIs were deprecated by the PyTorch team, and we're instead focusing on the supported APIs `dynamo` and `export`.\n+\n+## Quantization changes\n+\n+We clean up the quantization API in transformers, and significantly refactor the weight loading as highlighted\n+above.\n+\n+We drop support for two quantization arguments that have been deprecated for some time:\n+- `load_in_4bit`\n+- `load_in_8bit`\n+\n+We remove them in favor of the `quantization_config` argument which is much more complete. As an example, here is how\n+you would load a 4-bit bitsandbytes model using this argument:\n+\n+```python\n+from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+\n+model_4bit = AutoModelForCausalLM.from_pretrained(\n+    \"meta-llama/Llama-3.2-3B\",\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+```\n+\n+\n+## Configuration\n+\n+- Methods to init a nested config such as `from_xxx_config` are deleted. Configs can be init from the `__init__` method in the same way (https://github.com/huggingface/transformers/pull/41314)\n+\n+## Processing\n+\n+### Tokenization\n+\n+- Slow tokenizer files (aka: `tokenization_<model>.py` ) will be removed in favor of using fast tokenizer files `tokenization_<model>_fast.py` --> will be renamed to `tokenization_<model>.py`.  As fast tokenizers are :hugs:`tokenizers` - backend, they include a wider range of features that are maintainable and reliable. \n+- Other backends (sentence piece, tokenizers, etc.) will be supported with a light layer if loading a fast tokenizer fails\n+- Remove legacy files like special_tokens_map.json and added_tokens.json\n+- Remove _eventually_correct_t5_max_length \n+- `encode_plus` --> `__call__`\n+- `batch_decode` --> `decode`\n+\n+`apply_chat_template` by default returns naked `input_ids` rather than a `BatchEncoding` dict. \n+This was inconvenient - it should return a `BatchEncoding` dict like `tokenizer.__call__()`, but we were stuck with \n+it for backward compatibility. The method now returns a `BatchEncoding`.\n+\n+Linked PRs: \n+- https://github.com/huggingface/transformers/issues/40938\n+- https://github.com/huggingface/transformers/pull/40936\n+- https://github.com/huggingface/transformers/pull/41626\n+\n+### Processing classes\n+\n+- In processing classes each attribute will be serialized under `processor_config.json` as a nested dict, instead of serializing attributes in their own config files. Loading will be supported for all old format processors (https://github.com/huggingface/transformers/pull/41474)\n+- `XXXFeatureExtractors` classes are completely removed in favor of `XXXImageProcessor` class for all vision models (https://github.com/huggingface/transformers/pull/41174)\n+- Minor change: `XXXFastImageProcessorKwargs` is removed in favor of `XXXImageProcessorKwargs` which will be shared between fast and slow processors (https://github.com/huggingface/transformers/pull/40931)\n+\n+## Modeling\n+\n+- Some `RotaryEmbeddings` layers will start returning a dict of tuples, in case the model uses several RoPE configurations (Gemma2, ModernBert). Each value will be a tuple of \"cos, sin\" per RoPE type.\n+- Config attribute for `RotaryEmbeddings` layer will be unified and accessed via `config.rope_parameters`. Config attr for `rope_theta` might not be accessible anymore for some models, and instead will be in `config.rope_parameters['rope_theta']`. BC will be supported for a while as much as possible, and in the near future we'll gradually move to the new RoPE format  (https://github.com/huggingface/transformers/pull/39847)\n+\n+### Generate\n+\n+- Old, deprecated output type aliases were removed (e.g. `GreedySearchEncoderDecoderOutput`). We now only have 4 output classes built from the following matrix: decoder-only vs encoder-decoder, uses beams vs doesn't use beams (https://github.com/huggingface/transformers/pull/40998)\n+- Removed deprecated classes regarding decoding methods that were moved to the Hub due to low usage (constraints and beam scores) (https://github.com/huggingface/transformers/pull/41223)\n+- If `generate` doesn't receive any KV Cache argument, the default cache class used is now defined by the model (as opposed to always being `DynamicCache`) (https://github.com/huggingface/transformers/pull/41505)\n+\n+## Trainer\n+\n+### Removing arguments without deprecation cycle in `TrainingArguments` due to low usage\n+\n+- `mp_parameters` -> legacy param that was later on added to sagemaker trainer\n+- `_n_gpu` -> not intended for users to set, we will initialize it correctly instead of putting it in the `TrainingArguments`\n+- `overwrite_output_dir` - > replaced by `resume_from_checkpoint` and it was only used in examples script, no impact on Trainer. \n+- `logging_dir` -> only used for tensorboard, set `TENSORBOARD_LOGGING_DIR` env var instead\n+- `jit_mode_eval` -> use `use_torch_compile` instead as torchscript is not recommended anymore\n+- `tpu_num_cores`-> It is actually better to remove it as it is not recommended to set the number of cores. By default, all tpu cores are used . Set `TPU_NUM_CORES` env var instead\n+- `past_index` -> it was only used for a very small number of models that have special architecture like transformersxl + it was not documented at all how to train those model\n+- `ray_scope` -> only for a minor arg for ray integration. Set `RAY_SCOPE` var env instead \n+- `warmup_ratio` -> use `warmup_step` instead. We combined both args together by allowing passing float values in `warmup_step`. \n+\n+### Removing deprecated arguments in `TrainingArguments`\n+\n+- `fsdp_min_num_params` and `fsdp_transformer_layer_cls_to_wrap` -> use `fsdp_config`\n+- `tpu_metrics_debug` -> `debug` \n+- `push_to_hub_token` -> `hub_token`\n+- `push_to_hub_model_id` and `push_to_hub_organization` -> `hub_model_id`\n+- `include_inputs_for_metrics` -> `include_for_metrics`\n+- `per_gpu_train_batch_size` -> `per_device_train_batch_size`\n+- `per_gpu_eval_batch_size` -> `per_device_eval_batch_size`\n+- `use_mps_device` -> mps will be used by default if detected\n+- `fp16_backend` and `half_precision_backend` -> we will only rely on torch.amp as everything has been upstream to torch\n+- `no_cuda` -> `use_cpu`\n+- ` include_tokens_per_second` -> `include_num_input_tokens_seen`\n+- `use_legacy_prediction_loop` -> we only use `evaluation_loop` function from now on\n+\n+### Removing deprecated arguments in `Trainer`\n+\n+- `tokenizer` in initialization -> `processor`\n+- `model_path` in train() -> `resume_from_checkpoint`\n+\n+### Removed features for `Trainer`\n+\n+- sigpot integration for hp search was removed as the library was archived + the api stopped working\n+- drop support for sagemaker API <1.10\n+- bump accelerate minimum version to 1.1.0 \n+\n+###  New defaults for `Trainer`\n+\n+- `use_cache` in the model config will be set to `False`. You can still change the cache value through `TrainingArguments` `usel_cache` argument if needed. \n+\n+## CLI\n+\n+The deprecated `transformers-cli ...` command was deprecated, `transformers ...` is now the only CLI entry point.\n+\n+`transformers` CLI has been migrated to `Typer`, making it easier to maintain + adding some nice features out of \n+the box (improved `--help` section, autocompletion).\n+\n+Biggest breaking change is in `transformers chat`. This command starts a terminal UI to interact with a chat model. \n+It used to also be able to start a Chat Completion server powered by `transformers` and chat with it. In this revamped \n+version, this feature has been removed in favor of `transformers serve`. The goal of splitting `transformers chat` \n+and `transformers serve` is to define clear boundaries between client and server code. It helps with maintenance \n+but also makes the commands less bloated. The new signature of `transformers chat` is:\n+\n+```\n+Usage: transformers chat [OPTIONS] BASE_URL MODEL_ID [GENERATE_FLAGS]...\n+\n+  Chat with a model from the command line.\n+```\n+\n+Example:\n+\n+```sh\n+transformers chat https://router.huggingface.co/v1 HuggingFaceTB/SmolLM3-3B\n+```\n+\n+Linked PRs: \n+- https://github.com/huggingface/transformers/pull/40997\n+- https://github.com/huggingface/transformers/pull/41487"
        }
    ],
    "stats": {
        "total": 236,
        "additions": 236,
        "deletions": 0
    }
}