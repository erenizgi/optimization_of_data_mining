{
    "author": "qgallouedec",
    "message": "SmolVLM and InternVL: Ensure pixel values are converted to the correct dtype for fp16/bf16 (#40121)\n\n* Ensure pixel values are converted to the correct dtype for fp16/bf16\n\n* add to modular",
    "sha": "6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1",
    "files": [
        {
            "sha": "8e1d2709bc92038fc6c66b3d3c67fe9c85ff330e",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1",
            "patch": "@@ -604,6 +604,7 @@ def get_image_features(\n             if vision_feature_select_strategy is not None\n             else self.config.vision_feature_select_strategy\n         )\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n \n         downsample_ratio = self.config.downsample_ratio\n         if vision_feature_layer == -1:"
        },
        {
            "sha": "26be41d629beb357f27e5e678a98e22b8bc13203",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1",
            "patch": "@@ -544,6 +544,7 @@ def get_image_features(\n             if vision_feature_select_strategy is not None\n             else self.config.vision_feature_select_strategy\n         )\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n \n         downsample_ratio = self.config.downsample_ratio\n         if vision_feature_layer == -1:"
        },
        {
            "sha": "e13e4afdf9d994e60cf824af4c143d940397b8b9",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1",
            "patch": "@@ -661,6 +661,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_ma\n                 The attention mask indicating padded regions in the image.\n         \"\"\"\n         batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n         pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n \n         # Remove padding images - padding images are full 0."
        },
        {
            "sha": "9dd8353b94823ca12659fbb737bd39485f2baef3",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=6ceb13fb22dc4ac23abb2b4b35d6b311461fe5e1",
            "patch": "@@ -205,6 +205,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_ma\n                 The attention mask indicating padded regions in the image.\n         \"\"\"\n         batch_size, num_images, num_channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.to(dtype=self.dtype)  # fp16 compatibility\n         pixel_values = pixel_values.view(batch_size * num_images, *pixel_values.shape[2:])\n \n         # Remove padding images - padding images are full 0."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}