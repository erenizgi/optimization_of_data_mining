{
    "author": "RyanMullins",
    "message": "Reinstate self.scaling in Gemma3nTextAttention (#41751)\n\nmaintenance: make Gemma3nTextAttention more amenable to modular inheritance",
    "sha": "91d250efb12fd051481b90442597cff0c3a3825e",
    "files": [
        {
            "sha": "0760a5d743b7fe5f41031ad7919d4ce822eb3629",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/91d250efb12fd051481b90442597cff0c3a3825e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91d250efb12fd051481b90442597cff0c3a3825e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=91d250efb12fd051481b90442597cff0c3a3825e",
            "patch": "@@ -1237,6 +1237,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = 1.0\n         self.attention_dropout = self.config.attention_dropout\n         self.is_causal = True\n \n@@ -1335,7 +1336,7 @@ def forward(\n             value_states,\n             attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n-            scaling=1.0,\n+            scaling=self.scaling,\n             sliding_window=self.sliding_window,\n             **kwargs,\n         )"
        },
        {
            "sha": "ed1d3adfe1735ff2d33545fd8917ba52657d0297",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/91d250efb12fd051481b90442597cff0c3a3825e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91d250efb12fd051481b90442597cff0c3a3825e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=91d250efb12fd051481b90442597cff0c3a3825e",
            "patch": "@@ -1703,7 +1703,7 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.is_causal = True\n         del self.attn_logit_softcapping\n-        del self.scaling\n+        self.scaling = 1.0\n         self.v_norm = Gemma3nRMSNorm(dim=config.head_dim, eps=config.rms_norm_eps, with_scale=False)\n \n         first_kv_shared_layer_idx = self.config.num_hidden_layers - self.config.num_kv_shared_layers\n@@ -1782,7 +1782,7 @@ def forward(\n             value_states,\n             attention_mask,\n             dropout=self.attention_dropout if self.training else 0.0,\n-            scaling=1.0,\n+            scaling=self.scaling,\n             sliding_window=self.sliding_window,\n             **kwargs,\n         )"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}