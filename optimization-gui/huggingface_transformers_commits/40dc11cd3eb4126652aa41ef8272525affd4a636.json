{
    "author": "Cyrilvallez",
    "message": "Fix Gemma (#42847)\n\nfix",
    "sha": "40dc11cd3eb4126652aa41ef8272525affd4a636",
    "files": [
        {
            "sha": "3d72f492697e87c07ffe6361ffcf6030fc7a011b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/40dc11cd3eb4126652aa41ef8272525affd4a636/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40dc11cd3eb4126652aa41ef8272525affd4a636/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=40dc11cd3eb4126652aa41ef8272525affd4a636",
            "patch": "@@ -410,16 +410,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # It may already have been prepared by e.g. `generate`\n-        if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            causal_mask_mapping = create_causal_mask(\n-                config=self.config,\n-                input_embeds=inputs_embeds,\n-                attention_mask=attention_mask,\n-                cache_position=cache_position,\n-                past_key_values=past_key_values,\n-                position_ids=position_ids,\n-            )\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -434,7 +432,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask_mapping,\n+                attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        },
        {
            "sha": "35cf2b10f10cdec4b364394ae2e8f9c5d81896c7",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 11,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/40dc11cd3eb4126652aa41ef8272525affd4a636/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/40dc11cd3eb4126652aa41ef8272525affd4a636/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=40dc11cd3eb4126652aa41ef8272525affd4a636",
            "patch": "@@ -267,16 +267,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        # It may already have been prepared by e.g. `generate`\n-        if not isinstance(causal_mask_mapping := attention_mask, dict):\n-            causal_mask_mapping = create_causal_mask(\n-                config=self.config,\n-                input_embeds=inputs_embeds,\n-                attention_mask=attention_mask,\n-                cache_position=cache_position,\n-                past_key_values=past_key_values,\n-                position_ids=position_ids,\n-            )\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -291,7 +289,7 @@ def forward(\n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask_mapping,\n+                attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n                 use_cache=use_cache,"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 18,
        "deletions": 22
    }
}