{
    "author": "hutaiHang",
    "message": "fix(trainer): Correct loss scaling for incomplete gradient accumulation steps (#39659)\n\n* Fix issue[#38837]: wrong loss scaled in last step of epoch\n\n* chore: trigger CI\n\n* Update src/transformers/trainer.py\n\nCo-authored-by: Quentin Gallouédec <45557362+qgallouedec@users.noreply.github.com>\n\n* Update src/transformers/modeling_flash_attention_utils.py\n\nCo-authored-by: Quentin Gallouédec <45557362+qgallouedec@users.noreply.github.com>\n\n---------\n\nCo-authored-by: taihang <taihang@U-2RHYVWX7-2207.local>\nCo-authored-by: Quentin Gallouédec <45557362+qgallouedec@users.noreply.github.com>",
    "sha": "075dbbceaa2faf328ce54fdb7a8e842e596c55d2",
    "files": [
        {
            "sha": "3e5df5fb29f8b11eb0b62d2cdc3d7b4b8531dbf8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/075dbbceaa2faf328ce54fdb7a8e842e596c55d2/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/075dbbceaa2faf328ce54fdb7a8e842e596c55d2/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=075dbbceaa2faf328ce54fdb7a8e842e596c55d2",
            "patch": "@@ -2530,6 +2530,9 @@ def _inner_training_loop(\n                 update_step += 1\n                 num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\n                 batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)\n+                # Store the number of batches for current gradient accumulation\n+                # This is used to correctly scale the loss when the last accumulation step has fewer batches\n+                self.current_gradient_accumulation_steps = len(batch_samples)\n                 for i, inputs in enumerate(batch_samples):\n                     step += 1\n                     do_sync_step = (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == steps_in_epoch\n@@ -3830,7 +3833,8 @@ def training_step(\n         else:\n             # Finally we need to normalize the loss for reporting if GA loss bug is not fixed during compute loss\n             if (not self.model_accepts_loss_kwargs or num_items_in_batch is None) and self.compute_loss_func is None:\n-                loss = loss / self.args.gradient_accumulation_steps\n+                # If the model does not accept loss kwargs, we need to normalize the loss by the number of gradient accumulation steps\n+                loss = loss / self.current_gradient_accumulation_steps\n \n             # Turning off loss scaling w.r.t. gradient accumulation when DeepSpeed is enabled\n             # https://github.com/huggingface/transformers/pull/35808"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 5,
        "deletions": 1
    }
}