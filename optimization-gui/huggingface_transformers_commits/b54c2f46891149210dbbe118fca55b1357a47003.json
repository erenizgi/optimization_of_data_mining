{
    "author": "winglian",
    "message": "more fixes for post-training llama4 (#37329)\n\n* more fixes for post-training llama4\n\n* use target_length instead of guearded past_key_values",
    "sha": "b54c2f46891149210dbbe118fca55b1357a47003",
    "files": [
        {
            "sha": "0ba34ce37a420ab1fe5dc44f9c70e2b47ea8ee36",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b54c2f46891149210dbbe118fca55b1357a47003/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b54c2f46891149210dbbe118fca55b1357a47003/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=b54c2f46891149210dbbe118fca55b1357a47003",
            "patch": "@@ -730,6 +730,7 @@ def forward(\n         )\n         return output if return_dict else output.to_tuple()\n \n+    @torch.compiler.disable  # the operations in this method are not compilable\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -767,7 +768,7 @@ def _update_causal_mask(\n         )\n \n         if past_key_values is not None and past_key_values.is_compileable:\n-            target_length = past_key_values.get_max_cache_shape\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n \n@@ -780,7 +781,7 @@ def _update_causal_mask(\n                 attention_mask = make_flex_block_causal_mask(\n                     attention_mask,\n                     query_length=sequence_length,\n-                    key_length=past_key_values.get_max_cache_shape(),\n+                    key_length=target_length,\n                     offsets=None if sequence_length != 1 else (first_cache_position, 0),\n                 )\n                 return attention_mask, chunked_attention_mask"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 3,
        "deletions": 2
    }
}