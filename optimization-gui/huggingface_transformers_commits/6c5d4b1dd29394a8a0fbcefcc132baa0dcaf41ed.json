{
    "author": "bzantium",
    "message": "allow custom head_dim for qwen2_moe (#37188)\n\nallow custom head_dim\n\nCo-authored-by: ryan.agile <ryan.agile@kakaobrain.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "6c5d4b1dd29394a8a0fbcefcc132baa0dcaf41ed",
    "files": [
        {
            "sha": "7f81c331ccc91d558521a3c121a2786669a5fc1d",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c5d4b1dd29394a8a0fbcefcc132baa0dcaf41ed/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c5d4b1dd29394a8a0fbcefcc132baa0dcaf41ed/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=6c5d4b1dd29394a8a0fbcefcc132baa0dcaf41ed",
            "patch": "@@ -281,7 +281,7 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n \n         self.hidden_size = config.hidden_size\n         self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}