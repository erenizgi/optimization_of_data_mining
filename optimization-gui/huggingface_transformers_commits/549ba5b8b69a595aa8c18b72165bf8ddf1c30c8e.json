{
    "author": "jijihuny",
    "message": "[Docs] Add missing class documentation for optimizer_schedules (#31870,  #23010) (#40761)\n\n* Add missing class documentation for optimizer_schedules (#31870, #23010)\n\n* Add section level header to the optimizer schedules",
    "sha": "549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e",
    "files": [
        {
            "sha": "84d9ca7b907e1f54707679e4587c80ee8ac5839e",
            "filename": "docs/source/en/main_classes/optimizer_schedules.md",
            "status": "modified",
            "additions": 31,
            "deletions": 1,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foptimizer_schedules.md?ref=549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e",
            "patch": "@@ -29,32 +29,62 @@ The `.optimization` module provides:\n \n ## Schedules\n \n-### Learning Rate Schedules\n+### SchedulerType\n \n [[autodoc]] SchedulerType\n \n+### get_scheduler\n+\n [[autodoc]] get_scheduler\n \n+### get_constant_schedule\n+\n [[autodoc]] get_constant_schedule\n \n+### get_constant_schedule_with_warmup\n+\n [[autodoc]] get_constant_schedule_with_warmup\n \n <img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_constant_schedule.png\"/>\n \n+### get_cosine_schedule_with_warmup\n+\n [[autodoc]] get_cosine_schedule_with_warmup\n \n <img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_schedule.png\"/>\n \n+### get_cosine_with_hard_restarts_schedule_with_warmup\n+\n [[autodoc]] get_cosine_with_hard_restarts_schedule_with_warmup\n \n+### get_cosine_with_min_lr_schedule_with_warmup\n+\n+[[autodoc]] get_cosine_with_min_lr_schedule_with_warmup\n+\n+### get_cosine_with_min_lr_schedule_with_warmup_lr_rate\n+\n+[[autodoc]] get_cosine_with_min_lr_schedule_with_warmup_lr_rate\n+\n <img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_cosine_hard_restarts_schedule.png\"/>\n \n+### get_linear_schedule_with_warmup\n+\n [[autodoc]] get_linear_schedule_with_warmup\n \n <img alt=\"\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/warmup_linear_schedule.png\"/>\n \n+### get_polynomial_decay_schedule_with_warmup\n+\n [[autodoc]] get_polynomial_decay_schedule_with_warmup\n \n+### get_inverse_sqrt_schedule\n+\n [[autodoc]] get_inverse_sqrt_schedule\n \n+### get_reduce_on_plateau_schedule\n+\n+[[autodoc]] get_reduce_on_plateau_schedule\n+\n+### get_wsd_schedule\n+\n [[autodoc]] get_wsd_schedule"
        },
        {
            "sha": "a3daadcf1b4e019663c8465650a1779fb9e2cfe1",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e",
            "patch": "@@ -477,11 +477,14 @@\n         \"get_constant_schedule_with_warmup\",\n         \"get_cosine_schedule_with_warmup\",\n         \"get_cosine_with_hard_restarts_schedule_with_warmup\",\n+        \"get_cosine_with_min_lr_schedule_with_warmup\",\n+        \"get_cosine_with_min_lr_schedule_with_warmup_lr_rate\",\n         \"get_inverse_sqrt_schedule\",\n         \"get_linear_schedule_with_warmup\",\n         \"get_polynomial_decay_schedule_with_warmup\",\n         \"get_scheduler\",\n         \"get_wsd_schedule\",\n+        \"get_reduce_on_plateau_schedule\",\n     ]\n     _import_structure[\"pytorch_utils\"] = [\n         \"Conv1D\",\n@@ -796,6 +799,12 @@\n     from .optimization import (\n         get_cosine_with_hard_restarts_schedule_with_warmup as get_cosine_with_hard_restarts_schedule_with_warmup,\n     )\n+    from .optimization import (\n+        get_cosine_with_min_lr_schedule_with_warmup as get_cosine_with_min_lr_schedule_with_warmup,\n+    )\n+    from .optimization import (\n+        get_cosine_with_min_lr_schedule_with_warmup_lr_rate as get_cosine_with_min_lr_schedule_with_warmup_lr_rate,\n+    )\n     from .optimization import get_inverse_sqrt_schedule as get_inverse_sqrt_schedule\n     from .optimization import get_linear_schedule_with_warmup as get_linear_schedule_with_warmup\n     from .optimization import get_polynomial_decay_schedule_with_warmup as get_polynomial_decay_schedule_with_warmup"
        },
        {
            "sha": "e2a382db6c912b29daceeb14f547aab3abde7115",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=549ba5b8b69a595aa8c18b72165bf8ddf1c30c8e",
            "patch": "@@ -421,16 +421,17 @@ class SchedulerType(ExplicitEnum):\n     Scheduler names for the parameter `lr_scheduler_type` in [`TrainingArguments`].\n     By default, it uses \"linear\". Internally, this retrieves `get_linear_schedule_with_warmup` scheduler from [`Trainer`].\n     Scheduler types:\n-       - \"linear\" = get_linear_schedule_with_warmup\n-       - \"cosine\" = get_cosine_schedule_with_warmup\n-       - \"cosine_with_restarts\" = get_cosine_with_hard_restarts_schedule_with_warmup\n-       - \"polynomial\" = get_polynomial_decay_schedule_with_warmup\n-       - \"constant\" =  get_constant_schedule\n-       - \"constant_with_warmup\" = get_constant_schedule_with_warmup\n-       - \"inverse_sqrt\" = get_inverse_sqrt_schedule\n-       - \"reduce_lr_on_plateau\" = get_reduce_on_plateau_schedule\n-       - \"cosine_with_min_lr\" = get_cosine_with_min_lr_schedule_with_warmup\n-       - \"warmup_stable_decay\" = get_wsd_schedule\n+       - \"linear\" = [`get_linear_schedule_with_warmup`]\n+       - \"cosine\" = [`get_cosine_schedule_with_warmup`]\n+       - \"cosine_with_restarts\" = [`get_cosine_with_hard_restarts_schedule_with_warmup`]\n+       - \"polynomial\" = [`get_polynomial_decay_schedule_with_warmup`]\n+       - \"constant\" =  [`get_constant_schedule`]\n+       - \"constant_with_warmup\" = [`get_constant_schedule_with_warmup`]\n+       - \"inverse_sqrt\" = [`get_inverse_sqrt_schedule`]\n+       - \"reduce_lr_on_plateau\" = [`get_reduce_on_plateau_schedule`]\n+       - \"cosine_with_min_lr\" = [`get_cosine_with_min_lr_schedule_with_warmup`]\n+       - \"cosine_warmup_with_min_lr\" = [`get_cosine_with_min_lr_schedule_with_warmup_lr_rate`]\n+       - \"warmup_stable_decay\" = [`get_wsd_schedule`]\n     \"\"\"\n \n     LINEAR = \"linear\""
        }
    ],
    "stats": {
        "total": 62,
        "additions": 51,
        "deletions": 11
    }
}