{
    "author": "zucchini-nlp",
    "message": "[refactor] set attention implementation (#38974)\n\n* update\n\n* fix some tests\n\n* init from config, changes it in-place, add deepcopy in tests\n\n* fix modernbert\n\n* don't delete thsi config attr\n\n* update\n\n* style and copies\n\n* skip tests in generation\n\n* fix style\n\n* accidentally removed flash-attn-3, revert\n\n* docs\n\n* forgot about flags set to False\n\n* fix copies\n\n* address a few comments\n\n* fix copies\n\n* custom code BC",
    "sha": "8d6259b0b8290c2406949ce6342051b1f09a074c",
    "files": [
        {
            "sha": "8e10c8eef5d3e707aab859667c6d7d58e678e8e4",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 165,
            "deletions": 240,
            "changes": 405,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1961,11 +1961,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     supports_gradient_checkpointing = False\n     _is_stateful = False\n \n-    # Flash Attention 2 support\n-    _supports_flash_attn_2 = False\n-\n-    # Flash Attention 3 support\n-    _supports_flash_attn_3 = False\n+    # Flash Attention support\n+    _supports_flash_attn = False\n \n     # SDPA support\n     _supports_sdpa = False\n@@ -2074,12 +2071,15 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n                 \"`PretrainedConfig`. To create a model from a pretrained model use \"\n                 f\"`model = {self.__class__.__name__}.from_pretrained(PRETRAINED_MODEL_NAME)`\"\n             )\n-        if not getattr(config, \"_attn_implementation_autoset\", False):\n-            # config usually has a `torch_dtype` but we need the next line for the `no_super_init` tests\n-            dtype = config.torch_dtype if hasattr(config, \"torch_dtype\") else torch.get_default_dtype()\n-            config = self._autoset_attn_implementation(config, torch_dtype=dtype, check_device_map=False)\n         self.config = config\n \n+        # The `hasattr` here is used as some Transformers tests for some reason do not call\n+        # PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)\n+        if hasattr(config, \"_attn_implementation_internal\") and not getattr(\n+            config, \"_attn_implementation_autoset\", False\n+        ):\n+            self.set_attention_implementation(self.config._attn_implementation_internal)\n+\n         # for initialization of the loss\n         loss_type = self.__class__.__name__\n         if loss_type not in LOSS_MAPPING:\n@@ -2226,19 +2226,11 @@ def _from_config(cls, config, **kwargs):\n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in _from_config.\n \n         if config._attn_implementation_internal is not None:\n-            # In this case, the config has been created with the attn_implementation set by the user, which we\n-            # should respect.\n+            # In this case, the config has been created with the attn_implementation set by the user, which we should respect.\n             attn_implementation = config._attn_implementation_internal\n         else:\n             attn_implementation = None\n-\n         config._attn_implementation = kwargs.pop(\"attn_implementation\", attn_implementation)\n-        if not getattr(config, \"_attn_implementation_autoset\", False):\n-            config = cls._autoset_attn_implementation(\n-                config,\n-                check_device_map=False,\n-                torch_dtype=torch_dtype,\n-            )\n \n         if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n@@ -2260,81 +2252,65 @@ def _from_config(cls, config, **kwargs):\n         return model\n \n     @classmethod\n-    def _autoset_attn_implementation(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-    ):\n+    def _check_attn_implementation(cls, attn_implementation: Union[str, dict]) -> Union[str, dict]:\n         \"\"\"\n-        Automatically checks and dispatches to a default attention implementation. In order of priority:\n-            1. An implementation specified in `config._attn_implementation` (due for example to the argument attn_implementation=\"sdpa\" in from_pretrained).\n-            2. SDPA implementation, if available and supported by the model type. (`LlamaSdpaAttention` for example)\n-            3. The default model's implementation otherwise (`LlamaAttention` for example) .\n+        Checks that the requested attention implementation exists and tries to get the kernel from hub\n+        if `attn_implementation` matches hf kernels pattern.\n         \"\"\"\n-        # Here we use config._attn_implementation_internal to check whether the attention implementation was explicitly set by the user.\n-        # The property `PretrainedConfig._attn_implementation` is never `None`, for backward compatibility (always fall back on \"eager\").\n-        # The `hasattr` here is used as some Transformers tests for some reason do not call PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)\n-        requested_attn_implementation = None\n-        if hasattr(config, \"_attn_implementation_internal\") and config._attn_implementation_internal is not None:\n-            if isinstance(config._attn_implementation, str) and re.match(\n-                r\"^[^/:]+/[^/:]+:[^/:]+$\", config._attn_implementation\n-            ):\n-                if not is_kernels_available():\n-                    raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n+        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n+            if not is_kernels_available():\n+                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n \n-                # Extract repo_id and kernel_name from the string\n-                repo_id, kernel_name = config._attn_implementation.split(\":\")\n-                kernel_name = kernel_name.strip()\n-                repo_id = repo_id.strip()\n+            # Extract repo_id and kernel_name from the string\n+            repo_id, kernel_name = attn_implementation.split(\":\")\n+            kernel_name = kernel_name.strip()\n+            repo_id = repo_id.strip()\n \n-                try:\n-                    kernel = get_kernel(repo_id)\n-                    ALL_ATTENTION_FUNCTIONS.register(\n-                        f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name)\n-                    )\n-                    config._attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n-                except FileNotFoundError as e:\n-                    logger.warning(\n-                        f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n-                    )\n-                    config._attn_implementation = \"eager\"\n-                except AttributeError:\n-                    raise ValueError(\n-                        \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n-                                     Please check the documentation for the correct format, \\\n-                                     and check that the kernel exports the class and the function correctly.\"\n-                    )\n+            try:\n+                kernel = get_kernel(repo_id)\n+                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n+                attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n+            except FileNotFoundError as e:\n+                logger.warning(\n+                    f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n+                )\n+                attn_implementation = None  # try to dispatch SDPA and fallback eager if not available\n+            except AttributeError:\n+                raise ValueError(\n+                    \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n+                                 Please check the documentation for the correct format, \\\n+                                 and check that the kernel exports the class and the function correctly.\"\n+                )\n+        if (\n+            not isinstance(attn_implementation, dict)\n+            and attn_implementation not in [\"eager\", None] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n+        ):\n+            message = f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n+            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n+            if cls._supports_flash_attn or getattr(cls, \"_supports_flash_attn_2\", False):\n+                message += (\n+                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n+                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n+                )\n+            if cls._supports_sdpa:\n+                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n+            if cls._supports_flex_attn:\n+                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n+            raise ValueError(message + \".\")\n \n-            if (\n-                not isinstance(config._attn_implementation, dict)\n-                and config._attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n-            ):\n-                message = f'Specified `attn_implementation=\"{config._attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n-                if cls._supports_flash_attn_3:\n-                    message += ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n-                if cls._supports_flash_attn_2:\n-                    message += ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n-                if cls._supports_sdpa:\n-                    message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n-                if cls._supports_flex_attn:\n-                    message += (\n-                        ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n-                    )\n-                raise ValueError(message + \".\")\n+        return attn_implementation\n \n-            # If a config is passed with a preset attn_implementation, we skip the automatic dispatch and use the user-provided config, with hard checks that the requested attention implementation is available.\n-            requested_attn_implementation = config._attn_implementation_internal\n+    def set_attention_implementation(self, attn_implementation: Union[str, dict]):\n+        \"\"\"\n+        Checks and dispatches to the requested attention implementation.\n+        \"\"\"\n+        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n \n-        # Composite models consisting of several PretrainedModels have to specify attention impl as a dict\n-        # where keys are sub-config names. But most people will specify one `str` which means that should dispatch it\n-        # for all sub-models.\n-        # Below we check if a config is composite and manually prepare a dict of attn impl if not already passed as a dict.\n-        # Later each sub-module will dispatch with its own attn impl, by calling `XXXModel._from_config(config.text_config)`\n-        # If any of sub-modules doesn't support requested attn, an error will be raised. See https://github.com/huggingface/transformers/pull/32238\n-        for key in config.sub_configs.keys():\n-            sub_config = getattr(config, key)\n+        # Composite models consisting of several PretrainedModels can specify attention implementation as a dict where\n+        # keys are sub-config names. But most people will specify one `str` which means that should dispatch it for all sub-models.\n+        # See https://github.com/huggingface/transformers/pull/32238\n+        for key in self.config.sub_configs.keys():\n+            sub_config = getattr(self.config, key)\n             curr_attn_implementation = (\n                 requested_attn_implementation\n                 if not isinstance(requested_attn_implementation, dict)\n@@ -2349,50 +2325,26 @@ def _autoset_attn_implementation(\n             ):\n                 sub_config._attn_implementation_internal = curr_attn_implementation\n \n-        if config._attn_implementation == \"flash_attention_3\":\n-            cls._check_and_enable_flash_attn_3(\n-                config,\n-                torch_dtype=torch_dtype,\n-                device_map=device_map,\n-                hard_check_only=False,\n-                check_device_map=check_device_map,\n-            )\n-        elif config._attn_implementation == \"flash_attention_2\":\n-            cls._check_and_enable_flash_attn_2(\n-                config,\n-                torch_dtype=torch_dtype,\n-                device_map=device_map,\n-                hard_check_only=False,\n-                check_device_map=check_device_map,\n-            )\n-        elif requested_attn_implementation == \"flex_attention\":\n-            config = cls._check_and_enable_flex_attn(config, hard_check_only=True)\n-        elif requested_attn_implementation in [None, \"sdpa\"] and not is_torch_xla_available():\n-            # flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\n-            config = cls._check_and_enable_sdpa(\n-                config,\n-                hard_check_only=requested_attn_implementation is not None,\n-            )\n-\n-            if (\n-                torch.version.hip is not None\n-                and config._attn_implementation == \"sdpa\"\n-                and torch.cuda.device_count() > 1\n-                and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n-            ):\n-                logger.warning_once(\n-                    \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n-                )\n-                torch.backends.cuda.enable_flash_sdp(False)\n+        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n+            self.config._attn_implementation = \"flash_attention_3\"\n+        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n+            self.config._attn_implementation = \"flash_attention_2\"\n+        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n+            self.config._attn_implementation = \"flex_attention\"\n+        elif (\n+            requested_attn_implementation in [None, \"sdpa\"]\n+            and not is_torch_xla_available()\n+            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n+        ):\n+            self.config._attn_implementation = \"sdpa\"\n         elif requested_attn_implementation in ALL_ATTENTION_FUNCTIONS.valid_keys():\n-            config._attn_implementation = requested_attn_implementation\n+            self.config._attn_implementation = requested_attn_implementation\n         elif isinstance(requested_attn_implementation, dict):\n-            config._attn_implementation = None\n+            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n         else:\n-            config._attn_implementation = \"eager\"\n+            self.config._attn_implementation = \"eager\"\n \n-        config._attn_implementation_autoset = True\n-        return config\n+        self.config._attn_implementation_autoset = True\n \n     @classmethod\n     def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n@@ -2466,64 +2418,54 @@ def can_generate(cls) -> bool:\n         # Otherwise, can't generate\n         return False\n \n-    @classmethod\n-    def _check_and_enable_flash_attn_2(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-        hard_check_only: bool = False,\n-    ) -> PretrainedConfig:\n+    def _flash_attn_2_can_dispatch(self) -> bool:\n         \"\"\"\n         Checks the availability of Flash Attention 2 and compatibility with the current model.\n \n         If all checks pass and `hard_check_only` is False, the method will set the config attribute `attn_implementation` to \"flash_attention_2\" so that the model can initialize the correct attention module.\n         \"\"\"\n-        if not cls._supports_flash_attn_2:\n+        # Config always has `torch_dtype` but we need the next line for `no_super_init()` tests\n+        torch_dtype = self.config.torch_dtype if hasattr(self.config, \"torch_dtype\") else torch.get_default_dtype()\n+        device_map = self.hf_device_map if hasattr(self, \"hf_device_map\") else None\n+\n+        # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n+        if not (self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False)):\n             raise ValueError(\n-                f\"{cls.__name__} does not support Flash Attention 2.0 yet. Please request to add support where\"\n-                f\" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new\"\n+                f\"{self.__class__.__name__} does not support Flash Attention 2.0 yet. Please request to add support where\"\n+                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                 \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n             )\n \n         if not is_flash_attn_2_available():\n             preface = \"FlashAttention2 has been toggled on, but it cannot be used due to the following error:\"\n             install_message = \"Please refer to the documentation of https://huggingface.co/docs/transformers/perf_infer_gpu_one#flashattention-2 to install Flash Attention 2.\"\n \n-            if importlib.util.find_spec(\"flash_attn\") is None:\n-                # package `flash-attn` can not be installed on Ascend NPU, ignore related validation logic and early exit.\n-                if is_torch_npu_available():\n-                    if not hard_check_only:\n-                        config._attn_implementation = \"flash_attention_2\"\n-\n-                    logger.info(\"Detect using FlashAttention2 on Ascend NPU.\")\n-                    return config\n-                else:\n-                    raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n-\n-            flash_attention_version = version.parse(importlib.metadata.version(\"flash_attn\"))\n-            if torch.version.cuda:\n-                if flash_attention_version < version.parse(\"2.1.0\"):\n-                    raise ImportError(\n-                        f\"{preface} you need flash_attn package version to be greater or equal than 2.1.0. Detected version {flash_attention_version}. {install_message}\"\n-                    )\n-                elif not torch.cuda.is_available():\n-                    raise ValueError(\n-                        f\"{preface} Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\"\n-                    )\n-                else:\n-                    raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n-            elif torch.version.hip:\n-                if flash_attention_version < version.parse(\"2.0.4\"):\n-                    raise ImportError(\n-                        f\"{preface} you need flash_attn package version to be greater or equal than 2.0.4. Make sure to have that version installed - detected version {flash_attention_version}. {install_message}\"\n-                    )\n-                else:\n-                    raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n-\n-        _is_bettertransformer = getattr(cls, \"use_bettertransformer\", False)\n+            # package `flash-attn` can not be installed on Ascend NPU, ignore related validation logi\n+            if importlib.util.find_spec(\"flash_attn\") is None and not is_torch_npu_available():\n+                raise ImportError(f\"{preface} the package flash_attn seems to be not installed. {install_message}\")\n+            else:\n+                # Check FA2 installed version compatibility\n+                flash_attention_version = version.parse(importlib.metadata.version(\"flash_attn\"))\n+                if torch.version.cuda:\n+                    if flash_attention_version < version.parse(\"2.1.0\"):\n+                        raise ImportError(\n+                            f\"{preface} you need flash_attn package version to be greater or equal than 2.1.0. Detected version {flash_attention_version}. {install_message}\"\n+                        )\n+                    elif not torch.cuda.is_available():\n+                        raise ValueError(\n+                            f\"{preface} Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\"\n+                        )\n+                    else:\n+                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n+                elif torch.version.hip:\n+                    if flash_attention_version < version.parse(\"2.0.4\"):\n+                        raise ImportError(\n+                            f\"{preface} you need flash_attn package version to be greater or equal than 2.0.4. Detected version {flash_attention_version}. {install_message}\"\n+                        )\n+                    else:\n+                        raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n \n+        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n         if _is_bettertransformer:\n             raise ValueError(\n                 \"Flash Attention 2 and BetterTransformer API are not compatible. Please make sure to disable BetterTransformers by doing model.reverse_bettertransformer()\"\n@@ -2536,13 +2478,13 @@ def _check_and_enable_flash_attn_2(\n         elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n             logger.warning_once(\n                 \"Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n-                f\" the current dype in {cls.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n+                f\" the current dype in {self.__class__.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                 ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`'\n             )\n \n         # The check `torch.empty(0).device.type != \"cuda\"` is needed as the model may be initialized after `torch.set_default_device` has been called,\n         # or the model may be initialized under the context manager `with torch.device(\"cuda\"):`.\n-        if check_device_map and device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n+        if device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n             if torch.cuda.is_available():\n                 logger.warning_once(\n                     \"You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU\"\n@@ -2560,37 +2502,32 @@ def _check_and_enable_flash_attn_2(\n                     \"or initialising the model on CPU and then moving it to GPU.\"\n                 )\n         elif (\n-            check_device_map\n-            and device_map is not None\n+            device_map is not None\n             and isinstance(device_map, dict)\n             and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n         ):\n             raise ValueError(\n                 \"You are attempting to use Flash Attention 2.0 with a model dispatched on CPU or disk. This is not supported. Please make sure to \"\n                 \"initialise the model on a GPU by passing a device_map that contains only GPU devices as keys.\"\n             )\n-        if not hard_check_only:\n-            config._attn_implementation = \"flash_attention_2\"\n-        return config\n \n-    @classmethod\n-    def _check_and_enable_flash_attn_3(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-        hard_check_only: bool = False,\n-    ) -> PretrainedConfig:\n+        # If no error raise by this point, we can return `True`\n+        return True\n+\n+    def _flash_attn_3_can_dispatch(self) -> bool:\n         \"\"\"\n         Checks the availability of Flash Attention 3 and compatibility with the current model.\n \n         If all checks pass and `hard_check_only` is False, the method will set the config attribute `attn_implementation` to \"flash_attention_3\" so that the model can initialize the correct attention module.\n         \"\"\"\n-        if not cls._supports_flash_attn_3:\n+        # Config always has `torch_dtype` but we need the next line for `no_super_init()` tests\n+        torch_dtype = self.config.torch_dtype if hasattr(self.config, \"torch_dtype\") else torch.get_default_dtype()\n+        device_map = self.hf_device_map if hasattr(self, \"hf_device_map\") else None\n+\n+        if not self._supports_flash_attn:\n             raise ValueError(\n-                f\"{cls.__name__} does not support Flash Attention 3.0 yet. Please request to add support where\"\n-                f\" the model is hosted, on its model hub page: https://huggingface.co/{config._name_or_path}/discussions/new\"\n+                f\"{self.__class__.__name__} does not support Flash Attention 3.0 yet. Please request to add support where\"\n+                f\" the model is hosted, on its model hub page: https://huggingface.co/{self.config._name_or_path}/discussions/new\"\n                 \" or in the Transformers GitHub repo: https://github.com/huggingface/transformers/issues/new\"\n             )\n \n@@ -2620,22 +2557,22 @@ def _check_and_enable_flash_attn_3(\n         elif torch_dtype is not None and torch_dtype not in [torch.float16, torch.bfloat16]:\n             logger.warning_once(\n                 \"Flash Attention 3 only supports torch.float16 and torch.bfloat16 dtypes, but\"\n-                f\" the current dype in {cls.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n+                f\" the current dype in {self.__class__.__name__} is {torch_dtype}. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator,\"\n                 ' or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"meta-llama/Llama-3.2-1B\", attn_implementation=\"flash_attention_3\", torch_dtype=torch.float16)`'\n             )\n \n-        if getattr(config, \"alibi\", False) or getattr(config, \"use_alibi\", False):\n+        if getattr(self.config, \"alibi\", False) or getattr(self.config, \"use_alibi\", False):\n             raise ValueError(\"Model is configured to use ALiBi, which is not supported by Flash Attention 3.\")\n \n         # Check for attention dropout, which is incompatible with FA3\n-        if hasattr(config, \"attention_dropout\") and config.attention_dropout > 0:\n+        if hasattr(self.config, \"attention_dropout\") and self.config.attention_dropout > 0:\n             raise ValueError(\n-                f\"Model has attention_dropout={config.attention_dropout}, which is not supported by Flash Attention 3.\"\n+                f\"Model has attention_dropout={self.config.attention_dropout}, which is not supported by Flash Attention 3.\"\n             )\n \n         # The check `torch.empty(0).device.type != \"cuda\"` is needed as the model may be initialized after `torch.set_default_device` has been called,\n         # or the model may be initialized under the context manager `with torch.device(\"cuda\"):`.\n-        if check_device_map and device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n+        if device_map is None and torch.empty(0).device.type not in [\"cuda\", \"mlu\"]:\n             if torch.cuda.is_available():\n                 logger.warning_once(\n                     \"You are attempting to use Flash Attention 3 with a model not initialized on GPU. Make sure to move the model to GPU\"\n@@ -2648,30 +2585,26 @@ def _check_and_enable_flash_attn_3(\n                     \"or initialising the model on CPU and then moving it to GPU.\"\n                 )\n         elif (\n-            check_device_map\n-            and device_map is not None\n+            device_map is not None\n             and isinstance(device_map, dict)\n             and (\"cpu\" in device_map.values() or \"disk\" in device_map.values())\n         ):\n             raise ValueError(\n                 \"You are attempting to use Flash Attention 3 with a model dispatched on CPU or disk. This is not supported. Please make sure to \"\n                 \"initialise the model on a GPU by passing a device_map that contains only GPU devices as keys.\"\n             )\n-        if not hard_check_only:\n-            config._attn_implementation = \"flash_attention_3\"\n-        return config\n+        return True\n \n-    @classmethod\n-    def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n+    def _sdpa_can_dispatch(self, hard_check_only: bool = False) -> bool:\n         \"\"\"\n         Checks the availability of SDPA for a given model.\n \n         If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"sdpa\" so that the model can initialize the correct attention module.\n         \"\"\"\n         if hard_check_only:\n-            if not cls._supports_sdpa:\n+            if not self._supports_sdpa:\n                 raise ValueError(\n-                    f\"{cls.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n+                    f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n                     \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n                     ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n                 )\n@@ -2680,45 +2613,44 @@ def _check_and_enable_sdpa(cls, config, hard_check_only: bool = False):\n                     \"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\"\n                 )\n \n-        if not is_torch_sdpa_available() or not cls._supports_sdpa:\n-            return config\n+        if (\n+            torch.version.hip is not None\n+            and torch.cuda.device_count() > 1\n+            and version.parse(torch.__version__) < version.parse(\"2.4.1\")\n+        ):\n+            logger.warning_once(\n+                \"Using the `SDPA` attention implementation on multi-gpu setup with ROCM may lead to performance issues due to the FA backend. Disabling it to use alternative backends.\"\n+            )\n+            torch.backends.cuda.enable_flash_sdp(False)\n \n-        _is_bettertransformer = getattr(cls, \"use_bettertransformer\", False)\n-        if _is_bettertransformer:\n-            return config\n+        # This means we have `hard_check_only=False` and fallback to eager if SDPA isn't supported\n+        _is_bettertransformer = getattr(self, \"use_bettertransformer\", False)\n+        if not is_torch_sdpa_available() or not self._supports_sdpa or _is_bettertransformer:\n+            return False\n \n-        if not hard_check_only:\n-            config._attn_implementation = \"sdpa\"\n-        return config\n+        return True\n \n-    @classmethod\n-    def _check_and_enable_flex_attn(cls, config, hard_check_only: bool = False) -> PretrainedConfig:\n+    def _flex_attn_can_dispatch(self) -> bool:\n         \"\"\"\n         Checks the availability of Flex Attention for a given model.\n \n         If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"flex_attention\" so that the model can initialize the correct attention module.\n         \"\"\"\n-        if hard_check_only:\n-            if not cls._supports_flex_attn:\n-                raise ValueError(\n-                    f\"{cls.__name__} does not support an attention implementation through torch's flex_attention.\"\n-                    \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809.\"\n-                    \" If you believe this error is a bug, please open an issue in Transformers GitHub repository\"\n-                    ' and load your model with the argument `attn_implementation=\"eager\"` meanwhile.'\n-                    ' Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n-                )\n-            if not is_torch_flex_attn_available():\n-                raise ImportError(\n-                    \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n-                )\n-\n-        if not is_torch_flex_attn_available() or not cls._supports_flex_attn:\n-            return config\n-\n-        if not hard_check_only:\n-            config._attn_implementation = \"flex_attention\"\n+        if not self._supports_flex_attn:\n+            raise ValueError(\n+                f\"{self.__class__.__name__} does not support an attention implementation through torch's flex_attention.\"\n+                \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/34809.\"\n+                \" If you believe this error is a bug, please open an issue in Transformers GitHub repository\"\n+                ' and load your model with the argument `attn_implementation=\"eager\"` meanwhile.'\n+                ' Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n+            )\n+        if not is_torch_flex_attn_available():\n+            raise ImportError(\n+                \"PyTorch Flex Attention requirements in Transformers are not met. Please install torch>=2.5.0.\"\n+            )\n \n-        return config\n+        # If no error raise by this point, we can return `True`\n+        return True\n \n     def enable_input_require_grads(self):\n         \"\"\"\n@@ -4803,13 +4735,6 @@ def from_pretrained(\n         model_init_context = cls.get_init_context(is_quantized, _is_ds_init_called)\n \n         config = copy.deepcopy(config)  # We do not want to modify the config inplace in from_pretrained.\n-        if not getattr(config, \"_attn_implementation_autoset\", False):\n-            config = cls._autoset_attn_implementation(\n-                config,\n-                torch_dtype=torch_dtype,\n-                device_map=device_map,\n-            )\n-\n         with ContextManagers(model_init_context):\n             # Let's make sure we don't run the init function of buffer modules\n             model = cls(config, *model_args, **model_kwargs)"
        },
        {
            "sha": "c0d9f0990c90d96a117163eca54e87a99bee56c1",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -444,8 +444,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n         \"Aimv2TextEmbeddings\",\n     ]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "703c42e30887f56da61af68aebd0919991dea6d1",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -441,8 +441,7 @@ class Aimv2PreTrainedModel(PreTrainedModel):\n         \"Aimv2TextEmbeddings\",\n     ]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "8e1b1b168bfac3794cf981139d1af94e3e3dacd3",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -313,8 +313,7 @@ class ArceePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ArceeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a42d04717e00baf8eefda6da407aa24b95fd15b6",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -629,7 +629,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_attention_backend = True\n@@ -661,8 +661,7 @@ class AriaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"AriaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "fa371db78c7aa05c9a3b4bc648ad24272292701e",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1284,7 +1284,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "f37a5151feaccacc750def333af1ed3e81a3fef6",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -375,8 +375,7 @@ class ASTPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "ccd8d3a56acc677eaafc5f1755f89e6f013acc96",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -94,8 +94,7 @@ class AyaVisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = False\n     _supports_static_cache = False"
        },
        {
            "sha": "0113ce6b8c855e874926ecc23c7f7032309854d1",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1039,8 +1039,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "9c61066479c98e0faa1cd63f3891e2de6d0a06fc",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -810,8 +810,7 @@ class BambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "a4201faca56cbc906eeedf1a21300aa6b5e31a32",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -355,8 +355,7 @@ def forward(\n class BarkPreTrainedModel(PreTrainedModel):\n     config_class = BarkConfig\n     supports_gradient_checkpointing = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights.\"\"\"\n@@ -1684,42 +1683,6 @@ def generate(\n \n         return audio\n \n-    @classmethod\n-    def _check_and_enable_flash_attn_2(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        hard_check_only: bool = False,\n-        check_device_map: bool = False,\n-    ):\n-        \"\"\"\n-        `_check_and_enable_flash_attn_2` originally don't expand flash attention enabling to the model\n-        sub-configurations. We override the original method to make sure that Bark sub-models are using Flash Attention\n-        if necessary.\n-\n-        If you don't know about Flash Attention, check out the official repository of flash attention:\n-        https://github.com/Dao-AILab/flash-attention\n-\n-        For using Flash Attention 1.0 you can do it directly via the `BetterTransformer` API, have a look at this\n-        specific section of the documentation to learn more about it:\n-        https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#decoder-models\n-\n-        The method checks if the current setup is compatible with Flash Attention as it requires the model to be in\n-        half precision and not ran on CPU.\n-\n-        If all checks pass and `hard_check_only` is False, the method will set the config attribute `_attn_implementation` to \"flash_attention_2\" so that the model\n-        can initialize the correct attention module\n-        \"\"\"\n-        config = super()._check_and_enable_flash_attn_2(\n-            config, torch_dtype, device_map, hard_check_only=hard_check_only, check_device_map=check_device_map\n-        )\n-\n-        config.semantic_config._attn_implementation = config._attn_implementation\n-        config.coarse_acoustics_config._attn_implementation = config._attn_implementation\n-        config.fine_acoustics_config._attn_implementation = config._attn_implementation\n-        return config\n-\n \n __all__ = [\n     \"BarkFineModel\","
        },
        {
            "sha": "f2729321f059f173c356b922687fd142063bc11f",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -493,8 +493,7 @@ class BartPreTrainedModel(PreTrainedModel):\n     _keys_to_ignore_on_load_unexpected = [\"encoder.version\", \"decoder.version\"]\n     _no_split_modules = [r\"BartEncoderLayer\", r\"BartDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "390b57a4179d094b3bf1ea8821690261039003f9",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -346,8 +346,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     config_class = BioGptConfig\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "24d1c77fb688a3bf516b5ede67f0cbb9a5ccaa02",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -171,8 +171,7 @@ class BioGptPreTrainedModel(PreTrainedModel):\n     config_class = BioGptConfig\n     base_model_prefix = \"biogpt\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "c2e84df25685c6b6510fdd419751327913016313",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -308,8 +308,7 @@ class BitNetPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"BitNetDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "2af2d994489d4c71122389993052d45637c35c20",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -463,8 +463,7 @@ class BlenderbotPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "f81718d68cb1dde1f8cbf0a52e28f3568f6442ba",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -451,8 +451,7 @@ class BlenderbotSmallPreTrainedModel(PreTrainedModel):\n     config_class = BlenderbotSmallConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "21c533dd4f1378a2efa9f72e3e176d620fd8bcac",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -409,8 +409,7 @@ class Blip2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1048,7 +1047,7 @@ def forward(\n )\n class Blip2QFormerModel(Blip2PreTrainedModel):\n     _supports_attention_backend = False  # adds position on attn weights before last matmul\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n@@ -1238,7 +1237,7 @@ class Blip2Model(Blip2PreTrainedModel):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n-    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n+    _supports_flash_attn = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1623,7 +1622,7 @@ def forward(\n class Blip2TextModelWithProjection(Blip2PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n-    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n+    _supports_flash_attn = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1716,7 +1715,7 @@ def forward(\n class Blip2VisionModelWithProjection(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n-    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n+    _supports_flash_attn = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -1836,7 +1835,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n-    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n+    _supports_flash_attn = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2267,7 +2266,7 @@ def generate(\n class Blip2ForImageTextRetrieval(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n-    _supports_flash_attn_2 = False  # because self.qformer does not support FA2\n+    _supports_flash_attn = False  # because self.qformer does not support FA2\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "0100f191d278b7c312c046cb14d8bfc77cf451b9",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -821,8 +821,7 @@ class ChameleonPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ChameleonDecoderLayer\", \"ChameleonSwinDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "eb697fcccffd757fe0b64aafed44b008b00f1147",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -428,8 +428,7 @@ class CLIPPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"clip\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "6e43f257a38709b47e5ec22dc66ac50cb4ef812c",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -341,8 +341,7 @@ class CoherePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CohereDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "82ac1b2b61aeeedf054e801d613899205cc83b21",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -318,8 +318,7 @@ class Cohere2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Cohere2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "0a64bcc60e24871c2b700d94731c58e4f9576909",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -41,8 +41,7 @@ class ColQwen2PreTrainedModel(PreTrainedModel):\n     config_class = ColQwen2Config\n     base_model_prefix = \"model\"\n     _no_split_modules = []\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "4f5ce4aa8a812aee76c256dc8f84239ff53e3ff7",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -225,8 +225,7 @@ def __call__(\n \n \n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "f0f7eecbad7473c988a16329047ef2717ebc7591",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -366,8 +366,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True"
        },
        {
            "sha": "266327b13e5fbd1efae0733dbea1c100e59b0ff9",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -129,8 +129,7 @@ class CsmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"CsmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     # does not because of Mimi codec model\n     # _supports_flex_attn = True"
        },
        {
            "sha": "94dedbfb38db5789943460fc7e5776441d346cfd",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -505,8 +505,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "314e08ed4e79718e7cd45741c0ded48346db6df9",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -139,8 +139,7 @@ class Data2VecAudioPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     base_model_prefix = \"data2vec_audio\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "3bef3e3293793aaca02400d5c582def488b4e65b",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -809,8 +809,7 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DbrxBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "836b9dc9de2a85c36efeafc83f3f1b789da7f57d",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -455,8 +455,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeepseekV2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6c00b64eef79e7a9b3be16f4ef4f4c116e4c9d07",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -494,8 +494,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeepseekV3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "52ca6bdac9cff94c2ec713851c55fa61b8e77cd4",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -442,8 +442,7 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "f801a7f60372623711b8ad177bd86eac29755ac4",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -64,8 +64,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     config_class = DiaConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "5dfa78ce36442bfed6734e38101ad4339fa0c59c",
            "filename": "src/transformers/models/dia/modular_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodular_dia.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -59,8 +59,7 @@ class DiaPreTrainedModel(PreTrainedModel):\n     config_class = DiaConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "26de9466431d1aa083cb4588c8f6937f52333c3c",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -530,8 +530,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DiffLlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n     _supports_cache_class = True"
        },
        {
            "sha": "e266727bf9efd9fb71ceae019fef968980b2738d",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -486,8 +486,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2Layer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "69236ab67febb6b7b8897cb841b13ade81f48d95",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -504,8 +504,7 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2WithRegistersLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "feb8d6d8bcf85cc1f4a78db6d852b3de307a49ed",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -575,8 +575,7 @@ class DistilBertPreTrainedModel(PreTrainedModel):\n     load_tf_weights = None\n     base_model_prefix = \"distilbert\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "969150c7c7d40f36ad66807c47a7bdc962be22ff",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -491,8 +491,7 @@ class DogePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DogeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = False\n-    _supports_flash_attn_3 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a3d1b4f9bf321bf01457303942a64824f3b8a699",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -563,8 +563,7 @@ def forward(\n \n \n class DogePreTrainedModel(LlamaPreTrainedModel):\n-    _supports_flash_attn_3 = False\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_static_cache = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(DogeCDMoE, index=1),"
        },
        {
            "sha": "4ac1420e2d306008f56bdb2643b56ad6f8259081",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -414,8 +414,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dots1DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "8614a4de6e113c1806b8b733a05bec8de2504d3d",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -809,8 +809,7 @@ class DPTPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "cdf5eee993aac112f915426208e5176aa3cf5245",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -926,8 +926,7 @@ class Emu3VQVAE(PreTrainedModel):\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n     _no_split_modules = [\n@@ -1096,8 +1095,7 @@ class Emu3PreTrainedModel(PreTrainedModel):\n         \"Emu3DecoderLayer\",\n     ]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "58689de09b7e3a98c2ff46946a7fbadb2674afe9",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -678,8 +678,7 @@ class Emu3VQVAE(PreTrainedModel):\n     base_model_prefix = \"emuvideovq\"\n     main_input_name = \"pixel_values\"\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n     _no_split_modules = ["
        },
        {
            "sha": "48ec65a5e21e4704aabb23a7441a186629b990b3",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -78,8 +78,7 @@ class EncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "29c33cec057d7d5ea76480eec630c0e966a6e650",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1001,8 +1001,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range"
        },
        {
            "sha": "0a1b7dfd955d17d48cb2c906d926e7b35b6071ca",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -373,8 +373,7 @@ class EomtPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"EomtLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     def _init_weights(self, module: nn.Module) -> None:\n         std = self.config.initializer_range"
        },
        {
            "sha": "2cb697b08e2f3cf705e3da0a9b5ec637ddd2b9b3",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -738,8 +738,7 @@ class EsmPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"esm\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"EsmLayer\", \"EsmFoldTriangularSelfAttentionBlock\", \"EsmEmbeddings\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     # Copied from transformers.models.bert.modeling_bert.BertPreTrainedModel._init_weights with BertLMPredictionHead->EsmLMHead\n     def _init_weights(self, module):"
        },
        {
            "sha": "8c74afdc7c7aa2cd508b35b1630935bc41ba21ae",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1990,7 +1990,7 @@ def distogram(coords, min_bin, max_bin, num_bins):\n )\n class EsmForProteinFolding(EsmPreTrainedModel):\n     _no_split_modules = [\"EsmFoldStructureModule\", \"EsmFoldTriangularSelfAttentionBlock\"]\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "9e31eb1c90201fb0d21f9f017b969b6b3b850fe1",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -640,8 +640,7 @@ class FalconPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"transformer\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"FalconDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "82173de99faebc3747e32b6af0406aad640e452f",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1150,8 +1150,7 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"FalconH1DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "89c6abc411d7f2c911375c86cef8594112c0914d",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -928,8 +928,7 @@ class FalconH1PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"FalconH1DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports FalconHybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "d2838fa8e03b000113d5d3283dfd2db5bdc4daf0",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -38,8 +38,7 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"fuyu\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "e5b06aaa64f66099b3ccb867797b6bf23f21547c",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -310,8 +310,7 @@ class GemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GemmaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "fceb1cf9d005ef962f368f33370127be08d59eea",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -340,8 +340,7 @@ class Gemma2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Gemma2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3f50d5f17be42f4eb463fd8c8717162a2a8a2663",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -430,8 +430,7 @@ class Gemma3PreTrainedModel(PreTrainedModel):\n         \"SiglipMultiheadAttentionPoolingHead\",\n     ]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3eb10a1f22bdcffbf0ef83687df845f32b16f3f1",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1486,8 +1486,7 @@ class Gemma3nPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "ccb4cb583ac5b9ede1958c24b65a771d1b574c8a",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -327,8 +327,7 @@ class GlmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GlmDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1a1b5abe571001be6e948624e48955c9ff44056f",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -331,8 +331,7 @@ class Glm4PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1031969679f3aa4127dab75d0f6d94bdb5c01d14",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -404,8 +404,7 @@ class Glm4vPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Glm4vTextDecoderLayer\", \"Glm4vVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "c85d2c962f65fe4b2041a6aa8ab77471b4a41e38",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -281,8 +281,7 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "a98bf0c235e06fab90146eb9517cb1d37eabe6ff",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -559,8 +559,7 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPT2Block\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_attention_backend = True\n     _supports_cache_class = True"
        },
        {
            "sha": "2d2418d1f7002d97fab900ec35fb3376b3e72791",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -655,8 +655,7 @@ class GPTBigCodePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTBigCodeBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__(self, *inputs, **kwargs):"
        },
        {
            "sha": "896b2123c67b1589e06053f96816f53c2a6316f2",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -476,8 +476,7 @@ class GPTNeoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTNeoBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = False  # TODO: needs a HybridCache"
        },
        {
            "sha": "7359d7b46e390557ce0cf183951bc918aad50fab",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -360,8 +360,7 @@ class GPTNeoXPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTNeoXLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "5c8e9e81c2e5e124251ae4e6ed2f746f87082997",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -471,8 +471,7 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GPTJBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "f2cf41c249ccce400a3a65dcd20284b11e91c4d8",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -305,8 +305,7 @@ class GranitePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GraniteDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "447f222d8554ababbf1c231b98212b31e342ee6b",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -283,8 +283,7 @@ def forward(self, hidden_states: torch.Tensor):\n class GraniteSpeechPreTrainedModel(PreTrainedModel):\n     config_class = GraniteSpeechConfig\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module):"
        },
        {
            "sha": "824c7ccd8b7e0f81983c88e39afb58025960ca1e",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -589,8 +589,7 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GraniteMoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "fdfdae611221ef6b1c7ca300417d6554e141aa5d",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1162,8 +1162,7 @@ class GraniteMoeHybridPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GraniteMoeHybridDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "527ee691d4937a59efed9236cbc88caeefd2a83c",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -507,8 +507,7 @@ class GraniteMoeSharedPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"GraniteMoeSharedDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "c238fa200f950b23e0032b5f3e09b1ace2d46ed2",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -312,8 +312,7 @@ class HeliumPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"HeliumDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6f2d9cf3aec78168a4119fcf9df405700fd75509",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -684,8 +684,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"hubert\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "3e12c14e4c87b221eeac368513c969ac3ff5f0cb",
            "filename": "src/transformers/models/hubert/modular_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodular_hubert.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -129,8 +129,7 @@ class HubertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"hubert\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "cf5fad7bb5fd4f246940b8c741b8c5d5ddfc90b1",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -879,8 +879,7 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_static_cache = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "e596bce9dea47b9a83fad3367b3be65edad98805",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -456,8 +456,7 @@ class Idefics2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics2VisionAttention\", \"Idefics2MLP\", \"Idefics2PerceiverLayer\", \"Idefics2DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -495,8 +494,7 @@ def _init_weights(self, module):\n class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n     config_class = Idefics2VisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: Idefics2VisionConfig):"
        },
        {
            "sha": "2461f4b95678989b564869a6461735f4eca342c7",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -473,8 +473,7 @@ class Idefics3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Idefics3VisionAttention\", \"Idefics3DecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -506,8 +505,7 @@ def _init_weights(self, module):\n class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     config_class = Idefics3VisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: Idefics3VisionConfig):"
        },
        {
            "sha": "2c16928f0abb69c1aa3030df28deb5624dbd9247",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -151,8 +151,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "231753cea51f2d8a546da6b3c3cc5c1e1fcdd525",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -94,8 +94,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "ad94340b5060762693a345f51551ae77347b76f1",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -336,8 +336,7 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -966,7 +965,7 @@ class InstructBlipQFormerModel(InstructBlipPreTrainedModel):\n     \"\"\"\n \n     _supports_attention_backend = False  # adds position on attn weights before last matmul\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "68ed042a3d09b914c932683f7e95f2b1e9a2b929",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -823,8 +823,7 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -927,7 +926,7 @@ class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     \"\"\"\n \n     _supports_attention_backend = False  # adds position on attn weights before last matmul\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "687a6dd1e3a2009e3d5d1dbb2b213a0e16fdbcfc",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -179,8 +179,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n@@ -522,8 +521,7 @@ class InternVLPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "0e52b898723436e6ad21d0cee64cca20d5d93cda",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -141,8 +141,7 @@ class InternVLVisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"InternVLVisionLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "4b15f42eadc6938e94af11838ab267f545d36951",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1069,8 +1069,7 @@ class JambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"JambaAttentionDecoderLayer\", \"JambaMambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n     _is_stateful = True"
        },
        {
            "sha": "bb8dec3b46ca31fd1a9e6eb97a4b4bbd47dd5b6a",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -60,8 +60,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "df94df47b93897cb09f4b24aa00e0c86aeb8f124",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -387,8 +387,7 @@ class JanusPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\", \"JanusVisionEncoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\", \"causal_mask\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_cache_class = True"
        },
        {
            "sha": "7009c2d7d553e27e177dcded62204adbf8f6aef8",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -826,8 +826,7 @@ class JetMoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n     _no_split_modules = [\"JetMoeBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "da44dae8e3439f15067ef8289b60becda5ea1781",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1151,8 +1151,7 @@ class Kosmos2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Kosmos2VisionEncoderLayer\", \"Kosmos2TextBlock\"]\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "41b5800b2392319623d62f9062228d3f7ee59b65",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -117,8 +117,7 @@ class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     main_input_name = \"input_ids\""
        },
        {
            "sha": "1c9e026624475e9b07155fe5a1ca90a86a66d5b2",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -539,8 +539,7 @@ class Lfm2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Lfm2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "5b863a7a93086398d7ff8688818532849c9c3cad",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -423,8 +423,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "544cad5c79a312cc1e486ba74c4cae14161ac868",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -508,8 +508,7 @@ class LightGluePreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"lightglue\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module: nn.Module) -> None:"
        },
        {
            "sha": "3a078129b035aded829ac7a7ff9504dc092203a8",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -311,8 +311,7 @@ class LlamaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "b3781f65c08fea2e132058c9c1fea23a6b446d30",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -433,7 +433,7 @@ class Llama4PreTrainedModel(PreTrainedModel):\n     config_class = Llama4Config\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "b39360bb586309ab53e54005adca2650621f79b4",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -118,8 +118,7 @@ class LlavaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "249b85a5598d7c95c0bd71cb3d698b180428b782",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -229,8 +229,7 @@ class LlavaNextPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "8a218c5f4af753dac3f618bb19cba8793eadfba0",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -170,8 +170,7 @@ class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "65c26c78de99964d815c63614fd01baeafa16f5d",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -283,8 +283,7 @@ class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "1f33da304f5206f454356cfb22dbcf0fb86335f1",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -529,8 +529,7 @@ class M2M100PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"M2M100EncoderLayer\", \"M2M100DecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3782d4bc3dff99cdd9d59b61df77540c22a3144e",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -467,8 +467,7 @@ class MarianPreTrainedModel(PreTrainedModel):\n     config_class = MarianConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "6f3ca84a2485dbb533f444ee61106d6ef687ebc3",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -497,8 +497,7 @@ class MBartPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MBartDecoderLayer\", \"MBartEncoderLayer\", \"MBartAttention\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "3ca8fe2b391857b36a72bc0dee0e8cbabdc4536e",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1373,8 +1373,7 @@ class MimiPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MimiDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "a5ad9c68ad022a2d1d8bea1c7d15cf6c552b2d8d",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -585,8 +585,7 @@ class MiniMaxPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MiniMaxDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True  # Note: only supports MiniMaxCache"
        },
        {
            "sha": "cc03b14c553d23d1cfe4c782c3be3a12926a1fdb",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -256,8 +256,7 @@ class MistralPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MistralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "8abcab6d3647d32d6978a744d92a396184c9c965",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -183,8 +183,7 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "ac64ace0468314dcc2b44660e103a89e85dda49d",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -385,8 +385,7 @@ class MixtralPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MixtralDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "12fd0c68306caaa8ad28964a99310a1e7175e98a",
            "filename": "src/transformers/models/mlcd/modeling_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodeling_mlcd.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -508,8 +508,7 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config_class = MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "a640ed0b59f2cc36dc70d0877926bc61140c6516",
            "filename": "src/transformers/models/mlcd/modular_mlcd.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmlcd%2Fmodular_mlcd.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -442,8 +442,7 @@ class MLCDPreTrainedModel(PreTrainedModel):\n     config_class = MLCDVisionConfig\n     base_model_prefix = \"mlcd\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "0c418e70a91067bb82158542293ddde84aa9de2e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -852,8 +852,7 @@ class MllamaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_static_cache = False  # static cache cannot have different shapes for each layer\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_quantized_cache = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "e6d6c3e71249274313604d2572561f740b0b1409",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 28,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -560,8 +560,7 @@ class ModernBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n \n@@ -612,36 +611,25 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    @classmethod\n-    def _autoset_attn_implementation(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-    ):\n+    def set_attention_implementation(self, attn_implementation: Union[str, dict]):\n+        \"\"\"\n+        Checks and dispatches to hhe requested attention implementation.\n+        \"\"\"\n         # If the user didn't specify anything, try to use flash_attention_2 if available.\n         # Otherwise we fall back to the default SDPA -> Eager from the super() method.\n         # ModernBert's FA2 implementation correctly handles non-fp16/bf16 dtypes, we don't\n         # need the FA2 warning for non-fp16/bf16 dtypes so we set fp16 for the FA2 check.\n-        if config._attn_implementation_internal is None:\n-            config._attn_implementation_internal = \"flash_attention_2\"\n-            try:\n-                return cls._check_and_enable_flash_attn_2(\n-                    config,\n-                    torch_dtype=torch.float16,\n-                    device_map=device_map,\n-                    hard_check_only=False,\n-                    check_device_map=check_device_map,\n-                )\n-            except (ValueError, ImportError):\n-                config._attn_implementation_internal = None\n-        return super()._autoset_attn_implementation(\n-            config,\n-            torch_dtype=torch_dtype,\n-            device_map=device_map,\n-            check_device_map=check_device_map,\n-        )\n+\n+        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n+        try:\n+            attn_implementation = (\n+                \"flash_attention_2\"\n+                if requested_attn_implementation is None and self._flash_attn_2_can_dispatch()\n+                else attn_implementation\n+            )\n+        except (ValueError, ImportError):\n+            pass\n+        return super().set_attention_implementation(attn_implementation=attn_implementation)\n \n     def _maybe_set_compile(self):\n         if self.config.reference_compile is False:"
        },
        {
            "sha": "32e694d7d52130b22f752a98324dd7362466a986",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 16,
            "deletions": 28,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -760,8 +760,7 @@ class ModernBertPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ModernBertEmbeddings\", \"ModernBertEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False\n \n@@ -812,36 +811,25 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    @classmethod\n-    def _autoset_attn_implementation(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        check_device_map: bool = True,\n-    ):\n+    def set_attention_implementation(self, attn_implementation: Union[str, dict]):\n+        \"\"\"\n+        Checks and dispatches to hhe requested attention implementation.\n+        \"\"\"\n         # If the user didn't specify anything, try to use flash_attention_2 if available.\n         # Otherwise we fall back to the default SDPA -> Eager from the super() method.\n         # ModernBert's FA2 implementation correctly handles non-fp16/bf16 dtypes, we don't\n         # need the FA2 warning for non-fp16/bf16 dtypes so we set fp16 for the FA2 check.\n-        if config._attn_implementation_internal is None:\n-            config._attn_implementation_internal = \"flash_attention_2\"\n-            try:\n-                return cls._check_and_enable_flash_attn_2(\n-                    config,\n-                    torch_dtype=torch.float16,\n-                    device_map=device_map,\n-                    hard_check_only=False,\n-                    check_device_map=check_device_map,\n-                )\n-            except (ValueError, ImportError):\n-                config._attn_implementation_internal = None\n-        return super()._autoset_attn_implementation(\n-            config,\n-            torch_dtype=torch_dtype,\n-            device_map=device_map,\n-            check_device_map=check_device_map,\n-        )\n+\n+        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n+        try:\n+            attn_implementation = (\n+                \"flash_attention_2\"\n+                if requested_attn_implementation is None and self._flash_attn_2_can_dispatch()\n+                else attn_implementation\n+            )\n+        except (ValueError, ImportError):\n+            pass\n+        return super().set_attention_implementation(attn_implementation=attn_implementation)\n \n     def _maybe_set_compile(self):\n         if self.config.reference_compile is False:"
        },
        {
            "sha": "33d0a75f3722d3a6d8c941899eec45b438e7dc98",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -459,8 +459,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "5e56fee5e018e2fda3fda2a425fe785963510e6a",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -494,8 +494,7 @@ class MoonshinePreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoonshineEncoderLayer\", \"MoonshineDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "3b09eba5e0d97852360997a4bc516c61b5ec4c6e",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -805,8 +805,7 @@ class MoshiPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MoshiDecoderLayer\", \"MimiTransformerLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     main_input_name = \"input_ids\"\n@@ -1632,8 +1631,7 @@ class MoshiForConditionalGeneration(MoshiPreTrainedModel, GenerationMixin):\n     config_class = MoshiConfig\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__(self, config: MoshiConfig):"
        },
        {
            "sha": "2062a2333cc072d38ac7250272a28af1f49439f1",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -434,8 +434,7 @@ class MusicgenPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MusicgenDecoderLayer\", \"MusicgenAttention\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1346,8 +1345,7 @@ class MusicgenForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     base_model_prefix = \"encoder_decoder\"\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "7ad2b8c2d5d4e4854db93af96efbc7b772ed9470",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -400,8 +400,7 @@ class MusicgenMelodyPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"MusicgenMelodyDecoderLayer\", \"MusicgenMelodyAttention\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1274,8 +1273,7 @@ class MusicgenMelodyForConditionalGeneration(PreTrainedModel, GenerationMixin):\n     config_class = MusicgenMelodyConfig\n     main_input_name = \"input_ids\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "fe75aa212e565e895ffea576c019cff4daabda5e",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -586,8 +586,7 @@ class NemotronPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"NemotronDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "1e0dddeac9378e16db4b30cb9c0930c6a4a20118",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -853,7 +853,7 @@ class NllbMoePreTrainedModel(PreTrainedModel):\n     # TODO: If anyone is up to it to make sure tests pass etc\n     # Flash attention has problems due to not preparing masks the same way as eager/sdpa\n     # SDPA has more flaky logits which requires more time to look into tests\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "8459ba57a65875dd910d342b6b30ba4ae4c43b8d",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -290,8 +290,7 @@ class OlmoPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OlmoDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "8eb966f313ec7377fb54ef1ba0373f05ad7a6057",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -295,8 +295,7 @@ class Olmo2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Olmo2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "420d239b2d231fde157c1aba783249a915da3ed0",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -703,8 +703,7 @@ class OlmoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OlmoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "2f5a07f79ac6f1468be03674b25e5d40fb54f76f",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -309,8 +309,7 @@ class OPTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"OPTDecoderLayer\"]\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "e3c17bc18ede3cd4e41db1521dccdcd96272c8e2",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -116,8 +116,7 @@ class PaliGemmaPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "9d0f3a9dd880038462e6308e6638eddaa4d63e35",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -462,8 +462,7 @@ class PegasusPreTrainedModel(PreTrainedModel):\n     config_class = PegasusConfig\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "29dbbdf328323932c83bc0e0055ed3f0df6701e2",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -757,8 +757,7 @@ class PegasusXPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [r\"PegasusXEncoderLayer\", r\"PegasusXDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     # Flaky logits\n     _supports_sdpa = False\n     _supports_flex_attn = True"
        },
        {
            "sha": "e0f09b574ec2f87b8a504dad5c995bae2bab8d78",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -92,8 +92,7 @@ class PerceptionLMPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "af7d1db9e272b220b18625f6145161f3cbdfc2cd",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -392,8 +392,7 @@ class PersimmonPreTrainedModel(PreTrainedModel):\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "527574f613bf5ae961c34a13673a8116ba95d892",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -295,8 +295,7 @@ class PhiPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PhiDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "865a3973adf88f8d397f5e00c7c20d29239a2692",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -287,8 +287,7 @@ class Phi3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "a464e67e68f93ff330342c7bced47d1ba86ecde8",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -370,8 +370,7 @@ class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n@@ -996,8 +995,7 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     config_class = Phi4MultimodalAudioConfig\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1591,8 +1589,7 @@ class Phi4MultimodalPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "d8c7c03b76b0cdc09f749d2d9b6005c2a071c30f",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -540,8 +540,7 @@ class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n     supports_gradient_checkpointing = True\n \n     _no_split_modules = [\"Phi4MultimodalVisionEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n@@ -1121,8 +1120,7 @@ class Phi4MultimodalAudioPreTrainedModel(PreTrainedModel):\n     config_class = Phi4MultimodalAudioConfig\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi4MultimodalAudioConformerEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "328d749cd6bec09e34bee64a8f483f152079f0d2",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -887,8 +887,7 @@ class PhimoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PhimoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "616f5810b29af25d5ae86ca9fc64bd14a7f74cd0",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -405,13 +405,11 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_attention_backend = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "95835fd977cb5da8f7c567195362ee94654ec43d",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -77,8 +77,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "5202e61de846c6d21b1c940a75ec7f7dcebf3017",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -62,8 +62,7 @@ class PLBartPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PLBartDecoderLayer\", \"PLBartEncoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "28578956962692303b59faca9c988815bbc030e3",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -259,8 +259,7 @@ class Qwen2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "0136daf174fe146849816543c2c261ff82f99304",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -85,8 +85,7 @@ class Qwen2_5OmniPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\", \"Qwen2_5OmniVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = False"
        },
        {
            "sha": "b06150ccfa4fb56c40daf0ed966d5e5866093b94",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -323,8 +323,7 @@ class Qwen2_5_VLPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "a689b3ecf3aaf2306204895ac20ca0b4a9137643",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -267,8 +267,7 @@ class Qwen2AudioPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2AudioAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "7ec40f25814d3fab828a44dcdd7697d4d1a9d02c",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -744,8 +744,7 @@ class Qwen2MoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2MoeDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n "
        },
        {
            "sha": "a43c4d6a727fa2e005b7b0de1a02c77d4c21ce61",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -656,8 +656,7 @@ class Qwen2VLPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen2VLDecoderLayer\", \"Qwen2VLVisionBlock\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_static_cache = True"
        },
        {
            "sha": "2ebdbc97565e6160f9fdf9299416de15e87ee6b5",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -285,8 +285,7 @@ class Qwen3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1d0f3275235f97d1b89e6e0aa88cc80ff51c935b",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -408,8 +408,7 @@ class Qwen3MoePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Qwen3MoeDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "1af81cf95947292a3ddf89b9d0213b8febec7291",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -234,8 +234,7 @@ class RetrievAugLMOutput(ModelOutput):\n class RagPreTrainedModel(PreTrainedModel):\n     config_class = RagConfig\n     base_model_prefix = \"rag\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     @classmethod"
        },
        {
            "sha": "02af226c5bf57fcf21f6eb84f5f44cc007828ba9",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -509,7 +509,7 @@ class RecurrentGemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"RecurrentGemmaDecoderLayer\"]\n     _skip_keys_device_placement = [\"cache\"]\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False  # we can't compare with eager for now\n     _supports_cache_class = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "b3f4388cb53c4043bea0b56266a8f27abc39ab84",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -518,8 +518,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"sew\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False  # needs a proper look into the mask creation\n "
        },
        {
            "sha": "b09398754838f1395c90f196127665eb32a75391",
            "filename": "src/transformers/models/sew/modular_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodular_sew.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -263,8 +263,7 @@ class SEWPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"sew\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = False  # needs a proper look into the mask creation\n "
        },
        {
            "sha": "dfc252c4733448d5fa85578c120d0099794dfd58",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -479,8 +479,7 @@ class SiglipPreTrainedModel(PreTrainedModel):\n         \"SiglipEncoderLayer\",\n         \"SiglipMultiheadAttentionPoolingHead\",\n     ]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "2ff20c8b23eed7aa0e701f49f113dcb6ece59170",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -712,8 +712,7 @@ class Siglip2PreTrainedModel(PreTrainedModel):\n         \"Siglip2EncoderLayer\",\n         \"Siglip2MultiheadAttentionPoolingHead\",\n     ]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True"
        },
        {
            "sha": "da584a63fc47a62f8ca05cb30a5e17d57393d943",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -289,8 +289,7 @@ class SmolLM3PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SmolLM3DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "2fd0776edf719a986f8b573d4efc831b1ebe1568",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -54,8 +54,7 @@ class SmolVLMPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"SmolVLMVisionAttention\", \"SmolVLMDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True\n@@ -373,8 +372,7 @@ def forward(\n class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     config_class = SmolVLMVisionConfig\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n \n     def __init__(self, config: SmolVLMVisionConfig):"
        },
        {
            "sha": "cbb1915022b79724c181ffbdfa55741780db2b29",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -66,8 +66,7 @@ class SpeechEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"inputs\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "3fcdf1af3b834af85463a980959ac5e9c60f200c",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -528,7 +528,7 @@ class Speech2TextPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     # TODO: tests would need a rewrite to check for correct implementation\n     # Current tests always assume certain inputs to be passed\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "66551b53cbdc9e897199c470475e544f007d9d8a",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -618,8 +618,7 @@ class StableLmPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"StableLmDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_cache_class = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True"
        },
        {
            "sha": "c3f10cea1171f4fb59afcdff2bb85f95e069d7c7",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -293,8 +293,7 @@ class Starcoder2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Starcoder2DecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "f37995ae32ce14cdd228b631857a674aaa074998",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -581,8 +581,7 @@ class T5GemmaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"T5GemmaBlock\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "0732ec7a3c64c500f39dda472440a9ae49080bd3",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -635,7 +635,7 @@ class TimeSeriesTransformerPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     # TODO: tests would need a rewrite to check for correct implementation\n     # Current tests always assume certain inputs to be passed\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "d1c2c0e00a81d3b7bf57198b55e8e90565a01617",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -786,8 +786,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"unispeech\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "0f4a98f9bec35e1ced49b28707c894dbac398eb5",
            "filename": "src/transformers/models/unispeech/modular_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodular_unispeech.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -142,8 +142,7 @@ class UniSpeechPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"unispeech\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "21bd79613da18b527d6f9d1fbaca518b38c684dd",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -791,8 +791,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"unispeech_sat\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "087f83f95838ff07c993d43f212fa89a6bd5b2f5",
            "filename": "src/transformers/models/unispeech_sat/modular_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodular_unispeech_sat.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -154,8 +154,7 @@ class UniSpeechSatPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"unispeech_sat\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "ea94b07bfa8f0dca20e20f3292e9b14cafb1fdb6",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -132,8 +132,7 @@ class VideoLlavaPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "39784ca889cea12d0b9b099f28aa43e571845740",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -471,8 +471,7 @@ class VideoMAEPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "94ced611b0f024d83f72e74ac838e2245861cd98",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -119,8 +119,7 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "6bda9bf1221d5cdae0d49ba35f3b3d27998c3226",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -68,8 +68,7 @@ class VisionEncoderDecoderModel(PreTrainedModel, GenerationMixin):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_param_buffer_assignment = False\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "f0d806c3119b7012b6029825f5c5c3321d63657a",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -46,8 +46,7 @@ def clip_loss(similarity: torch.Tensor) -> torch.Tensor:\n class VisionTextDualEncoderModel(PreTrainedModel):\n     config_class = VisionTextDualEncoderConfig\n     base_model_prefix = \"vision_text_dual_encoder\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n \n     def __init__("
        },
        {
            "sha": "8e38f83cacaa7f6b8edf892dcf0e421b1a5f00f0",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -440,8 +440,7 @@ class ViTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "ddd582eca2a0dd08ff0d74037b9edd3b05794da2",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -605,8 +605,7 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "0c3da4fffa3738bbc1337be8b9ba8844115897d2",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -444,8 +444,7 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "af2ca9825e48c17827bf76b9a5d01aeb63e314dc",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -405,8 +405,7 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm, VitPoseBackboneEmbeddings]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "ca7c3046a52ce84f750b82e0565cd2ed27b984ec",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -448,8 +448,7 @@ class VivitPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "0226d7d9ad91e44e2d82174ccc3fabd17a24e8bf",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -989,8 +989,7 @@ class VJEPA2PreTrainedModel(PreTrainedModel):\n         \"VJEPA2PredictorEmbeddings\",\n     ]\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "88ca7c254095eb5b9e97714f9520d2e664cb200f",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1035,8 +1035,7 @@ class Wav2Vec2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"wav2vec2\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n "
        },
        {
            "sha": "9f571c5dbab9dc90209c68ec5c72f5ff6fe68302",
            "filename": "src/transformers/models/wavlm/modeling_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodeling_wavlm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -598,8 +598,7 @@ class WavLMPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"wavlm\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = False\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "7666ed05615dff9a21252756c0aa51c5eed6d436",
            "filename": "src/transformers/models/wavlm/modular_wavlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwavlm%2Fmodular_wavlm.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -508,7 +508,7 @@ class WavLMPreTrainedModel(PreTrainedModel, Wav2Vec2PreTrainedModel):\n     base_model_prefix = \"wavlm\"\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_flex_attn = False\n "
        },
        {
            "sha": "9442412b020244fac57dff010ecdbb8fcc7cedfb",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -560,8 +560,7 @@ class WhisperPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_features\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"WhisperEncoderLayer\", \"WhisperDecoderLayer\"]\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n     _supports_cache_class = True"
        },
        {
            "sha": "96ae1fe1d9c0eac2581f0c5699654432d4f5dcc2",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -518,8 +518,7 @@ class YolosPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n "
        },
        {
            "sha": "06d689ae680780381c0f39def90b622415e847c1",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 25,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -786,7 +786,7 @@ class ZambaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ZambaAttentionDecoderLayer\", \"ZambaMambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = False\n+    _supports_flash_attn = False\n     _supports_sdpa = False\n     _supports_cache_class = True  # Note: only supports ZambaHybridDynamicCache\n     _is_stateful = True\n@@ -823,30 +823,6 @@ def _init_weights(self, module):\n             module.A_log.data.copy_(torch.log(A).reshape(module.n_mamba_heads, module.mamba_head_dim, -1))\n             module.D.data.fill_(1.0)\n \n-    @classmethod\n-    @classmethod\n-    def _check_and_enable_flash_attn_2(\n-        cls,\n-        config,\n-        torch_dtype: Optional[torch.dtype] = None,\n-        device_map: Optional[Union[str, dict[str, int]]] = None,\n-        hard_check_only: bool = False,\n-        check_device_map: bool = False,\n-    ):\n-        \"\"\"\n-        Overloads `PreTrainedModel._check_and_enable_flash_attn_2` so as to DISABLE Flash Attention 2 by default on Zamba models.\n-        Flash attention 2 is currently not supported in the HuggingFace implementation of Zamba v1.\n-        \"\"\"\n-        config = super()._check_and_enable_flash_attn_2(\n-            config, torch_dtype, device_map, hard_check_only=hard_check_only, check_device_map=check_device_map\n-        )\n-\n-        # if using the default path -> swap sdpa by eager\n-        if not hard_check_only and config._attn_implementation == \"flash_attention_2\":\n-            config._attn_implementation = \"eager\"\n-\n-        return config\n-\n \n @auto_docstring\n class ZambaModel(ZambaPreTrainedModel):"
        },
        {
            "sha": "ce7555058a4ee862049de84cddcf4f5e1ddcb437",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -1176,8 +1176,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Zamba2AttentionDecoderLayer\", \"Zamba2MambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache"
        },
        {
            "sha": "032a2dd5cbf4de92f6148745cbe9c12d2fe57649",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -899,8 +899,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Zamba2AttentionDecoderLayer\", \"Zamba2MambaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_flash_attn_3 = True\n+    _supports_flash_attn = True\n     _supports_flex_attn = True\n     _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache"
        },
        {
            "sha": "f98bd89b34ad5e2b6e3e13059ff5c1c7c876659f",
            "filename": "src/transformers/utils/args_doc.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/src%2Ftransformers%2Futils%2Fargs_doc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fargs_doc.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -927,11 +927,8 @@ class ClassAttrs:\n     _skip_keys_device_placement = r\"\"\"\n     A list of keys to ignore when moving inputs or outputs between devices when using the `accelerate` library.\n     \"\"\"\n-    _supports_flash_attn_3 = r\"\"\"\n-    Whether the model's attention implementation supports FlashAttention 3.0.\n-    \"\"\"\n-    _supports_flash_attn_2 = r\"\"\"\n-    Whether the model's attention implementation supports FlashAttention 2.0.\n+    _supports_flash_attn = r\"\"\"\n+    Whether the model's attention implementation supports FlashAttention.\n     \"\"\"\n     _supports_sdpa = r\"\"\"\n     Whether the model's attention implementation supports SDPA (Scaled Dot Product Attention)."
        },
        {
            "sha": "ea00871d27baf83b13f6019a89028d198e29b88e",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 2,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -29,7 +29,15 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoConfig, AutoProcessor, AutoTokenizer, is_torch_available, logging, pipeline\n+from transformers import (\n+    AutoConfig,\n+    AutoProcessor,\n+    AutoTokenizer,\n+    PreTrainedModel,\n+    is_torch_available,\n+    logging,\n+    pipeline,\n+)\n from transformers.testing_utils import (\n     CaptureLogger,\n     is_flaky,\n@@ -2007,7 +2015,7 @@ def test_generate_with_static_cache(self):\n             max_new_tokens = 20\n \n             for dtype in (torch.float32, torch.float16):\n-                model = model_class(config).to(torch_device).to(dtype).eval()\n+                model = model_class(copy.deepcopy(config)).to(torch_device).to(dtype).eval()\n                 inputs_dict = {\n                     k: v.to(dtype) if isinstance(v, torch.Tensor) and torch.is_floating_point(v) else v\n                     for k, v in inputs_dict.items()\n@@ -2340,6 +2348,18 @@ def _test_attention_implementation(self, attn_implementation):\n             set_config_for_less_flaky_test(config)\n             model = model_class(config)\n \n+            # If not all sub-models support flex, skip the test. We could potentially set not supported backbones\n+            # to \"eager\" attention, leaving it for future updates on multimodality tests\n+            sub_models_supporting_attn = [\n+                getattr(module, support_flag[attn_implementation])\n+                for name, module in model.named_modules()\n+                if isinstance(module, PreTrainedModel) and name != \"\"\n+            ]\n+            if not all(sub_models_supporting_attn) and len(sub_models_supporting_attn) > 0:\n+                self.skipTest(\n+                    f\"One of {model_class.__name__}'s backbones does not support `attn_implementation={attn_implementation}`\"\n+                )\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 del model"
        },
        {
            "sha": "4d24fc6e70d12c8d6c01dddb7f3a4af61869a2ad",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch BLIP-2 model.\"\"\"\n \n+import copy\n import inspect\n import tempfile\n import unittest\n@@ -184,9 +185,8 @@ def test_model_get_set_embeddings(self):\n             self.assertTrue(x is None or isinstance(x, nn.Linear))\n \n     def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             signature = inspect.signature(model.forward)\n             # signature.parameters is an OrderedDict => so arg_names order is deterministic\n@@ -987,9 +987,8 @@ def test_sdpa_can_dispatch_composite_models(self):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n     def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             signature = inspect.signature(model.forward)\n             # signature.parameters is an OrderedDict => so arg_names order is deterministic\n@@ -1077,7 +1076,7 @@ def test_initialization(self):\n         for key in [\"vision_config\", \"qformer_config\", \"text_config\"]:\n             setattr(configs_no_init, key, _config_zero_init(getattr(configs_no_init, key)))\n         for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n+            model = model_class(config=copy.deepcopy(configs_no_init))\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n                     self.assertIn("
        },
        {
            "sha": "6b5f4b661458be46cb0a803e98f493b6461e046f",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -187,9 +187,9 @@ def test_model_common_attributes(self):\n             self.assertTrue(x is None or isinstance(x, nn.Linear))\n \n     def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n             model = model_class(config)\n             signature = inspect.signature(model.forward)\n             # signature.parameters is an OrderedDict => so arg_names order is deterministic\n@@ -541,9 +541,8 @@ def test_model_common_attributes(self):\n         pass\n \n     def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             signature = inspect.signature(model.forward)\n             # signature.parameters is an OrderedDict => so arg_names order is deterministic"
        },
        {
            "sha": "bed63445d336460028f4fff2a30aedbab8771157",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 25,
            "deletions": 26,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8d6259b0b8290c2406949ce6342051b1f09a074c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=8d6259b0b8290c2406949ce6342051b1f09a074c",
            "patch": "@@ -699,7 +699,7 @@ def check_save_load(out1, out2):\n     def test_from_pretrained_no_checkpoint(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             state_dict = model.state_dict()\n \n             new_model = model_class.from_pretrained(\n@@ -714,7 +714,7 @@ def test_keep_in_fp32_modules(self):\n             if model_class._keep_in_fp32_modules is None:\n                 self.skipTest(reason=\"Model class has no _keep_in_fp32_modules attribute defined\")\n \n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n@@ -730,7 +730,7 @@ def test_save_load_keys_to_ignore_on_save(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             _keys_to_ignore_on_save = getattr(model, \"_keys_to_ignore_on_save\", None)\n             if _keys_to_ignore_on_save is None:\n                 continue\n@@ -766,7 +766,7 @@ def test_gradient_checkpointing_backward_compatibility(self):\n                 continue\n \n             config.gradient_checkpointing = True\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             self.assertTrue(model.is_gradient_checkpointing)\n \n     def test_gradient_checkpointing_enable_disable(self):\n@@ -777,7 +777,7 @@ def test_gradient_checkpointing_enable_disable(self):\n                 continue\n \n             # at init model should have gradient checkpointing disabled\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n             # check enable works\n@@ -810,7 +810,7 @@ def test_peft_gradient_checkpointing_enable_disable(self):\n                 continue\n \n             # at init model should have gradient checkpointing disabled\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n             # check enable works\n@@ -871,7 +871,7 @@ def seeded_initialize_weights(self, module):\n \n             # First, initialize the model from config -> this ensure everything is correctly initialized, even if\n             # _init_weights() does not take all weights into account correctly\n-            model_from_config = model_class(config)\n+            model_from_config = model_class(copy.deepcopy(config))\n             # Here, passing an empty state dict will force all weights to be moved from meta to cpu, then be initialized\n             # by _init_weights()\n             model_from_pretrained = model_class.from_pretrained(None, config=config, state_dict={})\n@@ -944,7 +944,7 @@ class CopyClass(base_class):\n             base_class_copy._init_weights = _mock_init_weights\n             base_class_copy.init_weights = _mock_all_init_weights\n \n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             state_dict = model.state_dict()\n \n             def check_equal(loaded):\n@@ -969,7 +969,7 @@ def test_initialization(self):\n \n         configs_no_init = _config_zero_init(config)\n         for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n+            model = model_class(config=copy.deepcopy(configs_no_init))\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n                     data = torch.flatten(param.data)\n@@ -1000,7 +1000,7 @@ def check_determinism(first, second):\n             self.assertLessEqual(max_diff, 1e-5)\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             model.to(torch_device)\n             model.eval()\n             with torch.no_grad():\n@@ -1075,7 +1075,7 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n             if hasattr(self.model_tester, \"prepare_config_and_inputs_for_model_class\"):\n                 config, batched_input = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)\n             batched_input_prepared = self._prepare_for_class(batched_input, model_class)\n-            model = model_class(config).to(torch_device).eval()\n+            model = model_class(copy.deepcopy(config)).to(torch_device).eval()\n             set_model_for_less_flaky_test(model)\n \n             batch_size = self.model_tester.batch_size\n@@ -1932,7 +1932,7 @@ def test_head_pruning_integration(self):\n \n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             model.to(torch_device)\n             model.eval()\n \n@@ -2061,16 +2061,15 @@ def test_feed_forward_chunking(self):\n         ) = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             torch.manual_seed(0)\n-            config = copy.deepcopy(original_config)\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(original_config))\n             model.to(torch_device)\n             model.eval()\n \n             hidden_states_no_chunk = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n \n             torch.manual_seed(0)\n-            config.chunk_size_feed_forward = 1\n-            model = model_class(config)\n+            original_config.chunk_size_feed_forward = 1\n+            model = model_class(copy.deepcopy(original_config))\n             model.to(torch_device)\n             model.eval()\n \n@@ -2445,7 +2444,7 @@ def test_model_get_set_embeddings(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             self.assertIsInstance(model.get_input_embeddings(), nn.Embedding)\n \n             new_input_embedding_layer = nn.Embedding(10, 10)\n@@ -2505,7 +2504,7 @@ def check_same_values(layer_1, layer_2):\n \n         for model_class in self.all_model_classes:\n             config.torchscript = True\n-            model_not_tied = model_class(config)\n+            model_not_tied = model_class(copy.deepcopy(config))\n             if model_not_tied.get_output_embeddings() is None:\n                 continue\n \n@@ -2582,7 +2581,7 @@ def test_tied_weights_keys(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         config.get_text_config().tie_word_embeddings = True\n         for model_class in self.all_model_classes:\n-            model_tied = model_class(config)\n+            model_tied = model_class(copy.deepcopy(config))\n \n             ptrs = collections.defaultdict(list)\n             for name, tensor in model_tied.state_dict().items():\n@@ -2707,7 +2706,7 @@ def recursive_check(tuple_object, dict_object):\n                 recursive_check(tuple_output, dict_output)\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             model.to(torch_device)\n             model.eval()\n \n@@ -3033,7 +3032,7 @@ def test_disk_offload_bin(self):\n                 continue\n \n             inputs_dict_class = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config).eval()\n+            model = model_class(copy.deepcopy(config)).eval()\n             model = model.to(torch_device)\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n@@ -3077,7 +3076,7 @@ def test_disk_offload_safetensors(self):\n                 continue\n \n             inputs_dict_class = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config).eval()\n+            model = model_class(copy.deepcopy(config)).eval()\n             model = model.to(torch_device)\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n@@ -3115,7 +3114,7 @@ def test_cpu_offload(self):\n                 continue\n \n             inputs_dict_class = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config).eval()\n+            model = model_class(copy.deepcopy(config)).eval()\n             model = model.to(torch_device)\n \n             torch.manual_seed(0)\n@@ -3470,7 +3469,7 @@ def test_model_is_small(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n-            model = model_class(config)\n+            model = model_class(copy.deepcopy(config))\n             num_params = model.num_parameters()\n             assert num_params < 1000000, (\n                 f\"{model_class} is too big for the common tests ({num_params})! It should have 1M max.\"\n@@ -3628,7 +3627,7 @@ def test_attn_implementation_composite_models(self):\n \n             # set eager as it will be the one supported in all models\n             # we just need to test if passing 'attn_implementation' as a dict fails or not\n-            attn_implementation_per_subconfig = {}\n+            attn_implementation_per_subconfig = {\"\": \"eager\"}\n             for key in config.sub_configs.keys():\n                 attn_implementation_per_subconfig[key] = \"eager\"\n \n@@ -4717,7 +4716,7 @@ def test_can_be_initialized_on_meta(self):\n         for model_class in self.all_model_classes:\n             # If it does not raise here, the test passes\n             with torch.device(\"meta\"):\n-                _ = model_class(config)\n+                _ = model_class(copy.deepcopy(config))\n \n     @require_torch_accelerator\n     def test_can_load_with_device_context_manager(self):"
        }
    ],
    "stats": {
        "total": 1227,
        "additions": 451,
        "deletions": 776
    }
}