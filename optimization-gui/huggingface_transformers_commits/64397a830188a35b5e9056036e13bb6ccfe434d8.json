{
    "author": "gjamesgoenawan",
    "message": "Fixed wrong padding value in OWLv2 (#41938)\n\n* Update image_processing_owlv2_fast.py\n\nfixed padding value\n\n* fixed padding value\n\n* Change padding constant value from 0.5 to 0.0\n\n* Fixed missed padding value in modular_owlv2.py\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "64397a830188a35b5e9056036e13bb6ccfe434d8",
    "files": [
        {
            "sha": "a272c8fdbce78aca1d587179dc1cf7c5b0f67e66",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=64397a830188a35b5e9056036e13bb6ccfe434d8",
            "patch": "@@ -291,7 +291,7 @@ def pad(\n         image = pad(\n             image=image,\n             padding=((0, size - height), (0, size - width)),\n-            constant_values=0.5,\n+            constant_values=0.0,\n             data_format=data_format,\n             input_data_format=input_data_format,\n         )"
        },
        {
            "sha": "f1a8a79fb81e2c4afeb622026a50f41b9d40543c",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=64397a830188a35b5e9056036e13bb6ccfe434d8",
            "patch": "@@ -228,7 +228,7 @@ def post_process_image_guided_detection(self, outputs, threshold=0.0, nms_thresh\n \n         return results\n \n-    def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.5) -> \"torch.Tensor\":\n+    def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.0) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image with zeros to the given size.\n         \"\"\"\n@@ -245,7 +245,7 @@ def pad(\n         self,\n         images: list[\"torch.Tensor\"],\n         disable_grouping: Optional[bool],\n-        constant_value: float = 0.5,\n+        constant_value: float = 0.0,\n         **kwargs,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n@@ -351,7 +351,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         if do_pad:\n-            processed_images = self.pad(processed_images, constant_value=0.5, disable_grouping=disable_grouping)\n+            processed_images = self.pad(processed_images, constant_value=0.0, disable_grouping=disable_grouping)\n \n         grouped_images, grouped_images_index = group_images_by_shape(\n             processed_images, disable_grouping=disable_grouping"
        },
        {
            "sha": "590fa5b4b31cc0ab5ef96a002830193f807e9d9f",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/64397a830188a35b5e9056036e13bb6ccfe434d8/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=64397a830188a35b5e9056036e13bb6ccfe434d8",
            "patch": "@@ -52,7 +52,7 @@ class Owlv2ImageProcessorFast(OwlViTImageProcessorFast):\n     crop_size = None\n     do_center_crop = None\n \n-    def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.5) -> \"torch.Tensor\":\n+    def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.0) -> \"torch.Tensor\":\n         \"\"\"\n         Pad an image with zeros to the given size.\n         \"\"\"\n@@ -69,7 +69,7 @@ def pad(\n         self,\n         images: list[\"torch.Tensor\"],\n         disable_grouping: Optional[bool],\n-        constant_value: float = 0.5,\n+        constant_value: float = 0.0,\n         **kwargs,\n     ) -> list[\"torch.Tensor\"]:\n         \"\"\"\n@@ -175,7 +175,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         if do_pad:\n-            processed_images = self.pad(processed_images, constant_value=0.5, disable_grouping=disable_grouping)\n+            processed_images = self.pad(processed_images, constant_value=0.0, disable_grouping=disable_grouping)\n \n         grouped_images, grouped_images_index = group_images_by_shape(\n             processed_images, disable_grouping=disable_grouping"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}