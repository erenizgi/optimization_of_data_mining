{
    "author": "44670",
    "message": "support loading qwen3 gguf (#38645)\n\n* support loading qwen3 gguf\n\n* Add qwen3 into GGUF_TO_FAST_CONVERTERS for tokenizer conversion\n\n* Add testcase\n\n* Fix formatting",
    "sha": "2b79f143756bf10a2d43e5ab6d413374b633d2ce",
    "files": [
        {
            "sha": "17f86e1667373601166c31eae1e2a7b6fb2bd5f5",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b79f143756bf10a2d43e5ab6d413374b633d2ce/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b79f143756bf10a2d43e5ab6d413374b633d2ce/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=2b79f143756bf10a2d43e5ab6d413374b633d2ce",
            "patch": "@@ -90,6 +90,18 @@\n         \"expert_count\": \"num_experts\",\n         \"expert_used_count\": \"num_experts_per_tok\",\n     },\n+    \"qwen3\": {\n+        \"context_length\": \"max_position_embeddings\",\n+        \"block_count\": \"num_hidden_layers\",\n+        \"feed_forward_length\": \"intermediate_size\",\n+        \"embedding_length\": \"hidden_size\",\n+        \"rope.dimension_count\": None,\n+        \"rope.freq_base\": \"rope_theta\",\n+        \"attention.head_count\": \"num_attention_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_rms_epsilon\": \"rms_norm_eps\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n     \"falcon\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n@@ -676,6 +688,7 @@ def converted(self) -> Tokenizer:\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n     \"qwen2_moe\": GGUFQwen2Converter,\n+    \"qwen3\": GGUFQwen2Converter,\n     \"phi3\": GGUFPhi3Converter,\n     \"bloom\": GGUFGPTConverter,\n     \"falcon\": GGUFGPTConverter,"
        },
        {
            "sha": "d2ed7f7a745d0928b929457534f749b003b9a7fa",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/2b79f143756bf10a2d43e5ab6d413374b633d2ce/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2b79f143756bf10a2d43e5ab6d413374b633d2ce/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=2b79f143756bf10a2d43e5ab6d413374b633d2ce",
            "patch": "@@ -301,6 +301,7 @@ class GgufModelTests(unittest.TestCase):\n     gemma3_qat_model_id = \"google/gemma-3-1b-it-qat-q4_0-gguf\"\n     gemma3_text_model_id = \"unsloth/gemma-3-1b-it-GGUF\"\n     gemma3_vision_model_id = \"unsloth/gemma-3-4b-it-GGUF\"\n+    qwen3_model_id = \"Qwen/Qwen3-0.6B-GGUF\"\n \n     q4_0_phi3_model_id = \"Phi-3-mini-4k-instruct-q4.gguf\"\n     q4_0_mistral_model_id = \"mistral-7b-instruct-v0.2.Q4_0.gguf\"\n@@ -333,6 +334,7 @@ class GgufModelTests(unittest.TestCase):\n     q4_0_gemma3_qat_model_id = \"gemma-3-1b-it-q4_0.gguf\"\n     bf16_gemma3_text_model_id = \"gemma-3-1b-it-BF16.gguf\"\n     bf16_gemma3_vision_model_id = \"gemma-3-4b-it-BF16.gguf\"\n+    q8_0_qwen3_model_id = \"Qwen3-0.6B-Q8_0.gguf\"\n \n     example_text = \"Hello\"\n \n@@ -955,3 +957,19 @@ def test_gemma3_vision_weights_conversion_bf16(self):\n                 torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n             else:\n                 raise ValueError(f\"Layer {layer_name} is not presented in GGUF model\")\n+\n+    @require_read_token\n+    @unittest.skipUnless(is_gguf_available(\"0.16.0\"), \"test requires gguf version >= 0.16.0\")\n+    def test_qwen3_q8_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.qwen3_model_id, gguf_file=self.q8_0_qwen3_model_id)\n+        model = AutoModelForCausalLM.from_pretrained(\n+            self.qwen3_model_id,\n+            gguf_file=self.q8_0_qwen3_model_id,\n+            torch_dtype=torch.float16,\n+        )\n+\n+        text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n+        out = model.generate(text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"HelloED\\nI need to find the value of the\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 31,
        "deletions": 0
    }
}