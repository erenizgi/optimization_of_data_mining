{
    "author": "gante",
    "message": "[whisper] move processor test into processor test file ðŸ§¹  (#38266)\n\nmove processor tests",
    "sha": "aa02a5d9020c7afcd4ef512bf989387d1180661e",
    "files": [
        {
            "sha": "cda375632a947086f36d835dc6444b9ef40b54c0",
            "filename": "src/transformers/pipelines/automatic_speech_recognition.py",
            "status": "modified",
            "additions": 0,
            "deletions": 126,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa02a5d9020c7afcd4ef512bf989387d1180661e/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa02a5d9020c7afcd4ef512bf989387d1180661e/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fautomatic_speech_recognition.py?ref=aa02a5d9020c7afcd4ef512bf989387d1180661e",
            "patch": "@@ -79,26 +79,6 @@ def chunk_iter(inputs, feature_extractor, chunk_len, stride_left, stride_right,\n             break\n \n \n-def _fast_find_longest_common_sequence(sequence_left, sequence_right):\n-    seq_len_left = len(sequence_left)\n-    seq_len_right = len(sequence_right)\n-    counter = [[0] * (seq_len_right + 1) for _ in range(seq_len_left + 1)]\n-    longest = 0\n-    for i in range(seq_len_left):\n-        for j in range(seq_len_right):\n-            if sequence_left[i] == sequence_right[j]:\n-                previous_counter = counter[i][j] + 1\n-                counter[i + 1][j + 1] = previous_counter\n-                if previous_counter > longest:\n-                    longest = previous_counter\n-\n-    counter = np.array(counter)\n-    # we return the idx of the first element of the longest common sequence in the left sequence\n-    index_left = np.argwhere(counter == longest)[-1][0] - longest if longest != 0 else -1\n-    index_right = np.argwhere(counter == longest)[-1][1] - longest if longest != 0 else -1\n-    return index_left, index_right, longest\n-\n-\n def _find_longest_common_sequence(sequences, tokenizer):\n     # TODO  Use a faster algorithm this can probably be done in O(n)\n     # using suffix array.\n@@ -664,109 +644,3 @@ def postprocess(\n             for k, v in output.items():\n                 extra[k].append(v)\n         return {\"text\": text, **optional, **extra}\n-\n-\n-def _find_timestamp_sequence(sequences, tokenizer, feature_extractor, max_source_positions):\n-    \"\"\"\n-    Computes the final sequences by merging the end of the nth sequence with the beginning of the n+1th sequence. Since\n-    `WhisperForConditionalGeneration` produces the timestamps pairwise, we filter the consecutive timestamps and only\n-    iterate over them. We keep track of the `time` which indicates the actual starting time of the chunk that is\n-    processed. We need to make sure to offset the timestamps tokens by the `time` in order for the tokenizer to\n-    properly compute the final `offset`.\n-    \"\"\"\n-    # index of the first timestamp token\n-    timestamp_begin = tokenizer.convert_tokens_to_ids(\"<|notimestamps|>\") + 1\n-    items = []\n-    # approximation of the token to time ratio : ~0.2seconds\n-    time_precision = feature_extractor.chunk_length / max_source_positions\n-    time = 0\n-    for seq_idx, item in enumerate(sequences):\n-        sequence, stride = item\n-        if isinstance(sequence, list):\n-            sequence = np.array(sequence)\n-        chunk_len, stride_left, stride_right = stride\n-        sequence = sequence.squeeze(0)\n-        # get rid of the `forced_decoder_idx` that are use to parametrize the generation\n-        begin_idx = np.where(sequence == timestamp_begin)[0][0] if timestamp_begin in sequence else 0\n-        sequence = sequence[begin_idx:]\n-\n-        timestamp_tokens = sequence >= timestamp_begin\n-        if seq_idx != 0 and sum(timestamp_tokens) > 0:\n-            consecutive = np.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0] + 1\n-            last_timestamp = np.where(timestamp_tokens)[0][-1]\n-            consecutive = np.append(consecutive, last_timestamp) if last_timestamp not in consecutive else consecutive\n-            time -= stride_left + stride_right\n-            offset = int((time / feature_extractor.sampling_rate) / time_precision)\n-            overlap_time = int((stride_left / feature_extractor.sampling_rate) / time_precision)\n-            # relevant timestamps are in the overlapping part\n-            relevant_timestamp = np.where(sequence[consecutive] >= timestamp_begin + overlap_time)[0]\n-            if relevant_timestamp.shape[0] > 0:\n-                relevant_timestamp = (\n-                    consecutive[relevant_timestamp[0] - 1] if relevant_timestamp[0] > 0 else consecutive[0]\n-                )\n-                # if a big stride is used, we need to check some of the previous items for the best overlap\n-                best_match = 0\n-                sliced_sequence = []\n-                for idx, previous_sequence in enumerate(reversed(items)):\n-                    previous_tokens = previous_sequence[1:-1]\n-                    if previous_sequence[0] < (timestamp_begin + offset - overlap_time) and idx != 0:\n-                        break  # the previous sequence is too far in the past\n-                    if len(previous_tokens) > 0:\n-                        # find the longest common sequence between the overlapping parts\n-                        index_left, index_right, match_length = _fast_find_longest_common_sequence(\n-                            sequence[1:relevant_timestamp], previous_tokens\n-                        )\n-                        # don't do anything if only 1 token was matched\n-                        if match_length > 1 and match_length > best_match:\n-                            best_match = match_length\n-                            best_idx = idx\n-                            end_of_curr_sequence_idx = (\n-                                np.where(sequence[index_left + 1 :] >= timestamp_begin)[0][0] + 1\n-                            )\n-                            end_of_curr_sequence_idx = end_of_curr_sequence_idx + 1 + index_left\n-                            # if all the tokens are matched, suffix\n-                            if index_left == 0 and match_length == len(previous_tokens):\n-                                sliced_sequence = np.insert(\n-                                    sequence[index_left + 1 : end_of_curr_sequence_idx], 0, previous_sequence[0]\n-                                )\n-                                sliced_sequence[-1] = previous_sequence[-1]\n-                            # if part of the previous sequence is not taken\n-                            elif index_left >= 0:\n-                                sliced_sequence = sequence[index_left + 1 : end_of_curr_sequence_idx]\n-                                # let's insert the missing part of the previous sequence\n-                                previous_slice = (\n-                                    previous_sequence[: index_right + 1] if index_right > 0 else [previous_sequence[0]]\n-                                )\n-                                sliced_sequence = np.insert(sliced_sequence, 0, previous_slice)\n-                                sliced_sequence[-1] += offset\n-\n-                if len(sliced_sequence) > 0:\n-                    items[len(items) - best_idx - 1] = sliced_sequence\n-                    items = items[: len(items) - best_idx]\n-                    sequence = sequence[end_of_curr_sequence_idx:]\n-\n-        # sequence might have changed\n-        timestamp_tokens = sequence >= timestamp_begin\n-        consecutive = np.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0] + 1\n-        if sum(timestamp_tokens) > 0:\n-            last_timestamp = np.where(timestamp_tokens)[0][-1]\n-            consecutive = (\n-                np.append(consecutive, last_timestamp + 1) if last_timestamp not in consecutive else consecutive\n-            )\n-\n-        if len(consecutive) > 0:\n-            last_slice = 0\n-            for current_slice in consecutive:\n-                actual_offset = items[-1][-1] if seq_idx != 0 or last_slice != 0 else sequence[0]\n-                sliced_tokens = sequence[last_slice:current_slice]\n-                duration = sliced_tokens[-1] - sliced_tokens[0]\n-                sliced_tokens[0] = actual_offset\n-                sliced_tokens[-1] = actual_offset + duration\n-                items.append(sliced_tokens)\n-                last_slice = current_slice\n-\n-        time += chunk_len\n-    result = []\n-    for i in range(len(items)):\n-        result += items[i].tolist()\n-    return result"
        },
        {
            "sha": "b13d0d1867f0d868a6c1bc29c2a798b0b42a49eb",
            "filename": "tests/models/whisper/test_processor_whisper.py",
            "status": "modified",
            "additions": 294,
            "deletions": 0,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa02a5d9020c7afcd4ef512bf989387d1180661e/tests%2Fmodels%2Fwhisper%2Ftest_processor_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa02a5d9020c7afcd4ef512bf989387d1180661e/tests%2Fmodels%2Fwhisper%2Ftest_processor_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_processor_whisper.py?ref=aa02a5d9020c7afcd4ef512bf989387d1180661e",
            "patch": "@@ -16,6 +16,7 @@\n import tempfile\n import unittest\n \n+import numpy as np\n import pytest\n \n from transformers import WhisperTokenizer, is_speech_available\n@@ -177,3 +178,296 @@ def _test_prompt_error_raised_helper(prompt, special_token):\n         _test_prompt_error_raised_helper(\"<|startofprev|> test\", \"<|startofprev|>\")\n         _test_prompt_error_raised_helper(\"test <|notimestamps|>\", \"<|notimestamps|>\")\n         _test_prompt_error_raised_helper(\"test <|zh|> test <|transcribe|>\", \"<|zh|>\")\n+\n+    def test_find_longest_common_subsequence_old(self):\n+        \"\"\"Test using the old processing functions used in the ASR pipeline, but that serves as a BC reference.\"\"\"\n+        max_source_positions = 1500\n+        processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n+\n+        previous_sequence = [[51492, 406, 3163, 1953, 466, 13, 51612, 51612]]\n+        self.assertEqual(\n+            processor.decode(previous_sequence[0], output_offsets=True),\n+            {\n+                \"text\": \" not worth thinking about.\",\n+                \"offsets\": [{\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)}],\n+            },\n+        )\n+\n+        # Merge when the previous sequence is a suffix of the next sequence\n+        # fmt: off\n+        next_sequences_1 = [\n+            [50364, 295, 6177, 3391, 11, 19817, 3337, 507, 307, 406, 3163, 1953, 466, 13, 50614, 50614, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50834, 50257]\n+        ]\n+        # fmt: on\n+        self.assertEqual(\n+            processor.decode(next_sequences_1[0], output_offsets=True),\n+            {\n+                \"text\": (\n+                    \" of spectators, retrievality is not worth thinking about. His instant panic was followed by a\"\n+                    \" small, sharp blow high on his chest.<|endoftext|>\"\n+                ),\n+                \"offsets\": [\n+                    {\"text\": \" of spectators, retrievality is not worth thinking about.\", \"timestamp\": (0.0, 5.0)},\n+                    {\n+                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n+                        \"timestamp\": (5.0, 9.4),\n+                    },\n+                ],\n+            },\n+        )\n+        merge = _find_timestamp_sequence(\n+            [[previous_sequence, (480_000, 0, 0)], [next_sequences_1, (480_000, 120_000, 0)]],\n+            processor.tokenizer,\n+            processor.feature_extractor,\n+            max_source_positions,\n+        )\n+\n+        # fmt: off\n+        self.assertEqual(\n+            merge,\n+            [51492, 406, 3163, 1953, 466, 13, 51739, 51739, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51959],\n+        )\n+        # fmt: on\n+        self.assertEqual(\n+            processor.decode(merge, output_offsets=True),\n+            {\n+                \"text\": (\n+                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n+                    \" chest.\"\n+                ),\n+                \"offsets\": [\n+                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 27.5)},\n+                    {\n+                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n+                        \"timestamp\": (27.5, 31.900000000000002),\n+                    },\n+                ],\n+            },\n+        )\n+\n+        # Merge when the sequence is in the middle of the 1st next sequence\n+        # fmt: off\n+        next_sequences_2 = [\n+            [50364, 295, 6177, 3391, 11, 19817, 3337, 507, 307, 406, 3163, 1953, 466, 13, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50834, 50257]\n+        ]\n+        # fmt: on\n+        # {'text': ' of spectators, retrievality is not worth thinking about. His instant panic was followed by a small, sharp blow high on his chest.','timestamp': (0.0, 9.4)}\n+        merge = _find_timestamp_sequence(\n+            [[previous_sequence, (480_000, 0, 0)], [next_sequences_2, (480_000, 120_000, 0)]],\n+            processor.tokenizer,\n+            processor.feature_extractor,\n+            max_source_positions,\n+        )\n+        # fmt: off\n+        self.assertEqual(\n+            merge,\n+            [51492, 406, 3163, 1953, 466, 13, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51959],\n+        )\n+        # fmt: on\n+        self.assertEqual(\n+            processor.decode(merge, output_offsets=True),\n+            {\n+                \"text\": (\n+                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n+                    \" chest.\"\n+                ),\n+                \"offsets\": [\n+                    {\n+                        \"text\": (\n+                            \" not worth thinking about. His instant panic was followed by a small, sharp blow high on\"\n+                            \" his chest.\"\n+                        ),\n+                        \"timestamp\": (22.56, 31.900000000000002),\n+                    },\n+                ],\n+            },\n+        )\n+\n+        # Merge when the previous sequence is not included in the current sequence\n+        next_sequences_3 = [[50364, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50584, 50257]]  # fmt: skip\n+        # {'text': ' His instant panic was followed by a small, sharp blow high on his chest.','timestamp': (0.0, 9.4)}\n+        merge = _find_timestamp_sequence(\n+            [[previous_sequence, (480_000, 0, 0)], [next_sequences_3, (480_000, 120_000, 0)]],\n+            processor.tokenizer,\n+            processor.feature_extractor,\n+            max_source_positions,\n+        )\n+        self.assertEqual(\n+            merge,\n+            [51492, 406, 3163, 1953, 466, 13, 51612, 51612, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51832],\n+        )  # fmt: skip\n+        self.assertEqual(\n+            processor.decode(merge, output_offsets=True),\n+            {\n+                \"text\": (\n+                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n+                    \" chest.\"\n+                ),\n+                \"offsets\": [\n+                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)},\n+                    {\n+                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n+                        \"timestamp\": (24.96, 29.36),\n+                    },\n+                ],\n+            },\n+        )\n+        # last case is when the sequence is not in the first next predicted start and end of timestamp\n+        next_sequences_3 = [\n+            [50364, 2812, 9836, 14783, 390, 406, 3163, 1953, 466, 13, 50634, 50634, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50934]\n+        ]  # fmt: skip\n+        merge = _find_timestamp_sequence(\n+            [[previous_sequence, (480_000, 0, 0)], [next_sequences_3, (480_000, 167_000, 0)]],\n+            processor.tokenizer,\n+            processor.feature_extractor,\n+            max_source_positions,\n+        )\n+        self.assertEqual(\n+            merge,\n+            [51492, 406, 3163, 1953, 466, 13, 51612, 51612, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51912]\n+        )  # fmt: skip\n+        self.assertEqual(\n+            processor.decode(merge, output_offsets=True),\n+            {\n+                \"text\": (\n+                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n+                    \" chest.\"\n+                ),\n+                \"offsets\": [\n+                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)},\n+                    {\n+                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n+                        \"timestamp\": (24.96, 30.96),\n+                    },\n+                ],\n+            },\n+        )\n+\n+\n+def _fast_find_longest_common_sequence(sequence_left, sequence_right):\n+    \"\"\"Old processing function used in the ASR pipeline.\"\"\"\n+    seq_len_left = len(sequence_left)\n+    seq_len_right = len(sequence_right)\n+    counter = [[0] * (seq_len_right + 1) for _ in range(seq_len_left + 1)]\n+    longest = 0\n+    for i in range(seq_len_left):\n+        for j in range(seq_len_right):\n+            if sequence_left[i] == sequence_right[j]:\n+                previous_counter = counter[i][j] + 1\n+                counter[i + 1][j + 1] = previous_counter\n+                if previous_counter > longest:\n+                    longest = previous_counter\n+\n+    counter = np.array(counter)\n+    # we return the idx of the first element of the longest common sequence in the left sequence\n+    index_left = np.argwhere(counter == longest)[-1][0] - longest if longest != 0 else -1\n+    index_right = np.argwhere(counter == longest)[-1][1] - longest if longest != 0 else -1\n+    return index_left, index_right, longest\n+\n+\n+def _find_timestamp_sequence(sequences, tokenizer, feature_extractor, max_source_positions):\n+    \"\"\"\n+    Old processing function used in the ASR pipeline.\n+\n+    Computes the final sequences by merging the end of the nth sequence with the beginning of the n+1th sequence. Since\n+    `WhisperForConditionalGeneration` produces the timestamps pairwise, we filter the consecutive timestamps and only\n+    iterate over them. We keep track of the `time` which indicates the actual starting time of the chunk that is\n+    processed. We need to make sure to offset the timestamps tokens by the `time` in order for the tokenizer to\n+    properly compute the final `offset`.\n+    \"\"\"\n+    # index of the first timestamp token\n+    timestamp_begin = tokenizer.convert_tokens_to_ids(\"<|notimestamps|>\") + 1\n+    items = []\n+    # approximation of the token to time ratio : ~0.2seconds\n+    time_precision = feature_extractor.chunk_length / max_source_positions\n+    time = 0\n+    for seq_idx, item in enumerate(sequences):\n+        sequence, stride = item\n+        if isinstance(sequence, list):\n+            sequence = np.array(sequence)\n+        chunk_len, stride_left, stride_right = stride\n+        sequence = sequence.squeeze(0)\n+        # get rid of the `forced_decoder_idx` that are use to parametrize the generation\n+        begin_idx = np.where(sequence == timestamp_begin)[0][0] if timestamp_begin in sequence else 0\n+        sequence = sequence[begin_idx:]\n+\n+        timestamp_tokens = sequence >= timestamp_begin\n+        if seq_idx != 0 and sum(timestamp_tokens) > 0:\n+            consecutive = np.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0] + 1\n+            last_timestamp = np.where(timestamp_tokens)[0][-1]\n+            consecutive = np.append(consecutive, last_timestamp) if last_timestamp not in consecutive else consecutive\n+            time -= stride_left + stride_right\n+            offset = int((time / feature_extractor.sampling_rate) / time_precision)\n+            overlap_time = int((stride_left / feature_extractor.sampling_rate) / time_precision)\n+            # relevant timestamps are in the overlapping part\n+            relevant_timestamp = np.where(sequence[consecutive] >= timestamp_begin + overlap_time)[0]\n+            if relevant_timestamp.shape[0] > 0:\n+                relevant_timestamp = (\n+                    consecutive[relevant_timestamp[0] - 1] if relevant_timestamp[0] > 0 else consecutive[0]\n+                )\n+                # if a big stride is used, we need to check some of the previous items for the best overlap\n+                best_match = 0\n+                sliced_sequence = []\n+                for idx, previous_sequence in enumerate(reversed(items)):\n+                    previous_tokens = previous_sequence[1:-1]\n+                    if previous_sequence[0] < (timestamp_begin + offset - overlap_time) and idx != 0:\n+                        break  # the previous sequence is too far in the past\n+                    if len(previous_tokens) > 0:\n+                        # find the longest common sequence between the overlapping parts\n+                        index_left, index_right, match_length = _fast_find_longest_common_sequence(\n+                            sequence[1:relevant_timestamp], previous_tokens\n+                        )\n+                        # don't do anything if only 1 token was matched\n+                        if match_length > 1 and match_length > best_match:\n+                            best_match = match_length\n+                            best_idx = idx\n+                            end_of_curr_sequence_idx = (\n+                                np.where(sequence[index_left + 1 :] >= timestamp_begin)[0][0] + 1\n+                            )\n+                            end_of_curr_sequence_idx = end_of_curr_sequence_idx + 1 + index_left\n+                            # if all the tokens are matched, suffix\n+                            if index_left == 0 and match_length == len(previous_tokens):\n+                                sliced_sequence = np.insert(\n+                                    sequence[index_left + 1 : end_of_curr_sequence_idx], 0, previous_sequence[0]\n+                                )\n+                                sliced_sequence[-1] = previous_sequence[-1]\n+                            # if part of the previous sequence is not taken\n+                            elif index_left >= 0:\n+                                sliced_sequence = sequence[index_left + 1 : end_of_curr_sequence_idx]\n+                                # let's insert the missing part of the previous sequence\n+                                previous_slice = (\n+                                    previous_sequence[: index_right + 1] if index_right > 0 else [previous_sequence[0]]\n+                                )\n+                                sliced_sequence = np.insert(sliced_sequence, 0, previous_slice)\n+                                sliced_sequence[-1] += offset\n+\n+                if len(sliced_sequence) > 0:\n+                    items[len(items) - best_idx - 1] = sliced_sequence\n+                    items = items[: len(items) - best_idx]\n+                    sequence = sequence[end_of_curr_sequence_idx:]\n+\n+        # sequence might have changed\n+        timestamp_tokens = sequence >= timestamp_begin\n+        consecutive = np.where(timestamp_tokens[:-1] & timestamp_tokens[1:])[0] + 1\n+        if sum(timestamp_tokens) > 0:\n+            last_timestamp = np.where(timestamp_tokens)[0][-1]\n+            consecutive = (\n+                np.append(consecutive, last_timestamp + 1) if last_timestamp not in consecutive else consecutive\n+            )\n+\n+        if len(consecutive) > 0:\n+            last_slice = 0\n+            for current_slice in consecutive:\n+                actual_offset = items[-1][-1] if seq_idx != 0 or last_slice != 0 else sequence[0]\n+                sliced_tokens = sequence[last_slice:current_slice]\n+                duration = sliced_tokens[-1] - sliced_tokens[0]\n+                sliced_tokens[0] = actual_offset\n+                sliced_tokens[-1] = actual_offset + duration\n+                items.append(sliced_tokens)\n+                last_slice = current_slice\n+\n+        time += chunk_len\n+    result = []\n+    for i in range(len(items)):\n+        result += items[i].tolist()\n+    return result"
        },
        {
            "sha": "f18a35b83fe3dcc61045b25110278dda37216fab",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 1,
            "deletions": 164,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa02a5d9020c7afcd4ef512bf989387d1180661e/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa02a5d9020c7afcd4ef512bf989387d1180661e/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=aa02a5d9020c7afcd4ef512bf989387d1180661e",
            "patch": "@@ -33,7 +33,7 @@\n )\n from transformers.pipelines import AutomaticSpeechRecognitionPipeline, pipeline\n from transformers.pipelines.audio_utils import chunk_bytes_iter, ffmpeg_microphone_live\n-from transformers.pipelines.automatic_speech_recognition import _find_timestamp_sequence, chunk_iter\n+from transformers.pipelines.automatic_speech_recognition import chunk_iter\n from transformers.testing_utils import (\n     compare_pipeline_output_to_hub_spec,\n     is_pipeline_test,\n@@ -636,169 +636,6 @@ def test_torch_whisper_batched(self):\n         output = speech_recognizer(ds[\"audio\"], batch_size=2)\n         self.assertEqual(output, EXPECTED_OUTPUT)\n \n-    def test_find_longest_common_subsequence(self):\n-        max_source_positions = 1500\n-        processor = AutoProcessor.from_pretrained(\"openai/whisper-tiny\")\n-\n-        previous_sequence = [[51492, 406, 3163, 1953, 466, 13, 51612, 51612]]\n-        self.assertEqual(\n-            processor.decode(previous_sequence[0], output_offsets=True),\n-            {\n-                \"text\": \" not worth thinking about.\",\n-                \"offsets\": [{\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)}],\n-            },\n-        )\n-\n-        # Merge when the previous sequence is a suffix of the next sequence\n-        # fmt: off\n-        next_sequences_1 = [\n-            [50364, 295, 6177, 3391, 11, 19817, 3337, 507, 307, 406, 3163, 1953, 466, 13, 50614, 50614, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50834, 50257]\n-        ]\n-        # fmt: on\n-        self.assertEqual(\n-            processor.decode(next_sequences_1[0], output_offsets=True),\n-            {\n-                \"text\": (\n-                    \" of spectators, retrievality is not worth thinking about. His instant panic was followed by a\"\n-                    \" small, sharp blow high on his chest.<|endoftext|>\"\n-                ),\n-                \"offsets\": [\n-                    {\"text\": \" of spectators, retrievality is not worth thinking about.\", \"timestamp\": (0.0, 5.0)},\n-                    {\n-                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n-                        \"timestamp\": (5.0, 9.4),\n-                    },\n-                ],\n-            },\n-        )\n-        merge = _find_timestamp_sequence(\n-            [[previous_sequence, (480_000, 0, 0)], [next_sequences_1, (480_000, 120_000, 0)]],\n-            processor.tokenizer,\n-            processor.feature_extractor,\n-            max_source_positions,\n-        )\n-\n-        # fmt: off\n-        self.assertEqual(\n-            merge,\n-            [51492, 406, 3163, 1953, 466, 13, 51739, 51739, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51959],\n-        )\n-        # fmt: on\n-        self.assertEqual(\n-            processor.decode(merge, output_offsets=True),\n-            {\n-                \"text\": (\n-                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n-                    \" chest.\"\n-                ),\n-                \"offsets\": [\n-                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 27.5)},\n-                    {\n-                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n-                        \"timestamp\": (27.5, 31.900000000000002),\n-                    },\n-                ],\n-            },\n-        )\n-\n-        # Merge when the sequence is in the middle of the 1st next sequence\n-        # fmt: off\n-        next_sequences_2 = [\n-            [50364, 295, 6177, 3391, 11, 19817, 3337, 507, 307, 406, 3163, 1953, 466, 13, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50834, 50257]\n-        ]\n-        # fmt: on\n-        # {'text': ' of spectators, retrievality is not worth thinking about. His instant panic was followed by a small, sharp blow high on his chest.','timestamp': (0.0, 9.4)}\n-        merge = _find_timestamp_sequence(\n-            [[previous_sequence, (480_000, 0, 0)], [next_sequences_2, (480_000, 120_000, 0)]],\n-            processor.tokenizer,\n-            processor.feature_extractor,\n-            max_source_positions,\n-        )\n-        # fmt: off\n-        self.assertEqual(\n-            merge,\n-            [51492, 406, 3163, 1953, 466, 13, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51959],\n-        )\n-        # fmt: on\n-        self.assertEqual(\n-            processor.decode(merge, output_offsets=True),\n-            {\n-                \"text\": (\n-                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n-                    \" chest.\"\n-                ),\n-                \"offsets\": [\n-                    {\n-                        \"text\": (\n-                            \" not worth thinking about. His instant panic was followed by a small, sharp blow high on\"\n-                            \" his chest.\"\n-                        ),\n-                        \"timestamp\": (22.56, 31.900000000000002),\n-                    },\n-                ],\n-            },\n-        )\n-\n-        # Merge when the previous sequence is not included in the current sequence\n-        next_sequences_3 = [[50364, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50584, 50257]]  # fmt: skip\n-        # {'text': ' His instant panic was followed by a small, sharp blow high on his chest.','timestamp': (0.0, 9.4)}\n-        merge = _find_timestamp_sequence(\n-            [[previous_sequence, (480_000, 0, 0)], [next_sequences_3, (480_000, 120_000, 0)]],\n-            processor.tokenizer,\n-            processor.feature_extractor,\n-            max_source_positions,\n-        )\n-        self.assertEqual(\n-            merge,\n-            [51492, 406, 3163, 1953, 466, 13, 51612, 51612, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51832],\n-        )  # fmt: skip\n-        self.assertEqual(\n-            processor.decode(merge, output_offsets=True),\n-            {\n-                \"text\": (\n-                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n-                    \" chest.\"\n-                ),\n-                \"offsets\": [\n-                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)},\n-                    {\n-                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n-                        \"timestamp\": (24.96, 29.36),\n-                    },\n-                ],\n-            },\n-        )\n-        # last case is when the sequence is not in the first next predicted start and end of timestamp\n-        next_sequences_3 = [\n-            [50364, 2812, 9836, 14783, 390, 406, 3163, 1953, 466, 13, 50634, 50634, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 50934]\n-        ]  # fmt: skip\n-        merge = _find_timestamp_sequence(\n-            [[previous_sequence, (480_000, 0, 0)], [next_sequences_3, (480_000, 167_000, 0)]],\n-            processor.tokenizer,\n-            processor.feature_extractor,\n-            max_source_positions,\n-        )\n-        self.assertEqual(\n-            merge,\n-            [51492, 406, 3163, 1953, 466, 13, 51612, 51612, 2812, 9836, 14783, 390, 6263, 538, 257, 1359, 11, 8199, 6327, 1090, 322, 702, 7443, 13, 51912]\n-        )  # fmt: skip\n-        self.assertEqual(\n-            processor.decode(merge, output_offsets=True),\n-            {\n-                \"text\": (\n-                    \" not worth thinking about. His instant panic was followed by a small, sharp blow high on his\"\n-                    \" chest.\"\n-                ),\n-                \"offsets\": [\n-                    {\"text\": \" not worth thinking about.\", \"timestamp\": (22.56, 24.96)},\n-                    {\n-                        \"text\": \" His instant panic was followed by a small, sharp blow high on his chest.\",\n-                        \"timestamp\": (24.96, 30.96),\n-                    },\n-                ],\n-            },\n-        )\n-\n     @slow\n     @require_torch\n     @unittest.skip(\"TODO (joao, eustache): this test is failing, find the breaking PR and fix the cause or the test\")"
        }
    ],
    "stats": {
        "total": 585,
        "additions": 295,
        "deletions": 290
    }
}