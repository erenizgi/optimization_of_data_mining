{
    "author": "vasqu",
    "message": "[`Moe`] Post interface fixes (#43129)\n\n* fixes\n\n* style",
    "sha": "5cf0f5beb2752d65a978aca31a3c10cbccc607e8",
    "files": [
        {
            "sha": "9a06b48aa3b60aca46327315790c4c66b257ea51",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=5cf0f5beb2752d65a978aca31a3c10cbccc607e8",
            "patch": "@@ -152,6 +152,7 @@ def get_large_model(cls):\n         cls.model = Ernie4_5_MoeForCausalLM.from_pretrained(\n             \"baidu/ERNIE-4.5-21B-A3B-PT\",\n             device_map=\"auto\",\n+            experts_implementation=\"eager\",\n             quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n         )\n \n@@ -163,6 +164,7 @@ def get_small_model(cls):\n             \"hf-internal-testing/ERNIE-4.5-Small-Moe\",\n             device_map=\"auto\",\n             dtype=\"auto\",\n+            experts_implementation=\"eager\",\n         )\n \n         return cls.model"
        },
        {
            "sha": "67d2b74a6a905cf30c87335c440c5f2825012b0a",
            "filename": "tests/models/lfm2_moe/test_modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_moe%2Ftest_modeling_lfm2_moe.py?ref=5cf0f5beb2752d65a978aca31a3c10cbccc607e8",
            "patch": "@@ -164,7 +164,10 @@ def tearDown(self):\n     def get_model(cls):\n         if cls.model is None:\n             cls.model = Lfm2MoeForCausalLM.from_pretrained(\n-                \"LiquidAI/LFM2-8B-A1B\", device_map=\"auto\", dtype=torch.bfloat16\n+                \"LiquidAI/LFM2-8B-A1B\",\n+                device_map=\"auto\",\n+                dtype=torch.bfloat16,\n+                experts_implementation=\"eager\",\n             )\n         return cls.model\n "
        },
        {
            "sha": "d3e03910c1260ca319cacca50272e3d016dd6275",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5cf0f5beb2752d65a978aca31a3c10cbccc607e8/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=5cf0f5beb2752d65a978aca31a3c10cbccc607e8",
            "patch": "@@ -3566,7 +3566,7 @@ def test_sdpa_can_compile_dynamic(self):\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(tmpdirname, dtype=torch.float16, attn_implementation=\"sdpa\")\n+                model = model_class.from_pretrained(tmpdirname, dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n                 model.to(torch_device)\n \n                 # For PyTorch 2.1 - 2.3.0 set `dynamic=True`. In the future setting `dynamic=None` and using `torch._dynamo.mark_dynamic()`\n@@ -3577,7 +3577,7 @@ def test_sdpa_can_compile_dynamic(self):\n                 inputs_dict.pop(\"decoder_attention_mask\", None)\n                 for name, inp in inputs_dict.items():\n                     if isinstance(inp, torch.Tensor) and inp.dtype in [torch.float32, torch.float16]:\n-                        inputs_dict[name] = inp.to(torch.float16)\n+                        inputs_dict[name] = inp.to(torch.bfloat16)\n \n                 # use no_grad to save some memory\n                 with torch.no_grad():\n@@ -3914,7 +3914,12 @@ def test_torch_compile_for_training(self):\n         if attn_implementation is not None:\n             config._attn_implementation = attn_implementation\n \n-        model = cls(config).to(torch_device)\n+        model = cls(config).to(device=torch_device)\n+\n+        # torch._grouped_mm still only supports bfloat16 when used with torch.compile\n+        # bfloat16 is problematic with precisions so we keep an implementation with full precision\n+        if model.config._experts_implementation == \"grouped_mm\":\n+            model.set_experts_implementation(\"batched_mm\")\n \n         inputs = {\n             \"input_ids\": torch.randint(low=1, high=model.config.vocab_size, size=(2, 10), device=torch_device),"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 14,
        "deletions": 4
    }
}