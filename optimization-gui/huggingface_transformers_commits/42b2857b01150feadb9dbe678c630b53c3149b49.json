{
    "author": "qubvel",
    "message": "OmDet Turbo processor standardization (#34937)\n\n* Fix docstring\r\n\r\n* Fix docstring\r\n\r\n* Add `classes_structure` to model output\r\n\r\n* Update omdet postprocessing\r\n\r\n* Adjust tests\r\n\r\n* Update code example in docs\r\n\r\n* Add deprecation to \"classes\" key in output\r\n\r\n* Types, docs\r\n\r\n* Fixing test\r\n\r\n* Fix missed clip_boxes\r\n\r\n* [run-slow] omdet_turbo\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\r\n\r\n* Make CamelCase class\r\n\r\n---------\r\n\r\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "42b2857b01150feadb9dbe678c630b53c3149b49",
    "files": [
        {
            "sha": "91419919b6e02c317d65f18eda9d9a168b00b7f5",
            "filename": "docs/source/en/model_doc/omdet-turbo.md",
            "status": "modified",
            "additions": 45,
            "deletions": 42,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b2857b01150feadb9dbe678c630b53c3149b49/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b2857b01150feadb9dbe678c630b53c3149b49/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md?ref=42b2857b01150feadb9dbe678c630b53c3149b49",
            "patch": "@@ -44,37 +44,40 @@ One unique property of OmDet-Turbo compared to other zero-shot object detection\n Here's how to load the model and prepare the inputs to perform zero-shot object detection on a single image:\n \n ```python\n-import requests\n-from PIL import Image\n-\n-from transformers import AutoProcessor, OmDetTurboForObjectDetection\n-\n-processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n-model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n-\n-url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image = Image.open(requests.get(url, stream=True).raw)\n-classes = [\"cat\", \"remote\"]\n-inputs = processor(image, text=classes, return_tensors=\"pt\")\n-\n-outputs = model(**inputs)\n-\n-# convert outputs (bounding boxes and class logits)\n-results = processor.post_process_grounded_object_detection(\n-    outputs,\n-    classes=classes,\n-    target_sizes=[image.size[::-1]],\n-    score_threshold=0.3,\n-    nms_threshold=0.3,\n-)[0]\n-for score, class_name, box in zip(\n-    results[\"scores\"], results[\"classes\"], results[\"boxes\"]\n-):\n-    box = [round(i, 1) for i in box.tolist()]\n-    print(\n-        f\"Detected {class_name} with confidence \"\n-        f\"{round(score.item(), 2)} at location {box}\"\n-    )\n+>>> import torch\n+>>> import requests\n+>>> from PIL import Image\n+\n+>>> from transformers import AutoProcessor, OmDetTurboForObjectDetection\n+\n+>>> processor = AutoProcessor.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n+>>> model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+>>> text_labels = [\"cat\", \"remote\"]\n+>>> inputs = processor(image, text=text_labels, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> # convert outputs (bounding boxes and class logits)\n+>>> results = processor.post_process_grounded_object_detection(\n+...     outputs,\n+...     target_sizes=[(image.height, image.width)],\n+...     text_labels=text_labels,\n+...     threshold=0.3,\n+...     nms_threshold=0.3,\n+... )\n+>>> result = results[0]\n+>>> boxes, scores, text_labels = result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]\n+>>> for box, score, text_label in zip(boxes, scores, text_labels):\n+...     box = [round(i, 2) for i in box.tolist()]\n+...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n+Detected remote with confidence 0.768 at location [39.89, 70.35, 176.74, 118.04]\n+Detected cat with confidence 0.72 at location [11.6, 54.19, 314.8, 473.95]\n+Detected remote with confidence 0.563 at location [333.38, 75.77, 370.7, 187.03]\n+Detected cat with confidence 0.552 at location [345.15, 23.95, 639.75, 371.67]\n ```\n \n ### Multi image inference\n@@ -93,22 +96,22 @@ OmDet-Turbo can perform batched multi-image inference, with support for differen\n \n >>> url1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n >>> image1 = Image.open(BytesIO(requests.get(url1).content)).convert(\"RGB\")\n->>> classes1 = [\"cat\", \"remote\"]\n->>> task1 = \"Detect {}.\".format(\", \".join(classes1))\n+>>> text_labels1 = [\"cat\", \"remote\"]\n+>>> task1 = \"Detect {}.\".format(\", \".join(text_labels1))\n \n >>> url2 = \"http://images.cocodataset.org/train2017/000000257813.jpg\"\n >>> image2 = Image.open(BytesIO(requests.get(url2).content)).convert(\"RGB\")\n->>> classes2 = [\"boat\"]\n+>>> text_labels2 = [\"boat\"]\n >>> task2 = \"Detect everything that looks like a boat.\"\n \n >>> url3 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n >>> image3 = Image.open(BytesIO(requests.get(url3).content)).convert(\"RGB\")\n->>> classes3 = [\"statue\", \"trees\"]\n+>>> text_labels3 = [\"statue\", \"trees\"]\n >>> task3 = \"Focus on the foreground, detect statue and trees.\"\n \n >>> inputs = processor(\n ...     images=[image1, image2, image3],\n-...     text=[classes1, classes2, classes3],\n+...     text=[text_labels1, text_labels2, text_labels3],\n ...     task=[task1, task2, task3],\n ...     return_tensors=\"pt\",\n ... )\n@@ -119,19 +122,19 @@ OmDet-Turbo can perform batched multi-image inference, with support for differen\n >>> # convert outputs (bounding boxes and class logits)\n >>> results = processor.post_process_grounded_object_detection(\n ...     outputs,\n-...     classes=[classes1, classes2, classes3],\n-...     target_sizes=[image1.size[::-1], image2.size[::-1], image3.size[::-1]],\n-...     score_threshold=0.2,\n+...     text_labels=[text_labels1, text_labels2, text_labels3],\n+...     target_sizes=[(image.height, image.width) for image in [image1, image2, image3]],\n+...     threshold=0.2,\n ...     nms_threshold=0.3,\n ... )\n \n >>> for i, result in enumerate(results):\n-...     for score, class_name, box in zip(\n-...         result[\"scores\"], result[\"classes\"], result[\"boxes\"]\n+...     for score, text_label, box in zip(\n+...         result[\"scores\"], result[\"text_labels\"], result[\"boxes\"]\n ...     ):\n ...         box = [round(i, 1) for i in box.tolist()]\n ...         print(\n-...             f\"Detected {class_name} with confidence \"\n+...             f\"Detected {text_label} with confidence \"\n ...             f\"{round(score.item(), 2)} at location {box} in image {i}\"\n ...         )\n Detected remote with confidence 0.77 at location [39.9, 70.4, 176.7, 118.0] in image 0"
        },
        {
            "sha": "15d7c1f05916acf4839793014a00431e93e08b11",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 19,
            "deletions": 14,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b2857b01150feadb9dbe678c630b53c3149b49/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b2857b01150feadb9dbe678c630b53c3149b49/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=42b2857b01150feadb9dbe678c630b53c3149b49",
            "patch": "@@ -143,22 +143,24 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n             The predicted class of the objects from the encoder.\n         encoder_extracted_states (`torch.FloatTensor`):\n             The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n-        decoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):\n+        decoder_hidden_states (`Tuple[torch.FloatTensor]`, *optional*):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n             `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n             plus the initial embedding outputs.\n-        decoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):\n+        decoder_attentions (`Tuple[Tuple[torch.FloatTensor]]`, *optional*):\n             Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n             sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n             weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n-        encoder_hidden_states (`Optional[Tuple[torch.FloatTensor]]`):\n+        encoder_hidden_states (`Tuple[torch.FloatTensor]`, *optional*):\n             Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of shape\n             `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n             plus the initial embedding outputs.\n-        encoder_attentions (`Optional[Tuple[Tuple[torch.FloatTensor]]]`):\n+        encoder_attentions (`Tuple[Tuple[torch.FloatTensor]]`, *optional*):\n             Tuple of tuples of `torch.FloatTensor` (one for attention for each layer) of shape `(batch_size, num_heads,\n             sequence_length, sequence_length)`. Attentions weights after the attention softmax, used to compute the\n             weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n+        classes_structure (`torch.LongTensor`, *optional*):\n+            The number of queried classes for each image.\n     \"\"\"\n \n     loss: torch.FloatTensor = None\n@@ -173,6 +175,7 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n     decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n+    classes_structure: Optional[torch.LongTensor] = None\n \n \n # Copied from models.deformable_detr.load_cuda_kernels\n@@ -1667,16 +1670,16 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n     @replace_return_docstrings(output_type=OmDetTurboObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        pixel_values: Tensor,\n-        classes_input_ids: Tensor,\n-        classes_attention_mask: Tensor,\n-        tasks_input_ids: Tensor,\n-        tasks_attention_mask: Tensor,\n-        classes_structure: Tensor,\n-        labels: Optional[Tensor] = None,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n+        pixel_values: torch.FloatTensor,\n+        classes_input_ids: torch.LongTensor,\n+        classes_attention_mask: torch.LongTensor,\n+        tasks_input_ids: torch.LongTensor,\n+        tasks_attention_mask: torch.LongTensor,\n+        classes_structure: torch.LongTensor,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.FloatTensor], OmDetTurboObjectDetectionOutput]:\n         r\"\"\"\n         Returns:\n@@ -1770,6 +1773,7 @@ def forward(\n                     decoder_outputs[2],\n                     encoder_outputs[1],\n                     encoder_outputs[2],\n+                    classes_structure,\n                 ]\n                 if output is not None\n             )\n@@ -1787,6 +1791,7 @@ def forward(\n             decoder_attentions=decoder_outputs.attentions,\n             encoder_hidden_states=encoder_outputs.hidden_states,\n             encoder_attentions=encoder_outputs.attentions,\n+            classes_structure=classes_structure,\n         )\n \n "
        },
        {
            "sha": "f52840e1d0b662b55359874c40ce17277df404c1",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 152,
            "deletions": 97,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b2857b01150feadb9dbe678c630b53c3149b49/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b2857b01150feadb9dbe678c630b53c3149b49/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=42b2857b01150feadb9dbe678c630b53c3149b49",
            "patch": "@@ -16,7 +16,8 @@\n Processor class for OmDet-Turbo.\n \"\"\"\n \n-from typing import List, Optional, Tuple, Union\n+import warnings\n+from typing import TYPE_CHECKING, List, Optional, Tuple, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_transforms import center_to_corners_format\n@@ -28,12 +29,25 @@\n     is_torch_available,\n     is_torchvision_available,\n )\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_omdet_turbo import OmDetTurboObjectDetectionOutput\n \n \n class OmDetTurboTextKwargs(TextKwargs, total=False):\n     task: Optional[Union[str, List[str], TextInput, PreTokenizedInput]]\n \n \n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torchvision_available():\n+    from torchvision.ops.boxes import batched_nms\n+\n+\n class OmDetTurboProcessorKwargs(ProcessingKwargs, total=False):\n     text_kwargs: OmDetTurboTextKwargs\n     _defaults = {\n@@ -55,11 +69,23 @@ class OmDetTurboProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n-if is_torch_available():\n-    import torch\n+class DictWithDeprecationWarning(dict):\n+    message = (\n+        \"The `classes` key is deprecated for `OmDetTurboProcessor.post_process_grounded_object_detection` \"\n+        \"output dict and will be removed in a 4.51.0 version. Please use `text_labels` instead.\"\n+    )\n \n-if is_torchvision_available():\n-    from torchvision.ops.boxes import batched_nms\n+    def __getitem__(self, key):\n+        if key == \"classes\":\n+            warnings.warn(self.message, FutureWarning)\n+            return super().__getitem__(\"text_labels\")\n+        return super().__getitem__(key)\n+\n+    def get(self, key, *args, **kwargs):\n+        if key == \"classes\":\n+            warnings.warn(self.message, FutureWarning)\n+            return super().get(\"text_labels\", *args, **kwargs)\n+        return super().get(key, *args, **kwargs)\n \n \n def clip_boxes(box, box_size: Tuple[int, int]):\n@@ -97,76 +123,80 @@ def compute_score(boxes):\n \n \n def _post_process_boxes_for_image(\n-    boxes: TensorType,\n-    scores: TensorType,\n-    predicted_classes: TensorType,\n-    classes: List[str],\n+    boxes: \"torch.Tensor\",\n+    scores: \"torch.Tensor\",\n+    labels: \"torch.Tensor\",\n+    image_num_classes: int,\n     image_size: Tuple[int, int],\n-    num_classes: int,\n-    score_threshold: float,\n+    threshold: float,\n     nms_threshold: float,\n-    max_num_det: int = None,\n-) -> dict:\n+    max_num_det: Optional[int] = None,\n+) -> Tuple[\"torch.Tensor\", \"torch.Tensor\", \"torch.Tensor\"]:\n     \"\"\"\n     Filter predicted results using given thresholds and NMS.\n+\n     Args:\n-        boxes (torch.Tensor): A Tensor of predicted class-specific or class-agnostic\n-            boxes for the image. Shape : (num_queries, max_num_classes_in_batch * 4) if doing\n-            class-specific regression, or (num_queries, 4) if doing class-agnostic\n-            regression.\n-        scores (torch.Tensor): A Tensor of predicted class scores for the image.\n-            Shape : (num_queries, max_num_classes_in_batch + 1)\n-        predicted_classes (torch.Tensor): A Tensor of predicted classes for the image.\n-            Shape : (num_queries * (max_num_classes_in_batch + 1),)\n-        classes (List[str]): The input classes names.\n-        image_size (tuple): A tuple of (height, width) for the image.\n-        num_classes (int): The number of classes given for this image.\n-        score_threshold (float): Only return detections with a confidence score exceeding this\n-            threshold.\n-        nms_threshold (float):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n-        max_num_det (int, optional): The maximum number of detections to return. Default is None.\n+        boxes (`torch.Tensor`):\n+            A Tensor of predicted class-specific or class-agnostic boxes for the image.\n+            Shape (num_queries, max_num_classes_in_batch * 4) if doing class-specific regression,\n+            or (num_queries, 4) if doing class-agnostic regression.\n+        scores (`torch.Tensor` of shape (num_queries, max_num_classes_in_batch + 1)):\n+            A Tensor of predicted class scores for the image.\n+        labels (`torch.Tensor` of shape (num_queries * (max_num_classes_in_batch + 1),)):\n+            A Tensor of predicted labels for the image.\n+        image_num_classes (`int`):\n+            The number of classes queried for detection on the image.\n+        image_size (`Tuple[int, int]`):\n+            A tuple of (height, width) for the image.\n+        threshold (`float`):\n+            Only return detections with a confidence score exceeding this threshold.\n+        nms_threshold (`float`):\n+            The threshold to use for box non-maximum suppression. Value in [0, 1].\n+        max_num_det (`int`, *optional*):\n+            The maximum number of detections to return. Default is None.\n+\n     Returns:\n-        dict: A dictionary the following keys:\n+        Tuple: A tuple with the following:\n             \"boxes\" (Tensor): A tensor of shape (num_filtered_objects, 4), containing the predicted boxes in (x1, y1, x2, y2) format.\n             \"scores\" (Tensor): A tensor of shape (num_filtered_objects,), containing the predicted confidence scores for each detection.\n-            \"classes\" (List[str]): A list of strings, where each string is the predicted class for the\n-                corresponding detection\n+            \"labels\" (Tensor): A tensor of ids, where each id is the predicted class id for the corresponding detection\n     \"\"\"\n+\n+    # Filter by max number of detections\n     proposal_num = len(boxes) if max_num_det is None else max_num_det\n     scores_per_image, topk_indices = scores.flatten(0, 1).topk(proposal_num, sorted=False)\n-    classes_per_image = predicted_classes[topk_indices]\n-    box_pred_per_image = boxes.view(-1, 1, 4).repeat(1, num_classes, 1).view(-1, 4)\n-    box_pred_per_image = box_pred_per_image[topk_indices]\n-\n-    # Score filtering\n-    box_pred_per_image = center_to_corners_format(box_pred_per_image)\n-    box_pred_per_image = box_pred_per_image * torch.tensor(image_size[::-1]).repeat(2).to(box_pred_per_image.device)\n-    filter_mask = scores_per_image > score_threshold  # R x K\n+    labels_per_image = labels[topk_indices]\n+    boxes_per_image = boxes.view(-1, 1, 4).repeat(1, scores.shape[1], 1).view(-1, 4)\n+    boxes_per_image = boxes_per_image[topk_indices]\n+\n+    # Convert and scale boxes to original image size\n+    boxes_per_image = center_to_corners_format(boxes_per_image)\n+    boxes_per_image = boxes_per_image * torch.tensor(image_size[::-1]).repeat(2).to(boxes_per_image.device)\n+\n+    # Filtering by confidence score\n+    filter_mask = scores_per_image > threshold  # R x K\n     score_keep = filter_mask.nonzero(as_tuple=False).view(-1)\n-    box_pred_per_image = box_pred_per_image[score_keep]\n+    boxes_per_image = boxes_per_image[score_keep]\n     scores_per_image = scores_per_image[score_keep]\n-    classes_per_image = classes_per_image[score_keep]\n+    labels_per_image = labels_per_image[score_keep]\n \n-    filter_classes_mask = classes_per_image < len(classes)\n+    # Ensure we did not overflow to non existing classes\n+    filter_classes_mask = labels_per_image < image_num_classes\n     classes_keep = filter_classes_mask.nonzero(as_tuple=False).view(-1)\n-    box_pred_per_image = box_pred_per_image[classes_keep]\n+    boxes_per_image = boxes_per_image[classes_keep]\n     scores_per_image = scores_per_image[classes_keep]\n-    classes_per_image = classes_per_image[classes_keep]\n+    labels_per_image = labels_per_image[classes_keep]\n \n     # NMS\n-    keep = batched_nms(box_pred_per_image, scores_per_image, classes_per_image, nms_threshold)\n-    box_pred_per_image = box_pred_per_image[keep]\n+    keep = batched_nms(boxes_per_image, scores_per_image, labels_per_image, nms_threshold)\n+    boxes_per_image = boxes_per_image[keep]\n     scores_per_image = scores_per_image[keep]\n-    classes_per_image = classes_per_image[keep]\n-    classes_per_image = [classes[i] for i in classes_per_image]\n+    labels_per_image = labels_per_image[keep]\n \n-    # create an instance\n-    result = {}\n-    result[\"boxes\"] = clip_boxes(box_pred_per_image, image_size)\n-    result[\"scores\"] = scores_per_image\n-    result[\"classes\"] = classes_per_image\n+    # Clip to image size\n+    boxes_per_image = clip_boxes(boxes_per_image, image_size)\n \n-    return result\n+    return boxes_per_image, scores_per_image, labels_per_image\n \n \n class OmDetTurboProcessor(ProcessorMixin):\n@@ -274,11 +304,26 @@ def decode(self, *args, **kwargs):\n         \"\"\"\n         return self.tokenizer.decode(*args, **kwargs)\n \n+    def _get_default_image_size(self) -> Tuple[int, int]:\n+        height = (\n+            self.image_processor.size[\"height\"]\n+            if \"height\" in self.image_processor.size\n+            else self.image_processor.size[\"shortest_edge\"]\n+        )\n+        width = (\n+            self.image_processor.size[\"width\"]\n+            if \"width\" in self.image_processor.size\n+            else self.image_processor.size[\"longest_edge\"]\n+        )\n+        return height, width\n+\n+    @deprecate_kwarg(\"score_threshold\", new_name=\"threshold\", version=\"4.51.0\")\n+    @deprecate_kwarg(\"classes\", new_name=\"text_labels\", version=\"4.51.0\")\n     def post_process_grounded_object_detection(\n         self,\n-        outputs,\n-        classes: Union[List[str], List[List[str]]],\n-        score_threshold: float = 0.3,\n+        outputs: \"OmDetTurboObjectDetectionOutput\",\n+        text_labels: Optional[Union[List[str], List[List[str]]]] = None,\n+        threshold: float = 0.3,\n         nms_threshold: float = 0.5,\n         target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n         max_num_det: Optional[int] = None,\n@@ -290,67 +335,77 @@ def post_process_grounded_object_detection(\n         Args:\n             outputs ([`OmDetTurboObjectDetectionOutput`]):\n                 Raw outputs of the model.\n-            classes (Union[List[str], List[List[str]]]): The input classes names.\n-            score_threshold (float, defaults to 0.3): Only return detections with a confidence score exceeding this\n-                threshold.\n-            nms_threshold (float, defaults to 0.5):  The threshold to use for box non-maximum suppression. Value in [0, 1].\n-            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*, defaults to None):\n+            text_labels (Union[List[str], List[List[str]]], *optional*):\n+                The input classes names. If not provided, `text_labels` will be set to `None` in `outputs`.\n+            threshold (float, defaults to 0.3):\n+                Only return detections with a confidence score exceeding this threshold.\n+            nms_threshold (float, defaults to 0.5):\n+                The threshold to use for box non-maximum suppression. Value in [0, 1].\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n                 Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n                 `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n-            max_num_det (int, *optional*, defaults to None): The maximum number of detections to return.\n+            max_num_det (`int`, *optional*):\n+                The maximum number of detections to return.\n         Returns:\n             `List[Dict]`: A list of dictionaries, each dictionary containing the scores, classes and boxes for an image\n             in the batch as predicted by the model.\n         \"\"\"\n-        if isinstance(classes[0], str):\n-            classes = [classes]\n \n-        boxes_logits = outputs.decoder_coord_logits\n-        scores_logits = outputs.decoder_class_logits\n+        batch_size = len(outputs.decoder_coord_logits)\n \n-        # Inputs consistency check\n+        # Inputs consistency check for target sizes\n         if target_sizes is None:\n-            height = (\n-                self.image_processor.size[\"height\"]\n-                if \"height\" in self.image_processor.size\n-                else self.image_processor.size[\"shortest_edge\"]\n-            )\n-            width = (\n-                self.image_processor.size[\"width\"]\n-                if \"width\" in self.image_processor.size\n-                else self.image_processor.size[\"longest_edge\"]\n-            )\n-            target_sizes = ((height, width),) * len(boxes_logits)\n-        elif len(target_sizes[0]) != 2:\n+            height, width = self._get_default_image_size()\n+            target_sizes = [(height, width)] * batch_size\n+\n+        if any(len(image_size) != 2 for image_size in target_sizes):\n             raise ValueError(\n                 \"Each element of target_sizes must contain the size (height, width) of each image of the batch\"\n             )\n-        if len(target_sizes) != len(boxes_logits):\n+\n+        if len(target_sizes) != batch_size:\n             raise ValueError(\"Make sure that you pass in as many target sizes as output sequences\")\n-        if len(classes) != len(boxes_logits):\n+\n+        # Inputs consistency check for text labels\n+        if text_labels is not None and isinstance(text_labels[0], str):\n+            text_labels = [text_labels]\n+\n+        if text_labels is not None and len(text_labels) != batch_size:\n             raise ValueError(\"Make sure that you pass in as many classes group as output sequences\")\n \n         # Convert target_sizes to list for easier handling\n         if isinstance(target_sizes, torch.Tensor):\n             target_sizes = target_sizes.tolist()\n \n-        scores, predicted_classes = compute_score(scores_logits)\n-        num_classes = scores_logits.shape[2]\n+        batch_boxes = outputs.decoder_coord_logits\n+        batch_logits = outputs.decoder_class_logits\n+        batch_num_classes = outputs.classes_structure\n+\n+        batch_scores, batch_labels = compute_score(batch_logits)\n+\n         results = []\n-        for scores_img, box_per_img, image_size, class_names in zip(scores, boxes_logits, target_sizes, classes):\n-            results.append(\n-                _post_process_boxes_for_image(\n-                    box_per_img,\n-                    scores_img,\n-                    predicted_classes,\n-                    class_names,\n-                    image_size,\n-                    num_classes,\n-                    score_threshold=score_threshold,\n-                    nms_threshold=nms_threshold,\n-                    max_num_det=max_num_det,\n-                )\n+        for boxes, scores, image_size, image_num_classes in zip(\n+            batch_boxes, batch_scores, target_sizes, batch_num_classes\n+        ):\n+            boxes, scores, labels = _post_process_boxes_for_image(\n+                boxes=boxes,\n+                scores=scores,\n+                labels=batch_labels,\n+                image_num_classes=image_num_classes,\n+                image_size=image_size,\n+                threshold=threshold,\n+                nms_threshold=nms_threshold,\n+                max_num_det=max_num_det,\n             )\n+            result = DictWithDeprecationWarning(\n+                {\"boxes\": boxes, \"scores\": scores, \"labels\": labels, \"text_labels\": None}\n+            )\n+            results.append(result)\n+\n+        # Add text labels\n+        if text_labels is not None:\n+            for result, image_text_labels in zip(results, text_labels):\n+                result[\"text_labels\"] = [image_text_labels[idx] for idx in result[\"labels\"]]\n \n         return results\n "
        },
        {
            "sha": "d057b35006d3ee3d2fea997184915981df3fbab2",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b2857b01150feadb9dbe678c630b53c3149b49/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b2857b01150feadb9dbe678c630b53c3149b49/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=42b2857b01150feadb9dbe678c630b53c3149b49",
            "patch": "@@ -646,9 +646,9 @@ def prepare_img():\n \n \n def prepare_text():\n-    classes = [\"cat\", \"remote\"]\n-    task = \"Detect {}.\".format(\", \".join(classes))\n-    return classes, task\n+    text_labels = [\"cat\", \"remote\"]\n+    task = \"Detect {}.\".format(\", \".join(text_labels))\n+    return text_labels, task\n \n \n def prepare_img_batched():\n@@ -660,14 +660,14 @@ def prepare_img_batched():\n \n \n def prepare_text_batched():\n-    classes1 = [\"cat\", \"remote\"]\n-    classes2 = [\"boat\"]\n-    classes3 = [\"statue\", \"trees\", \"torch\"]\n+    text_labels1 = [\"cat\", \"remote\"]\n+    text_labels2 = [\"boat\"]\n+    text_labels3 = [\"statue\", \"trees\", \"torch\"]\n \n-    task1 = \"Detect {}.\".format(\", \".join(classes1))\n+    task1 = \"Detect {}.\".format(\", \".join(text_labels1))\n     task2 = \"Detect all the boat in the image.\"\n     task3 = \"Focus on the foreground, detect statue, torch and trees.\"\n-    return [classes1, classes2, classes3], [task1, task2, task3]\n+    return [text_labels1, text_labels2, text_labels3], [task1, task2, task3]\n \n \n @require_timm\n@@ -683,8 +683,8 @@ def test_inference_object_detection_head(self):\n \n         processor = self.default_processor\n         image = prepare_img()\n-        classes, task = prepare_text()\n-        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\").to(torch_device)\n+        text_labels, task = prepare_text()\n+        encoding = processor(images=image, text=text_labels, task=task, return_tensors=\"pt\").to(torch_device)\n \n         with torch.no_grad():\n             outputs = model(**encoding)\n@@ -706,7 +706,7 @@ def test_inference_object_detection_head(self):\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n-            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+            outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n         expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device)\n         expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n@@ -715,8 +715,8 @@ def test_inference_object_detection_head(self):\n         self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n         self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n \n-        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n-        self.assertListEqual(results[\"classes\"], expected_classes)\n+        expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n \n     def test_inference_object_detection_head_fp16(self):\n         model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(\n@@ -725,8 +725,8 @@ def test_inference_object_detection_head_fp16(self):\n \n         processor = self.default_processor\n         image = prepare_img()\n-        classes, task = prepare_text()\n-        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\").to(\n+        text_labels, task = prepare_text()\n+        encoding = processor(images=image, text=text_labels, task=task, return_tensors=\"pt\").to(\n             torch_device, dtype=torch.float16\n         )\n \n@@ -750,7 +750,7 @@ def test_inference_object_detection_head_fp16(self):\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n-            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+            outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n         expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device, dtype=torch.float16)\n         expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(\n@@ -761,16 +761,16 @@ def test_inference_object_detection_head_fp16(self):\n         self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n         self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-1))\n \n-        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n-        self.assertListEqual(results[\"classes\"], expected_classes)\n+        expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n \n     def test_inference_object_detection_head_no_task(self):\n         model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(torch_device)\n \n         processor = self.default_processor\n         image = prepare_img()\n-        classes, _ = prepare_text()\n-        encoding = processor(images=image, text=classes, return_tensors=\"pt\").to(torch_device)\n+        text_labels, _ = prepare_text()\n+        encoding = processor(images=image, text=text_labels, return_tensors=\"pt\").to(torch_device)\n \n         with torch.no_grad():\n             outputs = model(**encoding)\n@@ -792,7 +792,7 @@ def test_inference_object_detection_head_no_task(self):\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n-            outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+            outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n         expected_scores = torch.tensor([0.7675, 0.7196, 0.5634, 0.5524]).to(torch_device)\n         expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n@@ -801,19 +801,19 @@ def test_inference_object_detection_head_no_task(self):\n         self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n         self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n \n-        expected_classes = [\"remote\", \"cat\", \"remote\", \"cat\"]\n-        self.assertListEqual(results[\"classes\"], expected_classes)\n+        expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n+        self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n \n     def test_inference_object_detection_head_batched(self):\n         torch_device = \"cpu\"\n         model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\").to(torch_device)\n \n         processor = self.default_processor\n         images_batched = prepare_img_batched()\n-        classes_batched, tasks_batched = prepare_text_batched()\n-        encoding = processor(images=images_batched, text=classes_batched, task=tasks_batched, return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        text_labels_batched, tasks_batched = prepare_text_batched()\n+        encoding = processor(\n+            images=images_batched, text=text_labels_batched, task=tasks_batched, return_tensors=\"pt\"\n+        ).to(torch_device)\n \n         with torch.no_grad():\n             outputs = model(**encoding)\n@@ -837,7 +837,7 @@ def test_inference_object_detection_head_batched(self):\n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n             outputs,\n-            classes=classes_batched,\n+            text_labels=text_labels_batched,\n             target_sizes=[image.size[::-1] for image in images_batched],\n             score_threshold=0.2,\n         )\n@@ -858,19 +858,19 @@ def test_inference_object_detection_head_batched(self):\n             torch.allclose(torch.stack([result[\"boxes\"][0, :] for result in results]), expected_slice_boxes, atol=1e-2)\n         )\n \n-        expected_classes = [\n+        expected_text_labels = [\n             [\"remote\", \"cat\", \"remote\", \"cat\"],\n             [\"boat\", \"boat\", \"boat\", \"boat\"],\n             [\"statue\", \"trees\", \"trees\", \"torch\", \"statue\", \"statue\"],\n         ]\n-        self.assertListEqual([result[\"classes\"] for result in results], expected_classes)\n+        self.assertListEqual([result[\"text_labels\"] for result in results], expected_text_labels)\n \n     @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         processor = self.default_processor\n         image = prepare_img()\n-        classes, task = prepare_text()\n-        encoding = processor(images=image, text=classes, task=task, return_tensors=\"pt\")\n+        text_labels, task = prepare_text()\n+        encoding = processor(images=image, text=text_labels, task=task, return_tensors=\"pt\")\n         # 1. run model on CPU\n         model = OmDetTurboForObjectDetection.from_pretrained(\"omlab/omdet-turbo-swin-tiny-hf\")\n \n@@ -894,10 +894,10 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n \n         # verify grounded postprocessing\n         results_cpu = processor.post_process_grounded_object_detection(\n-            cpu_outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+            cpu_outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n         result_gpu = processor.post_process_grounded_object_detection(\n-            gpu_outputs, classes=[classes], target_sizes=[image.size[::-1]]\n+            gpu_outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n \n         self.assertTrue(torch.allclose(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), atol=1e-2))"
        },
        {
            "sha": "341c7a1d9a136589a0f0c2216f82672918dfd205",
            "filename": "tests/models/omdet_turbo/test_processor_omdet_turbo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b2857b01150feadb9dbe678c630b53c3149b49/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b2857b01150feadb9dbe678c630b53c3149b49/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py?ref=42b2857b01150feadb9dbe678c630b53c3149b49",
            "patch": "@@ -76,10 +76,13 @@ def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n     def get_fake_omdet_turbo_output(self):\n+        classes = self.get_fake_omdet_turbo_classes()\n+        classes_structure = torch.tensor([len(sublist) for sublist in classes])\n         torch.manual_seed(42)\n         return OmDetTurboObjectDetectionOutput(\n             decoder_coord_logits=torch.rand(self.batch_size, self.num_queries, 4),\n             decoder_class_logits=torch.rand(self.batch_size, self.num_queries, self.embed_dim),\n+            classes_structure=classes_structure,\n         )\n \n     def get_fake_omdet_turbo_classes(self):\n@@ -99,7 +102,7 @@ def test_post_process_grounded_object_detection(self):\n         )\n \n         self.assertEqual(len(post_processed), self.batch_size)\n-        self.assertEqual(list(post_processed[0].keys()), [\"boxes\", \"scores\", \"classes\"])\n+        self.assertEqual(list(post_processed[0].keys()), [\"boxes\", \"scores\", \"labels\", \"text_labels\"])\n         self.assertEqual(post_processed[0][\"boxes\"].shape, (self.num_queries, 4))\n         self.assertEqual(post_processed[0][\"scores\"].shape, (self.num_queries,))\n         expected_scores = torch.tensor([0.7310, 0.6579, 0.6513, 0.6444, 0.6252])"
        }
    ],
    "stats": {
        "total": 442,
        "additions": 254,
        "deletions": 188
    }
}