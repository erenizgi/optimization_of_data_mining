{
    "author": "yao-matrix",
    "message": "enable blip2 and emu3 cases on XPU (#37662)\n\n* enable blip2 and emu3 modeling cases on XPU\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* remove extra new line\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "ece79b068812190e95309598e3dadca773cd0d6f",
    "files": [
        {
            "sha": "b7dc7e541c998517aa4eefe51091130e82ef8d73",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece79b068812190e95309598e3dadca773cd0d6f/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece79b068812190e95309598e3dadca773cd0d6f/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=ece79b068812190e95309598e3dadca773cd0d6f",
            "patch": "@@ -27,7 +27,6 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n-    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_torch_sdpa,\n     require_vision,\n@@ -1400,7 +1399,7 @@ def test_forward_signature(self):\n             self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_model_from_pretrained(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n         model = Blip2VisionModelWithProjection.from_pretrained(model_name)\n@@ -1551,7 +1550,7 @@ def test_load_vision_qformer_text_config(self):\n             self.assertDictEqual(config.qformer_config.to_dict(), qformer_config.to_dict())\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_model_from_pretrained(self):\n         model_name = \"Salesforce/blip2-itm-vit-g\"\n         model = Blip2ForImageTextRetrieval.from_pretrained(model_name)"
        },
        {
            "sha": "b27b1d4c708a6eb7f459bf5019c6e23ea12f7dea",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 26,
            "deletions": 9,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/ece79b068812190e95309598e3dadca773cd0d6f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ece79b068812190e95309598e3dadca773cd0d6f/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=ece79b068812190e95309598e3dadca773cd0d6f",
            "patch": "@@ -23,9 +23,10 @@\n \n from transformers import Emu3Config, Emu3TextConfig, is_torch_available, is_vision_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_large_gpu,\n+    require_torch_large_accelerator,\n     slow,\n     torch_device,\n )\n@@ -416,7 +417,7 @@ def test_model_generation(self):\n \n     @slow\n     @require_bitsandbytes\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     def test_model_generation_batched(self):\n         model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", load_in_4bit=True)\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n@@ -434,17 +435,27 @@ def test_model_generation_batched(self):\n         )\n \n         # greedy generation outputs\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"USER: 64*64Describe what do you see here? ASSISTANT: The image depicts a black panther in a crouched position. The panther's body is elongated and curved, with its head lowered and ears pointed forward, suggesting alertness or focus.\",\n-            'USER: 64*64What can you say about the image? ASSISTANT: The image depicts a serene natural landscape. The foreground consists of a grassy area with some patches of bare earth. The middle ground shows a steep, reddish-brown cliff, which could be a'\n-        ]  # fmt: skip\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+            {\n+                (\"xpu\", 3): [\n+                    \"USER: 64*64Describe what do you see here? ASSISTANT: The image depicts a black panther in a crouched position. The panther's body is elongated and its head is lowered, suggesting a state of alertness or readiness. The animal's\",\n+                    \"USER: 64*64What can you say about the image? ASSISTANT: The image depicts a serene natural landscape. The foreground consists of a grassy area with some patches of bare earth. The middle ground shows a gently sloping hill with a reddish-brown hue,\",\n+                ],\n+                (\"cuda\", 7): [\n+                    \"USER: 64*64Describe what do you see here? ASSISTANT: The image depicts a black panther in a crouched position. The panther's body is elongated and curved, with its head lowered and ears pointed forward, suggesting alertness or focus.\",\n+                    \"USER: 64*64What can you say about the image? ASSISTANT: The image depicts a serene natural landscape. The foreground consists of a grassy area with some patches of bare earth. The middle ground shows a steep, reddish-brown cliff, which could be a\",\n+                ],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n+\n         generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n         text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n     @require_bitsandbytes\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     def test_model_generation_multi_image(self):\n         model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", load_in_4bit=True)\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n@@ -456,14 +467,20 @@ def test_model_generation_multi_image(self):\n         inputs = processor(images=[image, image_2], text=prompt, return_tensors=\"pt\").to(model.device, torch.float16)\n \n         # greedy generation outputs\n-        EXPECTED_TEXT_COMPLETION = [\"USER: 64*6464*64What do these two images have in common? ASSISTANT: Both images feature a black animal, but they are not the same animal. The top image shows a close-up of a black cow's head, while the bottom image depicts a black cow in a natural\"]  # fmt: skip\n+        EXPECTED_TEXT_COMPLETIONS = Expectations(\n+                {\n+                    (\"xpu\", 3): ['USER: 64*6464*64What do these two images have in common? ASSISTANT: The two images both depict a rhinoceros, yet they are significantly different in terms of focus and clarity. The rhinoceros in the upper image is in sharp focus, showing detailed textures'],\n+                    (\"cuda\", 7): [\"USER: 64*6464*64What do these two images have in common? ASSISTANT: Both images feature a black animal, but they are not the same animal. The top image shows a close-up of a black cow's head, while the bottom image depicts a black cow in a natural\"],\n+                }\n+            )  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n         generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n         text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n     @slow\n     @require_bitsandbytes\n-    @require_torch_large_gpu\n+    @require_torch_large_accelerator\n     def test_model_generate_images(self):\n         model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Gen-hf\", load_in_4bit=True)\n         processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Gen-hf\")"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 28,
        "deletions": 12
    }
}