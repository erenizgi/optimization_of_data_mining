{
    "author": "camilla-deckard",
    "message": "Add FastVLM (#41112)\n\n* Added an initial conversion script\n\n* Added a modular where FastVLM is different from LlaVA\n\n* Improved the conversion script\n\n* Adjusted the conversion script\n\n* Removed redundant labels from FastViT & improved the template\n\n* Added docs and changed default config\n\n* Fix default config\n\n* Fix default config\n\n* Fixed layer feature handling and more docs\n\n* Fixed documentation\n\n* Style fixed\n\n* Some small fixes\n\n* Improved the example script to be more inclusive\n\n* Fixes after the rebase\n\n* Made the code and docs more readable and consistent\n\n* Some fixes from the review\n\n* Reverted back to last layer only\n\n* Typos fixed\n\n* added initial tests - some still failing\n\n* Style and quality fixes\n\n* Updated modular according to the review\n\n* Tests passing and some suggested generic improvements\n\n* Docs updated with another usage tip and an auto model\n\n* Reversed changes to test_can_intialize_on_meta becuase it's not fully compatible with one existing model\n\n* Some tweaks\n\n* Typo fix\n\n* Consistency fixed\n\n* Review comment\n\n* Redundant config attr deleted\n\n* Consistency fixed\n\n* Fixed integration tests after rebase\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>",
    "sha": "a649767573430f2f9dbc81ac3a801f8546740109",
    "files": [
        {
            "sha": "852e3b06959a1ad48b1638c8fa4f4e689335522c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -1030,6 +1030,8 @@\n         title: Emu3\n       - local: model_doc/evolla\n         title: Evolla\n+      - local: model_doc/fast_vlm\n+        title: FastVLM\n       - local: model_doc/flava\n         title: FLAVA\n       - local: model_doc/florence2"
        },
        {
            "sha": "25cbe3bff12656bd7037d266145411a3a198c0a7",
            "filename": "docs/source/en/model_doc/fast_vlm.md",
            "status": "added",
            "additions": 175,
            "deletions": 0,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffast_vlm.md?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,175 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+*This model was released on 2025-05-06 and added to Hugging Face Transformers on 2025-10-07.*\n+\n+# FastVLM\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+FastVLM is an open-source vision-language model featuring a novel hybrid vision encoder, FastViTHD. Leveraging reparameterizable convolutional layers, scaled input resolution, and a reduced number of visual tokens, FastVLM delivers high accuracy with exceptional efficiency. Its optimized architecture enables deployment even on edge devices, achieving ultra-low TTFT (time to first token) without sacrificing performance.\n+\n+The model was proposed in [FastVLM: Efficient Vision Encoding for Vision Language Models](https://huggingface.co/papers/2412.13303) by Pavan Kumar Anasosalu Vasu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokul Santhanam, James Gabriel, Peter Grasch, Oncel Tuzel and Hadi Pouransari.\n+\n+The abstract from the paper is the following:\n+\n+*Scaling the input image resolution is essential for enhancing the performance of Vision Language Models (VLMs), particularly in text-rich image understanding tasks. However, popular visual encoders such as ViTs become inefficient at high resolutions due to the large number of tokens and high encoding latency. At different operational resolutions, the vision encoder of a VLM can be optimized along two axes: reducing encoding latency and  minimizing the number of visual tokens passed to the LLM, thereby lowering overall latency. Based on a comprehensive efficiency analysis of the interplay between image resolution, vision latency, token count, and LLM size, we introduce FastVLMâ€”a model that achieves an optimized trade-off between resolution, latency, and accuracy. FastVLM incorporates FastViTHD, a novel hybrid vision encoder designed to output fewer tokens and significantly reduce encoding time for high-resolution images. Unlike previous methods, FastVLM achieves the optimal balance between visual token count and image resolution solely by scaling the input image, eliminating the need for additional token pruning and simplifying the model design. In the LLaVA-1.5 setup, FastVLM achieves 3.2Ã— improvement in time-to-first-token (TTFT) while maintaining similar performance on VLM benchmarks compared to prior works. Compared to LLaVa-OneVision at the highest resolution (1152Ã—1152), FastVLM achieves better performance on key benchmarks like SeedBench, MMMU and DocVQA, using the same 0.5B LLM, but with 85Ã— faster TTFT and a vision encoder that is 3.4Ã— smaller.*\n+\n+This model was contributed by [Kamila](https://github.com/kamila-chay).\n+The original code can be found [here](https://github.com/apple/ml-fastvlm).\n+\n+## Usage tips\n+\n+- We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n+\n+- Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n+\n+**Important: **\n+\n+Hugging Face models use SDPA by default; however, this modelâ€™s visual backbone supports only eager attention, so it automatically falls back to `\"eager\"`.\n+\n+If you want to use a different attention implementation in the language decoder, make sure to set it explicitly, for example:\n+\n+`model = FastVlmForConditionalGeneration.from_pretrained(\"KamilaMila/FastVLM-0.5B\", attn_implementation={\"text_config\": \"flash_attention_2\"})`\n+\n+Setting it for the entire model, e.g.\n+\n+`model = FastVlmForConditionalGeneration.from_pretrained(\"KamilaMila/FastVLM-0.5B\", attn_implementation=\"flash_attention_2\")`\n+\n+will result in an error.\n+\n+### Formatting Prompts with Chat Templates  \n+\n+Each **checkpoint** is trained with a specific prompt format, depending on the underlying large language model backbone. To ensure correct formatting, use the processorâ€™s `apply_chat_template` method.  \n+\n+**Important:**  \n+- You must construct a conversation history â€” passing a plain string won't work.  \n+- Each message should be a dictionary with `\"role\"` and `\"content\"` keys.  \n+- The `\"content\"` should be a list of dictionaries for different modalities like `\"text\"` and `\"image\"`.  \n+\n+## Usage examples\n+\n+### Single input inference\n+\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, FastVlmForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = FastVlmForConditionalGeneration.from_pretrained(\"KamilaMila/FastVLM-0.5B\", dtype=torch.bfloat16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"KamilaMila/FastVLM-0.5B\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.bfloat16)\n+\n+# Generate\n+generate_ids = model.generate(**inputs, max_new_tokens=30)\n+processor.batch_decode(generate_ids, skip_special_tokens=True)\n+```\n+\n+\n+### Batched inference\n+\n+FastVLM also supports batched inference. Here is how you can do it:\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, FastVlmForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = FastVlmForConditionalGeneration.from_pretrained(\"KamilaMila/FastVLM-0.5B\", dtype=torch.bfloat16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"KamilaMila/FastVLM-0.5B\")\n+\n+\n+# Prepare a batch of two prompts\n+conversation_1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+conversation_2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    [conversation_1, conversation_2],\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    padding=True,\n+    return_tensors=\"pt\"\n+).to(model.device, torch.bfloat16)\n+\n+\n+# Generate\n+generate_ids = model.generate(**inputs, max_new_tokens=30)\n+processor.batch_decode(generate_ids, skip_special_tokens=True)\n+```\n+\n+\n+## Note regarding reproducing original implementation\n+\n+In order to match the logits of the [original implementation](https://github.com/apple/ml-fastvlm), one needs to use float32. In half precision the logit difference is higher due to tiny differences in how some ops are implemented in timm.\n+\n+### Using Flash Attention 2\n+\n+Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one).\n+\n+## FastVlmConfig\n+\n+[[autodoc]] FastVlmConfig\n+\n+## FastVlmModel\n+\n+[[autodoc]] FastVlmModel\n+\n+## FastVlmForConditionalGeneration\n+\n+[[autodoc]] FastVlmForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "fdcd98a853859b7a77407c30e9f8fa9f1d445bb4",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -126,6 +126,7 @@\n     from .falcon import *\n     from .falcon_h1 import *\n     from .falcon_mamba import *\n+    from .fast_vlm import *\n     from .fastspeech2_conformer import *\n     from .flaubert import *\n     from .flava import *"
        },
        {
            "sha": "3df37eeb3468134341569bdb09c0d5f279a2ac9c",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -148,6 +148,7 @@\n         (\"falcon\", \"FalconConfig\"),\n         (\"falcon_h1\", \"FalconH1Config\"),\n         (\"falcon_mamba\", \"FalconMambaConfig\"),\n+        (\"fast_vlm\", \"FastVlmConfig\"),\n         (\"fastspeech2_conformer\", \"FastSpeech2ConformerConfig\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGanConfig\"),\n         (\"flaubert\", \"FlaubertConfig\"),\n@@ -585,6 +586,7 @@\n         (\"falcon3\", \"Falcon3\"),\n         (\"falcon_h1\", \"FalconH1\"),\n         (\"falcon_mamba\", \"FalconMamba\"),\n+        (\"fast_vlm\", \"FastVlm\"),\n         (\"fastspeech2_conformer\", \"FastSpeech2Conformer\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGan\"),\n         (\"flan-t5\", \"FLAN-T5\"),"
        },
        {
            "sha": "dd49973926172167337b51151de9e2f196e4a0a8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -151,6 +151,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"falcon\", \"FalconModel\"),\n         (\"falcon_h1\", \"FalconH1Model\"),\n         (\"falcon_mamba\", \"FalconMambaModel\"),\n+        (\"fast_vlm\", \"FastVlmModel\"),\n         (\"fastspeech2_conformer\", \"FastSpeech2ConformerModel\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"FastSpeech2ConformerWithHifiGan\"),\n         (\"flaubert\", \"FlaubertModel\"),\n@@ -996,6 +997,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"deepseek_vl_hybrid\", \"DeepseekVLHybridForConditionalGeneration\"),\n         (\"emu3\", \"Emu3ForConditionalGeneration\"),\n         (\"evolla\", \"EvollaForProteinText2Text\"),\n+        (\"fast_vlm\", \"FastVlmForConditionalGeneration\"),\n         (\"florence2\", \"Florence2ForConditionalGeneration\"),\n         (\"fuyu\", \"FuyuForCausalLM\"),\n         (\"gemma3\", \"Gemma3ForConditionalGeneration\"),"
        },
        {
            "sha": "949f087650dd99ea1b04cb052708b9123b08b557",
            "filename": "src/transformers/models/fast_vlm/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2F__init__.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_fast_vlm import *\n+    from .modeling_fast_vlm import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "46e5a6ccbf762b22fbe85ecb0ce1e4979f407fba",
            "filename": "src/transformers/models/fast_vlm/configuration_fast_vlm.py",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconfiguration_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconfiguration_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconfiguration_fast_vlm.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,137 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/fast_vlm/modular_fast_vlm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_fast_vlm.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PreTrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class FastVlmConfig(PreTrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`FastVlmForConditionalGeneration`]. It is used to instantiate a\n+    FastVLM model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield the same configuration as the one of FastVLM-7B.\n+\n+    e.g. [KamilaMila/FastVLM-7B](https://huggingface.co/KamilaMila/FastVLM-7B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `TimmWrapperConfig` for `fastvit_mci3`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_id (`int`, *optional*, defaults to 151646):\n+            The image token index to encode the image prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function used by the multimodal projector.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"full\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Only \"full\" supported.\n+        vision_feature_layer (`Union[int, list[int]]`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features. Only -1 supported.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import FastVlmForConditionalGeneration, FastVlmConfig\n+\n+    >>> # Initializing a FastVLM-7B style configuration\n+    >>> configuration = FastVlmConfig()\n+\n+    >>> # Initializing a model from the FastVLM-7B style configuration\n+    >>> model = FastVlmForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"fast_vlm\"\n+    attribute_map = {\n+        \"image_token_id\": \"image_token_index\",\n+    }\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_id=151646,\n+        projector_hidden_act=\"gelu\",\n+        vision_feature_select_strategy=\"full\",\n+        vision_feature_layer=-1,\n+        multimodal_projector_bias=True,\n+        **kwargs,\n+    ):\n+        self.image_token_id = image_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        if vision_feature_select_strategy != \"full\":\n+            raise ValueError(\n+                f\"Unexpected select feature strategy: {vision_feature_select_strategy}. Only 'full' is supported in FastVLM.\"\n+            )\n+\n+        if vision_feature_layer != -1:\n+            raise ValueError(\n+                f\"Unexpected vision feature layer: {vision_feature_layer}. Only -1 is supported in FastVLM.\"\n+            )\n+\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"timm_wrapper\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"timm_wrapper\"](\n+                architecture=\"fastvit_mci3\",\n+                do_pooling=True,\n+                global_pool=\"avg\",\n+                hidden_size=3072,\n+                initializer_range=0.02,\n+                model_args={\"inference_mode\": True},\n+            )\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"](\n+                hidden_size=3584,\n+                vocab_size=152128,\n+                intermediate_size=18944,\n+                num_attention_heads=28,\n+                num_key_value_heads=4,\n+                num_hidden_layers=28,\n+            )\n+\n+        self.text_config = text_config\n+        self.multimodal_projector_bias = multimodal_projector_bias\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"FastVlmConfig\"]"
        },
        {
            "sha": "70fbedb2e2d68d9bf7ba32e00d5c5a43a5d341ee",
            "filename": "src/transformers/models/fast_vlm/convert_fastvlm_weights_to_hf.py",
            "status": "added",
            "additions": 247,
            "deletions": 0,
            "changes": 247,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconvert_fastvlm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconvert_fastvlm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fconvert_fastvlm_weights_to_hf.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,247 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import glob\n+import os\n+import re\n+\n+import requests\n+import torch\n+from huggingface_hub import snapshot_download\n+from PIL import Image\n+from safetensors import safe_open\n+\n+from transformers import (\n+    AddedToken,\n+    AutoConfig,\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    FastVlmConfig,\n+    FastVlmForConditionalGeneration,\n+    LlavaProcessor,\n+)\n+\n+\n+os.environ[\"TIMM_FUSED_ATTN\"] = (\n+    \"0\"  # to avoid logits diverging, needed because the original implementation uses regular (not fused) atteniton\n+)\n+\n+KEYS_TO_MODIFY_MAPPING = {\n+    \"model.vision_tower.vision_tower.model\": \"model.vision_tower.timm_model\",\n+    \"patch_embed\": \"stem\",\n+    \"layers\": \"language_model.layers\",\n+    \"embed_tokens\": \"language_model.embed_tokens\",\n+    \"layer_scale_1\": \"layer_scale_1.gamma\",\n+    \"layer_scale_2\": \"layer_scale_2.gamma\",\n+    \"mm_projector.0\": \"multi_modal_projector.linear_1\",\n+    \"mm_projector.2\": \"multi_modal_projector.linear_2\",\n+    \"conv_exp\": \"final_conv\",\n+    \"se.reduce\": \"se.fc1\",\n+    \"se.expand\": \"se.fc2\",\n+    \"convffn\": \"mlp\",\n+    \"lkb_reparam\": \"reparam_conv\",\n+}\n+\n+\n+def map_to_stage(number):\n+    number = int(number)\n+    if number == 0:\n+        return 0\n+    if number in {1, 2}:\n+        return 1\n+    if number in {3, 4}:\n+        return 2\n+    if number in {5, 6, 7}:\n+        return 3\n+    if number in {8, 9, 10}:\n+        return 4\n+\n+\n+def load_original_state_dict(model_id):\n+    directory_path = snapshot_download(repo_id=model_id, allow_patterns=[\"*.safetensors\"])\n+\n+    original_state_dict = {}\n+    for path in glob.glob(f\"{directory_path}/*\"):\n+        if path.endswith(\".safetensors\"):\n+            with safe_open(path, framework=\"pt\", device=\"cpu\") as f:\n+                for key in f.keys():\n+                    original_state_dict[key] = f.get_tensor(key)\n+\n+    if \"model.vision_tower.vision_tower.model.head.proj\" in original_state_dict:\n+        del original_state_dict[\"model.vision_tower.vision_tower.model.head.proj\"]\n+    return original_state_dict\n+\n+\n+def convert_state_dict_to_hf(state_dict):\n+    new_state_dict = {}\n+\n+    single_pattern = r\"network\\.(\\d{1,2})\"\n+    double_pattern = r\"network\\.(\\d{1,2})\\.(\\d{1,2})\"\n+    pos_embedding_pattern = r\"stages\\.(\\d{1,2})\\.reparam_conv\"\n+\n+    for key, value in state_dict.items():\n+        if key.endswith(\"layer_scale\"):\n+            key = key.replace(\"layer_scale\", \"layer_scale.gamma\")\n+        if key.startswith(\"model.norm\"):\n+            key = key.replace(\"model.norm\", \"model.language_model.norm\")\n+        if \"token_mixer\" not in key:\n+            key = key.replace(\".proj.\", \".downsample.proj.\")\n+\n+        matches = re.findall(double_pattern, key)\n+        if len(matches) == 1:\n+            match = matches[0]\n+            key = key.replace(f\"network.{match[0]}.{match[1]}\", f\"stages.{map_to_stage(match[0])}.blocks.{match[1]}\")\n+\n+        matches = re.findall(single_pattern, key)\n+        if len(matches) == 1:\n+            match = matches[0]\n+            key = key.replace(f\"network.{match[0]}\", f\"stages.{map_to_stage(match[0])}\")\n+\n+        matches = re.findall(pos_embedding_pattern, key)\n+        if len(matches) == 1:\n+            match = matches[0]\n+            key = key.replace(f\"stages.{match[0]}\", f\"stages.{match[0]}.pos_emb\")\n+\n+        for key_to_modify, new_key in KEYS_TO_MODIFY_MAPPING.items():\n+            if key_to_modify in key:\n+                key = key.replace(key_to_modify, new_key)\n+\n+        new_state_dict[key] = value\n+    return new_state_dict\n+\n+\n+def convert_fastvlm_to_hf(text_model_id, vision_model_id, output_hub_path, old_state_dict_id):\n+    torch.set_default_dtype(torch.bfloat16)\n+\n+    text_config = AutoConfig.from_pretrained(text_model_id)\n+    vision_config = AutoConfig.from_pretrained(vision_model_id)\n+    vision_config.model_args = {\"inference_mode\": True}\n+    vision_config.hidden_size = vision_config.num_features\n+    vision_config.label2id = {}\n+    vision_config.id2label = {}\n+    config = FastVlmConfig(\n+        text_config=text_config,\n+        vision_config=vision_config,\n+    )\n+    config.vision_feature_select_strategy = \"full\"\n+    config.vision_feature_layer = -1\n+    config.image_token_index = 151646\n+    config.image_seq_length = 256\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        text_model_id,\n+        chat_template=\"{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n' }}{% endif %}{{'<|im_start|>' + message['role'] + '\\n'}}{# Render all images first #}{% for content in message['content'] | selectattr('type', 'equalto', 'image') %}{{ '<image>' }}{% endfor %}{# Render all text next #}{% for content in message['content'] | selectattr('type', 'equalto', 'text') %}{{ '\\n' + content['text'] }}{% endfor %}{{'<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\",\n+    )\n+\n+    tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n+    image_processor = CLIPImageProcessor(\n+        crop_size={\"height\": 1024, \"width\": 1024},\n+        image_mean=[0.0, 0.0, 0.0],\n+        image_std=[1.0, 1.0, 1.0],\n+        size={\"shortest_edge\": 1024},\n+    )\n+\n+    processor = LlavaProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+    processor.patch_size = 64  # effective patch size (2^6)\n+\n+    model = FastVlmForConditionalGeneration(config)\n+\n+    state_dict = load_original_state_dict(old_state_dict_id)\n+    state_dict = convert_state_dict_to_hf(state_dict)\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+\n+    pre_expansion_embeddings = model.language_model.embed_tokens.weight.data\n+    mu = torch.mean(pre_expansion_embeddings, dim=0).float()\n+    n = pre_expansion_embeddings.size()[0]\n+    sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n+    dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5 * sigma)\n+\n+    # We add an image token so we resize the model and pad to 64 for performance reasons\n+    pad_shape = 64\n+    vocab_size = config.text_config.vocab_size\n+    model.resize_token_embeddings(config.text_config.vocab_size + 1, pad_shape)\n+    model.language_model.embed_tokens.weight.data[vocab_size:] = torch.stack(\n+        tuple(dist.sample() for _ in range(model.language_model.embed_tokens.weight.data[vocab_size:].shape[0])),\n+        dim=0,\n+    )\n+    model.lm_head.weight.data[vocab_size:] = torch.stack(\n+        tuple(dist.sample() for _ in range(model.lm_head.weight.data[vocab_size:].shape[0])),\n+        dim=0,\n+    )\n+\n+    conversation = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What are these?\"}, {\"type\": \"image\"}]}]\n+    prompt = tokenizer.apply_chat_template(conversation, add_generation_prompt=True, tokenize=False)\n+\n+    image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+    inputs = processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(\"cuda\")\n+    inputs = {k: (v.to(torch.bfloat16) if v.dtype == torch.float32 else v) for k, v in inputs.items()}\n+\n+    model = model.cuda()\n+    model.eval()\n+    with torch.no_grad():\n+        logits = model(**inputs).logits\n+\n+    # in order to get the same logits as in the Apple repo, we need to manually replace the original (Apple) LayerNorm2D with Timm's LayerNorm2D or vice versa\n+    # otherwise numerical errors accumulate\n+    if output_hub_path == \"KamilaMila/FastVLM-0.5B\":\n+        expected_shape = torch.Size([1, 280, 152000])\n+        expected_slice = torch.tensor([4.1250, 9.6875, 11.1875], device=\"cuda\")\n+    elif output_hub_path == \"KamilaMila/FastVLM-1.5B\":\n+        expected_shape = torch.Size([1, 280, 152000])\n+        expected_slice = torch.tensor([3.3750, 11.5000, 11.8125], device=\"cuda\")\n+    elif output_hub_path == \"KamilaMila/FastVLM-7B\":\n+        expected_shape = torch.Size([1, 280, 152128])\n+        expected_slice = torch.tensor([3.8281, 9.0625, 7.9062], device=\"cuda\")\n+\n+    logits_slice = logits[0, -1, :3]\n+    assert torch.allclose(expected_slice, logits_slice, atol=1e-8)\n+    assert logits.shape == expected_shape\n+\n+    model.push_to_hub(output_hub_path)\n+    processor.push_to_hub(output_hub_path)\n+    print(\"Successfully pushed to hub!\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(\n+        formatter_class=argparse.RawDescriptionHelpFormatter,\n+    )\n+\n+    parser.add_argument(\n+        \"--text_model_id\",\n+        default=\"Qwen/Qwen2-1.5B\",\n+        help=\"Hub location of the text model\",\n+    )\n+    parser.add_argument(\n+        \"--vision_model_id\",\n+        default=\"timm/fastvit_mci3.apple_mclip2_dfndr2b\",\n+        help=\"Hub location of the vision model\",\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        default=\"KamilaMila/FastVLM-1.5B\",\n+        help=\"Location on the hub of the converted model\",\n+    )\n+    parser.add_argument(\n+        \"--old_state_dict_id\",\n+        default=\"apple/FastVLM-1.5B\",\n+        help=\"Location on the hub of the raw state dict of the original model.\",\n+    )\n+    args = parser.parse_args()\n+    convert_fastvlm_to_hf(args.text_model_id, args.vision_model_id, args.output_hub_path, args.old_state_dict_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "82860e029f98da1cb6690a81a58655e40b73e54f",
            "filename": "src/transformers/models/fast_vlm/modeling_fast_vlm.py",
            "status": "added",
            "additions": 458,
            "deletions": 0,
            "changes": 458,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,458 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/fast_vlm/modular_fast_vlm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_fast_vlm.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ..auto import AutoModel\n+from .configuration_fast_vlm import FastVlmConfig\n+\n+\n+class FastVlmMultiModalProjector(nn.Module):\n+    def __init__(self, config: FastVlmConfig):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class FastVlmPreTrainedModel(PreTrainedModel):\n+    config: FastVlmConfig\n+    base_model_prefix = \"model\"\n+    input_modalities = [\"image\", \"text\"]\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for FastVlm outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class FastVlmModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The FastVlm model which consists of a vision backbone and a language model, without a language modeling head.\n+    \"\"\"\n+)\n+class FastVlmModel(FastVlmPreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: FastVlmConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+\n+        self.multi_modal_projector = FastVlmMultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n+                The index/indices of the layer to select the vision feature. Only -1 supported.\n+            vision_feature_select_strategy (`str`, *optional*):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Only \"full\" supported.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        image_outputs = self.vision_tower(pixel_values, **kwargs)\n+\n+        # since the vision tower is hybrid in FastVLM, its output needs to be handled differently from Llava\n+        selected_image_feature = image_outputs.last_hidden_state\n+        selected_image_feature = selected_image_feature.flatten(2).permute(0, 2, 1)\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = list(image_features)\n+        return image_features\n+\n+    def get_placeholder_mask(\n+        self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n+    ):\n+        \"\"\"\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        equal to the length of multimodal features. If the lengths are different, an error is raised.\n+        \"\"\"\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        n_image_tokens = special_image_mask.sum()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        n_image_features = image_features.shape[0] * image_features.shape[1]\n+        if inputs_embeds[special_image_mask].numel() != image_features.numel():\n+            raise ValueError(\n+                f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+            )\n+        return special_image_mask\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, FastVlmModelOutputWithPast]:\n+        r\"\"\"\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n+\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+            image_features = torch.cat(image_features, dim=0).to(inputs_embeds.device, inputs_embeds.dtype)\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids, inputs_embeds=inputs_embeds, image_features=image_features\n+            )\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        return FastVlmModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for FastVlm causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class FastVlmCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[Cache] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The FastVlm model which consists of a vision backbone and a language model.\n+    \"\"\"\n+)\n+class FastVlmForConditionalGeneration(FastVlmPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = {\"lm_head.weight\": \"model.language_model.embed_tokens.weight\"}\n+\n+    def __init__(self, config: FastVlmConfig):\n+        super().__init__(config)\n+        self.model = FastVlmModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        return self.model.get_image_features(\n+            pixel_values=pixel_values,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            **kwargs,\n+        )\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        image_sizes: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> Union[tuple, FastVlmCausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n+\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+        >>> import torch\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\"KamilaMila/FastVLM-0.5B\").to(device)\n+        >>> processor = AutoProcessor.from_pretrained(\"KamilaMila/FastVLM-0.5B\")\n+\n+        >>> conversation = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What are these?\"},\n+                        {\"type\": \"image\"}\n+                    ]\n+                }\n+            ]\n+\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> print(processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n+        system\\n You are a helpful assistant.\\n user\\n What are these?\\n assistant\\n The image depicts a traditional Chinese street...\n+        ```\"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            cache_position=cache_position,\n+            image_sizes=image_sizes,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        return FastVlmCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"FastVlmForConditionalGeneration\", \"FastVlmModel\", \"FastVlmPreTrainedModel\"]"
        },
        {
            "sha": "e5d8c0908307819406b03369eab8cdf566e36a8e",
            "filename": "src/transformers/models/fast_vlm/modular_fast_vlm.py",
            "status": "added",
            "additions": 276,
            "deletions": 0,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodular_fast_vlm.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,276 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...configuration_utils import PreTrainedConfig\n+from ...utils import auto_docstring\n+from ..auto import CONFIG_MAPPING\n+from ..llava.configuration_llava import LlavaConfig\n+from ..llava.modeling_llava import (\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaMultiModalProjector,\n+    LlavaPreTrainedModel,\n+)\n+\n+\n+class FastVlmConfig(LlavaConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`FastVlmForConditionalGeneration`]. It is used to instantiate a\n+    FastVLM model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield the same configuration as the one of FastVLM-7B.\n+\n+    e.g. [KamilaMila/FastVLM-7B](https://huggingface.co/KamilaMila/FastVLM-7B)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[AutoConfig, dict]`,  *optional*, defaults to `TimmWrapperConfig` for `fastvit_mci3`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        image_token_id (`int`, *optional*, defaults to 151646):\n+            The image token index to encode the image prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function used by the multimodal projector.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"full\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Only \"full\" supported.\n+        vision_feature_layer (`Union[int, list[int]]`, *optional*, defaults to -1):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features. Only -1 supported.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import FastVlmForConditionalGeneration, FastVlmConfig\n+\n+    >>> # Initializing a FastVLM-7B style configuration\n+    >>> configuration = FastVlmConfig()\n+\n+    >>> # Initializing a model from the FastVLM-7B style configuration\n+    >>> model = FastVlmForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"fast_vlm\"\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        image_token_id=151646,\n+        projector_hidden_act=\"gelu\",\n+        vision_feature_select_strategy=\"full\",\n+        vision_feature_layer=-1,\n+        multimodal_projector_bias=True,\n+        **kwargs,\n+    ):\n+        self.image_token_id = image_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        if vision_feature_select_strategy != \"full\":\n+            raise ValueError(\n+                f\"Unexpected select feature strategy: {vision_feature_select_strategy}. Only 'full' is supported in FastVLM.\"\n+            )\n+\n+        if vision_feature_layer != -1:\n+            raise ValueError(\n+                f\"Unexpected vision feature layer: {vision_feature_layer}. Only -1 is supported in FastVLM.\"\n+            )\n+\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+\n+        if isinstance(vision_config, dict):\n+            vision_config[\"model_type\"] = vision_config.get(\"model_type\", \"timm_wrapper\")\n+            vision_config = CONFIG_MAPPING[vision_config[\"model_type\"]](**vision_config)\n+        elif vision_config is None:\n+            vision_config = CONFIG_MAPPING[\"timm_wrapper\"](\n+                architecture=\"fastvit_mci3\",\n+                do_pooling=True,\n+                global_pool=\"avg\",\n+                hidden_size=3072,\n+                initializer_range=0.02,\n+                model_args={\"inference_mode\": True},\n+            )\n+\n+        self.vision_config = vision_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"](\n+                hidden_size=3584,\n+                vocab_size=152128,\n+                intermediate_size=18944,\n+                num_attention_heads=28,\n+                num_key_value_heads=4,\n+                num_hidden_layers=28,\n+            )\n+\n+        self.text_config = text_config\n+        self.multimodal_projector_bias = multimodal_projector_bias\n+\n+        PreTrainedConfig.__init__(**kwargs)\n+\n+\n+class FastVlmMultiModalProjector(LlavaMultiModalProjector):\n+    def __init__(self, config: FastVlmConfig):\n+        nn.Module.__init__()\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n+\n+\n+class FastVlmPreTrainedModel(LlavaPreTrainedModel):\n+    pass\n+\n+\n+class FastVlmModel(LlavaModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: FastVlmConfig):\n+        super().__init__(config)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Optional[Union[int, list[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            vision_feature_layer (`Union[int, list[int]]`, *optional*):\n+                The index/indices of the layer to select the vision feature. Only -1 supported.\n+            vision_feature_select_strategy (`str`, *optional*):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Only \"full\" supported.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        kwargs = {k: v for k, v in kwargs.items() if v is not None}\n+        image_outputs = self.vision_tower(pixel_values, **kwargs)\n+\n+        # since the vision tower is hybrid in FastVLM, its output needs to be handled differently from Llava\n+        selected_image_feature = image_outputs.last_hidden_state\n+        selected_image_feature = selected_image_feature.flatten(2).permute(0, 2, 1)\n+        image_features = self.multi_modal_projector(selected_image_feature)\n+        image_features = list(image_features)\n+        return image_features\n+\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n+\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+        \"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The FastVlm model which consists of a vision backbone and a language model.\n+    \"\"\"\n+)\n+class FastVlmForConditionalGeneration(LlavaForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone. Only \"full\" supported.\n+\n+        vision_feature_layer (`Union[int, list[int], NoneType]`, *optional*):\n+            The index of the layer to select the vision feature. If multiple indices are provided, the vision feature of the\n+            corresponding indices will be concatenated to form the vision features. Only -1 supported.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModelForImageTextToText\n+        >>> import torch\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+        >>> model = AutoModelForImageTextToText.from_pretrained(\"KamilaMila/FastVLM-0.5B\").to(device)\n+        >>> processor = AutoProcessor.from_pretrained(\"KamilaMila/FastVLM-0.5B\")\n+\n+        >>> conversation = [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": \"What are these?\"},\n+                        {\"type\": \"image\"}\n+                    ]\n+                }\n+            ]\n+\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device)\n+\n+        >>> # Generate\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=15)\n+        >>> print(processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0])\n+        system\\n You are a helpful assistant.\\n user\\n What are these?\\n assistant\\n The image depicts a traditional Chinese street...\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\"FastVlmForConditionalGeneration\", \"FastVlmModel\", \"FastVlmPreTrainedModel\", \"FastVlmConfig\"]"
        },
        {
            "sha": "36e1fc248a47bf3538086439b4f6a634358965c5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -1894,6 +1894,13 @@ def test_flash_attention_2_continue_generate_with_position_ids(self):\n                 config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n \n             model = model_class(config)\n+            if not all(\n+                getattr(submodel, \"_supports_flash_attn\")\n+                for submodel in model.modules()\n+                if isinstance(submodel, PreTrainedModel)\n+            ):\n+                self.skipTest(f\"At least some parts of {model_class.__name__} don't support flash attention\")\n+\n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 self.skipTest(\"Model does not support position_ids\")\n \n@@ -1994,6 +2001,14 @@ def attention_mask_padding_matches_padding_free_with_position_ids(\n                 config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n \n             model = model_class(config)\n+            if attn_implementation != \"eager\":\n+                if not all(\n+                    getattr(submodel, support_flag[attn_implementation])\n+                    for submodel in model.modules()\n+                    if isinstance(submodel, PreTrainedModel)\n+                ):\n+                    self.skipTest(f\"At least some parts of {model_class.__name__} don't support {attn_implementation}\")\n+\n             if \"position_ids\" not in inspect.signature(model.forward).parameters:\n                 self.skipTest(\"Model does not support position_ids\")\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/fast_vlm/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fmodels%2Ffast_vlm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fmodels%2Ffast_vlm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffast_vlm%2F__init__.py?ref=a649767573430f2f9dbc81ac3a801f8546740109"
        },
        {
            "sha": "19971dfa164932a0d2a3beec027874ea6f90604d",
            "filename": "tests/models/fast_vlm/test_modeling_fast_vlm.py",
            "status": "added",
            "additions": 302,
            "deletions": 0,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffast_vlm%2Ftest_modeling_fast_vlm.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -0,0 +1,302 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the FastVLM model.\"\"\"\n+\n+import copy\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    FastVlmConfig,\n+    FastVlmForConditionalGeneration,\n+    FastVlmModel,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class FastVlmVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        image_token_id=0,\n+        projector_hidden_act=\"gelu\",\n+        seq_length=7,\n+        vision_feature_select_strategy=\"full\",\n+        vision_feature_layer=-1,\n+        text_config={\n+            \"model_type\": \"qwen2\",\n+            \"is_training\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 512,\n+            \"initializer_range\": 0.02,\n+            \"pad_token_id\": 1,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"image_size\": 16,\n+            \"patch_size\": 8,\n+            \"num_channels\": 3,\n+            \"hidden_size\": 32,\n+            \"initializer_range\": 0.02,\n+            \"architecture\": \"fastvit_mci3\",\n+            \"do_pooling\": True,\n+            \"global_pool\": \"avg\",\n+            \"model_args\": {\n+                \"inference_mode\": True,\n+                \"layers\": (2, 2),\n+                \"embed_dims\": (8, 16),\n+                \"mlp_ratios\": (4, 4),\n+                \"se_downsamples\": (False, False),\n+                \"downsamples\": (False, True),\n+                \"pos_embs\": (None, None),\n+                \"token_mixers\": (\"repmixer\", \"repmixer\"),\n+                \"lkc_use_act\": True,\n+                \"stem_use_scale_branch\": False,\n+            },\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.image_token_id = image_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+        self.vision_feature_select_strategy = vision_feature_select_strategy\n+        self.vision_feature_layer = vision_feature_layer\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.num_image_tokens = (self.vision_config[\"image_size\"] // self.vision_config[\"patch_size\"]) ** 2\n+        self.seq_length = seq_length + self.num_image_tokens\n+\n+    def get_config(self):\n+        return FastVlmConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            ignore_index=self.ignore_index,\n+            image_token_id=self.image_token_id,\n+            projector_hidden_act=self.projector_hidden_act,\n+            vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            vision_feature_layer=self.vision_feature_layer,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.vision_config[\"num_channels\"],\n+                self.vision_config[\"image_size\"],\n+                self.vision_config[\"image_size\"],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        input_ids[input_ids == config.image_token_index] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_index\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class FastVlmForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `FastVlmForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            FastVlmModel,\n+            FastVlmForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\"image-to-text\": FastVlmForConditionalGeneration, \"image-text-to-text\": FastVlmForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = FastVlmVisionText2TextModelTester(self)\n+        common_properties = [\"image_token_id\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=FastVlmConfig, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that an explicit error is thrown when the number of image tokens\n+        doesn't match the number of image placeholders in the text.\n+        We also test multi-image cases when one prompt has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+            curr_input_dict = copy.deepcopy(input_dict)  # in-place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n+\n+            # remove one image but leave all the image tokens in text\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-2:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**curr_input_dict)\n+\n+            # simulate the multi-image/single set of placeholders case by concatenating\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+\n+            # two images and one set of image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two sets of image tokens don't raise an error\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+    @unittest.skip(\"Timm wrapper and backbone don't currently support full HF initialization\")\n+    def test_can_init_all_missing_weights(self):\n+        pass\n+\n+    @unittest.skip(\"Timm can't be initialized on meta\")\n+    def test_can_be_initialized_on_meta(self):\n+        pass\n+\n+\n+@require_torch\n+@slow\n+class FastVlmForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"KamilaMila/FastVLM-0.5B\")\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @require_vision\n+    def test_small_model_integration_test(self):\n+        model = FastVlmForConditionalGeneration.from_pretrained(\n+            \"KamilaMila/FastVLM-0.5B\", device_map=torch_device, dtype=torch.bfloat16\n+        )\n+\n+        prompt = \"user\\n<image>\\nWhat are the things I should be cautious about when I visit this place?\\nassistant\"\n+        image_file = \"https://llava-vl.github.io/static/images/view.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = self.processor(images=raw_image, text=prompt, return_tensors=\"pt\").to(torch_device)\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+        expected_decoded_texts = \"user\\n\\nWhat are the things I should be cautious about when I visit this place?\\nassistant\\n\\nWhen visiting this place, there are a few things you should be cautious about:\\n\\n1. **\"  # fmt: skip\n+\n+        EXPECTED_DECODED_TEXT = expected_decoded_texts\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    @require_vision\n+    def test_small_model_integration_test_batch(self):\n+        model = FastVlmForConditionalGeneration.from_pretrained(\n+            \"KamilaMila/FastVLM-0.5B\", device_map=torch_device, dtype=torch.bfloat16\n+        )\n+\n+        prompts = [\n+            \"user\\n<image>\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nassistant\",\n+            \"user\\n<image>\\nWhat is this?\\nassistant\",\n+        ]\n+        image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n+        image2 = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = self.processor(images=[image1, image2], text=prompts, return_tensors=\"pt\", padding=True).to(\n+            torch_device\n+        )\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+\n+        EXPECTED_DECODED_TEXT = [\n+            \"user\\n\\nWhat are the things I should be cautious about when I visit this place? What should I bring with me?\\nassistant\\n\\nWhen visiting this serene place, it's essential to be mindful of the following:\\n\\n1. **\",\n+            \"user\\n\\nWhat is this?\\nassistant\\nThe image depicts two cats lying on a pink surface, which could be a couch or a\"\n+        ]  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.batch_decode(output, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_generation_no_images(self):\n+        model_id = \"KamilaMila/FastVLM-0.5B\"\n+        model = FastVlmForConditionalGeneration.from_pretrained(\n+            model_id, device_map=torch_device, dtype=torch.bfloat16\n+        )\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        # Prepare inputs with no images\n+        inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n+\n+        # Make sure that `generate` works\n+        _ = model.generate(**inputs, max_new_tokens=20)"
        },
        {
            "sha": "c1773dd69c234cdae655cc03ad10e4baed8970d2",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a649767573430f2f9dbc81ac3a801f8546740109/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a649767573430f2f9dbc81ac3a801f8546740109",
            "patch": "@@ -1308,6 +1308,10 @@ def test_attention_outputs(self):\n             del inputs_dict[\"output_attentions\"]\n             config.output_attentions = True\n             for k in config.sub_configs:\n+                if (\n+                    self._is_composite and k == \"vision_config\"\n+                ):  # skip because it's not needed and causes errors e.g with Timm\n+                    continue\n                 if getattr(config, k) is not None:\n                     getattr(config, k).output_attentions = True\n \n@@ -1467,6 +1471,10 @@ def test_retain_grad_hidden_states_attentions(self):\n         config.output_attentions = self.has_attentions\n \n         for k in config.sub_configs:\n+            if (\n+                self._is_composite and k == \"vision_config\"\n+            ):  # skip because it's not needed and causes errors e.g with Timm\n+                continue\n             if getattr(config, k) is not None:\n                 getattr(config, k).output_attentions = self.has_attentions\n \n@@ -3309,6 +3317,11 @@ def test_flash_attn_2_fp32_ln(self):\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n+            if not all(\n+                submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=\"At least some parts of this model do not support flash attention\")\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n \n@@ -3405,6 +3418,12 @@ def flash_attn_from_config(self, attn_implementation: str, test_fwd_in_train: bo\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)  # let's construct it here to see if any submodels can't support flash attn\n+            if not all(\n+                submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=f\"At least some parts of this model do not support {attn_implementation}\")\n+\n             # TODO: to change it in the future with other relevant auto classes\n             fa_model = model_class._from_config(\n                 config, attn_implementation=attn_implementation, dtype=torch.bfloat16"
        }
    ],
    "stats": {
        "total": 1663,
        "additions": 1663,
        "deletions": 0
    }
}