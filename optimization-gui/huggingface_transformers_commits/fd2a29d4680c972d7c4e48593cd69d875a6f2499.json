{
    "author": "cyyever",
    "message": "Fix more typos (#40627)\n\nFix typos\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "fd2a29d4680c972d7c4e48593cd69d875a6f2499",
    "files": [
        {
            "sha": "f0a781cba4fcc6057722bd9c10fca45843afad1c",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=fd2a29d4680c972d7c4e48593cd69d875a6f2499",
            "patch": "@@ -102,7 +102,7 @@ You may want to consider offloading if you have a small GPU and you're getting o\n Offloading is available for both [`DynamicCache`] and [`StaticCache`]. You can enable it by configuring `cache_implementation=\"offloaded\"` for the dynamic version, or `cache_implementation=\"offloaded_static\"` for the static version, in either [`GenerationConfig`] or [`~GenerationMixin.generate`].\n Additionally, you can also instantiate your own [`DynamicCache`] or [`StaticCache`] with the `offloading=True` option, and pass this cache in `generate` or your model's `forward` (for example, `past_key_values=DynamicCache(config=model.config, offloading=True)` for a dynamic cache).\n \n-Note that the 2 [`Cache`] classes mentionned above have an additional option when instantiating them directly, `offload_only_non_sliding`.\n+Note that the 2 [`Cache`] classes mentioned above have an additional option when instantiating them directly, `offload_only_non_sliding`.\n This additional argument decides if the layers using sliding window/chunk attention (if any), will be offloaded as well. Since\n these layers are usually short anyway, it may be better to avoid offloading them, as offloading may incur a speed penalty. By default, this option is `False` for [`DynamicCache`], and `True` for [`StaticCache`].\n "
        },
        {
            "sha": "fc83f4846e04f61f4f09733669890dd3755cc064",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=fd2a29d4680c972d7c4e48593cd69d875a6f2499",
            "patch": "@@ -50,7 +50,7 @@ The `generate()` method can be used to generate text using GPTSAN-Japanese model\n >>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n >>> x_tok = tokenizer(\"は、\", prefix_text=\"織田信長\", return_tensors=\"pt\")\n >>> torch.manual_seed(0)\n->>> gen_tok = model.generate(x_tok.input_ids.to(model.device), token_type_ids=x_tok.token_type_ids.to(mdoel.device), max_new_tokens=20)\n+>>> gen_tok = model.generate(x_tok.input_ids.to(model.device), token_type_ids=x_tok.token_type_ids.to(model.device), max_new_tokens=20)\n >>> tokenizer.decode(gen_tok[0])\n '織田信長は、2004年に『戦国BASARA』のために、豊臣秀吉'\n ```"
        },
        {
            "sha": "340a1ab708f13519aa50b367c3d0af80435ce495",
            "filename": "docs/source/en/tasks/visual_document_retrieval.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd2a29d4680c972d7c4e48593cd69d875a6f2499/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md?ref=fd2a29d4680c972d7c4e48593cd69d875a6f2499",
            "patch": "@@ -79,7 +79,7 @@ Index the images offline, and during inference, return the query text embeddings\n Store the image and image embeddings by writing them to the dataset with [`~datasets.Dataset.map`] as shown below. Add an `embeddings` column that contains the indexed embeddings. ColPali embeddings take up a lot of storage, so remove them from the accelerator and store them in the CPU as NumPy vectors.\n \n ```python\n-ds_with_embeddings = dataset.map(lambda example: {'embeddings': model(**processor(images=example[\"image\"]).to(devide), return_tensors=\"pt\").embeddings.to(torch.float32).detach().cpu().numpy()})\n+ds_with_embeddings = dataset.map(lambda example: {'embeddings': model(**processor(images=example[\"image\"]).to(device), return_tensors=\"pt\").embeddings.to(torch.float32).detach().cpu().numpy()})\n ```\n \n For online inference, create a function to search the image embeddings in batches and retrieve the k-most relevant images. The function below returns the indices in the dataset and their scores for a given indexed dataset, text embeddings, number of top results, and the batch size."
        },
        {
            "sha": "b1ca97b5fb748501fd478e9d165b181724e726ae",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fd2a29d4680c972d7c4e48593cd69d875a6f2499/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fd2a29d4680c972d7c4e48593cd69d875a6f2499/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=fd2a29d4680c972d7c4e48593cd69d875a6f2499",
            "patch": "@@ -232,7 +232,7 @@ def get_placeholder_mask(\n         self, input_ids: torch.LongTensor, inputs_embeds: torch.FloatTensor, image_features: torch.FloatTensor\n     ):\n         \"\"\"\n-        Obtains multimodal placeholdr mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n+        Obtains multimodal placeholder mask from `input_ids` or `inputs_embeds`, and checks that the placeholder token count is\n         equal to the length of multimodal features. If the lengths are different, an error is raised.\n         \"\"\"\n         if input_ids is None:\n@@ -406,7 +406,7 @@ def get_decoder(self):\n     def get_image_features(self, pixel_values):\n         return self.model.get_image_features(pixel_values)\n \n-    # Make modules available throught conditional class for BC\n+    # Make modules available through conditional class for BC\n     @property\n     def language_model(self):\n         return self.model.language_model"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}