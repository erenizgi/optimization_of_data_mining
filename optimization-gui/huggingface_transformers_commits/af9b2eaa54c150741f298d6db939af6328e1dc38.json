{
    "author": "threewebcode",
    "message": "chore: fix typos in language models (#36586)\n\n* chore: fix typos in language models\n\n* chore: fix typos in mistral model\n\n* chore: fix model copy from issue\n\n* chore: fix model copy from issue\n\n* chore: fix model copy from issue\n\n* chore: fix model copy from issue\n\n* chore: fix model copy from issue",
    "sha": "af9b2eaa54c150741f298d6db939af6328e1dc38",
    "files": [
        {
            "sha": "202f4f8ad52f6caf8e9dd9a0a752160a5ab29618",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1094,7 +1094,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "558434ae6f84b4ba4deadf8483fa8578731c2ef3",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1399,7 +1399,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "b477a04fa0e73fc2cdee9d03d59a5cd163e1aec3",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1140,7 +1140,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "f287234893ea3ec019455b5a38eb8b366e674c47",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -201,7 +201,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "bec24c4f78f72646318d8a04398fc980401b1604",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -298,7 +298,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "18c8f6b85ccc8da02f8a0aeb78c069d93ebc754d",
            "filename": "src/transformers/models/bart/modeling_flax_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_flax_bart.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -274,7 +274,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "61939a53f4a4c1446596b71c71f4d0fb05158b18",
            "filename": "src/transformers/models/bert/modeling_flax_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_flax_bert.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -263,7 +263,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "5afda9c1eee28280579e76e1cefc179060159d77",
            "filename": "src/transformers/models/big_bird/modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_flax_big_bird.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -284,7 +284,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "1e0775cd08cc560d13f8a3885e0be201c67ac6a8",
            "filename": "src/transformers/models/blenderbot/modeling_flax_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_flax_blenderbot.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -262,7 +262,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "6aceaa611c956f2eabfee7fb04ee7bef81fbc097",
            "filename": "src/transformers/models/blenderbot_small/modeling_flax_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_flax_blenderbot_small.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -273,7 +273,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "f14dc879b7a666ef827b0e65881b086602a95674",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -824,7 +824,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "51ccb4c3625cde37d049e0703347ff58888b4f7c",
            "filename": "src/transformers/models/bloom/modeling_flax_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_flax_bloom.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -187,7 +187,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "7510782e5ed8a416b33f2a1bc414028b2496424f",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -376,7 +376,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1470,7 +1470,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "472c7d40983064885d3f3d3bb622dcdb918e1834",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -412,7 +412,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "e6db6427756df5e9ff336547da747cd7a9629a1d",
            "filename": "src/transformers/models/code_llama/tokenization_code_llama_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcode_llama%2Ftokenization_code_llama_fast.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -82,7 +82,7 @@ class CodeLlamaTokenizerFast(PreTrainedTokenizerFast):\n             [tokenizers](https://github.com/huggingface/tokenizers) file (generally has a .json extension) that\n             contains everything needed to load the tokenizer.\n         clean_up_tokenization_spaces (`str`, *optional*, defaults to `False`):\n-            Wether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n+            Whether to cleanup spaces after decoding, cleanup consists in removing potential artifacts like extra\n             spaces.\n         unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n             The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this"
        },
        {
            "sha": "11628e8ee61ebb3380b73b04e1d5e31c664bb207",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -667,7 +667,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "0522b5ec40ca32f67df4e3d98becbc78ec75d261",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -744,7 +744,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "3570eb15080cecaf2d6dd60d66a40f2a2abda80d",
            "filename": "src/transformers/models/cohere/tokenization_cohere_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Ftokenization_cohere_fast.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -406,7 +406,7 @@ def apply_grounded_generation_template(\n             conversation (Union[List[Dict[str, str]]]): A list of dicts\n                 with \"role\" and \"content\" keys, representing the chat history so far.\n             documents (List[Dict[str, str]): A list of dicts, representing documents or tool outputs to ground your\n-                generation on. A document is a semistructured dict, wiht a string to string mapping. Common fields are\n+                generation on. A document is a semistructured dict, with a string to string mapping. Common fields are\n                 `url`, `title`, `snippet` etc but should be descriptive of the key. They will get rendered into the prompt.\n             citation_mode: either \"accurate\" (prompt the model to generate an answer first, then rewrite it with citation\n                 spans in) or \"fast\", where the prompt instructs the model to generate an answer with citations in directly."
        },
        {
            "sha": "afd2125d09bd9f50d8f30aca92bf591900166fd8",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -745,7 +745,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "4d226567b565d47c1f3adc07c6d6eddbf5d9d7e4",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -493,7 +493,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "5e8d81415da4809550e82d65360b63b891aa201d",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -322,7 +322,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1199,7 +1199,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "6edf83f8a22aa0e7b5057049592846ce0e82ec4e",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -244,7 +244,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -983,7 +983,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "e116db288246a3df058a802b7be97f31c8f0f047",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -174,7 +174,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "c869355a046bc4cac02d710be9de49337a4a0fcf",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -249,7 +249,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "646f66a8782fcf0941225d6714e5932e7dcc9109",
            "filename": "src/transformers/models/electra/modeling_flax_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_flax_electra.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -230,7 +230,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "43b1d8f93f592acb1437cb6dbf77b81d9c681503",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1562,7 +1562,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "1910e32f29b34cf2d9374b1e594032126f2d405a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -470,7 +470,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1126,7 +1126,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2c3b2e57fd6e3d54c54ca9fede0a024c09468dbb",
            "filename": "src/transformers/models/gemma/modeling_flax_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_flax_gemma.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -234,7 +234,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "ddef6a8ca577d4bfaf4ca3893650029d3a4c858a",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -716,7 +716,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "b71fbe0b74fe62c554038ae7cccbd33da9392630",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -757,7 +757,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "a162a21c0bb37f10ba69fa9a73bc36ee04dc6ba8",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -725,7 +725,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2c52e32822b6342d3d4880ecf0cceaff949810ab",
            "filename": "src/transformers/models/gpt2/modeling_flax_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_flax_gpt2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -162,7 +162,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "d4691d014319a5064e041e9e037b71a7ff72f72d",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -282,7 +282,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "f282e117dd14c490c7e435cd659a6c29e47b4b71",
            "filename": "src/transformers/models/gpt_neo/modeling_flax_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_flax_gpt_neo.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -148,7 +148,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "bc48252578de2e1f0055a40b3e3e379b6537d915",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -278,7 +278,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -876,7 +876,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "8ff537630334e599f7e8200a704b3efcc53ae221",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -719,7 +719,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "7ea246f8519a5814a30ec9d31e61edb98a2318f1",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -746,7 +746,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "eb20180a8f59d14f10d2e6e78969ed5cb0f8c65a",
            "filename": "src/transformers/models/gptj/modeling_flax_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_flax_gptj.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -174,7 +174,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "9f5413e4b4634c18382531def964a2aae3caa22e",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -270,7 +270,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -975,7 +975,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "3a239450657c089a09077c4951b4d55087b0f99d",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -728,7 +728,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "92315e6bd68ed4777385bdda6744339686064aff",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -525,7 +525,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1202,7 +1202,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2c0c20e6ddd509c549b10a2d3954105fb364ef6a",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -402,7 +402,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1146,7 +1146,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "6793bbf201a3703fce5d16df63ce598d42a10b6a",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -712,7 +712,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "333c6df5dbcb210d382f742bf5a3552244910481",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -567,7 +567,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "6b872f421f911163c7791407c92da337a6cab430",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1447,7 +1447,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "603ab05d7990138d40c0141861d2b1a33bb00d39",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -277,7 +277,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -872,7 +872,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "82401437e23c6823ce1559f7412e88575dea6448",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -277,7 +277,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "aacd512b9794c0459f6551e8499c2c9f34e5badf",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -390,7 +390,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "aa9bf4fa68c406131ad267bf8b38446e03f07c12",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -676,7 +676,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1208,7 +1208,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "be457edbc5c1b512ad30f5402c7c459d01152e7c",
            "filename": "src/transformers/models/llama/modeling_flax_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_flax_llama.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -228,7 +228,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "d2bf73e80959736212041db637d49f0b06d686a6",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -714,7 +714,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "7c5fdf9c17ccfb14a8cdfa4b354661470ec2a1c5",
            "filename": "src/transformers/models/longt5/modeling_flax_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_flax_longt5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -431,7 +431,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "ab5f1b72ab083216ab8618f6754c31448e4b981b",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1684,7 +1684,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "8b6553b5292a31c2d209890ec259cbdc4575c222",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -352,7 +352,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "d4844b6fc3209f8330e55cb0710c4a93a94db5aa",
            "filename": "src/transformers/models/marian/modeling_flax_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_flax_marian.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -285,7 +285,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "2f1b650a5d60318d288cce526587f8a4188e6b87",
            "filename": "src/transformers/models/mbart/modeling_flax_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_flax_mbart.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -286,7 +286,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "17d2af3c13d1398feb87befc625275c45d69bd2a",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -295,7 +295,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "182f2bd00f552f94cbeeb71351ff8d5d116196fe",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -602,7 +602,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1170,7 +1170,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "e7f9da0015c58142db7074c66e3686e9dec0a3e2",
            "filename": "src/transformers/models/mistral/convert_mistral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -112,7 +112,7 @@ def get_concat_dim(key):\n \n \n def convert_state_dict_sharded(loaded_shards: list[dict], config: MistralConfig):\n-    \"\"\"Convert the state dict, when a single `nn.Module` is sharded accross different files.\"\"\"\n+    \"\"\"Convert the state dict, when a single `nn.Module` is sharded across different files.\"\"\"\n     new_dict = {}\n \n     num_shards = len(loaded_shards)"
        },
        {
            "sha": "9ad28772bcf12855410c60ee13dcf97f63be030b",
            "filename": "src/transformers/models/mistral/modeling_flax_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_flax_mistral.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -254,7 +254,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "4c0f6b8b68632ecc0849668ac60626c011d59fbc",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -703,7 +703,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "10337f4eef0404e2ba3ad463a10ffda05672c5fb",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -221,7 +221,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "4112bed373f78c8bc2f3a83e6cf93c1ba4eb7a49",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -837,7 +837,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2ec4cca6ebce5450eae741ebd8b0e0369ca127d0",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1162,7 +1162,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "aac5f40de44c4e5a9dd10d6b413123d76bc0d0a6",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1078,7 +1078,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "4a5f9d68a0f62c69acc155359eb0bb6104dc1a52",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -571,7 +571,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1400,7 +1400,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1714,7 +1714,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "0266292388af3927b93cb2554c87de3ec4c29053",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1276,7 +1276,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "bd62e6add8dd8e2c8d74451b5db5692e6b1a759e",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -328,7 +328,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "29d2da154071cb34a87d3425c6525200c347b06d",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -344,7 +344,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "a2c4805fafb38dbc03a4815f2640aa248c81c815",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -315,7 +315,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -963,7 +963,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "f9398ad1c5fc74aa44f92b601c9e17dfb40a463b",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -690,7 +690,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "76b8d88ce6bfe51569e07b95f8cab9a9d4f8e832",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -691,7 +691,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "009f7dc57ab0a8837d62c0dcc005423a853239e4",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -402,7 +402,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1122,7 +1122,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "fc023bb4ae84a178bb6ab1f2f817663cae0e44d6",
            "filename": "src/transformers/models/opt/modeling_flax_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_flax_opt.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -150,7 +150,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "22d2bb40b9e48dc709961205fb95838eb52327bd",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -199,7 +199,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -723,7 +723,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "1b8b48963f73100a2b0ef1e34716bbe2790d9c00",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -77,7 +77,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         dtype (`torch.dtype`):\n             The dtype to use for the 4D attention mask.\n         device (`torch.device`):\n-            The device to plcae the 4D attention mask on.\n+            The device to place the 4D attention mask on.\n         min_dtype (`float`):\n             The minimum value representable with the dtype `dtype`.\n         cache_position (`torch.Tensor`):"
        },
        {
            "sha": "bd450698937ccb8e52c1d02ca2cb3be750a7823f",
            "filename": "src/transformers/models/pegasus/modeling_flax_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_flax_pegasus.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -278,7 +278,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "b6060e3e24def109db607d644b3a59a384d2fb99",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -763,7 +763,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2950e27a2fd6af1a8af07fbb10c7c36522ecda26",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -688,7 +688,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "6712f24f413e8a9d156cb5fbd7d4a315a69144b6",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -778,7 +778,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "33123ff8ef2dd20aa33d5f8f668f75459170a284",
            "filename": "src/transformers/models/phimoe/configuration_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fconfiguration_phimoe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -88,7 +88,7 @@ class PhimoeConfig(PretrainedConfig):\n         num_local_experts (`int`, *optional*, defaults to 16):\n             Number of experts per Sparse MLP layer.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss. See [here]() for more details\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "f06255030e65795537a4ae234931afd6baf60798",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1284,7 +1284,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "591dbdfe5c24f2f53ee27772d3ba132e15c8611c",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1671,7 +1671,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "0b9ad5724b9e034dbcadc6703cd69e3d66f4776c",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1084,7 +1084,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "53f675192b44531a283c92ac179b97eae8ed27d3",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -716,7 +716,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "ed3505728a6331220a60ca71aa6e1dba7aef09a0",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -243,7 +243,7 @@ def __init__(\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'.\n-        # and change type from 'mrope' to 'default' because `mrope` does defeault RoPE calculations\n+        # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations\n         # one can set it to \"linear\"/\"dynamic\" etc. to have scaled RoPE\n         # TODO: @raushan update config in the hub\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:"
        },
        {
            "sha": "a90baf246edae199e3cc2d4139ae2c8ba762b111",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -604,7 +604,7 @@ def forward(self, x, position_ids):\n         if \"dynamic\" in self.rope_type:\n             self._dynamic_frequency_update(position_ids, device=x.device)\n \n-        # Core RoPE block. In contrast to other models, Qwen2_5_VL has different position ids for thw grids\n+        # Core RoPE block. In contrast to other models, Qwen2_5_VL has different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n@@ -646,7 +646,7 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     Explanation:\n         Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding\n         sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For\n-        vision embedding part, we apply rotary position embedding on temporal, height and width dimension seperately.\n+        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.\n         Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.\n         For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,\n         height and width) of text embedding is always the same, so the text embedding rotary position embedding has no\n@@ -815,7 +815,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1349,7 +1349,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1564,7 +1564,7 @@ def get_rope_index(\n                 width position_ids: [0, 1, 2, 3, 4]\n \n             For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part\n-            and 1D rotary position embeddin for text part.\n+            and 1D rotary position embedding for text part.\n             Examples:\n                 Temporal (Time): 3 patches, representing different segments of the video in time.\n                 Height: 2 patches, dividing each frame vertically."
        },
        {
            "sha": "f4397cf122cc50b753e95b623c1cf4755296382f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -432,7 +432,7 @@ def get_rope_index(\n                 width position_ids: [0, 1, 2, 3, 4]\n \n             For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part\n-            and 1D rotary position embeddin for text part.\n+            and 1D rotary position embedding for text part.\n             Examples:\n                 Temporal (Time): 3 patches, representing different segments of the video in time.\n                 Height: 2 patches, dividing each frame vertically."
        },
        {
            "sha": "1be779e29fbf03948101d8d5cd36e089a5e56c8b",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -227,7 +227,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "5c45a1c187f316aa91abe7f4261fda196e585e01",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -125,7 +125,7 @@ class Qwen2MoeConfig(PretrainedConfig):\n         norm_topk_prob (`bool`, *optional*, defaults to `False`):\n             Whether to normalize the topk probabilities.\n         output_router_logits (`bool`, *optional*, defaults to `False`):\n-            Whether or not the router logits should be returned by the model. Enabeling this will also\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n             allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n         router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n             The aux loss factor for the total loss."
        },
        {
            "sha": "036bdb26ca565443604710f37d6e41d925858072",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -410,7 +410,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1172,7 +1172,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "2917e2d8ba18cf843b99a662a7206bb9aa9b93bf",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -232,7 +232,7 @@ def __init__(\n \n         # Validate the correctness of rotary position embeddings parameters\n         # BC: if there is a 'type' field, move it to 'rope_type'.\n-        # and change type from 'mrope' to 'default' because `mrope` does defeault RoPE calculations\n+        # and change type from 'mrope' to 'default' because `mrope` does default RoPE calculations\n         # one can set it to \"linear\"/\"dynamic\" etc. to have scaled RoPE\n         # TODO: @raushan update config in the hub\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:"
        },
        {
            "sha": "abcc2895dc3180b04182ef348486ac0b7a2c3bb8",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -110,7 +110,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n         max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n             The max pixels of the image to resize the image.\n         patch_size (`int`, *optional*, defaults to 14):\n-            The spacial patch size of the vision encoder.\n+            The spatial patch size of the vision encoder.\n         temporal_patch_size (`int`, *optional*, defaults to 2):\n             The temporal patch size of the vision encoder.\n         merge_size (`int`, *optional*, defaults to 2):"
        },
        {
            "sha": "8f4c233c0bfa4f497783bf4c08b146f2f90d137e",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -86,7 +86,7 @@ class Qwen2VLFastImageProcessorInitKwargs(DefaultFastImageProcessorInitKwargs):\n         max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n             The max pixels of the image to resize the image.\n         patch_size (`int`, *optional*, defaults to 14):\n-            The spacial patch size of the vision encoder.\n+            The spatial patch size of the vision encoder.\n         temporal_patch_size (`int`, *optional*, defaults to 2):\n             The temporal patch size of the vision encoder.\n         merge_size (`int`, *optional*, defaults to 2):"
        },
        {
            "sha": "dfea703003dbf09ec7b18343692b216ecf52bf8d",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -140,7 +140,7 @@ def forward(self, x, position_ids):\n         if \"dynamic\" in self.rope_type:\n             self._dynamic_frequency_update(position_ids, device=x.device)\n \n-        # Core RoPE block. In contrast to other models, Qwen2_VL has different position ids for thw grids\n+        # Core RoPE block. In contrast to other models, Qwen2_VL has different position ids for the grids\n         # So we expand the inv_freq to shape (3, ...)\n         inv_freq_expanded = self.inv_freq[None, None, :, None].float().expand(3, position_ids.shape[1], -1, 1)\n         position_ids_expanded = position_ids[:, :, None, :].float()  # shape (3, bs, 1, positions)\n@@ -174,7 +174,7 @@ def apply_multimodal_rotary_pos_emb(q, k, cos, sin, mrope_section, unsqueeze_dim\n     Explanation:\n         Multimodal 3D rotary position embedding is an extension to 1D rotary position embedding. The input embedding\n         sequence contains vision (images / videos) embedding and text embedding or just contains text embedding. For\n-        vision embedding part, we apply rotary position embedding on temporal, height and width dimension seperately.\n+        vision embedding part, we apply rotary position embedding on temporal, height and width dimension separately.\n         Here we split the channel dimension to 3 chunks for the temporal, height and width rotary position embedding.\n         For text embedding part, we just apply 1D rotary position embedding. The three rotary position index (temporal,\n         height and width) of text embedding is always the same, so the text embedding rotary position embedding has no\n@@ -628,7 +628,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1296,7 +1296,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n@@ -1461,15 +1461,15 @@ def get_rope_index(\n         Explanation:\n             Each embedding sequence contains vision embedding and text embedding or just contains text embedding.\n \n-            For pure text embedding sequence, the rotary position embedding has no difference with mordern LLMs.\n+            For pure text embedding sequence, the rotary position embedding has no difference with modern LLMs.\n             Examples:\n                 input_ids: [T T T T T], here T is for text.\n                 temporal position_ids: [0, 1, 2, 3, 4]\n                 height position_ids: [0, 1, 2, 3, 4]\n                 width position_ids: [0, 1, 2, 3, 4]\n \n             For vision and text embedding sequence, we calculate 3D rotary position embedding for vision part\n-            and 1D rotary position embeddin for text part.\n+            and 1D rotary position embedding for text part.\n             Examples:\n                 Assume we have a video input with 3 temporal patches, 2 height patches and 2 width patches.\n                 input_ids: [V V V V V V V V V V V V T T T T T], here V is for vision."
        },
        {
            "sha": "2beb0a06b8d7c1c693f150e9d7201417c469cd62",
            "filename": "src/transformers/models/roberta/modeling_flax_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_flax_roberta.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -224,7 +224,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "1e691c047bd4fc23b7ef83a58f37407a21aa088f",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_flax_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_flax_roberta_prelayernorm.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -227,7 +227,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "4c9cf6cb0a0b7f08ba43e8761b02862a5c368951",
            "filename": "src/transformers/models/roformer/tokenization_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_utils.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -40,7 +40,7 @@ def __init__(self, vocab) -> None:\n     def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[NormalizedString]:\n         splits = []\n \n-        # this code slice normalized_string is too slow (6s) but test_alignement_methods can pass\n+        # this code slice normalized_string is too slow (6s) but test_alignment_methods can pass\n         for token, start, end in self.jieba.tokenize(str(normalized_string), hmm=False):\n             if token in self.vocab:\n                 splits.append(normalized_string[start:end])\n@@ -52,7 +52,7 @@ def jieba_split(self, i: int, normalized_string: NormalizedString) -> List[Norma\n                         splits.append(normalized_string[start:end])\n                         start = end\n \n-        # this code test_alignement_methods can't pass but fast (300ms)\n+        # this code test_alignment_methods can't pass but fast (300ms)\n         # for token in self.jieba.cut(str(normalized_string), False):\n         #     if token in self.vocab:\n         #         splits.append(NormalizedString(token))"
        },
        {
            "sha": "0ef4852fa5f2e29ecfad9e48786f81b1a89bce53",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -567,7 +567,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "2da27ae36a99ae8518d11f9dfdfbf45ea1c80239",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -449,7 +449,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "922bc9304dd6e99608944b1277140bb8ebc95111",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -340,7 +340,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "3e34025fac907cd264a72813bc2ae49293caaa9e",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -251,7 +251,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "faa6ac2c813a631a75fa22f2af8e707e4a4aa3d3",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -452,7 +452,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1018,7 +1018,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "0187f733abca667c806faa63a79f348712639a33",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -699,7 +699,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "0d13dfaabbb149fff22af8af6169ff418259dfa5",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1220,7 +1220,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "be76fe1b7724c0901f1f43661cd37e31a59a345a",
            "filename": "src/transformers/models/t5/modeling_flax_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_flax_t5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -291,7 +291,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "e6f3a74e6d141ffeee1d96f0537436bb6015bfa2",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1289,7 +1289,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "3614503e73176b44c102412ffbd1471a31d3b171",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -1622,7 +1622,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "5a410f1ff7f905f681225af224f33850d8e958d0",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -933,7 +933,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "46ed990b29bd5a883006ccd3b615204febeab7cf",
            "filename": "src/transformers/models/unispeech/modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech%2Fmodeling_unispeech.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -599,7 +599,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "f19d5f3789fffe5832677ab558493d583a206b39",
            "filename": "src/transformers/models/unispeech_sat/modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funispeech_sat%2Fmodeling_unispeech_sat.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -616,7 +616,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "f9043eba0ebf65e5270300cababa171eba9fcb90",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -662,7 +662,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n "
        },
        {
            "sha": "fb892677fc406e1ba228f5e6018f5686bfb263bc",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -358,7 +358,7 @@ def __init__(self, *args, **kwargs):\n         super().__init__(*args, **kwargs)\n \n         # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n \n@@ -1459,7 +1459,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n             dtype (`torch.dtype`):\n                 The dtype to use for the 4D attention mask.\n             device (`torch.device`):\n-                The device to plcae the 4D attention mask on.\n+                The device to place the 4D attention mask on.\n             cache_position (`torch.Tensor`):\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):"
        },
        {
            "sha": "3b7a933e4dbaddd14c9a4cc03d857a5f9171fa1b",
            "filename": "src/transformers/models/xglm/modeling_flax_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_flax_xglm.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -169,7 +169,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        },
        {
            "sha": "63432be06ddd91bb77021a3502ac6cf8f8d1cde8",
            "filename": "src/transformers/models/xlm_roberta/modeling_flax_xlm_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af9b2eaa54c150741f298d6db939af6328e1dc38/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_flax_xlm_roberta.py?ref=af9b2eaa54c150741f298d6db939af6328e1dc38",
            "patch": "@@ -228,7 +228,7 @@ def _merge_heads(self, hidden_states):\n     def _concatenate_to_cache(self, key, value, query, attention_mask):\n         \"\"\"\n         This function takes projected key, value states from a single input token and concatenates the states to cached\n-        states from previous steps. This function is slighly adapted from the official Flax repository:\n+        states from previous steps. This function is slightly adapted from the official Flax repository:\n         https://github.com/google/flax/blob/491ce18759622506588784b4fca0e4bf05f8c8cd/flax/linen/attention.py#L252\n         \"\"\"\n         # detect if we're initializing by absence of existing cache data."
        }
    ],
    "stats": {
        "total": 288,
        "additions": 144,
        "deletions": 144
    }
}