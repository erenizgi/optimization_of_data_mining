{
    "author": "ved1beta",
    "message": "Logging message for ``` is_bitsandbytes_available() ```  (#38528)\n\n* bnb import log\n\n* bnb import log\n\n* log mesage change\n\n* moved error issue in qunatizer_bnb_4_bit.py\n\n* ruff\n\n* arg added for bnb check\n\n* required changes\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "71f73859423f12888dcd460ae4ff1fcbb73d5625",
    "files": [
        {
            "sha": "b25d61cdb49caedbb0d7b7aa58c0d783c66b6761",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=71f73859423f12888dcd460ae4ff1fcbb73d5625",
            "patch": "@@ -72,10 +72,23 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\n                 f\"Using `bitsandbytes` 4-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available():\n+        if not is_bitsandbytes_available(check_library_only=True):\n             raise ImportError(\n                 \"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n             )\n+        if not is_torch_available():\n+            raise ImportError(\n+                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n+                \"You can install it with `pip install torch`.\"\n+            )\n+        # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n+        # so those versions of the library are practically only available when CUDA is too.\n+        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.1\"):\n+            if not torch.cuda.is_available():\n+                raise ImportError(\n+                    \"The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \"\n+                    \"You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\"\n+                )\n \n         from ..integrations import validate_bnb_backend_availability\n         from ..utils import is_bitsandbytes_multi_backend_available\n@@ -110,12 +123,6 @@ def validate_environment(self, *args, **kwargs):\n                     \"for more details. \"\n                 )\n \n-        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.39.0\"):\n-            raise ValueError(\n-                \"You have a version of `bitsandbytes` that is not compatible with 4bit inference and training\"\n-                \" make sure you have the latest version of `bitsandbytes` installed\"\n-            )\n-\n     def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n             from accelerate.utils import CustomDtype"
        },
        {
            "sha": "f88035459f8d5fcfaca6046c92f94b009588a61f",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=71f73859423f12888dcd460ae4ff1fcbb73d5625",
            "patch": "@@ -69,10 +69,23 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\n                 f\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`\"\n             )\n-        if not is_bitsandbytes_available():\n+        if not is_bitsandbytes_available(check_library_only=True):\n             raise ImportError(\n                 \"Using `bitsandbytes` 8-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\n             )\n+        if not is_torch_available():\n+            raise ImportError(\n+                \"The bitsandbytes library requires PyTorch but it was not found in your environment. \"\n+                \"You can install it with `pip install torch`.\"\n+            )\n+        # `bitsandbytes` versions older than 0.43.1 eagerly require CUDA at import time,\n+        # so those versions of the library are practically only available when CUDA is too.\n+        if version.parse(importlib.metadata.version(\"bitsandbytes\")) < version.parse(\"0.43.1\"):\n+            if not torch.cuda.is_available():\n+                raise ImportError(\n+                    \"The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \"\n+                    \"You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\"\n+                )\n \n         from ..integrations import validate_bnb_backend_availability\n         from ..utils import is_bitsandbytes_multi_backend_available"
        },
        {
            "sha": "5e024e5857ec5c9ab33460e77516fcb73e0ec252",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71f73859423f12888dcd460ae4ff1fcbb73d5625/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=71f73859423f12888dcd460ae4ff1fcbb73d5625",
            "patch": "@@ -995,8 +995,14 @@ def is_torch_xpu_available(check_device=False):\n \n \n @lru_cache()\n-def is_bitsandbytes_available():\n-    if not is_torch_available() or not _bitsandbytes_available:\n+def is_bitsandbytes_available(check_library_only=False) -> bool:\n+    if not _bitsandbytes_available:\n+        return False\n+\n+    if check_library_only:\n+        return True\n+\n+    if not is_torch_available():\n         return False\n \n     import torch"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 36,
        "deletions": 10
    }
}