{
    "author": "Cyrilvallez",
    "message": "[loading] Allow loading to happen without threading (#42619)\n\n* add env var\n\n* oupsi\n\n* fix\n\n* style\n\n* add test\n\n* style",
    "sha": "280c5d6d4e67ff05f6140f788379abe484a84e34",
    "files": [
        {
            "sha": "105ff6a0004a5f187a8c7799cb724419a3dcf3e9",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 81,
            "deletions": 59,
            "changes": 140,
            "blob_url": "https://github.com/huggingface/transformers/blob/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=280c5d6d4e67ff05f6140f788379abe484a84e34",
            "patch": "@@ -31,7 +31,7 @@\n \n from .integrations.accelerate import offload_weight\n from .integrations.tensor_parallel import ALL_PARALLEL_STYLES\n-from .utils import is_torch_greater_or_equal, logging\n+from .utils import is_env_variable_true, is_torch_greater_or_equal, logging\n \n \n _torch_distributed_available = torch.distributed.is_available()\n@@ -491,25 +491,37 @@ def convert(\n GLOBAL_WORKERS = min(4, os.cpu_count() or 4)\n \n \n-def _materialize_copy(tensor, device=None, dtype=None):\n+def _materialize_copy(tensor: torch.Tensor, device=None, dtype=None) -> torch.Tensor:\n+    # This slicing is what actually loads the tensor from the safetensors slice object\n     tensor = tensor[...]\n     if dtype is not None or device is not None:\n         tensor = tensor.to(device=device, dtype=dtype)\n     return tensor\n \n \n-def spawn_materialize(thread_pool, tensor, device=None, dtype=None) -> Future:\n-    def _job():\n+def spawn_materialize(\n+    thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, device=None, dtype=None\n+) -> Future | torch.Tensor:\n+    \"\"\"Materialize a tensor from file asynchronously if `thread_pool` is provided, or immediately otherwise.\"\"\"\n+    if thread_pool is not None:\n+        return thread_pool.submit(_materialize_copy, tensor, device, dtype)\n+    else:\n         return _materialize_copy(tensor, device, dtype)\n \n-    return thread_pool.submit(_job)\n \n+def spawn_tp_materialize(\n+    thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, sharding_method, tensor_idx, dtype=None\n+) -> Future | torch.Tensor:\n+    \"\"\"Materialize and shard a tensor (according to the TP-plan) from file asynchronously if `thread_pool` is provided, or\n+    immediately otherwise.\"\"\"\n \n-def spawn_tp_materialize(thread_pool, tensor, sharding_method, tensor_idx, dtype=None) -> Future:\n     def _job():\n         return sharding_method.shard_tensor(tensor, param_casting_dtype=dtype, tensor_idx=tensor_idx)[0]\n \n-    return thread_pool.submit(_job)\n+    if thread_pool is not None:\n+        return thread_pool.submit(_job)\n+    else:\n+        return _job()\n \n \n def dot_natural_key(s: str):\n@@ -783,13 +795,16 @@ def convert_and_load_state_dict_in_model(\n     misc = {}\n     mismatch_keys = set()\n     unexpected_keys = set()\n-    # Global thread_pool\n-    thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n+\n+    # We use threading by default, if not explicitly deactivated via env variable\n+    if not is_env_variable_true(\"HF_DEACTIVATE_ASYNC_LOAD\"):\n+        thread_pool = ThreadPoolExecutor(max_workers=GLOBAL_WORKERS)\n+    else:\n+        thread_pool = None\n \n     renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n     converters = [entry for entry in weight_mapping if isinstance(entry, WeightConverter)]\n-\n-    param_name_to_load: dict[str, Union[WeightRenaming | WeightConverter]] = {}\n+    param_name_to_load: dict[str, WeightRenaming | WeightConverter] = {}\n \n     # build '(?P<g0>.*.*\\\\.block_sparse_moe\\\\..*)' and group to source {'g0': '*.block_sparse_moe.'}\n     # and target to source {'g0': '*.mlp.'}. This allows us to quickly find which pattern matched.\n@@ -841,8 +856,8 @@ def convert_and_load_state_dict_in_model(\n             elif empty_param is not None and empty_param.dtype != _dtype:\n                 _dtype = empty_param.dtype  # usually correct when initializing\n \n-            # 4. Handle TP sharding or device_map placement -> scheduled materialization\n-            future = None\n+            # 4. Handle TP sharding or device_map placement\n+            future_or_tensor = None\n             if device_mesh:\n                 if matched_tp_pattern := tp_plan_alt.search(renamed_key):\n                     matched_tp_pattern = tp_plan_by_group_name[matched_tp_pattern.lastgroup]\n@@ -852,74 +867,81 @@ def convert_and_load_state_dict_in_model(\n                             device_mesh=device_mesh, rank=device_map[\"\"].index, empty_param=empty_param.clone()\n                         )\n                     shard_index = len(mapping.collected_tensors.get(original_key, []))\n-                    future = spawn_tp_materialize(\n+                    future_or_tensor = spawn_tp_materialize(\n                         thread_pool,\n                         tensor,\n                         mapping.distributed_operation,\n                         shard_index,\n                         _dtype,\n                     )\n \n-            if future is None:\n+            if future_or_tensor is None:\n                 device_match = device_map_regex.match(renamed_key)\n                 param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n                 # If disk, we need to materialize on cpu first\n                 param_device = \"cpu\" if param_device == \"disk\" else param_device\n-                future = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n+                future_or_tensor = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n \n-            mapping.add_tensor(renamed_key, original_key, source_pattern, future)\n+            mapping.add_tensor(renamed_key, original_key, source_pattern, future_or_tensor)\n         elif source_pattern is not None:  # add all target keys as unexpected\n             mapping = pattern_to_converter[source_pattern]\n             for k in mapping.target_patterns:\n                 unexpected_keys.add(renamed_key.replace(mapping.target_patterns[0], k))\n         else:\n             unexpected_keys.add(renamed_key)\n \n-    total_entries = len(param_name_to_load)\n-    with logging.tqdm(total=total_entries, desc=\"Loading weights\") as pbar:\n-        for first_param_name, mapping in param_name_to_load.items():\n-            pbar.update(1)\n-            pbar.set_postfix({\"Materializing param\": first_param_name})\n-            pbar.refresh()\n-            try:\n-                realized_value, misc = mapping.convert(\n-                    first_param_name,\n-                    model=model,\n-                    config=model.config,\n-                    hf_quantizer=hf_quantizer,\n-                    missing_keys=missing_keys,\n-                    misc=misc,\n-                )\n-                for target_name, param in realized_value.items():\n-                    param = param[0] if isinstance(param, list) else param\n-                    device_match = device_map_regex.match(target_name)\n-                    param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n-                    # Offloading support\n-                    if param_device == \"disk\":\n-                        disk_offload_index = offload_and_maybe_resave_param(\n-                            target_name, param, missing_keys, disk_offload_folder, disk_offload_index, mapping\n-                        )\n-                    else:\n-                        set_param_for_module(\n-                            model,\n-                            target_name,\n-                            param,\n-                            mismatch_keys,\n-                            missing_keys,\n-                            misc,\n-                            unexpected_keys,\n-                            mapping.distributed_operation,\n-                            hf_quantizer,\n-                        )\n-\n-                # Cleanup the tensors\n-                mapping.reset()\n-            except SkipLayer:\n-                continue\n+    try:\n+        total_entries = len(param_name_to_load)\n+        with logging.tqdm(total=total_entries, desc=\"Loading weights\") as pbar:\n+            for first_param_name, mapping in param_name_to_load.items():\n+                pbar.update(1)\n+                pbar.set_postfix({\"Materializing param\": first_param_name})\n+                pbar.refresh()\n+                try:\n+                    realized_value, misc = mapping.convert(\n+                        first_param_name,\n+                        model=model,\n+                        config=model.config,\n+                        hf_quantizer=hf_quantizer,\n+                        missing_keys=missing_keys,\n+                        misc=misc,\n+                    )\n+                    for target_name, param in realized_value.items():\n+                        param = param[0] if isinstance(param, list) else param\n+                        device_match = device_map_regex.match(target_name)\n+                        param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+                        # Offloading support\n+                        if param_device == \"disk\":\n+                            disk_offload_index = offload_and_maybe_resave_param(\n+                                target_name, param, missing_keys, disk_offload_folder, disk_offload_index, mapping\n+                            )\n+                        else:\n+                            set_param_for_module(\n+                                model,\n+                                target_name,\n+                                param,\n+                                mismatch_keys,\n+                                missing_keys,\n+                                misc,\n+                                unexpected_keys,\n+                                mapping.distributed_operation,\n+                                hf_quantizer,\n+                            )\n+\n+                    # Cleanup the tensors that were gathered internally in the mapping\n+                    mapping.reset()\n+\n+                except SkipLayer:\n+                    continue\n+\n+    # Close the pool, independently of whether the code was interrupted or finished successfully\n+    finally:\n+        if thread_pool is not None:\n+            # `cancel_futures=True` in case the program was interupted, to avoid wasting time on exit\n+            thread_pool.shutdown(wait=False, cancel_futures=True)\n \n     # Keep the current weight conversion mapping for later saving (in case it was coming directly from the user)\n     model._weight_conversions = weight_mapping\n-    thread_pool.shutdown(wait=False)\n     return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, misc\n \n "
        },
        {
            "sha": "3e3f4e1d80766d989576d9bd3b769f025c2c90fd",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=280c5d6d4e67ff05f6140f788379abe484a84e34",
            "patch": "@@ -129,6 +129,8 @@\n     is_datasets_available,\n     is_decord_available,\n     is_detectron2_available,\n+    is_env_variable_false,\n+    is_env_variable_true,\n     is_essentia_available,\n     is_faiss_available,\n     is_fbgemm_gpu_available,"
        },
        {
            "sha": "3c10269c44a5b48a39ab51efac2a53c95325405a",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/280c5d6d4e67ff05f6140f788379abe484a84e34/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=280c5d6d4e67ff05f6140f788379abe484a84e34",
            "patch": "@@ -77,6 +77,16 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> tuple[\n         return package_exists\n \n \n+def is_env_variable_true(env_variable: str) -> bool:\n+    \"\"\"Detect whether `env_variable` has been set to a true value in the environment\"\"\"\n+    return os.getenv(env_variable, \"false\").lower() in (\"true\", \"1\", \"y\", \"yes\", \"on\")\n+\n+\n+def is_env_variable_false(env_variable: str) -> bool:\n+    \"\"\"Detect whether `env_variable` has been set to a false value in the environment\"\"\"\n+    return os.getenv(env_variable, \"true\").lower() in (\"false\", \"0\", \"n\", \"no\", \"off\")\n+\n+\n ENV_VARS_TRUE_VALUES = {\"1\", \"ON\", \"YES\", \"TRUE\"}\n ENV_VARS_TRUE_AND_AUTO_VALUES = ENV_VARS_TRUE_VALUES.union({\"AUTO\"})\n "
        },
        {
            "sha": "fb6a8380cd32fae98c5549e4f017e385952c75a6",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/280c5d6d4e67ff05f6140f788379abe484a84e34/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/280c5d6d4e67ff05f6140f788379abe484a84e34/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=280c5d6d4e67ff05f6140f788379abe484a84e34",
            "patch": "@@ -2228,6 +2228,41 @@ def test_device_map_works_with_unexpected_keys_sharded(self):\n         # Unexpected keys (mtp) should be removed from the state dict, therefore this should not error out.\n         BaseModelWithUnexpectedKeys.from_pretrained(temp.name, device_map={\"linear\": \"cpu\", \"linear_2\": \"disk\"})\n \n+    def test_loading_respect_env_variable_for_threading(self):\n+        \"\"\"Test that we can correctly control threading during loading\"\"\"\n+        model = BaseModel(PreTrainedConfig())\n+\n+        # Monkey patch Thread.__init__ to add a counter of launched threads\n+        original_init = threading.Thread.__init__\n+        counter = 0\n+\n+        def tracking_init(self, *args, **kwargs):\n+            nonlocal counter\n+            counter += 1\n+            original_init(self, *args, **kwargs)\n+\n+        threading.Thread.__init__ = tracking_init\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.save_pretrained(tmpdirname)\n+\n+            # Use threading\n+            os.environ[\"HF_DEACTIVATE_ASYNC_LOAD\"] = \"0\"\n+            before = counter\n+            _ = BaseModel.from_pretrained(tmpdirname)\n+            after = counter\n+            self.assertTrue(after - before > 0, \"Loading should have spawned new threads!\")\n+\n+            # Deactivate threading\n+            os.environ[\"HF_DEACTIVATE_ASYNC_LOAD\"] = \"1\"\n+            before = counter\n+            _ = BaseModel.from_pretrained(tmpdirname)\n+            after = counter\n+            self.assertTrue(after == before, \"It looks like loading did spawn new threads, but it should not have!\")\n+\n+        # Reverse monkey patch\n+        threading.Thread.__init__ = original_init\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 187,
        "additions": 128,
        "deletions": 59
    }
}