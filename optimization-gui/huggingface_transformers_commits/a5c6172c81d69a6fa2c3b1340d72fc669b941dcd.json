{
    "author": "zucchini-nlp",
    "message": "[VLM] fix loading issues (#38051)\n\n* fix qwen2-vl loading\n\n* fix a few nore models\n\n* delete print\n\n* fix copies",
    "sha": "a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
    "files": [
        {
            "sha": "f994d9b0876950052c4bf375d2b2937383e34383",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
            "patch": "@@ -219,22 +219,19 @@ def is_local_dist_rank_0():\n # DO NOT MODIFY, KEPT FOR BC ONLY\n VLMS = [\n     \"aria\",\n-    \"aya_vision\",\n+    \"ayavision\",\n     \"emu3\",\n     \"fuyu\",\n-    \"got_ocr2\",\n+    \"gotocr2\",\n     \"gemma3\",\n     \"internvl\",\n-    \"llava\",\n-    \"llava_next\",\n-    \"llava_next_video\",\n-    \"llava_onevision\",\n+    \"llava\",  # all llava prefixed models fall under this check\n     \"mistral3\",\n     \"mllama\",\n     \"paligemma\",\n-    \"qwen2_vl\",\n-    \"qwem2_5_vl\",\n-    \"video_llava\",\n+    \"qwen2vl\",\n+    \"qwen2_5_vl\",\n+    \"videollava\",\n     \"vipllava\",\n ]\n "
        },
        {
            "sha": "82ac330dd988f32edc0a4ef36b64f8e52cc26d84",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
            "patch": "@@ -1381,6 +1381,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n @auto_docstring\n class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n+    base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n     config_class = Qwen2_5_VLConfig\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]"
        },
        {
            "sha": "4f9882d22336022c66e8e1ef9f901875fe1bef33",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
            "patch": "@@ -414,6 +414,7 @@ class Qwen2_5_VLModelOutputWithPast(Qwen2VLModelOutputWithPast):\n \n class Qwen2_5_VLModel(Qwen2VLModel):\n     config_class = Qwen2_5_VLConfig\n+    base_model_prefix = \"\"\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "4e78a2591541c36439afb3d4a51b7d1c9587ebf3",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
            "patch": "@@ -1341,6 +1341,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n @auto_docstring\n class Qwen2VLModel(Qwen2VLPreTrainedModel):\n+    base_model_prefix = \"\"\n     _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n \n     def __init__(self, config: Qwen2VLConfig):"
        },
        {
            "sha": "785ad723f444b04a97e6e98dc746047539a3a099",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5c6172c81d69a6fa2c3b1340d72fc669b941dcd/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=a5c6172c81d69a6fa2c3b1340d72fc669b941dcd",
            "patch": "@@ -144,7 +144,6 @@ def prepare_config_and_inputs_for_common(self):\n         config, pixel_values = config_and_inputs\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n-        print(\"attention_mask\", attention_mask.shape)\n         # input_ids[:, -1] = self.pad_token_id\n         input_ids[input_ids == self.image_token_index] = self.pad_token_id\n         input_ids[:, : self.image_seq_length] = self.image_token_index\n@@ -366,7 +365,6 @@ def test_small_model_integration_forward(self):\n             output = model(**inputs)\n \n         actual_logits = output.logits[0, -1, :5].cpu()\n-        print(\"actual_logits\", actual_logits)\n         expected_logits = torch.tensor([0.4109, 0.1532, 0.8018, 2.1328, 0.5483], dtype=torch.float16)\n         self.assertTrue(\n             torch.allclose(actual_logits, expected_logits, atol=0.1),\n@@ -400,7 +398,6 @@ def test_small_model_integration_generate_text_only(self):\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        print(\"decoded_output\", decoded_output)\n \n         expected_outputs = Expectations(\n             {\n@@ -437,7 +434,6 @@ def test_small_model_integration_generate_chat_template(self):\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n-        print(\"decoded_output\", decoded_output)\n         expected_output = \"The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,\"  # fmt: skip\n         self.assertEqual(decoded_output, expected_output)\n \n@@ -477,7 +473,6 @@ def test_small_model_integration_batched_generate(self):\n \n         # Check first output\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-        print(\"decoded_output\", decoded_output)\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n@@ -494,7 +489,6 @@ def test_small_model_integration_batched_generate(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-        print(\"decoded_output\", decoded_output)\n         expected_output = 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a'  # fmt: skip\n \n         self.assertEqual(\n@@ -558,7 +552,6 @@ def test_small_model_integration_batched_generate_multi_image(self):\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n \n-        print(\"decoded_output\", decoded_output)\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n@@ -567,7 +560,6 @@ def test_small_model_integration_batched_generate_multi_image(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-        print(\"decoded_output\", decoded_output)\n         expected_outputs = Expectations(\n             {\n                 (\"xpu\", 3): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at \","
        }
    ],
    "stats": {
        "total": 26,
        "additions": 9,
        "deletions": 17
    }
}