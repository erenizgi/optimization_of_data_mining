{
    "author": "ylacombe",
    "message": "Fix distil whisper segment computation (#33920)\n\n* Fix distil whisper segment computation\r\n\r\n* [run-slow] whisper",
    "sha": "124713c32b62416bf7a773676866fd53924bc472",
    "files": [
        {
            "sha": "a3de765137b84d86385ed58ac99ce0cc93aae037",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/124713c32b62416bf7a773676866fd53924bc472/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/124713c32b62416bf7a773676866fd53924bc472/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=124713c32b62416bf7a773676866fd53924bc472",
            "patch": "@@ -994,7 +994,10 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n                     for v in range(len(values)):\n                         layer_past_key_values = []\n                         for w in values[v]:\n-                            layer_past_key_values.append(w[batch_idx][None].cpu())\n+                            if len(w) != 0:\n+                                layer_past_key_values.append(w[batch_idx][None].cpu())\n+                            else:\n+                                layer_past_key_values.append(w)\n                         all_past_key_values.append(tuple(layer_past_key_values))\n                     return tuple(all_past_key_values)\n "
        },
        {
            "sha": "e0eb27813ec03d7f8d70d4dadacc698fc9bd2b73",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/124713c32b62416bf7a773676866fd53924bc472/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/124713c32b62416bf7a773676866fd53924bc472/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=124713c32b62416bf7a773676866fd53924bc472",
            "patch": "@@ -2100,6 +2100,21 @@ def test_tiny_timestamp_generation(self):\n         transcript = processor.batch_decode(generated_ids, skip_special_tokens=True, output_offsets=True)\n         self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n \n+    @slow\n+    def test_distil_token_timestamp_generation(self):\n+        # we actually just want to check that returning segments with distil model works\n+        processor = WhisperProcessor.from_pretrained(\"distil-whisper/distil-large-v3\")\n+        model = WhisperForConditionalGeneration.from_pretrained(\"distil-whisper/distil-large-v3\")\n+        model.to(torch_device)\n+\n+        input_speech = np.concatenate(self._load_datasamples(4))\n+        input_features = processor(input_speech, return_tensors=\"pt\", sampling_rate=16_000).input_features\n+        input_features = input_features.to(torch_device)\n+\n+        _ = model.generate(\n+            input_features, max_length=448, return_timestamps=True, return_token_timestamps=True, return_segments=True\n+        )\n+\n     @slow\n     def test_tiny_longform_timestamps_generation(self):\n         set_seed(0)"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 19,
        "deletions": 1
    }
}