{
    "author": "tomaarsen",
    "message": "Add missing ModelOutput subclass return type hints (#41219)\n\n* Add missing ModelOutput subclass return type hints\n\n* Fix incorrect type hint: FlavaOutput (nn.Module subclass) -> FlavaModelOutput",
    "sha": "6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
    "files": [
        {
            "sha": "2aa873070b4ac599756df84dac8ca093850788cc",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional\n+from typing import Optional, Union\n \n import numpy as np\n import torch\n@@ -583,7 +583,7 @@ def encode(\n         input_values: torch.Tensor,\n         n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n-    ):\n+    ) -> Union[tuple, DacEncoderOutput]:\n         r\"\"\"\n         input_values (`torch.Tensor of shape `(batch_size, 1, time_steps)`):\n             Input audio data to encode,\n@@ -610,7 +610,7 @@ def decode(\n         quantized_representation: Optional[torch.Tensor] = None,\n         audio_codes: Optional[torch.Tensor] = None,\n         return_dict: Optional[bool] = None,\n-    ):\n+    ) -> Union[tuple, DacDecoderOutput]:\n         r\"\"\"\n         quantized_representation (torch.Tensor of shape `(batch_size, dimension, time_steps)`, *optional*):\n             Quantized continuous representation of input.\n@@ -643,7 +643,7 @@ def forward(\n         input_values: torch.Tensor,\n         n_quantizers: Optional[int] = None,\n         return_dict: Optional[bool] = None,\n-    ):\n+    ) -> Union[tuple, DacOutput]:\n         r\"\"\"\n         input_values (`torch.Tensor` of shape `(batch_size, 1, time_steps)`):\n             Audio data to encode."
        },
        {
            "sha": "56d2f8ebbf93da3efbab9def5fd9369dd0de25bc",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -196,7 +196,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ):\n+    ) -> DeepseekVLBaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -268,7 +268,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ):\n+    ) -> DeepseekVLCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "e53f45c9bee513746e264e736c49c96fb446666e",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -314,7 +314,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ):\n+    ) -> DeepseekVLHybridBaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -424,7 +424,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ):\n+    ) -> DeepseekVLHybridCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "0292b9ef4c5fd3f69b4ff13eb5f0ad91826118f4",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -297,7 +297,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ):\n+    ) -> DeepseekVLHybridBaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -361,7 +361,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ):\n+    ) -> DeepseekVLHybridCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "fd0c99dc35b5e0d61e838d6a4d916dbe78a09b19",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -1107,7 +1107,7 @@ def forward(\n         output_hidden_states: bool = True,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[tuple, FlavaOutput]:\n+    ) -> Union[tuple, FlavaModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, image_num_patches + text_seq_len)`):\n             Indices of input sequence tokens in the vocabulary. Indices can be obtained using [`AutoTokenizer`]. See"
        },
        {
            "sha": "5ea5056796090cff08eea1e0ad68e5e606a2d982",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -1511,7 +1511,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, GroundingDinoEncoderOutput]:\n         r\"\"\"\n         Args:\n             vision_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -1666,7 +1666,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, GroundingDinoDecoderOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n@@ -2059,7 +2059,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, GroundingDinoModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide"
        },
        {
            "sha": "e52ba900b5dfab27d3773ead09970d68fbd262c7",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -1124,7 +1124,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ):\n+    ) -> JanusBaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -1201,7 +1201,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ):\n+    ) -> JanusCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "aaf5e0d7a11a204465e537fdd2557207ed5d1f18",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -942,7 +942,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs,\n-    ):\n+    ) -> JanusBaseModelOutputWithPast:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n@@ -1019,7 +1019,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **kwargs: Unpack[TransformersKwargs],\n-    ):\n+    ) -> JanusCausalLMOutputWithPast:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,"
        },
        {
            "sha": "b20fa92ece6925552fbbd50cd839e194e45e8064",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -19,7 +19,7 @@\n import collections.abc\n import math\n from dataclasses import dataclass\n-from typing import Optional\n+from typing import Optional, Union\n \n import torch\n from torch import Tensor, nn\n@@ -656,7 +656,7 @@ def forward(\n         output_attentions=False,\n         output_hidden_states=False,\n         return_dict=True,\n-    ):\n+    ) -> Union[tuple, MaskFormerSwinBaseModelOutput]:\n         all_hidden_states = () if output_hidden_states else None\n         all_input_dimensions = ()\n         all_self_attentions = () if output_attentions else None\n@@ -739,7 +739,7 @@ def forward(\n         interpolate_pos_encoding=False,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, MaskFormerSwinModelOutputWithPooling]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "a8fe024b074fa86ddf4c047e9e18efc926c118d9",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -1181,7 +1181,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, MMGroundingDinoEncoderOutput]:\n         r\"\"\"\n         Args:\n             vision_features (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -1478,7 +1478,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, MMGroundingDinoDecoderOutput]:\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n@@ -1954,7 +1954,7 @@ def forward(\n         output_hidden_states=None,\n         return_dict=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, MMGroundingDinoModelOutput]:\n         r\"\"\"\n         input_ids (`torch.LongTensor` of shape `(batch_size, text_sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide"
        },
        {
            "sha": "d7c9d2f073eee70abdbe9cbe1a296552b8b5fd08",
            "filename": "src/transformers/models/tvp/modeling_tvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fmodeling_tvp.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -16,7 +16,7 @@\n \n import math\n from dataclasses import dataclass\n-from typing import Optional\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -462,7 +462,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ):\n+    ) -> Union[tuple, BaseModelOutput]:\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -722,7 +722,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n         r\"\"\"\n         Examples:\n         ```python\n@@ -824,7 +824,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, TvpVideoGroundingOutput]:\n         r\"\"\"\n         labels (`torch.FloatTensor` of shape `(batch_size, 3)`, *optional*):\n             The labels contains duration, start time, and end time of the video corresponding to the text."
        },
        {
            "sha": "676dc7e6ffcaa4595cc109d7b5f997c2b118cd9c",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6c7c992fafcfabf5832c185faa9e26b34bb4a2fd/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=6c7c992fafcfabf5832c185faa9e26b34bb4a2fd",
            "patch": "@@ -1106,7 +1106,7 @@ def forward(\n         return_dict=None,\n         cache_position=None,\n         **kwargs,\n-    ):\n+    ) -> Union[tuple, BaseModelOutputWithAttentionMask]:\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1476,7 +1476,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[Tensor, ...]:\n+    ) -> Union[tuple, Seq2SeqModelOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `({0}, 4)`, *optional*):\n             Bounding boxes of each input sequence tokens. Selected in the range `[0,\n@@ -1655,7 +1655,7 @@ def forward(\n         labels: Optional[Tensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n-    ) -> tuple[Tensor, ...]:\n+    ) -> Union[tuple, Seq2SeqLMOutput]:\n         r\"\"\"\n         bbox (`torch.LongTensor` of shape `({0}, 4)`, *optional*):\n             Bounding boxes of each input sequence tokens. Selected in the range `[0,"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 31,
        "deletions": 31
    }
}