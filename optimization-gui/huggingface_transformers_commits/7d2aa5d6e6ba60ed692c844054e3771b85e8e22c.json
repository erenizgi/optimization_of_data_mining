{
    "author": "vasqu",
    "message": ":rotating_light: [`Flash Attention`] Fix sliding window size (#40163)\n\n* swa fix\n\n* add comment, make fix symmetrical\n\n* modify fa inference test to force swa correctness check\n\n* fixup comment",
    "sha": "7d2aa5d6e6ba60ed692c844054e3771b85e8e22c",
    "files": [
        {
            "sha": "4377c734edc04694b25d39d8418225937532581e",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2aa5d6e6ba60ed692c844054e3771b85e8e22c/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2aa5d6e6ba60ed692c844054e3771b85e8e22c/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=7d2aa5d6e6ba60ed692c844054e3771b85e8e22c",
            "patch": "@@ -532,7 +532,12 @@ def _process_flash_attention_kwargs(\n         flash_kwargs[\"dropout_p\"] = dropout\n \n     if supports_mapping[\"window_size\"] and sliding_window is not None and key_length > sliding_window:\n-        flash_kwargs[\"window_size\"] = (sliding_window, sliding_window)\n+        # The flash attention API sets inclusive boundaries, i.e. (4, 0) would take 4 tokens to the left\n+        # and the current token for a total size of 5. However, we usually define our window sizes by\n+        # their total window size (when causal). Encoder models as of now seldom use SWA and when they\n+        # do, they have a custom workaround (e.g. ModernBERT) which would align with this symmetric logic, i.e.\n+        # for a total of `2*sliding_window + 1`.\n+        flash_kwargs[\"window_size\"] = (sliding_window - 1, sliding_window - 1)\n \n     if supports_mapping[\"deterministic\"]:\n         flash_kwargs[\"deterministic\"] = ("
        },
        {
            "sha": "e8104d3550e0f1e8f1f60f748bba38ae0d260508",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d2aa5d6e6ba60ed692c844054e3771b85e8e22c/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d2aa5d6e6ba60ed692c844054e3771b85e8e22c/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=7d2aa5d6e6ba60ed692c844054e3771b85e8e22c",
            "patch": "@@ -3494,6 +3494,10 @@ def flash_attn_inference_equivalence(self, attn_implementation: str, padding_sid\n             # flash attention variants does not always support arbitrary headim\n             config = self._prepare_config_headdim(config, 16)\n \n+            # forcing the prefill size to go over sliding window size to check for SWA correctness\n+            if getattr(config, \"sliding_window\", None):\n+                config.sliding_window = 2\n+\n             # TODO it is unclear why saving and reloading with dtype works while\n             # casting with `.to(dtype=..., device=...)` does not.\n             # Discovered on tests with `Bart` models."
        }
    ],
    "stats": {
        "total": 11,
        "additions": 10,
        "deletions": 1
    }
}