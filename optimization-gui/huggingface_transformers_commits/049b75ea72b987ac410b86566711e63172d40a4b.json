{
    "author": "molbap",
    "message": "Flag SpeechT5 flaky test (#37587)\n\nflag flaky test",
    "sha": "049b75ea72b987ac410b86566711e63172d40a4b",
    "files": [
        {
            "sha": "41d4e82c535cd7b4557fa6970e6232e0b0cd22ac",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/049b75ea72b987ac410b86566711e63172d40a4b/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/049b75ea72b987ac410b86566711e63172d40a4b/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=049b75ea72b987ac410b86566711e63172d40a4b",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers import SpeechT5Config, SpeechT5HifiGanConfig\n from transformers.testing_utils import (\n+    is_flaky,\n     is_torch_available,\n     require_deterministic_for_xpu,\n     require_sentencepiece,\n@@ -722,6 +723,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @is_flaky(max_attempts=5, description=\"Flaky for some input configurations.\")\n+    def test_past_key_values_format(self):\n+        super().test_past_key_values_format()\n+\n     # overwrite from test_modeling_common\n     def _mock_init_weights(self, module):\n         if hasattr(module, \"weight\") and module.weight is not None:"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 5,
        "deletions": 0
    }
}