{
    "author": "akibjawad",
    "message": "Add support for including in-memory videos (not just files/urls) in apply_chat_template (#39494)\n\n* added code for handling video object ,as dictionary of frames and metadata, in chat template\n\n* added new test where videos are passed as objects (dict of frames, metadata) in the chat template\n\n* modified hardcoded video_len check that does not match with increased number of tests cases.\n\n* Modify hardcoded video_len check that fails with increased number of tests\n\n* update documentation of multi-modal chat templating with extra information about including video object in chat template.\n\n* add array handling in load_video()\n\n* temporary test video inlcuded\n\n* skip testing smolvlm with videos that are list of frames\n\n* update documentation & make fixup\n\n* Address review comments",
    "sha": "2a9febd632dde21d3f873ebce176d2ac4edc24b8",
    "files": [
        {
            "sha": "367cfe6be922e22db5a9831c7dacb581b060c8e1",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -111,6 +111,7 @@ Some vision models also support video inputs. The message format is very similar\n \n - The content `\"type\"` should be `\"video\"` to indicate the content is a video.\n - For videos, it can be a link to the video (`\"url\"`) or it could be a file path (`\"path\"`). Videos loaded from a URL can only be decoded with [PyAV](https://pyav.basswood-io.com/docs/stable/) or [Decord](https://github.com/dmlc/decord).\n+- In addition to loading videos from a URL or file path, you can also pass decoded video data directly. This is useful if youâ€™ve already preprocessed or decoded video frames elsewhere in memory (e.g., using OpenCV, decord, or torchvision). You don't need to save to files or store it in an URL.\n \n > [!WARNING]\n > Loading a video from `\"url\"` is only supported by the PyAV or Decord backends.\n@@ -137,6 +138,52 @@ messages = [\n ]\n ```\n \n+### Example: Passing decoded video objects\n+```python\n+import numpy as np\n+\n+video_object1 = np.random.randint(0, 255, size=(16, 224, 224, 3), dtype=np.uint8),\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"video\": video_object1},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"}\n+        ],\n+    },\n+]\n+```\n+You can also use existing (`\"load_video()\"`) function to load a video, edit the video in memory and pass it in the messages.\n+```python\n+\n+# Make sure a video backend library (pyav, decord, or torchvision) is available.\n+from transformers.video_utils import load_video\n+\n+# load a video file in memory for testing\n+video_object2, _ = load_video(\n+    \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"You are a friendly chatbot who always responds in the style of a pirate\"}],\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"video\": video_object2},\n+            {\"type\": \"text\", \"text\": \"What do you see in this video?\"}\n+        ],\n+    },\n+]\n+```\n+\n Pass `messages` to [`~ProcessorMixin.apply_chat_template`] to tokenize the input content. There are a few extra parameters to include in [`~ProcessorMixin.apply_chat_template`] that controls the sampling process.\n \n The `video_load_backend` parameter refers to a specific framework to load a video. It supports [PyAV](https://pyav.basswood-io.com/docs/stable/), [Decord](https://github.com/dmlc/decord), [OpenCV](https://github.com/opencv/opencv), and [torchvision](https://pytorch.org/vision/stable/index.html)."
        },
        {
            "sha": "367beccf5152707f848ae7abb6517b587e2f8161",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -31,6 +31,8 @@\n import typing_extensions\n from huggingface_hub.errors import EntryNotFoundError\n \n+from transformers.utils import is_torch_available\n+\n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n@@ -42,6 +44,7 @@\n if is_vision_available():\n     from .image_utils import PILImageResampling\n \n+\n from .tokenization_utils_base import (\n     PaddingStrategy,\n     PreTokenizedInput,\n@@ -63,7 +66,6 @@\n     download_url,\n     is_offline_mode,\n     is_remote_url,\n-    is_torch_available,\n     list_repo_templates,\n     logging,\n )\n@@ -1559,15 +1561,16 @@ def apply_chat_template(\n \n                     for fname in video_fnames:\n                         if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n+                            # Case a: Video is provided as a list of image file names\n                             video = [np.array(load_image(image_fname)) for image_fname in fname]\n-                            # create a 4D video because `load_video` always returns a 4D array\n                             video = np.stack(video)\n                             metadata = None\n                             logger.warning(\n                                 \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n                                 \"If your model requires metadata during processing, please load the whole video and let the processor sample frames instead.\"\n                             )\n                         else:\n+                            # Case b: Video is provided as a single file path or URL or decoded frames in a np.ndarray or torch.tensor\n                             video, metadata = load_video(\n                                 fname,\n                                 backend=mm_load_kwargs[\"video_load_backend\"],"
        },
        {
            "sha": "3e55bdc21b7a848adde22871d2bc111a80861c6a",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -563,6 +563,14 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n \n         sample_indices_fn = sample_indices_fn_func\n \n+    if is_valid_image(video) or (isinstance(video, (list, tuple)) and is_valid_image(video[0])):\n+        # Case 1: Video is provided as a 4D numpy array or torch tensor (frames, height, width, channels)\n+        if not is_valid_video(video):\n+            raise ValueError(\n+                f\"When passing video as decoded frames, video should be a 4D numpy array or torch tensor, but got {video.ndim} dimensions instead.\"\n+            )\n+        return video, None\n+\n     if urlparse(video).netloc in [\"www.youtube.com\", \"youtube.com\"]:\n         if not is_yt_dlp_available():\n             raise ImportError(\"To load a video from YouTube url you have  to install `yt_dlp` first.\")\n@@ -579,8 +587,6 @@ def sample_indices_fn_func(metadata, **fn_kwargs):\n         file_obj = BytesIO(requests.get(video).content)\n     elif os.path.isfile(video):\n         file_obj = video\n-    elif is_valid_image(video) or (isinstance(video, (list, tuple)) and is_valid_image(video[0])):\n-        file_obj = None\n     else:\n         raise TypeError(\"Incorrect format used for video. Should be an url linking to an video or a local path.\")\n "
        },
        {
            "sha": "d192a6c049eae0efea9aa07434b64f1fc2644205",
            "filename": "tests/models/internvl/test_processor_internvl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -267,7 +267,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 2)\n \n     @require_av\n-    @parameterized.expand([(1, \"pt\"), (2, \"pt\")])\n+    @parameterized.expand([(1, \"pt\"), (2, \"pt\"), (3, \"pt\")])\n     def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n@@ -340,7 +340,12 @@ def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 2 if batch_size == 1 else 3  # InternVL patches out and removes frames after processing\n+        # InternVL internally collects frames from all the videos in a batch and flattens the batch dimension (B T C H W) -> (B*T C H W) then patches and removes the frames\n+        # hence output length does not equal batch size\n+        # removed hardcoded video length check video_len = 2 if batch_size == 1 else 3\n+        # from experiment video_len looks like batch_size + 1\n+        # TODO: update expected video_len calculation based on the internal processing logic of InternVLProcessor\n+        video_len = batch_size + 1\n         self.assertEqual(len(out_dict[self.videos_input_name]), video_len)\n         for k in out_dict:\n             self.assertIsInstance(out_dict[k], torch.Tensor)"
        },
        {
            "sha": "8295084b78246627091390934c44e7f0056cd33e",
            "filename": "tests/models/qwen2_5_omni/test_processor_qwen2_5_omni.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processor_qwen2_5_omni.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -422,8 +422,14 @@ def _test_apply_chat_template(\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 2880 if batch_size == 1 else 5808  # qwen pixels don't scale with bs same way as other models\n-        mm_len = batch_size * 1564 if modality == \"image\" else video_len\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 1564\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n         return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}"
        },
        {
            "sha": "5b8a3beb1dc425606acf6d58866779771bd9e0b0",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -239,8 +239,14 @@ def _test_apply_chat_template(\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n-        video_len = 180 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n-        mm_len = batch_size * 192 if modality == \"image\" else video_len\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 192\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n         return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}"
        },
        {
            "sha": "ccc9b3834fd4e86fba5e47998523f0817cde2cc3",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -239,9 +239,14 @@ def _test_apply_chat_template(\n         self.assertTrue(input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n-\n-        video_len = 180 if batch_size == 1 else 320  # qwen pixels don't scale with bs same way as other models\n-        mm_len = batch_size * 192 if modality == \"image\" else video_len\n+        if modality == \"video\":\n+            # qwen pixels don't scale with bs same way as other models, calculate expected video token count based on video_grid_thw\n+            expected_video_token_count = 0\n+            for thw in out_dict[\"video_grid_thw\"]:\n+                expected_video_token_count += thw[0] * thw[1] * thw[2]\n+            mm_len = expected_video_token_count\n+        else:\n+            mm_len = batch_size * 192\n         self.assertEqual(len(out_dict[input_name]), mm_len)\n \n         return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}"
        },
        {
            "sha": "9a75575bd9077e95eabc05302a2462e8af9a7f19",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -596,3 +596,9 @@ def test_special_mm_token_truncation(self):\n     @unittest.skip(\"SmolVLM cannot accept image URL as video frames, because it needs to know video fps and duration\")\n     def test_apply_chat_template_video_1(self):\n         pass\n+\n+    @unittest.skip(\n+        \"SmolVLM cannot accept list of decoded video frames, because it needs to know video fps and duration\"\n+    )\n+    def test_apply_chat_template_video_2(self):\n+        pass"
        },
        {
            "sha": "6abc3de1c689f1ffd0959b9d2d605e4a37bfa1d9",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2a9febd632dde21d3f873ebce176d2ac4edc24b8/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=2a9febd632dde21d3f873ebce176d2ac4edc24b8",
            "patch": "@@ -33,7 +33,7 @@\n     require_torch,\n     require_vision,\n )\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_av_available, is_torch_available, is_vision_available\n \n \n global_rng = random.Random()\n@@ -44,7 +44,6 @@\n if is_torch_available():\n     import torch\n \n-\n MODALITY_INPUT_DATA = {\n     \"images\": [\n         \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n@@ -60,6 +59,13 @@\n     ],\n }\n \n+if is_av_available():\n+    from transformers.video_utils import load_video\n+\n+    # load a video file in memory for testing\n+    video, _ = load_video(\"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\")\n+    MODALITY_INPUT_DATA[\"videos\"].append(video)\n+\n \n def prepare_image_inputs():\n     \"\"\"This function prepares a list of PIL images\"\"\"\n@@ -931,7 +937,7 @@ def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n         )\n \n     @require_av\n-    @parameterized.expand([(1, \"pt\"), (2, \"pt\")])  # video processor supports only torchvision\n+    @parameterized.expand([(1, \"pt\"), (2, \"pt\"), (3, \"pt\")])  # video processor supports only torchvision\n     def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n         self._test_apply_chat_template(\n             \"video\", batch_size, return_tensors, \"videos_input_name\", \"video_processor\", MODALITY_INPUT_DATA[\"videos\"]"
        }
    ],
    "stats": {
        "total": 122,
        "additions": 106,
        "deletions": 16
    }
}