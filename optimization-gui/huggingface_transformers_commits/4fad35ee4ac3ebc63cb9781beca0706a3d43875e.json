{
    "author": "NielsRogge",
    "message": "[VisionEncoderDecoderModel] Update loss function (#40863)\n\nUpdate loss function",
    "sha": "4fad35ee4ac3ebc63cb9781beca0706a3d43875e",
    "files": [
        {
            "sha": "c20080d532660a3e4bcc080b1183b1d1af3c81fb",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fad35ee4ac3ebc63cb9781beca0706a3d43875e/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fad35ee4ac3ebc63cb9781beca0706a3d43875e/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=4fad35ee4ac3ebc63cb9781beca0706a3d43875e",
            "patch": "@@ -18,6 +18,7 @@\n \n import torch\n from torch import nn\n+from torch.nn import CrossEntropyLoss\n \n from ...cache_utils import Cache\n from ...configuration_utils import PreTrainedConfig\n@@ -374,9 +375,6 @@ def forward(\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        # num_items_in_batch is only needed for loss computation\n-        num_items_in_batch = kwargs.pop(\"num_items_in_batch\", None)\n-\n         kwargs_encoder = {argument: value for argument, value in kwargs.items() if not argument.startswith(\"decoder_\")}\n \n         kwargs_decoder = {\n@@ -435,12 +433,8 @@ def forward(\n         if labels is not None:\n             logits = decoder_outputs.logits if return_dict else decoder_outputs[0]\n \n-            loss = self.loss_function(\n-                logits=logits,\n-                labels=labels,\n-                vocab_size=self.decoder.config.vocab_size,\n-                num_items_in_batch=num_items_in_batch,\n-            )\n+            loss_fct = CrossEntropyLoss()\n+            loss = loss_fct(logits.reshape(-1, self.decoder.config.vocab_size), labels.reshape(-1))\n \n         if not return_dict:\n             if loss is not None:"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 3,
        "deletions": 9
    }
}