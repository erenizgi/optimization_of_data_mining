{
    "author": "Cyrilvallez",
    "message": "[TP plans] Fix some incorrects TP plans (#42448)\n\n* gemma3\n\n* qwen3 and modulars\n\n* fix tp plans\n\n---------\n\nCo-authored-by: vasqu <antonprogamer@gmail.com>",
    "sha": "6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
    "files": [
        {
            "sha": "e01e4a10fc73aefb9cb2310ef4400141c433f0e7",
            "filename": "src/transformers/models/apertus/configuration_apertus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fconfiguration_apertus.py?ref=6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
            "patch": "@@ -101,10 +101,10 @@ class ApertusConfig(PreTrainedConfig):\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     default_theta = 12000000.0\n     base_model_tp_plan = {\n-        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }"
        },
        {
            "sha": "1d7306b2c827be6b202fbb2027a89516d7699840",
            "filename": "src/transformers/models/apertus/modular_apertus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodular_apertus.py?ref=6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
            "patch": "@@ -119,10 +119,10 @@ class ApertusConfig(PreTrainedConfig):\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     default_theta = 12000000.0\n     base_model_tp_plan = {\n-        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }"
        },
        {
            "sha": "415544d4680b6138f606413ab9d3166cb722f817",
            "filename": "src/transformers/models/doge/configuration_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fconfiguration_doge.py?ref=6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
            "patch": "@@ -117,11 +117,6 @@ class DogeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.dt_proj\": \"rowwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.input_residual\": \"sequence_parallel\",\n-        \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.post_attention_residual\": \"sequence_parallel\",\n-        \"norm.weight\": \"sequence_parallel\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "b0688b95be4ce39c6d80c0a5146fde5a5075621d",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
            "patch": "@@ -146,11 +146,6 @@ class DogeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.dt_proj\": \"rowwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.input_residual\": \"sequence_parallel\",\n-        \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.post_attention_residual\": \"sequence_parallel\",\n-        \"norm.weight\": \"sequence_parallel\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "2e8a49f64a5a672fc98f3adf933cd20f6668184e",
            "filename": "src/transformers/models/nanochat/configuration_nanochat.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e3f2f8f52f29e7080e5f286f92368dde30d6f83/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconfiguration_nanochat.py?ref=6e3f2f8f52f29e7080e5f286f92368dde30d6f83",
            "patch": "@@ -94,10 +94,10 @@ class NanoChatConfig(PretrainedConfig):\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     base_model_tp_plan = {\n-        \"layers.*.self_attn.q_proj\": \"colwise_rep\",\n-        \"layers.*.self_attn.k_proj\": \"colwise_rep\",\n-        \"layers.*.self_attn.v_proj\": \"colwise_rep\",\n-        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.fc1\": \"colwise\",\n         \"layers.*.mlp.fc2\": \"rowwise\",\n     }"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 12,
        "deletions": 22
    }
}