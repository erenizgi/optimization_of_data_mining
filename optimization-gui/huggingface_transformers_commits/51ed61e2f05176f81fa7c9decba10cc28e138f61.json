{
    "author": "NouamaneTazi",
    "message": "Mention UltraScale Playbook ðŸŒŒ in docs (#36589)",
    "sha": "51ed61e2f05176f81fa7c9decba10cc28e138f61",
    "files": [
        {
            "sha": "347db1a3f0b8cef94e6334f681721ba1edd8c49f",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/51ed61e2f05176f81fa7c9decba10cc28e138f61/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/51ed61e2f05176f81fa7c9decba10cc28e138f61/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=51ed61e2f05176f81fa7c9decba10cc28e138f61",
            "patch": "@@ -19,6 +19,8 @@ Multi-GPU setups are effective for accelerating training and fitting large model\n \n This guide will discuss the various parallelism methods, combining them, and choosing an appropriate strategy for your setup. For more details about distributed training, refer to the [Accelerate](https://hf.co/docs/accelerate/index) documentation.\n \n+For a comprehensive guide on scaling large language models, check out the [Ultrascale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook), which provides detailed strategies and best practices for training at scale.\n+\n ## Scalability strategy\n \n Use the [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/model-memory-usage) to calculate how much memory a model requires. Then refer to the table below to select a strategy based on your setup."
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}