{
    "author": "Rocketknight1",
    "message": "Add a doc section on writing generation prompts (#34248)\n\nAdd a section on writing generation prompts",
    "sha": "f701b98e4a10d3b882834308f2f0c9398b19d343",
    "files": [
        {
            "sha": "1bdf05a26c8d08f9e9286ff713450a73b9f1cc7f",
            "filename": "docs/source/en/chat_templating.md",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f701b98e4a10d3b882834308f2f0c9398b19d343/docs%2Fsource%2Fen%2Fchat_templating.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f701b98e4a10d3b882834308f2f0c9398b19d343/docs%2Fsource%2Fen%2Fchat_templating.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating.md?ref=f701b98e4a10d3b882834308f2f0c9398b19d343",
            "patch": "@@ -943,6 +943,35 @@ all implementations of Jinja:\n - Directly rendering a dict or list may give different results in other implementations (for example, string entries\n   might change from single-quoted to double-quoted). Adding the `tojson` filter can help to ensure consistency here.\n \n+### Writing generation prompts\n+\n+We mentioned above that `add_generation_prompt` is a special variable that will be accessible inside your template,\n+and is controlled by the user setting the `add_generation_prompt` flag. If your model expects a header for\n+assistant messages, then your template must support adding the header when `add_generation_prompt` is set.\n+\n+Here is an example of a template that formats messages ChatML-style, with generation prompt support:\n+\n+```text\n+{{- bos_token }}\n+{%- for message in messages %}\n+    {{- '<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n' }}\n+{%- endfor %}\n+{%- if add_generation_prompt %}\n+    {{- '<|im_start|>assistant\\n' }}\n+{%- endif %}\n+```\n+\n+The exact content of the assistant header will depend on your specific model, but it should always be **the string\n+that represents the start of an assistant message**, so that if the user applies your template with \n+`add_generation_prompt=True` and then generates text, the model will write an assistant response. Also note that some\n+models do not need a generation prompt, because assistant messages always begin immediately after user messages. \n+This is particularly common for LLaMA and Mistral models, where assistant messages begin immediately after the `[/INST]`\n+token that ends user messages. In these cases, the template can ignore the `add_generation_prompt` flag.\n+\n+Generation prompts are important! If your model requires a generation prompt but it is not set in the template, then\n+model generations will likely be severely degraded, or the model may display unusual behaviour like continuing \n+the final user message! \n+\n ### Writing and debugging larger templates\n \n When this feature was introduced, most templates were quite small, the Jinja equivalent of a \"one-liner\" script. "
        }
    ],
    "stats": {
        "total": 29,
        "additions": 29,
        "deletions": 0
    }
}