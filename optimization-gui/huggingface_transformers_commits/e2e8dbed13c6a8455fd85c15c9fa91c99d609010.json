{
    "author": "ydshieh",
    "message": "CI workflow for Flash Attn (#41857)\n\nci for flash attn\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
    "files": [
        {
            "sha": "9c946d7974a1fcd085e5fdcd9863e3b3528d47e7",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
            "patch": "@@ -28,6 +28,9 @@ on:\n       report_repo_id:\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n \n env:\n   HF_HOME: /mnt/cache\n@@ -137,7 +140,7 @@ jobs:\n       - name: Run all tests on GPU\n         working-directory: /transformers\n         run: |\n-          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n+          script -q -c \"PATCH_TESTING_METHODS_TO_COLLECT_OUTPUTS=yes _PATCHED_TESTING_METHODS_OUTPUT_DIR=/transformers/reports/${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports python3 -m pytest -rsfE -v -m '${{ inputs.pytest_marker }}' --make-reports=${{ env.machine_type }}_${{ inputs.report_name_prefix }}_${{ env.matrix_folders }}_test_reports tests/${{ matrix.folders }}\" test_outputs.txt\n           ls -la\n           # Extract the exit code from the output file\n           EXIT_CODE=$(tail -1 test_outputs.txt | grep -o 'COMMAND_EXIT_CODE=\"[0-9]*\"' | cut -d'\"' -f2)"
        },
        {
            "sha": "5dc18f609b74ec015daaa28c5ec1d7d0dbab387b",
            "filename": ".github/workflows/self-scheduled-flash-attn-caller.yml",
            "status": "added",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fself-scheduled-flash-attn-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fself-scheduled-flash-attn-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-flash-attn-caller.yml?ref=e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
            "patch": "@@ -0,0 +1,60 @@\n+name: Nvidia CI - Flash Attn\n+\n+on:\n+  repository_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n+  push:\n+    branches:\n+      - run_nvidia_ci_flash_attn*\n+  workflow_dispatch:\n+    inputs:\n+      prev_workflow_run_id:\n+        description: 'previous workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+      other_workflow_run_id:\n+        description: 'other workflow run id to compare'\n+        type: string\n+        required: false\n+        default: \"\"\n+\n+\n+# Used for `push` to easily modify the target workflow runs to compare against\n+env:\n+    prev_workflow_run_id: \"\"\n+    other_workflow_run_id: \"\"\n+\n+\n+jobs:\n+  setup:\n+    name: Setup\n+    runs-on: ubuntu-22.04\n+    steps:\n+      - name: Setup\n+        run: |\n+          mkdir \"setup_values\"\n+          echo \"${{ inputs.prev_workflow_run_id || env.prev_workflow_run_id }}\" > \"setup_values/prev_workflow_run_id.txt\"\n+          echo \"${{ inputs.other_workflow_run_id || env.other_workflow_run_id }}\" > \"setup_values/other_workflow_run_id.txt\"\n+\n+      - name: Upload artifacts\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: setup_values\n+          path: setup_values\n+\n+\n+  model-ci:\n+    name: Model CI\n+    uses: ./.github/workflows/self-scheduled.yml\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-flash-attn\"\n+      docker: huggingface/transformers-all-latest-gpu\n+      ci_event: Daily CI\n+      runner_type: \"a10\"\n+      report_repo_id: hf-internal-testing/transformers_flash_attn_ci\n+      commit_sha: ${{ github.sha }}\n+      pytest_marker: \"flash_attn_test or flash_attn_3_test\"\n+    secrets: inherit"
        },
        {
            "sha": "d499f2f65aa2ddb9de072c498fadc3c90bae2e59",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
            "patch": "@@ -38,6 +38,10 @@ on:\n         default: \"\"\n         required: false\n         type: string\n+      pytest_marker:\n+        required: false\n+        type: string\n+\n \n env:\n   HF_HOME: /mnt/cache\n@@ -127,6 +131,7 @@ jobs:\n       commit_sha: ${{ inputs.commit_sha || github.sha }}\n       runner_type: ${{ inputs.runner_type }}\n       report_repo_id: ${{ inputs.report_repo_id }}\n+      pytest_marker: ${{ inputs.pytest_marker }}\n     secrets: inherit\n \n   run_trainer_and_fsdp_gpu:"
        },
        {
            "sha": "5ef297f7913c9c41f13b89a543123e154c25f382",
            "filename": "utils/notification_service.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/utils%2Fnotification_service.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2e8dbed13c6a8455fd85c15c9fa91c99d609010/utils%2Fnotification_service.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnotification_service.py?ref=e2e8dbed13c6a8455fd85c15c9fa91c99d609010",
            "patch": "@@ -1407,7 +1407,10 @@ def pop_default(l: list[Any], i: int, default: Any) -> Any:\n     if not os.path.isdir(os.path.join(os.getcwd(), f\"ci_results_{job_name}\")):\n         os.makedirs(os.path.join(os.getcwd(), f\"ci_results_{job_name}\"))\n \n-    nvidia_daily_ci_workflow = \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\"\n+    nvidia_daily_ci_workflow = (\n+        \"huggingface/transformers/.github/workflows/self-scheduled-caller.yml\",\n+        \"huggingface/transformers/.github/workflows/self-scheduled-flash-attn-caller.yml\",\n+    )\n     amd_daily_ci_workflows = (\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi325-caller.yml\",\n         \"huggingface/transformers/.github/workflows/self-scheduled-amd-mi355-caller.yml\","
        }
    ],
    "stats": {
        "total": 75,
        "additions": 73,
        "deletions": 2
    }
}