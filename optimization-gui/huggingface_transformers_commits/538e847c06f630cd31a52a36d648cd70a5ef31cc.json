{
    "author": "miniMaddy",
    "message": "Updated Zoedepth model card (#37898)\n\n* Edited zoedepth model card according to specifications.\n\n* Edited Zoedepth model file\n\n* made suggested changes.",
    "sha": "538e847c06f630cd31a52a36d648cd70a5ef31cc",
    "files": [
        {
            "sha": "59bc483d8cf8e25e805b23ee906d5f8c6a1951b6",
            "filename": "docs/source/en/model_doc/zoedepth.md",
            "status": "modified",
            "additions": 82,
            "deletions": 81,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/538e847c06f630cd31a52a36d648cd70a5ef31cc/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/538e847c06f630cd31a52a36d648cd70a5ef31cc/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fzoedepth.md?ref=538e847c06f630cd31a52a36d648cd70a5ef31cc",
            "patch": "@@ -14,100 +14,101 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# ZoeDepth\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The ZoeDepth model was proposed in [ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth](https://arxiv.org/abs/2302.12288) by Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, Matthias MÃ¼ller. ZoeDepth extends the [DPT](dpt) framework for metric (also called absolute) depth estimation. ZoeDepth is pre-trained on 12 datasets using relative depth and fine-tuned on two domains (NYU and KITTI) using metric depth. A lightweight head is used with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier.\n-\n-The abstract from the paper is the following:\n+# ZoeDepth\n \n-*This paper tackles the problem of depth estimation from a single image. Existing work either focuses on generalization performance disregarding metric scale, i.e. relative depth estimation, or state-of-the-art results on specific datasets, i.e. metric depth estimation. We propose the first approach that combines both worlds, leading to a model with excellent generalization performance while maintaining metric scale. Our flagship model, ZoeD-M12-NK, is pre-trained on 12 datasets using relative depth and fine-tuned on two datasets using metric depth. We use a lightweight head with a novel bin adjustment design called metric bins module for each domain. During inference, each input image is automatically routed to the appropriate head using a latent classifier. Our framework admits multiple configurations depending on the datasets used for relative depth pre-training and metric fine-tuning. Without pre-training, we can already significantly improve the state of the art (SOTA) on the NYU Depth v2 indoor dataset. Pre-training on twelve datasets and fine-tuning on the NYU Depth v2 indoor dataset, we can further improve SOTA for a total of 21% in terms of relative absolute error (REL). Finally, ZoeD-M12-NK is the first model that can jointly train on multiple datasets (NYU Depth v2 and KITTI) without a significant drop in performance and achieve unprecedented zero-shot generalization performance to eight unseen datasets from both indoor and outdoor domains.*\n+[ZoeDepth](https://huggingface.co/papers/2302.12288) is a depth estimation model that combines the generalization performance of relative depth estimation (how far objects are from each other) and metric depth estimation (precise depth measurement on metric scale) from a single image. It is pre-trained on 12 datasets using relative depth and 2 datasets (NYU Depth v2 and KITTI) for metric accuracy. A lightweight head with a metric bin module for each domain is used, and during inference, it automatically selects the appropriate head for each input image with a latent classifier.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/zoedepth_architecture_bis.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> ZoeDepth architecture. Taken from the <a href=\"https://arxiv.org/abs/2302.12288\">original paper.</a> </small>\n-\n-This model was contributed by [nielsr](https://huggingface.co/nielsr).\n-The original code can be found [here](https://github.com/isl-org/ZoeDepth).\n-\n-## Usage tips\n-\n-- ZoeDepth is an absolute (also called metric) depth estimation model, unlike DPT which is a relative depth estimation model. This means that ZoeDepth is able to estimate depth in metric units like meters.\n-\n-The easiest to perform inference with ZoeDepth is by leveraging the [pipeline API](../main_classes/pipelines.md):\n-\n-```python\n->>> from transformers import pipeline\n->>> from PIL import Image\n->>> import requests\n-\n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n-\n->>> pipe = pipeline(task=\"depth-estimation\", model=\"Intel/zoedepth-nyu-kitti\")\n->>> result = pipe(image)\n->>> depth = result[\"depth\"]\n+You can find all the original ZoeDepth checkpoints under the [Intel](https://huggingface.co/Intel?search=zoedepth) organization.\n+\n+The example below demonstrates how to estimate depth with [`Pipeline`] or the [`AutoModel`] class.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import requests\n+import torch\n+from transformers import pipeline\n+from PIL import Image\n+\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+pipeline = pipeline(\n+    task=\"depth-estimation\",\n+    model=\"Intel/zoedepth-nyu-kitti\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+results = pipeline(image)\n+results[\"depth\"]\n ```\n \n-Alternatively, one can also perform inference using the classes:\n-\n-```python\n->>> from transformers import AutoImageProcessor, ZoeDepthForDepthEstimation\n->>> import torch\n->>> import numpy as np\n->>> from PIL import Image\n->>> import requests\n-\n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n->>> model = ZoeDepthForDepthEstimation.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n-\n->>> # prepare image for the model\n->>> inputs = image_processor(images=image, return_tensors=\"pt\")\n-\n->>> with torch.no_grad():   \n-...     outputs = model(inputs)\n-\n->>> # interpolate to original size and visualize the prediction\n->>> ## ZoeDepth dynamically pads the input image. Thus we pass the original image size as argument\n->>> ## to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n->>> post_processed_output = image_processor.post_process_depth_estimation(\n-...     outputs,\n-...     source_sizes=[(image.height, image.width)],\n-... )\n-\n->>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n->>> depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n->>> depth = depth.detach().cpu().numpy() * 255\n->>> depth = Image.fromarray(depth.astype(\"uint8\"))\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoModelForDepthEstimation, AutoImageProcessor\n+\n+image_processor = AutoImageProcessor.from_pretrained(\n+    \"Intel/zoedepth-nyu-kitti\"\n+)\n+model = AutoModelForDepthEstimation.from_pretrained(\n+    \"Intel/zoedepth-nyu-kitti\",\n+    device_map=\"auto\"\n+)\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+inputs = image_processor(image, return_tensors=\"pt\").to(\"cuda\")\n+\n+with torch.no_grad():\n+  outputs = model(inputs)\n+\n+# interpolate to original size and visualize the prediction\n+## ZoeDepth dynamically pads the input image, so pass the original image size as argument\n+## to `post_process_depth_estimation` to remove the padding and resize to original dimensions.\n+post_processed_output = image_processor.post_process_depth_estimation(\n+    outputs,\n+    source_sizes=[(image.height, image.width)],\n+)\n+\n+predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n+depth = depth.detach().cpu().numpy() * 255\n+Image.fromarray(depth.astype(\"uint8\"))\n ```\n \n-<Tip>\n-<p>In the <a href=\"https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131\">original implementation</a> ZoeDepth model performs inference on both the original and flipped images and averages out the results. The <code>post_process_depth_estimation</code> function can handle this for us by passing the flipped outputs to the optional <code>outputs_flipped</code> argument:</p>\n-<pre><code class=\"language-Python\">&gt;&gt;&gt; with torch.no_grad():   \n-...     outputs = model(pixel_values)\n-...     outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n-&gt;&gt;&gt; post_processed_output = image_processor.post_process_depth_estimation(\n-...     outputs,\n-...     source_sizes=[(image.height, image.width)],\n-...     outputs_flipped=outputs_flipped,\n-... )\n-</code></pre>\n-</Tip>\n-\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+- In the [original implementation](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131) ZoeDepth performs inference on both the original and flipped images and averages the results. The `post_process_depth_estimation` function handles this by passing the flipped outputs to the optional `outputs_flipped` argument as shown below.\n+   ```py\n+    with torch.no_grad():\n+        outputs = model(pixel_values)\n+        outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n+        post_processed_output = image_processor.post_process_depth_estimation(\n+            outputs,\n+            source_sizes=[(image.height, image.width)],\n+            outputs_flipped=outputs_flipped,\n+        )\n+   ```\n+   \n ## Resources\n-\n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with ZoeDepth.\n-\n-- A demo notebook regarding inference with ZoeDepth models can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ZoeDepth). ðŸŒŽ\n+- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/ZoeDepth) for an inference example.\n \n ## ZoeDepthConfig\n "
        }
    ],
    "stats": {
        "total": 163,
        "additions": 82,
        "deletions": 81
    }
}