{
    "author": "co63oc",
    "message": "chore: Fix typos in docs and examples (#36524)\n\nFix typos in docs and examples\n\nSigned-off-by: co63oc <co63oc@users.noreply.github.com>",
    "sha": "37508816d650a8074bc31c761e20872c2e5eaec4",
    "files": [
        {
            "sha": "29f50184ec3d119653b90fd5af4a1b2a1b7fd493",
            "filename": "awesome-transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/awesome-transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/awesome-transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/awesome-transformers.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -47,7 +47,7 @@ Keywords: LLMs, Large Language Models, Agents, Chains\n \n ## [LlamaIndex](https://github.com/run-llama/llama_index)\n \n-[LlamaIndex](https://github.com/run-llama/llama_index) is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retreival mechanisms to perform different LLM tasks and obtain knowledge-augmented results.\n+[LlamaIndex](https://github.com/run-llama/llama_index) is a project that provides a central interface to connect your LLM's with external data. It provides various kinds of indices and retrieval mechanisms to perform different LLM tasks and obtain knowledge-augmented results.\n \n Keywords: LLMs, Large Language Models, Data Retrieval, Indices, Knowledge Augmentation \n "
        },
        {
            "sha": "6f8e5499ba47867257b60688a329b53d7d7c984b",
            "filename": "docs/source/en/add_new_model.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fadd_new_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fadd_new_model.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -57,7 +57,7 @@ There is never more than two levels of abstraction for any model to keep the cod\n \n Other important functions like the forward method are defined in the `modeling.py` file.\n \n-Specific model heads (for example, sequence classification or language modeling) should call the base model in the forward pass rather than inherting from it to keep abstraction low.\n+Specific model heads (for example, sequence classification or language modeling) should call the base model in the forward pass rather than inheriting from it to keep abstraction low.\n \n New models require a configuration, for example `BrandNewLlamaConfig`, that is stored as an attribute of [`PreTrainedModel`].\n \n@@ -233,7 +233,7 @@ If you run into issues, you'll need to choose one of the following debugging str\n This strategy relies on breaking the original model into smaller sub-components, such as when the code can be easily run in eager mode. While more difficult, there are some advantages to this approach.\n \n 1. It is easier later to compare the original model to your implementation. You can automatically verify that each individual component matches its corresponding component in the Transformers' implementation. This is better than relying on a visual comparison based on print statements.\n-2. It is easier to port individal components instead of the entire model.\n+2. It is easier to port individual components instead of the entire model.\n 3. It is easier for understanding how a model works by breaking it up into smaller parts.\n 4. It is easier to prevent regressions at a later stage when you change your code thanks to component-by-component tests.\n \n@@ -328,7 +328,7 @@ def _init_weights(self, module):\n \n The initialization scheme can look different if you need to adapt it to your model. For example, [`Wav2Vec2ForPreTraining`] initializes [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) in its last two linear layers.\n \n-The `_is_hf_initialized` flag makes sure the submodule is only initialized once. Setting `module.project_q` and `module.project_hid` to `True` ensures the custom initialization is not overriden later. The `_init_weights` function won't be applied to these modules.\n+The `_is_hf_initialized` flag makes sure the submodule is only initialized once. Setting `module.project_q` and `module.project_hid` to `True` ensures the custom initialization is not overridden later. The `_init_weights` function won't be applied to these modules.\n \n ```py\n def _init_weights(self, module):\n@@ -457,7 +457,7 @@ Don't be discouraged if your forward pass isn't identical with the output from t\n Your output should have a precision of *1e-3*. Ensure the output shapes and output values are identical. Common reasons for why the outputs aren't identical include:\n \n - Some layers were not added (activation layer or a residual connection).\n-- The word embedding matix is not tied.\n+- The word embedding matrix is not tied.\n - The wrong positional embeddings are used because the original implementation includes an offset.\n - Dropout is applied during the forward pass. Fix this error by making sure `model.training` is `False` and passing `self.training` to [torch.nn.functional.dropout](https://pytorch.org/docs/stable/nn.functional.html?highlight=dropout#torch.nn.functional.dropout).\n "
        },
        {
            "sha": "bd24d8ce30cc7e0699aae68e88498f0b834baa8f",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -159,7 +159,7 @@ Here are a few examples using notional tools:\n ---\n {examples}\n \n-Above example were using notional tools that might not exist for you. You only have acces to those tools:\n+Above example were using notional tools that might not exist for you. You only have access to those tools:\n <<tool_names>>\n You also can perform computations in the python code you generate.\n "
        },
        {
            "sha": "4d1df98e50a225537228d450ce1e6ebee40bbdcf",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -840,7 +840,7 @@ Unless you have a lot of free CPU memory, fp32 weights shouldn't be saved during\n <hfoptions id=\"save\">\n <hfoption id=\"offline\">\n \n-DeepSpeed provies a [zero_to_fp32.py](https://github.com/microsoft/DeepSpeed/blob/91829476a8fd4d0d9268c03c1d56795d20a51c12/deepspeed/utils/zero_to_fp32.py#L14) script at the top-level checkpoint folder for extracting weights at any point. This is a standalone script and you don't need a config file or [`Trainer`].\n+DeepSpeed provides a [zero_to_fp32.py](https://github.com/microsoft/DeepSpeed/blob/91829476a8fd4d0d9268c03c1d56795d20a51c12/deepspeed/utils/zero_to_fp32.py#L14) script at the top-level checkpoint folder for extracting weights at any point. This is a standalone script and you don't need a config file or [`Trainer`].\n \n For example, if your checkpoint folder looks like the one shown below, then you can run the following command to create and consolidate the fp32 weights from multiple GPUs into a single `pytorch_model.bin` file. The script automatically discovers the subfolder `global_step1` which contains the checkpoint.\n \n@@ -942,7 +942,7 @@ import deepspeed\n ds_config = {...}\n # must run before instantiating the model to detect zero 3\n dschf = HfDeepSpeedConfig(ds_config)  # keep this object alive\n-# randomly intialize model weights\n+# randomly initialize model weights\n config = AutoConfig.from_pretrained(\"openai-community/gpt2\")\n model = AutoModel.from_config(config)\n engine = deepspeed.initialize(model=model, config_params=ds_config, ...)"
        },
        {
            "sha": "19ac987807261da527b9de5358fd007fe474087e",
            "filename": "docs/source/en/generation_features.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fgeneration_features.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fgeneration_features.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_features.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -50,7 +50,7 @@ The `streamer` parameter is compatible with any class with a [`~TextStreamer.put\n \n Watermarking is useful for detecting whether text is generated. The [watermarking strategy](https://hf.co/papers/2306.04634) in Transformers randomly \"colors\" a subset of the tokens green. When green tokens are generated, they have a small bias added to their logits, and a higher probability of being generated. You can detect generated text by comparing the proportion of green tokens to the amount of green tokens typically found in human-generated text.\n \n-Watermarking is supported for any generative model in Transformers and doesn't require an extra classfication model to detect the watermarked text.\n+Watermarking is supported for any generative model in Transformers and doesn't require an extra classification model to detect the watermarked text.\n \n Create a [`WatermarkingConfig`] with the bias value to add to the logits and watermarking algorithm. The example below uses the `\"selfhash\"` algorithm, where the green token selection only depends on the current token. Pass the [`WatermarkingConfig`] to [`~GenerationMixin.generate`].\n "
        },
        {
            "sha": "d86765720241d08805b8f3fd49b9a45fadb83726",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -87,7 +87,7 @@ You can customize [`~GenerationMixin.generate`] by overriding the parameters and\n model.generate(**inputs, num_beams=4, do_sample=True)\n ```\n \n-[`~GenerationMixin.generate`] can also be extended with external libraries or custom code. The `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manupulating the next token probability distribution. `stopping_criteria` supports custom [`StoppingCriteria`] to stop text generation. Check out the [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo) for more examples of external [`~GenerationMixin.generate`]-compatible extensions.\n+[`~GenerationMixin.generate`] can also be extended with external libraries or custom code. The `logits_processor` parameter accepts custom [`LogitsProcessor`] instances for manipulating the next token probability distribution. `stopping_criteria` supports custom [`StoppingCriteria`] to stop text generation. Check out the [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo) for more examples of external [`~GenerationMixin.generate`]-compatible extensions.\n \n Refer to the [Generation strategies](./generation_strategies) guide to learn more about search, sampling, and decoding strategies.\n "
        },
        {
            "sha": "bc65ea79655ff1ac96bb9d79a0990dc2441a4138",
            "filename": "docs/source/en/model_doc/speech_to_text.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fspeech_to_text.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -74,7 +74,7 @@ be installed as follows: `apt install libsndfile1-dev`\n   For multilingual speech translation models, `eos_token_id` is used as the `decoder_start_token_id` and\n   the target language id is forced as the first generated token. To force the target language id as the first\n   generated token, pass the `forced_bos_token_id` parameter to the `generate()` method. The following\n-  example shows how to transate English speech to French text using the *facebook/s2t-medium-mustc-multilingual-st*\n+  example shows how to translate English speech to French text using the *facebook/s2t-medium-mustc-multilingual-st*\n   checkpoint.\n \n ```python"
        },
        {
            "sha": "cadb6e71f07408c98ac676556ddc6ef165cdc5ab",
            "filename": "docs/source/en/model_doc/tvp.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftvp.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -111,7 +111,7 @@ def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps\n     Returns:\n         frames (tensor): decoded frames from the video.\n     '''\n-    assert clip_idx >= -2, \"Not a valied clip_idx {}\".format(clip_idx)\n+    assert clip_idx >= -2, \"Not a valid clip_idx {}\".format(clip_idx)\n     frames, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)\n     clip_size = sampling_rate * num_frames / target_fps * fps\n     index = np.linspace(0, clip_size - 1, num_frames)"
        },
        {
            "sha": "4d79f86148443c6681a4d8298dfb61e08580f1a2",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -355,7 +355,7 @@ class Olmo2Model(OlmoModel):\n         )\n ```\n \n-You only need to change the *type* of the `self.norm` attribute to use `RMSNorm` isntead of `LayerNorm`. This change doesn't affect the logic in the forward method (layer name and usage is identical to the parent class), so you don't need to overwrite it. The linter automatically unravels it.\n+You only need to change the *type* of the `self.norm` attribute to use `RMSNorm` instead of `LayerNorm`. This change doesn't affect the logic in the forward method (layer name and usage is identical to the parent class), so you don't need to overwrite it. The linter automatically unravels it.\n \n ### Model head\n \n@@ -374,7 +374,7 @@ The logic is identical to `OlmoForCausalLM` which means you don't need to make a\n \n The [modeling_olmo2.py](https://github.com/huggingface/transformers/blob/main/src/transformers/models/olmo2/modeling_olmo2.py) generated by the linter also contains some classes (`Olmo2MLP`, `Olmo2RotaryEmbedding`, `Olmo2PreTrainedModel`) that weren't explicitly defined in `modular_olmo2.py`.\n \n-Classes that are a dependency of an inherited class but aren't explicitly defined are automatically added as a part of depdendency tracing. This is similar to how some functions were added to the `Attention` class without drrectly importing them.\n+Classes that are a dependency of an inherited class but aren't explicitly defined are automatically added as a part of dependency tracing. This is similar to how some functions were added to the `Attention` class without directly importing them.\n \n For example, `OlmoDecoderLayer` has an attribute defined as `self.mlp = OlmoMLP(config)`. This class was never explicitly redefined in `Olmo2MLP`, so the linter automatically created a `Olmo2MLP` class similar to `OlmoMLP`. It is identical to the code below if it was explicitly written in `modular_olmo2.py`.\n "
        },
        {
            "sha": "49ba739be28f73027378cfa545d902383fec1341",
            "filename": "docs/source/en/perf_hardware.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fperf_hardware.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fperf_hardware.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_hardware.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -29,7 +29,7 @@ It is important the PSU has stable voltage otherwise it may not be able to suppl\n \n ## Cooling\n \n-An overheated GPU throttles its performance and can even shutdown if it's too hot to prevent damage. Keeping the GPU temperature low, anywhere between 158 - 167F, is essential for delivering full perfomance and maintaining its lifespan. Once temperatures reach 183 - 194F, the GPU may begin to throttle performance.\n+An overheated GPU throttles its performance and can even shutdown if it's too hot to prevent damage. Keeping the GPU temperature low, anywhere between 158 - 167F, is essential for delivering full performance and maintaining its lifespan. Once temperatures reach 183 - 194F, the GPU may begin to throttle performance.\n \n ## Multi-GPU connectivity\n "
        },
        {
            "sha": "7dfd4cd63cfa2243e18c0903463e18b435af7509",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -33,7 +33,7 @@ Use the [Model Memory Calculator](https://huggingface.co/spaces/hf-accelerate/mo\n \n ## Data parallelism\n \n-Data parallelism evenly distributes data across multiple GPUs. Each GPU holds a copy of the model and concurrently proccesses their portion of the data. At the end, the results from each GPU are synchronized and combined.\n+Data parallelism evenly distributes data across multiple GPUs. Each GPU holds a copy of the model and concurrently processes their portion of the data. At the end, the results from each GPU are synchronized and combined.\n \n Data parallelism significantly reduces training time by processing data in parallel, and it is scalable to the number of GPUs available. However, synchronizing results from each GPU can add overhead.\n "
        },
        {
            "sha": "24fa6275acee86e1b344db9f4d532bcdff68c23b",
            "filename": "docs/source/en/pipeline_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Fpipeline_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fpipeline_tutorial.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -24,7 +24,7 @@ Tailor the [`Pipeline`] to your task with task specific parameters such as addin\n \n Transformers has two pipeline classes, a generic [`Pipeline`] and many individual task-specific pipelines like [`TextGenerationPipeline`] or [`VisualQuestionAnsweringPipeline`]. Load these individual pipelines by setting the task identifier in the `task` parameter in [`Pipeline`]. You can find the task identifier for each pipeline in their API documentation.\n \n-Each task is configured to use a default pretrained model and preprocessor, but this can be overriden with the `model` parameter if you want to use a different model.\n+Each task is configured to use a default pretrained model and preprocessor, but this can be overridden with the `model` parameter if you want to use a different model.\n \n For example, to use the [`TextGenerationPipeline`] with [Gemma 2](./model_doc/gemma2), set `task=\"text-generation\"` and `model=\"google/gemma-2-2b\"`.\n "
        },
        {
            "sha": "dd0b9cbb4268df904beeea86735702457eea3f8b",
            "filename": "docs/source/en/testing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fen%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftesting.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -220,7 +220,7 @@ Just run the following line to automatically test every docstring example in the\n ```bash\n pytest --doctest-modules <path_to_file_or_dir>\n ```\n-If the file has a markdown extention, you should add the `--doctest-glob=\"*.md\"` argument.\n+If the file has a markdown extension, you should add the `--doctest-glob=\"*.md\"` argument.\n \n ### Run only modified tests\n "
        },
        {
            "sha": "b10fe43608599836dbed81524986ac27181d1acf",
            "filename": "docs/source/zh/agents.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fzh%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/docs%2Fsource%2Fzh%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fagents.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -233,7 +233,7 @@ Here are a few examples using notional tools:\n ---\n {examples}\n \n-Above example were using notional tools that might not exist for you. You only have acces to those tools:\n+Above example were using notional tools that might not exist for you. You only have access to those tools:\n <<tool_names>>\n You also can perform computations in the python code you generate.\n "
        },
        {
            "sha": "f3bcaa1562c91e0030dd5bbdcb12fed786fd44c0",
            "filename": "examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -265,7 +265,7 @@ class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor ([`Wav2Vec2Processor`])\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         decoder_start_token_id (:obj: `int`)\n             The begin-of-sentence of the decoder.\n         input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):"
        },
        {
            "sha": "e64ef981899a90dfb4491d15393373287484cee7",
            "filename": "examples/pytorch/speech-pretraining/run_wav2vec2_pretraining_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-pretraining%2Frun_wav2vec2_pretraining_no_trainer.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -296,7 +296,7 @@ class DataCollatorForWav2Vec2Pretraining:\n             The Wav2Vec2 model used for pretraining. The data collator needs to have access\n             to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n         feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:\n@@ -445,7 +445,7 @@ def main():\n     accelerator.wait_for_everyone()\n \n     # 1. Download and create train, validation dataset\n-    # We load all dataset configuration and datset split pairs passed in\n+    # We load all dataset configuration and dataset split pairs passed in\n     # ``args.dataset_config_names`` and ``args.dataset_split_names``\n     datasets_splits = []\n     for dataset_config_name, train_split_name in zip(args.dataset_config_names, args.dataset_split_names):"
        },
        {
            "sha": "f4a692238604c9b3d18da869f927cb66afe2bba7",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -292,7 +292,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.AutoProcessor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        },
        {
            "sha": "1f18998e93f92a31c464d520e770600d0d738588",
            "filename": "examples/pytorch/speech-recognition/run_speech_recognition_ctc_adapter.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-recognition%2Frun_speech_recognition_ctc_adapter.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -275,7 +275,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.AutoProcessor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:\n@@ -559,7 +559,7 @@ def remove_special_characters(batch):\n                 )\n \n                 # if we doing adapter language training, save\n-                # vocab with adpter language\n+                # vocab with adapter language\n                 if data_args.target_language is not None:\n                     vocab_dict[data_args.target_language] = lang_dict\n "
        },
        {
            "sha": "1af1d86913837ebfe2da21297736df5bd566ecf1",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -429,7 +429,7 @@ def main():\n     if is_regression:\n         label_list = None\n         num_labels = 1\n-        # regession requires float as label type, let's cast it if needed\n+        # regression requires float as label type, let's cast it if needed\n         for split in raw_datasets.keys():\n             if raw_datasets[split].features[\"label\"].dtype not in [\"float32\", \"float64\"]:\n                 logger.warning("
        },
        {
            "sha": "b96bcd92241c0b1ba8f8ebaf55b42f864875af74",
            "filename": "examples/pytorch/text-generation/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftext-generation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftext-generation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2FREADME.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n Based on the script [`run_generation.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-generation/run_generation.py).\n \n Conditional text generation using the auto-regressive models of the library: GPT, GPT-2, GPT-J, Transformer-XL, XLNet, CTRL, BLOOM, LLAMA, OPT.\n-A similar script is used for our official demo [Write With Transfomer](https://transformer.huggingface.co), where you\n+A similar script is used for our official demo [Write With Transformer](https://transformer.huggingface.co), where you\n can try out the different models available in the library.\n \n Example usage:"
        },
        {
            "sha": "734a1a1d1aef1a1d3217f60950d2b9b1daf231bb",
            "filename": "examples/pytorch/token-classification/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftoken-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fpytorch%2Ftoken-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2FREADME.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -19,7 +19,7 @@ limitations under the License.\n ## PyTorch version\n \n Fine-tuning the library models for token classification task such as Named Entity Recognition (NER), Parts-of-speech\n-tagging (POS) or phrase extraction (CHUNKS). The main scrip `run_ner.py` leverages the ðŸ¤— Datasets library and the Trainer API. You can easily\n+tagging (POS) or phrase extraction (CHUNKS). The main script `run_ner.py` leverages the ðŸ¤— Datasets library and the Trainer API. You can easily\n customize it to your needs if you need extra processing on your datasets.\n \n It will either run on a datasets hosted on our [hub](https://huggingface.co/datasets) or with your own text files for"
        },
        {
            "sha": "3e7222d490fc1035369c7408c9c2af243af1cf5f",
            "filename": "examples/research_projects/bertabs/configuration_bertabs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fconfiguration_bertabs.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -37,7 +37,7 @@ class BertAbsConfig(PretrainedConfig):\n         max_pos: int\n             The maximum sequence length that this model will be used with.\n         enc_layer: int\n-            The numner of hidden layers in the Transformer encoder.\n+            The number of hidden layers in the Transformer encoder.\n         enc_hidden_size: int\n             The size of the encoder's layers.\n         enc_heads: int\n@@ -49,7 +49,7 @@ class BertAbsConfig(PretrainedConfig):\n             embeddings, layers, pooler and also the attention probabilities in\n             the encoder.\n         dec_layer: int\n-            The numner of hidden layers in the decoder.\n+            The number of hidden layers in the decoder.\n         dec_hidden_size: int\n             The size of the decoder's layers.\n         dec_heads: int"
        },
        {
            "sha": "f6222d35d40bfbc3ea0454b4bee03b6c63ea0c34",
            "filename": "examples/research_projects/bertabs/convert_bertabs_original_pytorch_checkpoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fconvert_bertabs_original_pytorch_checkpoint.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -130,7 +130,7 @@ def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n     mask_tgt = decoder_attention_mask = None\n     mask_cls = None\n \n-    # The original model does not apply the geneator layer immediatly but rather in\n+    # The original model does not apply the generator layer immediatly but rather in\n     # the beam search (where it combines softmax + linear layer). Since we already\n     # apply the softmax in our generation process we only apply the linear layer here.\n     # We make sure that the outputs of the full stack are identical\n@@ -143,9 +143,9 @@ def convert_bertabs_checkpoints(path_to_checkpoints, dump_path):\n     output_converted_generator = new_model.generator(output_converted_model)\n \n     maximum_absolute_difference = torch.max(torch.abs(output_converted_model - output_original_model)).item()\n-    print(\"Maximum absolute difference beween weights: {:.2f}\".format(maximum_absolute_difference))\n+    print(\"Maximum absolute difference between weights: {:.2f}\".format(maximum_absolute_difference))\n     maximum_absolute_difference = torch.max(torch.abs(output_converted_generator - output_original_generator)).item()\n-    print(\"Maximum absolute difference beween weights: {:.2f}\".format(maximum_absolute_difference))\n+    print(\"Maximum absolute difference between weights: {:.2f}\".format(maximum_absolute_difference))\n \n     are_identical = torch.allclose(output_converted_model, output_original_model, atol=1e-3)\n     if are_identical:"
        },
        {
            "sha": "d65a0ca59d1036f8147f0a6506e75237f9bcd800",
            "filename": "examples/research_projects/bertabs/modeling_bertabs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Fmodeling_bertabs.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -390,7 +390,7 @@ class MultiHeadedAttention(nn.Module):\n     :cite:`DBLP:journals/corr/VaswaniSPUJGKP17`.\n \n     Similar to standard `dot` attention but uses\n-    multiple attention distributions simulataneously\n+    multiple attention distributions simultaneously\n     to select relevant items.\n \n     .. mermaid::"
        },
        {
            "sha": "bc13de558999542de32ce09dd615d70dc88b0b18",
            "filename": "examples/research_projects/bertabs/run_summarization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fbertabs%2Frun_summarization.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -260,7 +260,7 @@ def main():\n         default=None,\n         type=str,\n         required=False,\n-        help=\"The folder in wich the summaries should be written. Defaults to the folder where the documents are\",\n+        help=\"The folder in which the summaries should be written. Defaults to the folder where the documents are\",\n     )\n     parser.add_argument(\n         \"--compute_rouge\",\n@@ -315,7 +315,7 @@ def main():\n     )\n     args = parser.parse_args()\n \n-    # Select device (distibuted not available)\n+    # Select device (distributed not available)\n     args.device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n \n     # Check the existence of directories"
        },
        {
            "sha": "549627d6ca7355506bcdfd6088b4dbec2dcf7ca8",
            "filename": "examples/research_projects/codeparrot/scripts/codeparrot_training.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fcodeparrot_training.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -24,7 +24,7 @@ class ConstantLengthDataset(IterableDataset):\n     \"\"\"\n     Iterable dataset that returns constant length chunks of tokens from stream of text files.\n         Args:\n-            tokenizer (Tokenizer): The processor used for proccessing the data.\n+            tokenizer (Tokenizer): The processor used for processing the data.\n             dataset (dataset.Dataset): Dataset with text files.\n             infinite (bool): If True the iterator is reset after dataset reaches end else stops.\n             seq_length (int): Length of token sequences to return."
        },
        {
            "sha": "3e932c8ef61990ede6712fa52cd5d92e5eff518b",
            "filename": "examples/research_projects/codeparrot/scripts/preprocessing.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fcodeparrot%2Fscripts%2Fpreprocessing.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -84,7 +84,7 @@ def is_config_or_test(example, scan_width=5, coeff=0.05):\n \n \n def has_no_keywords(example):\n-    \"\"\"Check if a python file has none of the keywords for: funcion, class, for loop, while loop.\"\"\"\n+    \"\"\"Check if a python file has none of the keywords for: function, class, for loop, while loop.\"\"\"\n     keywords = [\"def \", \"class \", \"for \", \"while \"]\n     lines = example[\"content\"].splitlines()\n     for line in lines:"
        },
        {
            "sha": "c52425093812cdc2964b1cbb86c02d44671c940d",
            "filename": "examples/research_projects/performer/modeling_flax_performer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fperformer%2Fmodeling_flax_performer_utils.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -252,7 +252,7 @@ def make_fast_generalized_attention(\n     unidirectional=False,\n     lax_scan_unroll=1,\n ):\n-    \"\"\"Construct a fast generalized attention menthod.\"\"\"\n+    \"\"\"Construct a fast generalized attention method.\"\"\"\n     logging.info(\"Fast generalized attention.: %s features and renormalize=%s\", nb_features, renormalize_attention)\n     if features_type == \"ortho\":\n         matrix_creator = functools.partial(GaussianOrthogonalRandomMatrix, nb_features, qkv_dim, scaling=False)"
        },
        {
            "sha": "9aa0bc5dbcb1d5f890252e78d2b2a7704da4e565",
            "filename": "examples/research_projects/rag-end2end-retriever/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2FREADME.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -11,7 +11,7 @@ Please read the [accompanying blog post](https://shamanesiri.medium.com/how-to-f\n The original RAG code has also been modified to work with the latest versions of pytorch lightning (version 1.2.10) and RAY (version 1.3.0). All other implementation details remain the same as the [original RAG code](https://github.com/huggingface/transformers/tree/main/examples/research_projects/rag).\n Read more about RAG  at https://arxiv.org/abs/2005.11401.\n \n-This code can be modified to experiment with other research on retrival augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).\n+This code can be modified to experiment with other research on retrieval augmented models which include training of the retriever (e.g. [REALM](https://arxiv.org/abs/2002.08909) and [MARGE](https://arxiv.org/abs/2006.15020)).\n \n To start training, use the bash script (finetune_rag_ray_end2end.sh) in this folder. This script also includes descriptions on each command-line argument used.\n "
        },
        {
            "sha": "c1a271e88d138cd39fc59fe249eb3746e9be59fd",
            "filename": "examples/research_projects/rag-end2end-retriever/lightning_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag-end2end-retriever%2Flightning_base.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -134,7 +134,7 @@ def configure_optimizers(self):\n             {\n                 \"params\": [\n                     p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n-                ],  # check this named paramters\n+                ],  # check this named parameters\n                 \"weight_decay\": self.hparams.weight_decay,\n             },\n             {\n@@ -279,7 +279,7 @@ def on_sanity_check_start(self, trainer, pl_module):\n \n \n class CheckParamCallback(pl.Callback):\n-    # check whether new added model paramters are differentiable\n+    # check whether new added model parameters are differentiable\n     def on_after_backward(self, trainer, pl_module):\n         # print(pl_module.model.rag)\n         for name, param in pl_module.model.rag.named_parameters():"
        },
        {
            "sha": "59aa46a89522a1ddbfb756d3b1239d3d86734330",
            "filename": "examples/research_projects/rag/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2FREADME.md?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -98,7 +98,7 @@ Our evaluation script enables two modes of evaluation (controlled by the `eval_m\n \n The evaluation script expects paths to two files:\n - `evaluation_set` - a path to a file specifying the evaluation dataset, a single input per line.\n-- `gold_data_path` - a path to a file contaning ground truth answers for datapoints from the `evaluation_set`, a single output per line. Check below for expected formats of the gold data files.\n+- `gold_data_path` - a path to a file containing ground truth answers for datapoints from the `evaluation_set`, a single output per line. Check below for expected formats of the gold data files.\n \n \n ## Retrieval evaluation"
        },
        {
            "sha": "b8c4b6fc3c50eaca1033f530dedd26df4ea4bc65",
            "filename": "examples/research_projects/rag/distributed_pytorch_retriever.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Fdistributed_pytorch_retriever.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -70,7 +70,7 @@ def init_retrieval(self, distributed_port: int):\n             logger.info(\"dist not initialized / main\")\n             self.index.init_index()\n \n-        # all processes wait untill the retriever is initialized by the main process\n+        # all processes wait until the retriever is initialized by the main process\n         if dist.is_initialized():\n             torch.distributed.barrier(group=self.process_group)\n "
        },
        {
            "sha": "af3acd4def67b42ee49847b706bea1f23bce9dce",
            "filename": "examples/research_projects/rag/finetune_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frag%2Ffinetune_rag.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -458,7 +458,7 @@ def add_retriever_specific_args(parser):\n             default=None,\n             help=(\n                 \"Name of the index to use: 'hf' for a canonical dataset from the datasets library (default), 'custom'\"\n-                \" for a local index, or 'legacy' for the orignal one)\"\n+                \" for a local index, or 'legacy' for the original one)\"\n             ),\n         )\n         parser.add_argument("
        },
        {
            "sha": "cb489ea28d686e50384b57cd00e566e667d64218",
            "filename": "examples/research_projects/robust-speech-event/run_speech_recognition_ctc_bnb.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_bnb.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -266,7 +266,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.AutoProcessor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        },
        {
            "sha": "37f91b9ef6171b9885c81e1d5bb91c7ec369fa09",
            "filename": "examples/research_projects/robust-speech-event/run_speech_recognition_ctc_streaming.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Frobust-speech-event%2Frun_speech_recognition_ctc_streaming.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -257,7 +257,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.AutoProcessor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        },
        {
            "sha": "796d271583b049843c4965a7778457006f228c47",
            "filename": "examples/research_projects/wav2vec2/run_asr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_asr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_asr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fwav2vec2%2Frun_asr.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -226,7 +226,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.Wav2Vec2Processor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        },
        {
            "sha": "09a8458ca2aea4d18dbcfe45d5163ba2f361134a",
            "filename": "examples/research_projects/wav2vec2/run_common_voice.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_common_voice.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_common_voice.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fwav2vec2%2Frun_common_voice.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -145,7 +145,7 @@ class DataCollatorCTCWithPadding:\n     Data collator that will dynamically pad the inputs received.\n     Args:\n         processor (:class:`~transformers.Wav2Vec2Processor`)\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        },
        {
            "sha": "00ef4edb37e08d54ed22ededed782122ca5eb317",
            "filename": "examples/research_projects/wav2vec2/run_pretrain.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_pretrain.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37508816d650a8074bc31c761e20872c2e5eaec4/examples%2Fresearch_projects%2Fwav2vec2%2Frun_pretrain.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fwav2vec2%2Frun_pretrain.py?ref=37508816d650a8074bc31c761e20872c2e5eaec4",
            "patch": "@@ -142,7 +142,7 @@ class DataCollatorForWav2Vec2Pretraining:\n             The Wav2Vec2 model used for pretraining. The data collator needs to have access\n             to config and ``_get_feat_extract_output_lengths`` function for correct padding.\n         feature_extractor (:class:`~transformers.Wav2Vec2FeatureExtractor`):\n-            The processor used for proccessing the data.\n+            The processor used for processing the data.\n         padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n             Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n             among:"
        }
    ],
    "stats": {
        "total": 100,
        "additions": 50,
        "deletions": 50
    }
}