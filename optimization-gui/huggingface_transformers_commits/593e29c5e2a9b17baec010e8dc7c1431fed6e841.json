{
    "author": "1himan",
    "message": "Updated Aria model card (#38472)\n\n* Update aria.md\n\n* Update aria.md\n\n* Suggested Updates - aria.md",
    "sha": "593e29c5e2a9b17baec010e8dc7c1431fed6e841",
    "files": [
        {
            "sha": "1c974bf5e26637bb1031e78110ac3b8efe2430c5",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 91,
            "deletions": 31,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/593e29c5e2a9b17baec010e8dc7c1431fed6e841/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/593e29c5e2a9b17baec010e8dc7c1431fed6e841/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=593e29c5e2a9b17baec010e8dc7c1431fed6e841",
            "patch": "@@ -14,60 +14,119 @@ rendered properly in your Markdown viewer.\n \n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # Aria\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[Aria](https://huggingface.co/papers/2410.05993) is a multimodal mixture-of-experts (MoE) model. The goal of this model is to open-source a training recipe for creating a multimodal native model from scratch. Aria has 3.9B and 3.5B activated parameters per visual and text token respectively. Text is handled by a MoE decoder and visual inputs are handled by a lightweight visual encoder. It is trained in 4 stages, language pretraining, multimodal pretraining, multimodal long-context pretraining, and multimodal post-training.\n \n-## Overview\n+You can find all the original Aria checkpoints under the [Aria](https://huggingface.co/rhymes-ai?search_models=aria) organization.\n \n-The Aria model was proposed in [Aria: An Open Multimodal Native Mixture-of-Experts Model](https://huggingface.co/papers/2410.05993) by Li et al. from the Rhymes.AI team.\n+> [!TIP]\n+> Click on the Aria models in the right sidebar for more examples of how to apply Aria to different multimodal tasks.\n \n-Aria is an open multimodal-native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. It has a Mixture-of-Experts architecture, with respectively 3.9B and 3.5B activated parameters per visual token and text token. \n+The example below demonstrates how to generate text based on an image with [`Pipeline`] or the [`AutoModel`] class.\n \n-The abstract from the paper is the following:\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-*Information comes in diverse modalities. Multimodal native AI models are essential to integrate real-world information and deliver comprehensive understanding. While proprietary multimodal native models exist, their lack of openness imposes obstacles for adoptions, let alone adaptations. To fill this gap, we introduce Aria, an open multimodal native model with best-in-class performance across a wide range of multimodal, language, and coding tasks. Aria is a mixture-of-expert model with 3.9B and 3.5B activated parameters per visual token and text token, respectively. It outperforms Pixtral-12B and Llama3.2-11B, and is competitive against the best proprietary models on various multimodal tasks. We pre-train Aria from scratch following a 4-stage pipeline, which progressively equips the model with strong capabilities in language understanding, multimodal understanding, long context window, and instruction following. We open-source the model weights along with a codebase that facilitates easy adoptions and adaptations of Aria in real-world applications.*\n+```python\n+import torch\n+from transformers import pipeline\n \n-This model was contributed by [m-ric](https://huggingface.co/m-ric).\n-The original code can be found [here](https://github.com/rhymes-ai/Aria).\n+pipeline = pipeline(\n+    \"image-to-text\",\n+    model=\"rhymes-ai/Aria\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+pipeline(\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+    text=\"What is shown in this image?\"\n+)\n+```\n \n-## Usage tips\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n-Here's how to use the model for vision tasks:\n ```python\n-import requests\n import torch\n-from PIL import Image\n+from transformers import AutoModelForCausalLM, AutoProcessor\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"rhymes-ai/Aria\",\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    attn_implementation=\"sdpa\"\n+)\n+\n+processor = AutoProcessor.from_pretrained(\"rhymes-ai/Aria\")\n \n-from transformers import AriaProcessor, AriaForConditionalGeneration\n+messages = [\n+    {\n+        \"role\": \"user\", \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ]\n+    },\n+]\n \n-model_id_or_path = \"rhymes-ai/Aria\"\n+inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n+ipnuts = inputs.to(model.device, torch.bfloat16)\n \n-model = AriaForConditionalGeneration.from_pretrained(\n-    model_id_or_path, device_map=\"auto\"\n+output = model.generate(\n+    **inputs,\n+    max_new_tokens=15,\n+    stop_strings=[\"<|im_end|>\"],\n+    tokenizer=processor.tokenizer,\n+    do_sample=True,\n+    temperature=0.9,\n )\n+output_ids = output[0][inputs[\"input_ids\"].shape[1]:]\n+response = processor.decode(output_ids, skip_special_tokens=True)\n+print(response)\n+```\n+\n+</hfoption>\n+</hfoptions>\n \n-processor = AriaProcessor.from_pretrained(model_id_or_path)\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\t\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to int4 and the [rhymes-ai/Aria-sequential_mlp](https://huggingface.co/rhymes-ai/Aria-sequential_mlp) checkpoint. This checkpoint replaces grouped GEMM with `torch.nn.Linear` layers for easier quantization.\n \n-image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+```py\n+# pip install torchao\n+import torch\n+from transformers import TorchAoConfig, AutoModelForCausalLM, AutoProcessor\n+\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"rhymes-ai/Aria-sequential_mlp\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+processor = AutoProcessor.from_pretrained(\n+    \"rhymes-ai/Aria-sequential_mlp\",\n+)\n \n messages = [\n     {\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"image\"},\n-            {\"text\": \"what is the image?\", \"type\": \"text\"},\n-        ],\n-    }\n+        \"role\": \"user\", \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ]\n+    },\n ]\n \n-text = processor.apply_chat_template(messages, add_generation_prompt=True)\n-inputs = processor(text=text, images=image, return_tensors=\"pt\")\n-inputs.to(model.device)\n+inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\")\n+inputs = inputs.to(model.device, torch.bfloat16)\n \n output = model.generate(\n     **inputs,\n@@ -79,6 +138,7 @@ output = model.generate(\n )\n output_ids = output[0][inputs[\"input_ids\"].shape[1]:]\n response = processor.decode(output_ids, skip_special_tokens=True)\n+print(response)\n ```\n \n "
        }
    ],
    "stats": {
        "total": 122,
        "additions": 91,
        "deletions": 31
    }
}