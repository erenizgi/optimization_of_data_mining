{
    "author": "Cyrilvallez",
    "message": "[cache] Merge static sliding and static chunked layer (#40893)\n\n* merge\n\n* get rid of tensors in get_mask_sizes!!\n\n* remove branch\n\n* add comment explanation\n\n* re-add the class with deprecation cycle",
    "sha": "087775d10e0171f0cf7010fba70954aafd450d98",
    "files": [
        {
            "sha": "e519db4d8f2d107107249dd0ac0f1eb56ba3e609",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 49,
            "deletions": 97,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/087775d10e0171f0cf7010fba70954aafd450d98/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/087775d10e0171f0cf7010fba70954aafd450d98/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=087775d10e0171f0cf7010fba70954aafd450d98",
            "patch": "@@ -372,85 +372,6 @@ def __init__(self, max_cache_len: int, sliding_window: int):\n         super().__init__(max_cache_len=effective_max_cache_len)\n         self.cumulative_length = 0\n \n-    def update(\n-        self,\n-        key_states: torch.Tensor,\n-        value_states: torch.Tensor,\n-        cache_kwargs: Optional[dict[str, Any]] = None,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"\n-        Update the key and value caches in-place, and return the necessary keys and value states.\n-\n-        Args:\n-            key_states (`torch.Tensor`): The new key states to cache.\n-            value_states (`torch.Tensor`): The new value states to cache.\n-            cache_kwargs (`dict[str, Any]`, *optional*): Additional arguments for the cache.\n-\n-        Returns:\n-            tuple[`torch.Tensor`, `torch.Tensor`]: The key and value states.\n-        \"\"\"\n-        # Lazy initialization\n-        if self.keys is None:\n-            self.lazy_initialization(key_states)\n-\n-        cache_position = cache_kwargs.get(\"cache_position\")\n-\n-        is_full = self.cumulative_length >= self.max_cache_len\n-        # Update it now that we saved the value above\n-        self.cumulative_length += key_states.shape[-2]\n-\n-        # Handle prefill phase when prompt length > sliding_window_size.\n-        # Note that we store cropped key/value states in the cache but return the full key/value states.\n-        if cache_position.shape[0] > self.max_cache_len:\n-            self.keys.copy_(key_states[:, :, -self.max_cache_len :, :])\n-            self.values.copy_(value_states[:, :, -self.max_cache_len :, :])\n-            # Return the full states here\n-            return key_states, value_states\n-\n-        # Here we only assume decoding stage, i.e. 1 token at a time\n-        if is_full:\n-            # Roll all values to the left by 1 position\n-            new_keys = self.keys.roll(-1, dims=-2)\n-            new_values = self.values.roll(-1, dims=-2)\n-            # Overwrite the last position with new states\n-            # (note: very important to use a tensor to index here, see https://github.com/pytorch/pytorch/issues/159855)\n-            index = torch.tensor([-1], dtype=int, device=self.device)\n-            new_keys[:, :, index] = key_states\n-            new_values[:, :, index] = value_states\n-\n-            # Copy back into `self` (do not just assign again) in order to keep the static dynamo address\n-            self.keys.copy_(new_keys)\n-            self.values.copy_(new_values)\n-        else:\n-            try:\n-                self.keys.index_copy_(2, cache_position, key_states)\n-                self.values.index_copy_(2, cache_position, value_states)\n-            except NotImplementedError:\n-                self.keys[:, :, cache_position] = key_states\n-                self.values[:, :, cache_position] = value_states\n-\n-        return self.keys, self.values\n-\n-    def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n-        \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n-        query_length = cache_position.shape[0]\n-        first_cache_position = cache_position[0]\n-\n-        kv_offset = torch.clamp(first_cache_position - self.max_cache_len + 1, min=0)\n-        # This is not general (see HybridChunkedCache for the whole general case), but it's what the cache returns\n-        kv_length = max(query_length, self.max_cache_len)\n-        return kv_length, kv_offset\n-\n-    def get_seq_length(self) -> int:\n-        \"\"\"Returns the sequence length of the cached states.\"\"\"\n-        return self.cumulative_length\n-\n-\n-class ChunkedSlidingLayer(SlidingWindowLayer):\n-    \"\"\"\n-    An extended SlidingWindowLayer that supports prefill chunking, originally implemented for Llama 4.\n-    \"\"\"\n-\n     def update(\n         self,\n         key_states: torch.Tensor,\n@@ -480,16 +401,29 @@ def update(\n         self.cumulative_length += key_states.shape[-2]\n \n         if is_full:\n-            full_key_states = torch.cat((self.keys[:, :, 1:, :], key_states), dim=-2)\n-            full_value_states = torch.cat((self.values[:, :, 1:, :], value_states), dim=-2)\n-            # Fast decoding path -> here as the effective size is still sliding window, it is extremely important\n-            # to return `self.key_cache[layer_idx]` and `self.value_cache[layer_idx]`, as they have the fixed address\n-            # in memory (the values are the same as the full states, but not the address!!)\n+            # In general, we should use a much simpler `cat` here as well, independently of the states size. However,\n+            # dynamo is currently bugged when doing it - see https://github.com/pytorch/pytorch/issues/159855 for more details\n             if key_states.shape[-2] == 1:\n-                self.keys.copy_(full_key_states)\n-                self.values.copy_(full_value_states)\n+                # Roll all values to the left by 1 position\n+                new_keys = self.keys.roll(-1, dims=-2)\n+                new_values = self.values.roll(-1, dims=-2)\n+                # Overwrite the last position with new states\n+                # (note: very important to use a tensor to index here, see https://github.com/pytorch/pytorch/issues/159855)\n+                index = torch.tensor([-1], dtype=int, device=self.device)\n+                new_keys[:, :, index] = key_states\n+                new_values[:, :, index] = value_states\n+\n+                # Copy back into `self` (do not just assign again) in order to keep the static dynamo address\n+                self.keys.copy_(new_keys)\n+                self.values.copy_(new_values)\n+                # Very important to return the `self` tensors here, as they have the static dynamo address\n                 return self.keys, self.values\n-        elif not is_full and cumulative_length + key_states.shape[2] > self.max_cache_len:\n+            # Already full but using more than 1 new token (e.g. prefill caching, chat continuation, etc...)\n+            else:\n+                full_key_states = torch.cat((self.keys[:, :, 1:, :], key_states), dim=-2)\n+                full_value_states = torch.cat((self.values[:, :, 1:, :], value_states), dim=-2)\n+        # Not yet full, but becoming full on this update\n+        elif cumulative_length + key_states.shape[2] > self.max_cache_len:\n             # Fast prefill path, no need to cat() in this case, as the cache is currently empty\n             if cumulative_length == 0:\n                 full_key_states = key_states\n@@ -504,33 +438,38 @@ def update(\n             except NotImplementedError:\n                 self.keys[:, :, cache_position] = key_states\n                 self.values[:, :, cache_position] = value_states\n+\n+            # Very important to return the `self` tensors here, as they have the static dynamo address\n             return self.keys, self.values\n \n+        # We only cache the last `sliding_window` tokens\n         self.keys.copy_(full_key_states[:, :, -self.max_cache_len :, :])\n         self.values.copy_(full_value_states[:, :, -self.max_cache_len :, :])\n         # we should return the whole states instead of `self.keys/values` here, as otherwise we lose some context\n-        # which is outside the window\n         return full_key_states, full_value_states\n \n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n         query_length = cache_position.shape[0]\n-        first_cache_position = cache_position[0]\n         sliding_window = self.max_cache_len\n+        is_full = self.cumulative_length >= self.max_cache_len\n \n-        kv_offset = torch.clamp(first_cache_position - sliding_window + 1, min=0)\n-        # This is the true general case for any Cache using local attention (sliding or chunked)\n-        if first_cache_position >= sliding_window:\n-            # Here the Cache is already full\n+        kv_offset = max(self.cumulative_length - sliding_window + 1, 0)\n+        # The cache is already full\n+        if is_full:\n             kv_length = sliding_window + query_length - 1\n-        elif first_cache_position < sliding_window and first_cache_position + query_length > sliding_window:\n-            # Here the Cache becomes full with the new input\n-            kv_length = first_cache_position + query_length\n+        # Not yet full, but becoming full on this update\n+        elif self.cumulative_length + query_length > sliding_window:\n+            kv_length = self.cumulative_length + query_length\n         else:\n             # Here the Cache is still smaller than the local size, but we return the local size as it's static\n             kv_length = sliding_window\n         return kv_length, kv_offset\n \n+    def get_seq_length(self) -> int:\n+        \"\"\"Returns the sequence length of the cached states.\"\"\"\n+        return self.cumulative_length\n+\n \n class QuantizedLayer(DynamicLayer):\n     \"\"\"\n@@ -1023,6 +962,8 @@ def __init__(\n                 layer_types = layer_types[: -config.num_kv_shared_layers]\n \n             for layer_type in layer_types:\n+                # From a cache point of view, both sliding and chunked are the same in how they should behave and how many\n+                # states they should return - only the mask changes to make them different at the end!\n                 if layer_type in (\"sliding_attention\", \"chunked_attention\"):\n                     layers.append(DynamicSlidingWindowLayer(sliding_window=sliding_window))\n                 else:\n@@ -1141,7 +1082,9 @@ def __init__(\n             if layer_type == \"sliding_attention\":\n                 layer = SlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.sliding_window)\n             elif layer_type == \"chunked_attention\":\n-                layer = ChunkedSlidingLayer(max_cache_len=max_cache_len, sliding_window=config.attention_chunk_size)\n+                # From a cache point of view, both sliding and chunked are the same in how they should behave and how many\n+                # states they should return - only the mask changes to make them different at the end!\n+                layer = SlidingWindowLayer(max_cache_len=max_cache_len, sliding_window=config.attention_chunk_size)\n             else:\n                 layer = StaticLayer(max_cache_len=max_cache_len)\n             layers.append(layer)\n@@ -1414,6 +1357,15 @@ def is_compileable(self) -> bool:\n ### Deprecated classes\n \n \n+class ChunkedSlidingLayer(SlidingWindowLayer):\n+    def __init__(self, max_cache_len: int, sliding_window: int):\n+        logger.warning_once(\n+            \"`ChunkedSlidingLayer` is deprecated and will be removed in version v4.59 \"\n+            \"Use `SlidingWindowLayer` instead, which has the exact same functionalities.\"\n+        )\n+        super().__init__(max_cache_len, sliding_window)\n+\n+\n class OffloadedCache(DynamicCache):\n     def __init__(self) -> None:\n         logger.warning_once("
        }
    ],
    "stats": {
        "total": 146,
        "additions": 49,
        "deletions": 97
    }
}