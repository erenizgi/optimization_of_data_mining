{
    "author": "zucchini-nlp",
    "message": "[modenbert] fix regression (#39750)\n\n* fix regression\n\n* add FA2 test",
    "sha": "1ad216bd7dced23e7e38ab4881a9528486588bd0",
    "files": [
        {
            "sha": "7b6476bd612edecbfcdaf7e1a27c59cbd3966022",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ad216bd7dced23e7e38ab4881a9528486588bd0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ad216bd7dced23e7e38ab4881a9528486588bd0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=1ad216bd7dced23e7e38ab4881a9528486588bd0",
            "patch": "@@ -19,6 +19,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n import math\n from contextlib import nullcontext\n from typing import Optional, Union\n@@ -459,20 +460,21 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n \n         if layer_id % config.global_attn_every_n_layers != 0:\n             self.local_attention = (config.local_attention // 2, config.local_attention // 2)\n+            rope_theta = config.local_rope_theta if config.local_rope_theta is not None else config.global_rope_theta\n+            max_position_embeddings = config.local_attention\n         else:\n             self.local_attention = (-1, -1)\n-\n-        max_position_embeddings = config.max_position_embeddings\n-        if self.local_attention != (-1, -1):\n-            rope_theta = config.global_rope_theta if config.local_rope_theta is None else config.local_rope_theta\n-            max_position_embeddings = config.local_attention\n+            max_position_embeddings = config.max_position_embeddings\n+            rope_theta = config.global_rope_theta\n \n         if config._attn_implementation == \"flash_attention_2\":\n             self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(config=config)\n+            config_copy = copy.deepcopy(config)\n+            config_copy.rope_theta = rope_theta\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config_copy)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n@@ -611,7 +613,9 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n+    def _check_and_adjust_attn_implementation(\n+        self, attn_implementation: Optional[str], is_init_check: bool = False\n+    ) -> str:\n         \"\"\"\n         Checks and dispatches to hhe requested attention implementation.\n         \"\"\"\n@@ -620,16 +624,17 @@ def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n         # ModernBert's FA2 implementation correctly handles non-fp16/bf16 dtypes, we don't\n         # need the FA2 warning for non-fp16/bf16 dtypes so we set fp16 for the FA2 check.\n \n-        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n         try:\n             attn_implementation = (\n                 \"flash_attention_2\"\n-                if requested_attn_implementation is None and self._flash_attn_2_can_dispatch()\n+                if attn_implementation is None and self._flash_attn_2_can_dispatch()\n                 else attn_implementation\n             )\n         except (ValueError, ImportError):\n             pass\n-        return super().set_attention_implementation(attn_implementation=attn_implementation)\n+        return super()._check_and_adjust_attn_implementation(\n+            attn_implementation=attn_implementation, is_init_check=is_init_check\n+        )\n \n     def _maybe_set_compile(self):\n         if self.config.reference_compile is False:"
        },
        {
            "sha": "3648e30ac942140ab12e2537db481d2f1fc3e5a9",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ad216bd7dced23e7e38ab4881a9528486588bd0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ad216bd7dced23e7e38ab4881a9528486588bd0/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=1ad216bd7dced23e7e38ab4881a9528486588bd0",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import copy\n import math\n from contextlib import nullcontext\n from typing import Literal, Optional, Union\n@@ -659,20 +660,21 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n \n         if layer_id % config.global_attn_every_n_layers != 0:\n             self.local_attention = (config.local_attention // 2, config.local_attention // 2)\n+            rope_theta = config.local_rope_theta if config.local_rope_theta is not None else config.global_rope_theta\n+            max_position_embeddings = config.local_attention\n         else:\n             self.local_attention = (-1, -1)\n-\n-        max_position_embeddings = config.max_position_embeddings\n-        if self.local_attention != (-1, -1):\n-            rope_theta = config.global_rope_theta if config.local_rope_theta is None else config.local_rope_theta\n-            max_position_embeddings = config.local_attention\n+            max_position_embeddings = config.max_position_embeddings\n+            rope_theta = config.global_rope_theta\n \n         if config._attn_implementation == \"flash_attention_2\":\n             self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(config=config)\n+            config_copy = copy.deepcopy(config)\n+            config_copy.rope_theta = rope_theta\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config_copy)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()\n@@ -811,7 +813,9 @@ def init_weight(module: nn.Module, std: float):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n \n-    def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n+    def _check_and_adjust_attn_implementation(\n+        self, attn_implementation: Optional[str], is_init_check: bool = False\n+    ) -> str:\n         \"\"\"\n         Checks and dispatches to hhe requested attention implementation.\n         \"\"\"\n@@ -820,16 +824,17 @@ def set_attention_implementation(self, attn_implementation: Union[dict, str]):\n         # ModernBert's FA2 implementation correctly handles non-fp16/bf16 dtypes, we don't\n         # need the FA2 warning for non-fp16/bf16 dtypes so we set fp16 for the FA2 check.\n \n-        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n         try:\n             attn_implementation = (\n                 \"flash_attention_2\"\n-                if requested_attn_implementation is None and self._flash_attn_2_can_dispatch()\n+                if attn_implementation is None and self._flash_attn_2_can_dispatch()\n                 else attn_implementation\n             )\n         except (ValueError, ImportError):\n             pass\n-        return super().set_attention_implementation(attn_implementation=attn_implementation)\n+        return super()._check_and_adjust_attn_implementation(\n+            attn_implementation=attn_implementation, is_init_check=is_init_check\n+        )\n \n     def _maybe_set_compile(self):\n         if self.config.reference_compile is False:"
        },
        {
            "sha": "a6bb02e8b8f34a5862edf18bcb0e57a0710f9d8a",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ad216bd7dced23e7e38ab4881a9528486588bd0/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ad216bd7dced23e7e38ab4881a9528486588bd0/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=1ad216bd7dced23e7e38ab4881a9528486588bd0",
            "patch": "@@ -375,6 +375,16 @@ def test_saved_config_excludes_reference_compile(self):\n                 config_dict = json.load(f)\n             self.assertNotIn(\"reference_compile\", config_dict)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    def test_flash_attention_dispatches_by_defaul(self):\n+        \"ModernBert should dispatch to FA2 by default, not SDPA\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=config)\n+            self.assertTrue(model.config._attn_implementation == \"flash_attention_2\")\n+\n \n @require_torch\n class ModernBertModelIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 40,
        "deletions": 20
    }
}