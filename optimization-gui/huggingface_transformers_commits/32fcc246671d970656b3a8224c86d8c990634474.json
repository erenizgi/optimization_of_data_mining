{
    "author": "yao-matrix",
    "message": "rename get_cuda_warm_up_factor to get_accelerator_warm_up_factor (#40363)\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "32fcc246671d970656b3a8224c86d8c990634474",
    "files": [
        {
            "sha": "d5b1159ca1bdddedb8ab68a3537f2ec1d4a02e6f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=32fcc246671d970656b3a8224c86d8c990634474",
            "patch": "@@ -6202,7 +6202,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n     - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n     However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n-    factor = 2 if hf_quantizer is None else hf_quantizer.get_cuda_warm_up_factor()\n+    factor = 2 if hf_quantizer is None else hf_quantizer.get_accelerator_warm_up_factor()\n \n     # Remove disk, cpu and meta devices, and cast to proper torch.device\n     accelerator_device_map = {"
        },
        {
            "sha": "ec23955a6286055a3942c227f127878eb323d621",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=32fcc246671d970656b3a8224c86d8c990634474",
            "patch": "@@ -280,9 +280,9 @@ def dequantize(self, model):\n \n         return model\n \n-    def get_cuda_warm_up_factor(self):\n+    def get_accelerator_warm_up_factor(self):\n         \"\"\"\n-        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up cuda.\n+        The factor to be used in `caching_allocator_warmup` to get the number of bytes to pre-allocate to warm up accelerator.\n         A factor of 2 means we allocate all bytes in the empty model (since we allocate in fp16), a factor of 4 means\n         we allocate half the memory of the weights residing in the empty model, etc...\n         \"\"\""
        },
        {
            "sha": "dc30221b590ed969b3a0cb9512a60b1d37ea1851",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=32fcc246671d970656b3a8224c86d8c990634474",
            "patch": "@@ -59,8 +59,9 @@ def validate_environment(self, *args, **kwargs):\n         device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n-                \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \"\n-                \"your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \"\n+                \"You have loaded an FP8 model on CPU and have a CUDA or XPU device available, make sure to set \"\n+                \"your model on a GPU or XPU device in order to run your model. To remove this warning, \"\n+                \"pass device_map = 'cuda' or 'xpu'. \"\n             )\n         elif device_map is not None:\n             if (\n@@ -227,6 +228,6 @@ def is_serializable(self, safe_serialization=None):\n     def is_trainable(self) -> bool:\n         return False\n \n-    def get_cuda_warm_up_factor(self):\n+    def get_accelerator_warm_up_factor(self):\n         # Pre-processing is done cleanly, so we can allocate everything here\n         return 2"
        },
        {
            "sha": "cba023a7d8117580a1ad28241268bf08556b775f",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/32fcc246671d970656b3a8224c86d8c990634474/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=32fcc246671d970656b3a8224c86d8c990634474",
            "patch": "@@ -314,9 +314,9 @@ def is_serializable(self, safe_serialization=None) -> bool:\n             return False\n         return _is_torchao_serializable\n \n-    def get_cuda_warm_up_factor(self):\n+    def get_accelerator_warm_up_factor(self):\n         \"\"\"\n-        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for CUDA warmup.\n+        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for accelerator warmup.\n         - A factor of 2 means we pre-allocate the full memory footprint of the model.\n         - A factor of 4 means we pre-allocate half of that, and so on\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 9,
        "deletions": 8
    }
}