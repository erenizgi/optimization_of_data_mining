{
    "author": "Cyrilvallez",
    "message": "Remove deprecation warning (#41425)\n\n* remove\n\n* fix space",
    "sha": "242eb9cbdc2513d347fe528f4ff7111e90bb4372",
    "files": [
        {
            "sha": "fdc768237bed38941bc25615db0a0926269488a1",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -10,8 +10,6 @@\n import torch\n from torch import nn\n \n-from ...utils.deprecation import deprecate_kwarg\n-\n \n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n@@ -64,6 +62,5 @@ class TestAttention(nn.Module):\n     def __init__(self):\n         pass\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(self) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "cc08f89f235d16d5c5c0dfc6c35f37f44c298bc5",
            "filename": "examples/modular-transformers/modeling_global_indexing.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_global_indexing.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -14,7 +14,6 @@\n from ...cache_utils import Cache\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_global_indexing import GlobalIndexingConfig\n \n \n@@ -126,7 +125,6 @@ def __init__(self, config: GlobalIndexingConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "6337ea5cfca9016fcad530a2cc1bb50e0e53baaa",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -15,7 +15,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n@@ -153,7 +152,6 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -209,7 +207,6 @@ def __init__(self, config: MyNewModel2Config, layer_idx: int):\n         self.input_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MyNewModel2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "eb17364518c28c629dc9ce829cd20448afe4986d",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -19,7 +19,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_super import SuperConfig\n \n@@ -195,7 +194,6 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -251,7 +249,6 @@ def __init__(self, config: SuperConfig, layer_idx: int):\n         self.input_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = SuperRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c7ba8feec46362c3ca9fc25c93c587443eb47a15",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -14,7 +14,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_switch_function import SwitchFunctionConfig\n \n \n@@ -117,7 +116,6 @@ def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f4115f7f0acd697143c51b36c5082596d46815b0",
            "filename": "src/transformers/models/apertus/modeling_apertus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fapertus%2Fmodeling_apertus.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_apertus import ApertusConfig\n \n@@ -211,7 +210,6 @@ def __init__(self, config: ApertusConfig, layer_idx: Optional[int] = None):\n         self.q_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n         self.k_norm = ApertusRMSNorm(self.head_dim, config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -268,7 +266,6 @@ def __init__(self, config: ApertusConfig, layer_idx: int):\n         self.attention_layernorm = ApertusRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.feedforward_layernorm = ApertusRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "23bf41aab3243aab2bf912d12029af5763d99096",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_arcee import ArceeConfig\n \n@@ -216,7 +215,6 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -272,7 +270,6 @@ def __init__(self, config: ArceeConfig, layer_idx: int):\n         self.input_layernorm = ArceeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ArceeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "afec16c8a4232d0ca34a9fb23c765d23010421f3",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_aria import AriaConfig, AriaTextConfig\n@@ -468,7 +467,6 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -535,7 +533,6 @@ def __init__(self, config: AriaTextConfig, layer_idx: int):\n         self.input_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = AriaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2004ca3cd644f14be61cd7c6be5771ec5a045790",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_autoformer import AutoformerConfig\n \n \n@@ -446,7 +445,6 @@ def __init__(\n \n         self.autocorrelation_factor = autocorrelation_factor\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -734,7 +732,6 @@ def __init__(self, config: AutoformerConfig, layer_idx=None):\n             bias=False,\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2773bf1d4ba59545400e630e1ad6b9e1c14102ed",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -341,7 +340,6 @@ def __init__(self, config: BambaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1007,7 +1005,6 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n         else:\n             raise ValueError(\"Invalid layer_type\")\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "85b6fed82efb1e0361d8e795fc752a34b629a69f",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -50,7 +50,6 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -721,7 +720,6 @@ def __init__(self, config: BambaConfig, layer_idx: int, layer_type: str = \"mamba\n         else:\n             raise ValueError(\"Invalid layer_type\")\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "caef6160f901cbb401ddec506595a7dab85c8599",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -50,7 +50,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bart import BartConfig\n \n \n@@ -189,7 +188,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -372,7 +370,6 @@ def __init__(self, config: BartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "e53490955021175d7bbb0bf9c5372b03ff5d00a5",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_big_bird import BigBirdConfig\n \n \n@@ -152,7 +151,6 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1169,7 +1167,6 @@ def set_attention_type(self, value: str, layer_idx=None):\n         if not self.training:\n             self.self.eval()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1277,7 +1274,6 @@ def set_attention_type(self, value: str, layer_idx=None):\n         if self.add_cross_attention:\n             self.crossattention.set_attention_type(value, layer_idx=layer_idx)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "f81a4aa2eba7b4637519052749ed7030846ed79c",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -50,7 +50,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_bigbird_pegasus import BigBirdPegasusConfig\n \n \n@@ -136,7 +135,6 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1241,7 +1239,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ce985972f7ae05e98ea90f3b3a4fb0df5fc71e2e",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_biogpt import BioGptConfig\n \n \n@@ -164,7 +163,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -272,7 +270,6 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.intermediate_size, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "420e511792c179662969b72cc6594ad349bd201f",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n     is_torch_flex_attn_available,\n     logger,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..bart.modeling_bart import (\n     BartAttention,\n     BartDecoderLayer,\n@@ -97,7 +96,6 @@ def __init__(self, config: BioGptConfig, layer_idx: Optional[int] = None):\n         del self.encoder_attn\n         del self.encoder_attn_layer_norm\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0fda5e6bfb7a43b850aca87d618e29299b3320ee",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_bitnet import BitNetConfig\n \n@@ -177,7 +176,6 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         )\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -235,7 +233,6 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         self.input_layernorm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "4ddf7a1d7f02a54baa681df02dbccd4acc2c0309",
            "filename": "src/transformers/models/bitnet/modular_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodular_bitnet.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -59,7 +58,6 @@ def __init__(self, config: BitNetConfig, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attn_sub_norm = BitNetRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b6681fd9a1eb6be8f55a4afaea2a93a1273b004d",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -49,7 +49,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..blenderbot_small import BlenderbotSmallForConditionalGeneration, BlenderbotSmallModel\n from .configuration_blenderbot import BlenderbotConfig\n \n@@ -185,7 +184,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -362,7 +360,6 @@ def __init__(self, config: BlenderbotConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b8961541028eb971bd719855a29433bc7614d5ef",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -47,7 +47,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_blenderbot_small import BlenderbotSmallConfig\n \n \n@@ -169,7 +168,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -354,7 +352,6 @@ def __init__(self, config: BlenderbotSmallConfig, layer_idx: Optional[int] = Non\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "aec11500f912a1092deddbd03cd755ab4c13a2f9",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_blip import BlipTextConfig\n \n \n@@ -127,7 +126,6 @@ def save_attention_map(self, attention_map):\n     def get_attention_map(self):\n         return self.attention_map\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -255,7 +253,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -324,7 +321,6 @@ def __init__(self, config, layer_num):\n         self.intermediate = BlipTextIntermediate(config)\n         self.output = BlipTextOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d5c726ef67e17fc529dba25160dd48b97d1aaa36",
            "filename": "src/transformers/models/blt/modeling_blt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblt%2Fmodeling_blt.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_blt import (\n     BltConfig,\n@@ -134,7 +133,6 @@ def __init__(self, config, layer_idx: int):\n \n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -288,7 +286,6 @@ def __init__(self, config: BltConfig, layer_idx: int):\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n         self.is_causal = True\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -362,7 +359,6 @@ def __init__(self, config: BltConfig, layer_idx: int, hidden_size: Optional[int]\n         self.k_norm = BltRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.is_causal = False\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "778ae73cf4bdfcfa255a29e7b8e9e6b83c2164b5",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n \n@@ -311,7 +310,6 @@ def _init_rope(self):\n             else:\n                 raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -380,7 +378,6 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -453,7 +450,6 @@ def __init__(self, config: ChameleonConfig, layer_idx: int):\n         self.input_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = ChameleonRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0db1f10077dc3752e30a765be40dad779f378ffb",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_clvp import (\n     ClvpConfig,\n     ClvpDecoderConfig,\n@@ -298,7 +297,6 @@ def __init__(self, config, layer_idx=None):\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.FloatTensor,\n@@ -603,7 +601,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = ClvpDecoderMLP(inner_dim, config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "505b9cc48f5cd873ab86b32ce2968cb6c4064a71",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cohere import CohereConfig\n \n@@ -228,7 +227,6 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n                 hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -289,7 +287,6 @@ def __init__(self, config: CohereConfig, layer_idx: int):\n         self.mlp = CohereMLP(config)\n         self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "151dd1022870c2cc4b56bff91f087a1f27f78947",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaForCausalLM,\n@@ -145,7 +144,6 @@ def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n                 hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -206,7 +204,6 @@ def __init__(self, config: CohereConfig, layer_idx: int):\n         self.mlp = CohereMLP(config)\n         self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "30ab8d6e8d6a1e1161b81f4715d7a76221563652",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_cohere2 import Cohere2Config\n \n@@ -196,7 +195,6 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -267,7 +265,6 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "aeb330b8b7821a82ac2a77ee6f0bca43d29fa9d5",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..cohere.modeling_cohere import (\n     CohereAttention,\n     CohereDecoderLayer,\n@@ -285,7 +284,6 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -336,7 +334,6 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "edebfcb4f9518e086cca199b2620cffa5bb580a0",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n@@ -266,7 +265,6 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -322,7 +320,6 @@ def __init__(self, config: CsmConfig, layer_idx: int):\n         self.input_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = CsmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0dc363195c76b7a6671affb1189ab8af3cc9bd6a",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -34,7 +34,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_dbrx import DbrxConfig\n \n@@ -355,7 +354,6 @@ def __init__(self, config: DbrxConfig, layer_idx: Optional[int] = None):\n         )\n         self.norm_2 = nn.LayerNorm(config.d_model, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -398,7 +396,6 @@ def __init__(self, config: DbrxConfig, layer_idx: int):\n         )\n         self.ffn = DbrxFFN(config=config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c2b17aacfff39caa685712fce8593738392e13f8",
            "filename": "src/transformers/models/dbrx/modular_dbrx.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodular_dbrx.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..llama.modeling_llama import (\n     LlamaRotaryEmbedding,\n@@ -254,7 +253,6 @@ def __init__(self, config: DbrxConfig, layer_idx: Optional[int] = None):\n         )\n         self.norm_2 = nn.LayerNorm(config.d_model, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -297,7 +295,6 @@ def __init__(self, config: DbrxConfig, layer_idx: int):\n         )\n         self.ffn = DbrxFFN(config=config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b941a6fdb9ef01d56af15581053493aa8426d70a",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_decision_transformer import DecisionTransformerConfig\n \n \n@@ -189,7 +188,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n \n         return attn_output, attn_weights\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n@@ -320,7 +318,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = DecisionTransformerGPT2MLP(inner_dim, config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "0ea55ab53686e13db064654b6a644663f5cbadf3",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n@@ -303,7 +302,6 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -383,7 +381,6 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n         self.input_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "e8be9d72f36aeb9b3691a82a4f1231f4f393ca8b",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -25,7 +25,6 @@\n from ...utils import (\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -354,7 +353,6 @@ def __init__(self, config: DeepseekV2Config, layer_idx: Optional[int] = None):\n \n         self.scaling = self.qk_head_dim ** (-0.5)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d7aee681e3bf68e8a3d6248eb33703dbd04e4788",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n@@ -376,7 +375,6 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -461,7 +459,6 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n         self.input_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DeepseekV3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d0a496dc3eea21672a9ecbfd5f5bf0603d88c239",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -11,7 +11,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n@@ -216,7 +215,6 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n                 mscale = yarn_get_mscale(scaling_factor, mscale_all_dim)\n                 self.scaling = self.scaling * mscale * mscale\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "15acf140f3acaac4c7d72ae15bfccc3a822542ec",
            "filename": "src/transformers/models/deprecated/ernie_m/modeling_ernie_m.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fernie_m%2Fmodeling_ernie_m.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -34,7 +34,6 @@\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_ernie_m import ErnieMConfig\n \n \n@@ -119,7 +118,6 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -242,7 +240,6 @@ def prune_heads(self, heads):\n         self.self_attn.all_head_size = self.self_attn.attention_head_size * self.self_attn.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -285,7 +282,6 @@ def __init__(self, config):\n         else:\n             self.activation = config.hidden_act\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "702bb626d8b4495d2e6ebf2d4722fdc432bbb343",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ....modeling_outputs import MoECausalLMOutputWithPast, MoEModelOutputWithPastAndCrossAttentions\n from ....modeling_utils import PreTrainedModel\n from ....utils import DUMMY_INPUTS, DUMMY_MASK, auto_docstring, is_torch_fx_proxy\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_gptsan_japanese import GPTSanJapaneseConfig\n \n \n@@ -244,7 +243,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -371,7 +369,6 @@ def __init__(self, config, has_relative_attention_bias=False):\n         )\n         self.norm = nn.LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n@@ -445,7 +442,6 @@ def __init__(self, config, ext_layer=False):\n         self.self_attn = GPTSanJapaneseLayerSelfAttention(config)\n         self.feed_forward = GPTSanJapaneseLayerDenseFF(config) if ext_layer else GPTSanJapaneseLayerSparseFF(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "fb3c9276a2dd14b65afe6e0da075b5d6a227da12",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_mega import MegaConfig\n \n \n@@ -1172,7 +1171,6 @@ def __init__(self, config: MegaConfig):\n         else:\n             self.cross_attn = None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "80737a3e9d60958070e59167268de23802e01063",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_nezha import NezhaConfig\n \n \n@@ -167,7 +166,6 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -307,7 +305,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -374,7 +371,6 @@ def __init__(self, config):\n         self.intermediate = NezhaIntermediate(config)\n         self.output = NezhaOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d0ad72f1b09fac8f3386548ec6241f466f9ab9ac",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ....modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_open_llama import OpenLlamaConfig\n \n \n@@ -268,7 +267,6 @@ def _init_rope(self):\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -360,7 +358,6 @@ def __init__(self, config: OpenLlamaConfig):\n         self.input_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = OpenLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c086aa0388a8dc154bc9dbb52edd45d47de4fd73",
            "filename": "src/transformers/models/deprecated/qdqbert/modeling_qdqbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fqdqbert%2Fmodeling_qdqbert.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -48,7 +48,6 @@\n     replace_return_docstrings,\n     requires_backends,\n )\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_qdqbert import QDQBertConfig\n \n \n@@ -167,7 +166,6 @@ def transpose_for_scores(self, x):\n         x = x.view(*new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -311,7 +309,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -387,7 +384,6 @@ def __init__(self, config):\n         self.intermediate = QDQBertIntermediate(config)\n         self.output = QDQBertOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "a430789e77cdeb2061f3bbc38bba4ff050f1d38f",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -34,7 +34,6 @@\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ....utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_realm import RealmConfig\n \n \n@@ -139,7 +138,6 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         x = x.view(new_x_shape)\n         return x.permute(0, 2, 1, 3)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -283,7 +281,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -350,7 +347,6 @@ def __init__(self, config):\n         self.intermediate = RealmIntermediate(config)\n         self.output = RealmOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "5d6d7e30f669ead526f5c10f6fbd41bfdcc91d94",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -28,7 +28,6 @@\n from ....modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ....modeling_utils import PreTrainedModel\n from ....utils import add_start_docstrings, logging, replace_return_docstrings\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_speech_to_text_2 import Speech2Text2Config\n \n \n@@ -144,7 +143,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -286,7 +284,6 @@ def __init__(self, config: Speech2Text2Config):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "291239c28a621c3b16f56e9e832a7cf2b365eaa3",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ....utils.deprecation import deprecate_kwarg\n from .configuration_xlm_prophetnet import XLMProphetNetConfig\n \n \n@@ -627,7 +626,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_attn_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -773,7 +771,6 @@ def _shape(self, tensor, seq_len, batch_size):\n     def prepare_for_onnx_export_(self):\n         self.onnx_trace = True\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1101,7 +1098,6 @@ def __init__(self, config: XLMProphetNetConfig):\n         self.feed_forward = XLMProphetNetFeedForward(config, config.decoder_ffn_dim)\n         self.feed_forward_layer_norm = LayerNorm(config.hidden_size)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "fea80e9714601a9a5df3ca593869e7b79bbfd480",
            "filename": "src/transformers/models/dia/modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fmodeling_dia.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -48,7 +48,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_dia import DiaConfig, DiaDecoderConfig, DiaEncoderConfig\n from .generation_dia import DiaGenerationMixin\n \n@@ -269,7 +268,6 @@ def __init__(self, config: Union[DiaEncoderConfig, DiaDecoderConfig], layer_idx:\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "58212c9841ef6bab141a4433f30cc0107ec83375",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -44,7 +44,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_diffllama import DiffLlamaConfig\n \n@@ -155,7 +154,6 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.lambda_k2 = nn.Parameter(torch.normal(0, config.lambda_std_dev, size=(self.head_dim,)))\n         self.groupnorm = nn.RMSNorm(2 * self.head_dim, eps=config.rms_norm_eps, elementwise_affine=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -234,7 +232,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -376,7 +373,6 @@ class DiffLlamaSdpaAttention(DiffLlamaAttention):\n     \"\"\"\n \n     # Adapted from DiffLlamaAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -492,7 +488,6 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: int):\n         self.input_layernorm = DiffLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = DiffLlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "aadd28fed6876afb0d0d58b5deda9b759e73fe56",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -25,7 +25,6 @@\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n from ...modeling_utils import PreTrainedModel\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaForCausalLM\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -92,7 +91,6 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: Optional[int] = None):\n         self.lambda_k2 = nn.Parameter(torch.normal(0, config.lambda_std_dev, size=(self.head_dim,)))\n         self.groupnorm = nn.RMSNorm(2 * self.head_dim, eps=config.rms_norm_eps, elementwise_affine=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -171,7 +169,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -313,7 +310,6 @@ class DiffLlamaSdpaAttention(DiffLlamaAttention):\n     \"\"\"\n \n     # Adapted from DiffLlamaAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a1f1fc0477bc84ef05f81209a1bb0321a70c608e",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_doge import DogeConfig\n \n@@ -259,7 +258,6 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -440,7 +438,6 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n         self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "12cc98b08b649bb59dca7dd970333fd162dfca9c",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, is_torch_flex_attn_available\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..llama.modeling_llama import (\n     LlamaForSequenceClassification,\n@@ -355,7 +354,6 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.q_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = DogeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -524,7 +522,6 @@ def __init__(self, config: DogeConfig, layer_idx: Optional[int] = None):\n         self.mlp = DogeMLP(config) if not config.is_moe else DogeCDMoE(config)\n         self.post_attention_residual = nn.Parameter(torch.ones(config.hidden_size))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "5dd5dca524df83484e1d298cebf82f437b00bc05",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_dots1 import Dots1Config\n \n@@ -199,7 +198,6 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.k_norm = Dots1RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -380,7 +378,6 @@ def __init__(self, config: Dots1Config, layer_idx: int):\n         self.post_attention_layernorm = Dots1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ef54860684c5c2d85b041c5877cd1df72411bed2",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n@@ -142,7 +141,6 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -236,7 +234,6 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         self.post_attention_layernorm = Emu3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.dropout = nn.Dropout(config.attention_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "be46f0b115df7f31f633358e2e998046823ef6b5",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..chameleon.modeling_chameleon import (\n     ChameleonPreTrainedModel,\n     ChameleonVQVAEEncoderConvDownsample,\n@@ -51,7 +50,6 @@ def __init__(self, config: Emu3Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.dropout = nn.Dropout(config.attention_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "43cddbcdacdbeda093aceb9ffd798e28213937a7",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -34,7 +34,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_ernie4_5 import Ernie4_5Config\n \n@@ -193,7 +192,6 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -270,7 +268,6 @@ def __init__(self, config: Ernie4_5Config, layer_idx: int):\n         self.input_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3ce962707cc12a77d08a23e1b1a1736ff43643fb",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_ernie4_5_moe import Ernie4_5_MoeConfig\n \n@@ -215,7 +214,6 @@ def __init__(self, config: Ernie4_5_MoeConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -409,7 +407,6 @@ def __init__(self, config, layer_idx):\n         self.input_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n         self.post_attention_layernorm = Ernie4_5_MoeRMSNorm(config.hidden_size, config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c71dcaf91c159dfccb350d8140ce60e771fcf641",
            "filename": "src/transformers/models/evolla/modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodeling_evolla.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -45,7 +45,6 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_evolla import EvollaConfig, SaProtConfig\n \n@@ -934,7 +933,6 @@ def cross_attention(\n \n         return context_layer\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         query_states,\n@@ -1154,7 +1152,6 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1215,7 +1212,6 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n                 protein_encoder_dim=config.hidden_size,\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1116c2690b9a18a383a53d9bd3e2d477c7d1ba1d",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n     is_torch_flex_attn_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..esm.modeling_esm import (\n     EsmAttention,\n@@ -602,7 +601,6 @@ def cross_attention(\n \n         return context_layer\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         query_states,\n@@ -702,7 +700,6 @@ def __init__(self, config: EvollaConfig, layer_idx: int):\n                 protein_encoder_dim=config.hidden_size,\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "21e5610664f2317c93369dd4721c976c2bf1bc93",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_exaone4 import Exaone4Config\n \n \n@@ -201,7 +200,6 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -280,7 +278,6 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.post_attention_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Exaone4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "7e3ce3a89b9d2a9c617e878dd9f0bb64929edeae",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n     TransformersKwargs,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaForCausalLM,\n     LlamaForQuestionAnswering,\n@@ -287,7 +286,6 @@ def __init__(self, config: Exaone4Config, layer_idx: int):\n         self.q_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Exaone4RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "dcf995bf28b06fc8d59ab6be9dc86658adacc116",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -357,7 +356,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         )\n         self.key_multiplier = config.key_multiplier\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1076,7 +1074,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.input_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b670bbec7d83757b1ba24b811e4d63592207dc51",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -51,7 +51,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config\n \n@@ -204,7 +203,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.key_multiplier = config.key_multiplier\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -842,7 +840,6 @@ def __init__(self, config: FalconH1Config, layer_idx: int):\n         self.input_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = FalconH1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "492949be837a1bf692fe8a1ec8aef51160c585e5",
            "filename": "src/transformers/models/flex_olmo/modeling_flex_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodeling_flex_olmo.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_flex_olmo import FlexOlmoConfig\n \n@@ -213,7 +212,6 @@ def __init__(self, config: FlexOlmoConfig, layer_idx: Optional[int] = None):\n         self.q_norm = FlexOlmoRMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = FlexOlmoRMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -335,7 +333,6 @@ def __init__(self, config: FlexOlmoConfig, layer_idx: int):\n         self.post_attention_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = FlexOlmoRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "39a616c9b09b40a0dd8db4532a838feb93ddad0b",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gemma import GemmaConfig\n \n@@ -213,7 +212,6 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -270,7 +268,6 @@ def __init__(self, config: GemmaConfig, layer_idx: int):\n         self.post_attention_layernorm = GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3b64292bc67cde46b76b5336ca652449c0950921",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gemma2 import Gemma2Config\n \n@@ -228,7 +227,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -289,7 +287,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "61de0258b2213746f8c2894d9e9ba4e18a3fdfd2",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -267,7 +266,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.scaling = config.query_pre_attn_scalar**-0.5\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -328,7 +326,6 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9669b8691584008edff944b0f28d2181fa663975",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n@@ -292,7 +291,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -355,7 +353,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8d92700c3edb9aa76bac0d037f84ccc3ad49ba14",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -398,7 +397,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.q_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = Gemma3RMSNorm(dim=config.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -461,7 +459,6 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "7717caa34b6bbddcbb1ad2f53d0b7665e0a987c7",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_gemma3n import Gemma3nAudioConfig, Gemma3nConfig, Gemma3nTextConfig, Gemma3nVisionConfig\n \n@@ -1309,7 +1308,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n                 config.layer_types[layer_idx]\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1406,7 +1404,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.per_layer_projection = nn.Linear(self.hidden_size_per_layer_input, self.hidden_size, bias=False)\n         self.post_per_layer_input_norm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "507c6498315e9121e34492022f31721a2a722fa4",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n@@ -1759,7 +1758,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n                 config.layer_types[layer_idx]\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1848,7 +1846,6 @@ def __init__(self, config: Gemma3nTextConfig, layer_idx: int):\n         self.per_layer_projection = nn.Linear(self.hidden_size_per_layer_input, self.hidden_size, bias=False)\n         self.post_per_layer_input_norm = Gemma3nRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ef06802d35fa7f444b7a3d39dbbbb031199717e9",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n     logging,\n     torch_int,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_git import GitConfig, GitVisionConfig\n \n \n@@ -142,7 +141,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -247,7 +245,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -307,7 +304,6 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = GitIntermediate(config)\n         self.output = GitOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "669fcb4baa4159c32fc1993c769850dcf3f7d3e9",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm import GlmConfig\n \n@@ -173,7 +172,6 @@ def __init__(self, config: GlmConfig, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -286,7 +284,6 @@ def __init__(self, config: GlmConfig, layer_idx: int):\n         self.input_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "08e16ab39ee53f9cd39ab5268d874206bf9c9b43",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4 import Glm4Config\n \n@@ -75,7 +74,6 @@ def __init__(self, config: Glm4Config, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -223,7 +221,6 @@ def __init__(self, config: Glm4Config, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "7b0ee26811a7f6267af072268862c2cbb7037b02",
            "filename": "src/transformers/models/glm4/modular_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodular_glm4.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..glm.modeling_glm import GlmAttention, GlmForCausalLM, GlmForSequenceClassification, GlmForTokenClassification\n from ..phi3.modeling_phi3 import Phi3MLP\n from .configuration_glm4 import Glm4Config\n@@ -51,7 +50,6 @@ def __init__(self, config: Glm4Config, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f341a86569293632af14f7bdc5364496665b4b55",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4_moe import Glm4MoeConfig\n \n@@ -154,7 +153,6 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: Optional[int] = None):\n             self.q_norm = Glm4MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n             self.k_norm = Glm4MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -367,7 +365,6 @@ def __init__(self, config: Glm4MoeConfig, layer_idx: int):\n         self.input_layernorm = Glm4MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Glm4MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f00469a19767a28706be6b345edd0a8b5f7e9465",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_glm4v_moe import Glm4vMoeConfig, Glm4vMoeTextConfig, Glm4vMoeVisionConfig\n \n@@ -188,7 +187,6 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.rope_scaling = config.rope_scaling\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -399,7 +397,6 @@ def __init__(self, config: Glm4vMoeTextConfig, layer_idx: int):\n         self.input_layernorm = Glm4vMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Glm4vMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9a5c33a28cbd7cf2ecd3745292f2210e37c664f4",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_gpt2 import GPT2Config\n \n \n@@ -198,7 +197,6 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n \n         return attn_output, attn_weights\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],\n@@ -324,7 +322,6 @@ def __init__(self, config, layer_idx=None):\n \n         self.mlp = GPT2MLP(inner_dim, config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.FloatTensor]],"
        },
        {
            "sha": "a16cc0a044bcf2e4ad82645721645ecf1977f024",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_gpt_neox import GPTNeoXConfig\n \n@@ -313,7 +312,6 @@ def __init__(self, config: GPTNeoXConfig, layer_idx: int):\n         self.input_layernorm = GPTNeoXRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = GPTNeoXRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "628f2f4c23ec544249f944ecf4e9b22da8599ddb",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_gpt_oss import GptOssConfig\n \n@@ -297,7 +296,6 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -353,7 +351,6 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "616fd35d7f4ac05ffc37b5e7f803f266f613ac5f",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -255,7 +254,6 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         )\n         self.sinks = nn.Parameter(torch.empty(config.num_attention_heads))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -311,7 +309,6 @@ def __init__(self, config: GptOssConfig, layer_idx: int):\n         self.post_attention_layernorm = GptOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3855471d6ecf46277c64a7304eb0b84049b5e089",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_granite import GraniteConfig\n \n@@ -141,7 +140,6 @@ def __init__(self, config: GraniteConfig, layer_idx: Optional[int] = None):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -234,7 +232,6 @@ def __init__(self, config: GraniteConfig, layer_idx: int):\n         self.post_attention_layernorm = GraniteRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.residual_multiplier = config.residual_multiplier\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "fa18c122e112f305b5c5be3225e7da98aa90f891",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -51,7 +50,6 @@ def __init__(self, config: GraniteConfig, layer_idx: int):\n         self.residual_multiplier = config.residual_multiplier\n         self.self_attn = GraniteAttention(config=config, layer_idx=layer_idx)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b1f3d71066b22fc62217ae3190a666176164212f",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_granitemoe import GraniteMoeConfig\n \n@@ -333,7 +332,6 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -389,7 +387,6 @@ def __init__(self, config: GraniteMoeConfig, layer_idx: int):\n \n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a09b5ef5f2e8edde873d1223c711ee798039127d",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_granitemoehybrid import GraniteMoeHybridConfig\n@@ -122,7 +121,6 @@ def __init__(self, config: GraniteMoeHybridConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "13a506de2ccd42ad4f7d08f6624c4d52d9e2a246",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import can_return_tuple, check_model_inputs\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n@@ -352,7 +351,6 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -409,7 +407,6 @@ def __init__(self, config: GraniteMoeSharedConfig, layer_idx: int):\n         self.residual_multiplier = config.residual_multiplier  # Only diff with mixtral!\n         self.shared_mlp = None if config.shared_intermediate_size == 0 else GraniteMoeSharedMLP(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "5abbf500886a3fcc0e9506794e63d501e6caa234",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_helium import HeliumConfig\n \n@@ -215,7 +214,6 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n         )\n         self.o_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -271,7 +269,6 @@ def __init__(self, config: HeliumConfig, layer_idx: Optional[int] = None):\n         self.input_layernorm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = HeliumRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "acd2252777252b5484262cf29c46e141ba48f02d",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_hunyuan_v1_dense import HunYuanDenseV1Config\n \n@@ -180,7 +179,6 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n         self.query_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -239,7 +237,6 @@ def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n         self.post_attention_layernorm = HunYuanDenseV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "73af97375b040bb5e6075251214f304ed088ee42",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n \n@@ -180,7 +179,6 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n         self.query_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.key_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -318,7 +316,6 @@ def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n         self.post_attention_layernorm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2c39e9629e00eecafc10bcdbc28ab7818fdf2aac",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_idefics import IdeficsConfig\n from .perceiver import IdeficsPerceiverResampler\n@@ -563,7 +562,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -651,7 +649,6 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = IdeficsRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.dropout = config.dropout\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     @auto_docstring\n     def forward(\n         self,\n@@ -755,7 +752,6 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         if not (hasattr(self, \"alpha_cross_attn\") and hasattr(self, \"alpha_dense\")):\n             raise ValueError(\"Alpha parameters not initialized correctly!\")\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "778ca95adef2671d80a050f5542f34a39d35d0c0",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n@@ -572,7 +571,6 @@ def __init__(self, config, layer_idx: Optional[int] = None) -> None:\n \n         self.is_causal = False\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         latents: torch.Tensor,\n@@ -653,7 +651,6 @@ def __init__(self, config, layer_idx: int):\n             hidden_act=config.hidden_act,\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         latents: torch.Tensor,"
        },
        {
            "sha": "5c0bdea176bb2ebc845d9fd621e6ea3d68ad553c",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_informer import InformerConfig\n \n \n@@ -426,7 +425,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -549,7 +547,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -860,7 +857,6 @@ def __init__(self, config: InformerConfig, layer_idx: Optional[int] = None):\n                 layer_idx=layer_idx,\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b8461929dcf1b8a248ce2b2e470e249110b7a35f",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n     auto_docstring,\n     is_torch_flex_attn_available,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..bart.modeling_bart import BartAttention\n from ..time_series_transformer.modeling_time_series_transformer import (\n     TimeSeriesFeatureEmbedder,\n@@ -239,7 +238,6 @@ def __init__(\n     def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n         return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8be936945022a6f17520961173ceb308932bd275",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_jamba import JambaConfig\n@@ -229,7 +228,6 @@ def __init__(self, config: JambaConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1775ee6cbe99dc1449127acf2cd9f1649bfdce9c",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_jetmoe import JetMoeConfig\n \n@@ -495,7 +494,6 @@ def __init__(self, config: JetMoeConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = JetMoeRMSNorm(config.hidden_size)\n         self.self_attention = JetMoeAttention(config, layer_idx)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "cc65b4e65d3a38c94bf2b66fbed2b484fe9abd22",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_kosmos2 import Kosmos2Config, Kosmos2TextConfig, Kosmos2VisionConfig\n \n \n@@ -700,7 +699,6 @@ def __init__(\n         if add_inner_attn_layernorm:\n             self.inner_attn_ln = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -834,7 +832,6 @@ def __init__(self, config: Kosmos2TextConfig, layer_idx=None):\n         self.ffn = Kosmos2TextFFN(config)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f8f59614b02cd03d4477f48f63cbd38684d24a2b",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_kyutai_speech_to_text import KyutaiSpeechToTextConfig\n \n@@ -429,7 +428,6 @@ def __init__(\n             self.rope_theta = config.rope_theta\n             self.rotary_emb = KyutaiSpeechToTextRotaryEmbedding(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -511,7 +509,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -627,7 +624,6 @@ class KyutaiSpeechToTextSdpaAttention(KyutaiSpeechToTextAttention):\n     \"\"\"\n \n     # Adapted from KyutaiSpeechToTextAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -737,7 +733,6 @@ def __init__(self, config: KyutaiSpeechToTextConfig, layer_idx: int, use_flexibl\n \n         self._attn_implementation = config._attn_implementation\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ab73f67456aa1107268dfd393b4ef4c318eac730",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_led import LEDConfig\n \n \n@@ -767,7 +766,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -963,7 +961,6 @@ def __init__(self, config: LEDConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3ea3d2258655b78189f7de94b666745bd44ca693",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available\n from .configuration_lfm2 import Lfm2Config\n@@ -349,7 +348,6 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.q_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2RMSNorm(self.head_dim, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -430,7 +428,6 @@ def __init__(\n         self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def cuda_kernels_forward(\n         self,\n         x: torch.Tensor,\n@@ -465,7 +462,6 @@ def cuda_kernels_forward(\n         y = self.out_proj(y.transpose(-1, -2).contiguous())\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def slow_forward(\n         self,\n         x: torch.Tensor,\n@@ -504,7 +500,6 @@ def slow_forward(\n         y = self.out_proj(y)\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -530,7 +525,6 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0f3dad129548f6944ced8e4eb2de40043925cb8c",
            "filename": "src/transformers/models/lfm2/modular_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodular_lfm2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available\n from ..bamba.modeling_bamba import apply_mask_to_padding_states\n from ..llama.modeling_llama import (\n@@ -229,7 +228,6 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         del self.o_proj\n         del self.attention_dropout\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -295,7 +293,6 @@ def __init__(\n         self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def cuda_kernels_forward(\n         self,\n         x: torch.Tensor,\n@@ -330,7 +327,6 @@ def cuda_kernels_forward(\n         y = self.out_proj(y.transpose(-1, -2).contiguous())\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def slow_forward(\n         self,\n         x: torch.Tensor,\n@@ -369,7 +365,6 @@ def slow_forward(\n         y = self.out_proj(y)\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -395,7 +390,6 @@ def __init__(self, config: Lfm2Config, layer_idx: int):\n         self.operator_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.ffn_norm = Lfm2RMSNorm(config.hidden_size, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f6b4d97a289f573387670726e955ee31b2ceaf14",
            "filename": "src/transformers/models/lfm2_moe/modeling_lfm2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_moe%2Fmodeling_lfm2_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import is_causal_conv1d_available\n from .configuration_lfm2_moe import Lfm2MoeConfig\n@@ -412,7 +411,6 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n         self.q_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n         self.k_layernorm = Lfm2MoeRMSNorm(self.head_dim, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -493,7 +491,6 @@ def __init__(\n         self.in_proj = nn.Linear(config.hidden_size, 3 * config.hidden_size, bias=self.bias)\n         self.out_proj = nn.Linear(config.hidden_size, config.hidden_size, bias=self.bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def cuda_kernels_forward(\n         self,\n         x: torch.Tensor,\n@@ -528,7 +525,6 @@ def cuda_kernels_forward(\n         y = self.out_proj(y.transpose(-1, -2).contiguous())\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def slow_forward(\n         self,\n         x: torch.Tensor,\n@@ -567,7 +563,6 @@ def slow_forward(\n         y = self.out_proj(y)\n         return y\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -597,7 +592,6 @@ def __init__(self, config: Lfm2MoeConfig, layer_idx: int):\n         self.operator_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n         self.ffn_norm = Lfm2MoeRMSNorm(config.hidden_size, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d38cabcc9413d25862beb7c5fdd273ce995b3d21",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import can_return_tuple\n from ..auto.modeling_auto import AutoModelForKeypointDetection\n from .configuration_lightglue import LightGlueConfig\n@@ -200,7 +199,6 @@ def __init__(self, config: LightGlueConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2b29468d671e20dd445a7c288d80640d7b215652",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_llama import LlamaConfig\n \n@@ -220,7 +219,6 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -276,7 +274,6 @@ def __init__(self, config: LlamaConfig, layer_idx: int):\n         self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "3a9696a43eebae83eaf890011f48f45c1d607786",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -309,7 +309,6 @@ def __init__(self, config: Llama4TextConfig, layer_idx):\n         if self.config.use_qk_norm and self.use_rope:\n             self.qk_norm = Llama4TextL2Norm(config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -386,7 +385,6 @@ def __init__(self, config, layer_idx):\n         self.input_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "4cfe67d7bef285f333afc62a7204e3fca7c2574a",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_longcat_flash import LongcatFlashConfig\n \n@@ -336,7 +335,6 @@ def __init__(self, config, layer_idx: int):\n         self.mla_scale_q_lora = (config.hidden_size / self.q_lora_rank) ** 0.5\n         self.mla_scale_kv_lora = (config.hidden_size / self.kv_lora_rank) ** 0.5\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f7d655acded1a2368178e8ed8407347683ffb250",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -44,7 +44,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_longt5 import LongT5Config\n \n \n@@ -440,7 +439,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1004,7 +1002,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1098,7 +1095,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = LongT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -1152,7 +1148,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(LongT5LayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "ccbc72539831b54872c97d86b78c3061c854e095",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -50,7 +50,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_m2m_100 import M2M100Config\n \n \n@@ -266,7 +265,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -443,7 +441,6 @@ def __init__(self, config: M2M100Config, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "01715e4607e05a9e5a249ad1486ea7b8a8fe5e82",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -49,7 +49,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_marian import MarianConfig\n \n \n@@ -186,7 +185,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -371,7 +369,6 @@ def __init__(self, config: MarianConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "096f26546c67fa50053a54cafaa2e21f00bd355a",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -51,7 +51,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mbart import MBartConfig\n \n \n@@ -197,7 +196,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -372,7 +370,6 @@ def __init__(self, config: MBartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "87d74d8aad966cd67f3c282d3431d7273187ba51",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_megatron_bert import MegatronBertConfig\n \n \n@@ -125,7 +124,6 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -243,7 +241,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -313,7 +310,6 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = MegatronBertIntermediate(config)\n         self.output = MegatronBertOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "90291151aaacaf3c9badb308752172c1da65be80",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mimi import MimiConfig\n \n \n@@ -643,7 +642,6 @@ def __init__(self, config: MimiConfig, layer_idx: Optional[int] = None):\n         self.rotary_emb = MimiRotaryEmbedding(config)\n         self.sliding_window = config.sliding_window  # Ignore copy\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -720,7 +718,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -831,7 +828,6 @@ class MimiSdpaAttention(MimiAttention):\n     \"\"\"\n \n     # Adapted from MimiAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -932,7 +928,6 @@ def __init__(self, config: MimiConfig, layer_idx: int):\n         self.self_attn_layer_scale = MimiLayerScale(config)\n         self.mlp_layer_scale = MimiLayerScale(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "908bd1429abd50faf509b91cf5e4dc8f7483f1d3",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_minimax import MiniMaxConfig\n \n@@ -163,7 +162,6 @@ def decay_factors(self, slope_rate):\n \n         return query_decay, key_decay, diagonal_decay\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -352,7 +350,6 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -502,7 +499,6 @@ def __init__(self, config: MiniMaxConfig, layer_idx: int):\n             self.attn_alpha_factor = config.full_attn_alpha_factor\n             self.attn_beta_factor = config.full_attn_beta_factor\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "66fa77c78f9d9f11dac955135ec2308f8d6c9a74",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_outputs import MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..mixtral.configuration_mixtral import MixtralConfig\n from ..mixtral.modeling_mixtral import (\n@@ -279,7 +278,6 @@ def decay_factors(self, slope_rate):\n \n         return query_decay, key_decay, diagonal_decay\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "25ec419b46d9bd5624494f5a37e9039f6d455011",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -26,7 +26,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_ministral import MinistralConfig\n \n@@ -138,7 +137,6 @@ def __init__(self, config, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -217,7 +215,6 @@ def __init__(self, config: MinistralConfig, layer_idx: int):\n         self.post_attention_layernorm = MinistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "392664a970473464ff532a28d2ecd2f48c93414e",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mistral import MistralConfig\n \n \n@@ -137,7 +136,6 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -213,7 +211,6 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.input_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "68283279075a0c1b4c9cb0cd822081b3bdaded09",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -15,7 +15,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -51,7 +50,6 @@ def __init__(self, config: MistralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a5f127ca391fcbe870965f9d534b61df0297cb51",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -48,7 +48,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from .configuration_mixtral import MixtralConfig\n \n@@ -242,7 +241,6 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -299,7 +297,6 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "7c394c744e6458cc3c665b65fe645e154f19377c",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n@@ -230,7 +229,6 @@ def __init__(self, config: MixtralConfig, layer_idx: int):\n         self.input_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = MixtralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "21c85e8e004eee3e9c832f071e9afa6f407004ad",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n@@ -409,7 +408,6 @@ def __init__(\n         self.q_norm = MllamaTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n         self.k_norm = MllamaTextRMSNorm(self.head_dim, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -526,7 +524,6 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -609,7 +606,6 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int):\n \n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -685,7 +681,6 @@ def __init__(self, config: MllamaTextConfig, layer_idx: int) -> None:\n         self.post_attention_layernorm = MllamaTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.cross_attn_mlp_gate = torch.nn.Parameter(torch.zeros(1))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1ada5eae1ba028bd1347a564202c89b2bd0755a4",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_modernbert_decoder import ModernBertDecoderConfig\n \n@@ -228,7 +227,6 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n \n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -289,7 +287,6 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.mlp = ModernBertDecoderMLP(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "4eba5d74b7fd3ef67b53f8a6decbdebbfb37d445",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ..modernbert.modeling_modernbert import (\n     ModernBertEmbeddings,\n@@ -303,7 +302,6 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n \n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -364,7 +362,6 @@ def __init__(self, config: ModernBertDecoderConfig, layer_idx: Optional[int] = N\n         self.mlp_norm = nn.LayerNorm(config.hidden_size, eps=config.norm_eps, bias=config.norm_bias)\n         self.mlp = ModernBertDecoderMLP(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2a6025dd5d7b38f5fb4aec8828a1dabfcd9e8d9d",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -44,7 +44,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_moonshine import MoonshineConfig\n \n \n@@ -206,7 +205,6 @@ def __init__(\n         else:\n             self.head_dim_padding = 0\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -348,7 +346,6 @@ def __init__(self, config: MoonshineConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -408,7 +405,6 @@ def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9f6257f85a92ab1767519c3a4479ccefed7e76b4",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..glm.modeling_glm import GlmAttention, GlmRotaryEmbedding, apply_rotary_pos_emb\n from ..llama.modeling_llama import LlamaDecoderLayer, LlamaModel, eager_attention_forward\n from ..whisper.modeling_whisper import WhisperModel, shift_tokens_right\n@@ -305,7 +304,6 @@ def __init__(\n         else:\n             self.head_dim_padding = 0\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -440,7 +438,6 @@ def __init__(self, config: MoonshineConfig, layer_idx: Optional[int] = None):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n         self.final_layernorm = nn.LayerNorm(config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "4d815182c6e1ac9a9f43a7053a1f87dd0a0b645c",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -429,7 +428,6 @@ def __init__(self, config: MoshiConfig, layer_idx: Optional[int] = None, use_fle\n             self.rope_theta = config.rope_theta\n             self.rotary_emb = MoshiRotaryEmbedding(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -511,7 +509,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -627,7 +624,6 @@ class MoshiSdpaAttention(MoshiAttention):\n     \"\"\"\n \n     # Adapted from MoshiAttention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -737,7 +733,6 @@ def __init__(self, config: MoshiConfig, layer_idx: int, use_flexible_linear: boo\n \n         self._attn_implementation = config._attn_implementation\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c18bb9721a82e3d3e621b771aa4aa8291f35b30b",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mpt import MptConfig\n \n \n@@ -86,7 +85,6 @@ def __init__(self, config: MptConfig, layer_idx: Optional[int] = None):\n         self.out_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=False)\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -245,7 +243,6 @@ def _init_weights(self, module: nn.Module):\n             module.weight.data.fill_(1.0)\n \n     @staticmethod\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def _convert_to_mpt_cache(\n         past_key_values: tuple[tuple[torch.Tensor, torch.Tensor]],\n     ) -> tuple[tuple[torch.Tensor, torch.Tensor]]:"
        },
        {
            "sha": "39fd52f905a341d109ed3327be0b716a064037ae",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -47,7 +47,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mt5 import MT5Config\n \n \n@@ -277,7 +276,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -395,7 +393,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -429,7 +426,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = MT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -473,7 +469,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(MT5LayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "98b40e0b921d28882ec586add600cea28b6cefeb",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -55,7 +55,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel\n from .configuration_musicgen import MusicgenConfig, MusicgenDecoderConfig\n@@ -219,7 +218,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -335,7 +333,6 @@ def __init__(self, config: MusicgenDecoderConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=False)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1508b8f9aadb9d911535e4fdf96eea4a617f90cb",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -47,7 +47,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..auto.configuration_auto import AutoConfig\n from ..auto.modeling_auto import AutoModel, AutoModelForTextEncoding\n from .configuration_musicgen_melody import MusicgenMelodyConfig, MusicgenMelodyDecoderConfig\n@@ -226,7 +225,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -333,7 +331,6 @@ def __init__(self, config: MusicgenMelodyDecoderConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=False)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ad70b2cb851230cf02db875bd820a776ab037153",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -40,7 +40,6 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_mvp import MvpConfig\n \n \n@@ -124,7 +123,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -335,7 +333,6 @@ def __init__(self, config: MvpConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1ea97b9d3de08196e1d6d0567c6bd03e299f68be",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -40,7 +40,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -224,7 +223,6 @@ def __init__(self, config: NemotronConfig, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n         self.o_proj = nn.Linear(self.head_dim * self.num_heads, self.hidden_size, bias=config.attention_bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -298,7 +296,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -411,7 +408,6 @@ class NemotronSdpaAttention(NemotronAttention):\n     SDPA API.\n     \"\"\"\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -516,7 +512,6 @@ def __init__(self, config: NemotronConfig, layer_idx: int):\n         self.input_layernorm = NemotronLayerNorm1P(config.hidden_size, eps=config.norm_eps)\n         self.post_attention_layernorm = NemotronLayerNorm1P(config.hidden_size, eps=config.norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "079a8e0eb50f453319d6f6783a526a24e0f196e7",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, can_return_tuple, check_model_inputs\n from .configuration_nllb_moe import NllbMoeConfig\n \n@@ -456,7 +455,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "37cfd7944483df1c5f3cd9ea0ab1bc8e3c6e7cb0",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -20,7 +20,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_olmo import OlmoConfig\n \n@@ -154,7 +153,6 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -218,7 +216,6 @@ def __init__(self, config: OlmoConfig, layer_idx: int):\n         self.input_layernorm = OlmoLayerNorm(config.hidden_size)\n         self.post_attention_layernorm = OlmoLayerNorm(config.hidden_size)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a30d38e94be10c524ed6523b10102c9987a81db9",
            "filename": "src/transformers/models/olmo/modular_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodular_olmo.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -7,7 +7,6 @@\n from ...cache_utils import Cache\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -75,7 +74,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n \n class OlmoAttention(LlamaAttention):\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b91a27dd7425871d8c1506794f5778e846560376",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -22,7 +22,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_olmo2 import Olmo2Config\n \n@@ -149,7 +148,6 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -224,7 +222,6 @@ def __init__(self, config: Olmo2Config, layer_idx: int):\n         self.post_attention_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Olmo2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "daeb045593dda010d8b1fced610bad9d6448a821",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -9,7 +9,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import LlamaPreTrainedModel, LlamaRMSNorm, eager_attention_forward\n from ..olmo.configuration_olmo import OlmoConfig\n from ..olmo.modeling_olmo import (\n@@ -195,7 +194,6 @@ def __init__(self, config: Olmo2Config, layer_idx: Optional[int] = None):\n         self.q_norm = Olmo2RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n         self.k_norm = Olmo2RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -255,7 +253,6 @@ def __init__(self, config: Olmo2Config, layer_idx: int):\n         self.self_attn = Olmo2Attention(config=config, layer_idx=layer_idx)\n         del self.input_layernorm\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f710e2c7ec74871bd0c18f8c2a936c6ff3ea8505",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -37,7 +37,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_olmo3 import Olmo3Config\n \n@@ -167,7 +166,6 @@ def __init__(self, config: Olmo3Config, layer_idx: int):\n         self.attention_type = config.layer_types[layer_idx]\n         self.sliding_window = config.sliding_window if self.attention_type == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -243,7 +241,6 @@ def __init__(self, config: Olmo3Config, layer_idx: int):\n         self.post_attention_layernorm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9a9b4cd1d4b7864c42a9923de4b710c78b931ae2",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_olmoe import OlmoeConfig\n \n@@ -212,7 +211,6 @@ def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n             (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -339,7 +337,6 @@ def __init__(self, config: OlmoeConfig, layer_idx: int):\n         self.input_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = OlmoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0642a179f781b8e01ba69e4774433ff11a0ca560",
            "filename": "src/transformers/models/olmoe/modular_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodular_olmoe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n@@ -62,7 +61,6 @@ def __init__(self, config: OlmoeConfig, layer_idx: Optional[int] = None):\n             (config.hidden_size // config.num_attention_heads) * config.num_key_value_heads, eps=config.rms_norm_eps\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a233686a3c968b475209357bb53fcf5fbf2b0e36",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_opt import OPTConfig\n \n \n@@ -138,7 +137,6 @@ def __init__(\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=self.enable_bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -213,7 +211,6 @@ def __init__(self, config: OPTConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim, bias=config.enable_bias)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim, elementwise_affine=config.layer_norm_elementwise_affine)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d0b0e8170ab72bda641aeba965636da1275b57c4",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n \n@@ -231,7 +230,6 @@ def __init__(self, config: ParakeetEncoderConfig, layer_idx: int):\n         # global positional bias\n         self.bias_v = nn.Parameter(torch.zeros(config.num_attention_heads, self.head_dim))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "5da08e872723618ddc1fc0e7cde92209f4196762",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -49,7 +49,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_pegasus import PegasusConfig\n \n \n@@ -185,7 +184,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -362,7 +360,6 @@ def __init__(self, config: PegasusConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "e03b1f1aa6208e3294334ec6c7e98cf32f772b04",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -48,7 +48,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_pegasus_x import PegasusXConfig\n \n \n@@ -208,7 +207,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -674,7 +672,6 @@ def __init__(self, config: PegasusXConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f90721a25b4787e88ea6e66c7e3699a29a1aa196",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -223,7 +222,6 @@ def _split_heads(self, fused_qkv: torch.Tensor) -> tuple[torch.Tensor, torch.Ten\n         fused_qkv = fused_qkv.view(batch_size, seq_length, self.num_heads, 3, self.head_dim)\n         return fused_qkv[..., 0, :], fused_qkv[..., 1, :], fused_qkv[..., 2, :]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -315,7 +313,6 @@ def __init__(self, config: PersimmonConfig, layer_idx: int):\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.dropout = nn.Dropout(config.hidden_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "fa8e2431a79c1e39468926dabad5dc1e1699ed5c",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -23,7 +23,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_phi import PhiConfig\n \n@@ -129,7 +128,6 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n                 config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -215,7 +213,6 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "1143753f450bdee78693a0ccc711a87f0481f3e5",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -12,7 +12,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..clip.modeling_clip import CLIPMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -51,7 +50,6 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n                 config.hidden_size // config.num_attention_heads, eps=config.layer_norm_eps, elementwise_affine=True\n             )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -126,7 +124,6 @@ def __init__(self, config: PhiConfig, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d4a6d8bd785f47a6d43b418d506f4bef22122686",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -43,7 +43,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi3 import Phi3Config\n \n \n@@ -160,7 +159,6 @@ def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -245,7 +243,6 @@ def __init__(self, config: Phi3Config, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "44ff2b1e4ae5fbad0c75b18929b3ce8b354d2b7f",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -27,7 +27,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..mistral.modeling_mistral import (\n     MistralDecoderLayer,\n     MistralForCausalLM,\n@@ -114,7 +113,6 @@ def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -175,7 +173,6 @@ def __init__(self, config: Phi3Config, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0f1c7ab1ffaa273e9e7ad19d9883a6f2b9121afd",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, torch_int\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import TransformersKwargs, check_model_inputs\n from .configuration_phi4_multimodal import Phi4MultimodalAudioConfig, Phi4MultimodalConfig, Phi4MultimodalVisionConfig\n \n@@ -1321,7 +1320,6 @@ def __init__(self, config: Phi4MultimodalConfig, layer_idx: Optional[int] = None\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1385,7 +1383,6 @@ def __init__(self, config: Phi4MultimodalConfig, layer_idx: int):\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "be49fe59cb34bca0d80fa0d91e8f1204faf9a4cb",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -36,7 +36,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_phimoe import PhimoeConfig\n \n@@ -178,7 +177,6 @@ def __init__(self, config: PhimoeConfig, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -556,7 +554,6 @@ def __init__(self, config: PhimoeConfig, layer_idx: int):\n         self.input_layernorm = PhimoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = PhimoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0c1864436cdae6bd15aff55fe228c675bbd94f75",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_pix2struct import Pix2StructConfig, Pix2StructTextConfig, Pix2StructVisionConfig\n \n \n@@ -710,7 +709,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         return values\n \n     # Adapted from transformers.models.t5.modeling_t5.T5Attention.forward\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -827,7 +825,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -861,7 +858,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = Pix2StructLayerNorm(config.hidden_size, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -908,7 +904,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.mlp = Pix2StructTextLayerFF(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "15d84c3d0bf0d6ca5ac5e6c8931e1b88a90da831",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -53,7 +53,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_plbart import PLBartConfig\n \n \n@@ -374,7 +373,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -704,7 +702,6 @@ def __init__(self, config: PLBartConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ca4d1d0532bdbf6a4bbdf008bbbe2bf2c19df29c",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -33,7 +33,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, is_torch_flex_attn_available, is_torch_fx_proxy, is_torchdynamo_compiling, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_pop2piano import Pop2PianoConfig\n \n \n@@ -283,7 +282,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -401,7 +399,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -435,7 +432,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = Pop2PianoLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -481,7 +477,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(Pop2PianoLayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "1de3f9709838884561e729ac664361809ee84117",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -31,7 +31,6 @@\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_prophetnet import ProphetNetConfig\n \n \n@@ -433,7 +432,6 @@ def __init__(self, config: ProphetNetConfig, num_attn_heads: int, layer_idx: Opt\n \n         self.out_proj = nn.Linear(hidden_size, hidden_size)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -591,7 +589,6 @@ def _shape(self, tensor, seq_len, batch_size):\n     def prepare_for_onnx_export_(self):\n         self.onnx_trace = True\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -921,7 +918,6 @@ def __init__(self, config: ProphetNetConfig, layer_idx=None):\n         self.feed_forward = ProphetNetFeedForward(config, config.decoder_ffn_dim)\n         self.feed_forward_layer_norm = LayerNorm(config.hidden_size)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "d318e9f6448e0d998eac583fe7728e7a32ad902c",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -26,7 +26,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen2 import Qwen2Config\n \n@@ -137,7 +136,6 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -216,7 +214,6 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "e0f2225ed700ee504c27b3306b5a64a1834d59c8",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -14,7 +14,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from ...utils.import_utils import get_torch_version\n from ..llama.modeling_llama import (\n@@ -53,7 +52,6 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "d4079e36a65f2cadf6aba8e71d6c405dddbbfef3",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, check_torch_load_is_safe, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.hub import cached_file\n from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_omni import (\n@@ -1345,7 +1344,6 @@ def __init__(self, config: Qwen2_5OmniConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = Qwen2_5OmniRotaryEmbedding(config=config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1430,7 +1428,6 @@ def __init__(self, config: Qwen2_5OmniTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "9c112ac0bbb28f9e9407be2959b301ee4d41f88d",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..qwen2.modeling_qwen2 import Qwen2RMSNorm\n from .configuration_qwen2_5_vl import Qwen2_5_VLConfig, Qwen2_5_VLTextConfig, Qwen2_5_VLVisionConfig\n \n@@ -627,7 +626,6 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: Optional[int] = None\n \n         self.rotary_emb = Qwen2_5_VLRotaryEmbedding(config=config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -698,7 +696,6 @@ def __init__(self, config: Qwen2_5_VLTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "173bfb9d2e639e711fce24df45d410555285de8c",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n@@ -216,7 +215,6 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         if self.config.layer_types[layer_idx] == \"sliding_attention\":\n             self.sliding_window = config.sliding_window\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -344,7 +342,6 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.hidden_size = config.hidden_size\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ef4e81c35faccf91f01ea809e1b2df3a45c15d83",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -44,7 +44,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ..qwen2.modeling_qwen2 import (\n     Qwen2RMSNorm,\n )\n@@ -487,7 +486,6 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: Optional[int] = None):\n \n         self.rotary_emb = Qwen2VLRotaryEmbedding(config=config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -558,7 +556,6 @@ def __init__(self, config: Qwen2VLTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "889562a861dcee0b4a8cd748a2800d6e5f86d3c2",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3 import Qwen3Config\n \n@@ -184,7 +183,6 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -242,7 +240,6 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a45445ffa4e7d9c1f0a7e3790b079515c9645e6c",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -24,7 +24,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n@@ -64,7 +63,6 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "84c9ae9e5b1c0851f3d752c28a4d8830a4ad6109",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n@@ -148,7 +147,6 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n         self.sliding_window = getattr(config, \"sliding_window\", None)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -304,7 +302,6 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen3MoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.hidden_size = config.hidden_size\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "ae9a3e4c00c66eed810db4e02e2ff42b713a0213",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n@@ -347,7 +346,6 @@ def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n             self.head_dim, eps=config.rms_norm_eps\n         )  # thus post q_norm does not need reshape\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -882,7 +880,6 @@ def __init__(self, config: Qwen3NextConfig, layer_idx: int):\n         self.input_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen3NextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c897e06ced8b7ab2a754567149a000a3e96d8cd4",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -48,7 +48,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n from .configuration_qwen3_omni_moe import (\n     Qwen3OmniMoeAudioEncoderConfig,\n@@ -1430,7 +1429,6 @@ def __init__(self, config, layer_idx):\n         )  # thus post q_norm does not need reshape\n         self.sliding_window = None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1490,7 +1488,6 @@ def __init__(self, config, layer_idx):\n         self.post_attention_layernorm = Qwen3OmniMoeThinkerTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.hidden_size = config.hidden_size\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -2276,7 +2273,6 @@ def __init__(self, config: Qwen3OmniMoeConfig, layer_idx: int):\n         )  # thus post q_norm does not need reshape\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -2349,7 +2345,6 @@ def __init__(self, config, layer_idx):\n         self.post_attention_layernorm = Qwen3OmniMoeRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -2727,7 +2722,6 @@ def __init__(self, config, layer_idx):\n         self.hidden_size = config.hidden_size\n         self.mlp = Qwen3OmniMoeTalkerTextSparseMoeBlock(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -3298,7 +3292,6 @@ def __init__(self, config: Qwen3OmniMoeCode2WavConfig, layer_idx):\n         self.k_norm = nn.Identity()\n         self.sliding_window = config.sliding_window\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a8c0013cc438f08ad980327b08484456a5d97237",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3_vl import Qwen3VLConfig, Qwen3VLTextConfig, Qwen3VLVisionConfig\n \n@@ -412,7 +411,6 @@ def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n             self.head_dim, eps=config.rms_norm_eps\n         )  # thus post q_norm does not need reshape\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -484,7 +482,6 @@ def __init__(self, config: Qwen3VLTextConfig, layer_idx: int):\n         self.input_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen3VLTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b10224d9e012d5e8baf33894080b637dfda7c28f",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, is_torchdynamo_compiling\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_vl_moe import Qwen3VLMoeConfig, Qwen3VLMoeTextConfig, Qwen3VLMoeVisionConfig\n \n@@ -256,7 +255,6 @@ def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n             self.head_dim, eps=config.rms_norm_eps\n         )  # thus post q_norm does not need reshape\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -331,7 +329,6 @@ def __init__(self, config: Qwen3VLMoeTextConfig, layer_idx: int):\n         self.post_attention_layernorm = Qwen3VLMoeTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.hidden_size = config.hidden_size\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "cc62dba44001674205a7a10c19a66a8250b99d63",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_rembert import RemBertConfig\n \n \n@@ -135,7 +134,6 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "b833e163aa2a01e56db7dc021ffa09c5f979a551",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_roformer import RoFormerConfig\n \n \n@@ -135,7 +134,6 @@ def __init__(self, config, layer_idx=None):\n         self.rotary_value = config.rotary_value\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -299,7 +297,6 @@ def prune_heads(self, heads):\n         self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n         self.pruned_heads = self.pruned_heads.union(heads)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -370,7 +367,6 @@ def __init__(self, config, layer_idx=None):\n         self.intermediate = RoFormerIntermediate(config)\n         self.output = RoFormerOutput(config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "29049b9e9f3c7538aa6b00c11c12c72ee933e52d",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_seamless_m4t import SeamlessM4TConfig\n \n \n@@ -1029,7 +1028,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1261,7 +1259,6 @@ def __init__(self, config: SeamlessM4TConfig, decoder_ffn_dim=None, decoder_atte\n         self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n         self.ffn_dropout = nn.Dropout(config.activation_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8cc187fcd710f1dbf70dfbfe8f51081103389d8a",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_seamless_m4t_v2 import SeamlessM4Tv2Config\n \n \n@@ -900,7 +899,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1094,7 +1092,6 @@ def __init__(\n         self.ffn_layer_norm = nn.LayerNorm(config.hidden_size)\n         self.ffn_dropout = nn.Dropout(config.activation_dropout)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a861bb750c207adeaddaa990622387c6696119b0",
            "filename": "src/transformers/models/seed_oss/modeling_seed_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodeling_seed_oss.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_seed_oss import SeedOssConfig\n \n@@ -183,7 +182,6 @@ def __init__(self, config: SeedOssConfig, layer_idx: int):\n \n         self.residual_dropout = config.residual_dropout\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -241,7 +239,6 @@ def __init__(self, config: SeedOssConfig, layer_idx: int):\n         self.input_layernorm = SeedOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = SeedOssRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0612d7ffffe3afcd492b8712d16350890f6f3c1f",
            "filename": "src/transformers/models/seed_oss/modular_seed_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodular_seed_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodular_seed_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseed_oss%2Fmodular_seed_oss.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -24,7 +24,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n@@ -95,7 +94,6 @@ def __init__(self, config: SeedOssConfig, layer_idx: int):\n \n         self.residual_dropout = config.residual_dropout\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "dc1f2dde6c1d5d83fed5940f29711e36d41f2998",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_smollm3 import SmolLM3Config\n \n@@ -151,7 +150,6 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n             else None\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -246,7 +244,6 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n         self.post_attention_layernorm = SmolLM3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.attention_type = config.layer_types[layer_idx]\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "eed216e3f4c4bbd05872fd74461cb9fa3a7ec492",
            "filename": "src/transformers/models/smollm3/modular_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodular_smollm3.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -24,7 +24,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -274,7 +273,6 @@ def __init__(self, config: SmolLM3Config, layer_idx: int):\n             else None\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "33c57de16c6dedb4f3a4b1b4062e694031665d9c",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n     is_torch_flex_attn_available,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_speech_to_text import Speech2TextConfig\n \n \n@@ -244,7 +243,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -420,7 +418,6 @@ def __init__(self, config: Speech2TextConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward\n     def forward(\n         self,"
        },
        {
            "sha": "76c7d8ecc11bfca6dd71ec88c51b3921bd80eb7e",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n )\n from ...modeling_utils import EmbeddingAccessMixin, PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_speecht5 import SpeechT5Config, SpeechT5HifiGanConfig\n \n \n@@ -876,7 +875,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1103,7 +1101,6 @@ def __init__(self, config: SpeechT5Config, layer_idx=None):\n         self.feed_forward = SpeechT5FeedForward(config, config.decoder_ffn_dim)\n         self.final_layer_norm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "a7bc61f3f44ef2b50feb28225bf3464269d9afbf",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -220,7 +219,6 @@ def __init__(self, config: StableLmConfig, layer_idx: Optional[int] = None):\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.rotary_emb = StableLmRotaryEmbedding(config=self.config)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -308,7 +306,6 @@ def forward(\n \n \n class StableLmSdpaAttention(StableLmAttention):\n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -431,7 +428,6 @@ def __init__(self, *args, **kwargs):\n         # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n         self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "755240a06cab47cd6f7116210bd45e57eb259e4b",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_starcoder2 import Starcoder2Config\n \n \n@@ -157,7 +156,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n         self.residual_dropout = config.residual_dropout\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -216,7 +214,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: int):\n         self.input_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n         self.post_attention_layernorm = nn.LayerNorm(config.hidden_size, eps=config.norm_epsilon)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "10356f067477167f3305e807fa96e19de845c2c6",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -34,7 +34,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ..mistral.modeling_mistral import (\n     MistralAttention,\n     MistralDecoderLayer,\n@@ -78,7 +77,6 @@ def __init__(self, config: Starcoder2Config, layer_idx: Optional[int] = None):\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "0eb69ad37c31d0f17aa105eac7572ad80689c9ce",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -49,7 +49,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, can_return_tuple, check_model_inputs\n from .configuration_switch_transformers import SwitchTransformersConfig\n \n@@ -356,7 +355,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -473,7 +471,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -508,7 +505,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = SwitchTransformersLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "8629e3961b824dedb8bd366c7f8d5e370d4bcfad",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -47,7 +47,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_t5 import T5Config\n \n \n@@ -287,7 +286,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -404,7 +402,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -437,7 +434,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -480,7 +476,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(T5LayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "5383891366c26dc28cf35507fab5d1727805b1b4",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n \n@@ -235,7 +234,6 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -313,7 +311,6 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n         if config.cross_attention_hidden_size is None:\n             raise ValueError(\"Cross-attention needs cross_attention_hidden_size to be specified.\")\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -429,7 +426,6 @@ def __init__(self, config, layer_idx: int):\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "2a94d8e7712532cd6360411a92fd245b524eb889",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -39,7 +39,6 @@\n     can_return_tuple,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import OutputRecorder, check_model_inputs\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n@@ -384,7 +383,6 @@ def __init__(self, config: T5GemmaModuleConfig, layer_idx: int):\n             config.cross_attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n         )\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -524,7 +522,6 @@ def __init__(self, config, layer_idx: int):\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "38c5b85e8814a7ea940fb06661cfcde99557d57a",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -30,7 +30,6 @@\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_tapas import TapasConfig\n \n \n@@ -163,7 +162,6 @@ def __init__(self, config, layer_idx=None):\n         self.is_decoder = config.is_decoder\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "d1f231d108a9762a7110e901270600fd1326b7d8",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -42,7 +42,6 @@\n from ...processing_utils import Unpack\n from ...time_series_utils import NegativeBinomialOutput, NormalOutput, StudentTOutput\n from ...utils import TransformersKwargs, auto_docstring, is_torch_flex_attn_available, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_time_series_transformer import TimeSeriesTransformerConfig\n \n \n@@ -351,7 +350,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -536,7 +534,6 @@ def __init__(self, config: TimeSeriesTransformerConfig, layer_idx: Optional[int]\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "6bf34c491b18db5953804bbc3e5c2322a2fbca71",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -32,7 +32,6 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_trocr import TrOCRConfig\n \n \n@@ -181,7 +180,6 @@ def __init__(\n \n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -325,7 +323,6 @@ def __init__(self, config: TrOCRConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "fc74bcba2b51562fce2bc90ea6bac448edbcbda1",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -47,7 +47,6 @@\n     is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n )\n-from ...utils.deprecation import deprecate_kwarg\n \n \n if is_torch_flex_attn_available():\n@@ -554,7 +553,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -672,7 +670,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n         self.layer_norm = UdopLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -706,7 +703,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = UdopLayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -752,7 +748,6 @@ def __init__(self, config, has_relative_attention_bias=False, layer_idx: Optiona\n \n         self.layer.append(UdopLayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "0e3bdc6792aff004bdaa3cc8f1057ed2c861e3cd",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -46,7 +46,6 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_umt5 import UMT5Config\n \n \n@@ -257,7 +256,6 @@ def compute_bias(self, query_length, key_length, device=None, cache_position=Non\n         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)\n         return values\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -356,7 +354,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = UMT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -383,7 +380,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n         self.layer_norm = UMT5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)\n         self.dropout = nn.Dropout(config.dropout_rate)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,\n@@ -416,7 +412,6 @@ def __init__(self, config, layer_idx: Optional[int] = None):\n \n         self.layer.append(UMT5LayerFF(config))\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states,"
        },
        {
            "sha": "4bbf0b2548e651233c7b89cd01e5907662c949e9",
            "filename": "src/transformers/models/vaultgemma/modeling_vaultgemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvaultgemma%2Fmodeling_vaultgemma.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.generic import check_model_inputs\n from .configuration_vaultgemma import VaultGemmaConfig\n \n@@ -188,7 +187,6 @@ def __init__(self, config: VaultGemmaConfig, layer_idx: int):\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n         self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -247,7 +245,6 @@ def __init__(self, config: VaultGemmaConfig, layer_idx: int):\n \n         self.pre_feedforward_layernorm = VaultGemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "c54ea9faa39e2370bea2f734dabf2cc892ab5d8c",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -41,7 +41,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_whisper import WhisperConfig\n from .generation_whisper import WhisperGenerationMixin\n \n@@ -280,7 +279,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -454,7 +452,6 @@ def __init__(self, config: WhisperConfig, layer_idx: Optional[int] = None):\n         self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "8e62c683484b49e61d864565e991cdfaccd0bcce",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -28,7 +28,6 @@\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, CausalLMOutputWithCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from .configuration_xglm import XGLMConfig\n \n \n@@ -133,7 +132,6 @@ def __init__(\n         self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n         self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -283,7 +281,6 @@ def __init__(self, config: XGLMConfig, layer_idx=None):\n         self.fc2 = nn.Linear(config.ffn_dim, self.embed_dim)\n         self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     # Copied from transformers.models.musicgen.modeling_musicgen.MusicgenDecoderLayer.forward\n     def forward(\n         self,"
        },
        {
            "sha": "daf945b02b7d89071eae48a6388744f9ddd1624f",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -35,7 +35,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba import ZambaConfig\n \n@@ -248,7 +247,6 @@ def __init__(self, config: ZambaConfig, layer_idx: int):\n         self.v_proj = nn.Linear(config.attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -589,7 +587,6 @@ def __init__(self, config: ZambaConfig, layer_idx: Optional[int] = None):\n         self.input_layernorm = ZambaRMSNorm(config.attention_hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = ZambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -651,7 +648,6 @@ def __init__(self, config: ZambaConfig, layer_idx: int):\n         self.input_layernorm = ZambaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -720,7 +716,6 @@ def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear,\n         self.linear = linear\n         self.mamba_decoder = mamba\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "73d4bf32e61298b00b721f5c8b7c3b34a2786da0",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -38,7 +38,6 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available\n from .configuration_zamba2 import Zamba2Config\n \n@@ -388,7 +387,6 @@ def __init__(\n \n         self.layer_dic = {value: index for index, value in enumerate(self.layer_block_map)}\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -978,7 +976,6 @@ def __init__(self, config: Zamba2Config, block_id: Optional[int] = None, layer_i\n         self.input_layernorm = Zamba2RMSNorm(config.attention_hidden_size, eps=config.rms_norm_eps)\n         self.pre_ff_layernorm = Zamba2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1040,7 +1037,6 @@ def __init__(self, config: Zamba2Config, layer_idx: int):\n         self.input_layernorm = Zamba2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.layer_idx = layer_idx\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -1111,7 +1107,6 @@ def __init__(\n         self.mamba_decoder = mamba\n         self.shared_transformer = shared_transformer\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "f8e1506da48a1c2e3e1d534d310a17d87fb886ac",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/242eb9cbdc2513d347fe528f4ff7111e90bb4372/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=242eb9cbdc2513d347fe528f4ff7111e90bb4372",
            "patch": "@@ -29,7 +29,6 @@\n from ...utils import (\n     logging,\n )\n-from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n     is_mamba_ssm_available,\n@@ -227,7 +226,6 @@ def __init__(\n \n         self.layer_dic = {value: index for index, value in enumerate(self.layer_block_map)}\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -758,7 +756,6 @@ def __init__(self, config: Zamba2Config, block_id: Optional[int] = None, layer_i\n         self.self_attn = Zamba2Attention(config, layer_idx=-1, num_fwd_mem_blocks=num_gs, block_id=block_id)\n         self.feed_forward = Zamba2MLP(config, num_fwd_mem_blocks=num_gs, block_id=block_id)\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -828,7 +825,6 @@ def __init__(\n         del self.shared_transf\n         self.shared_transformer = shared_transformer\n \n-    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        }
    ],
    "stats": {
        "total": 608,
        "additions": 0,
        "deletions": 608
    }
}