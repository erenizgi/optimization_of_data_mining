{
    "author": "Wheest",
    "message": "fix: Fully remove legacy cache from Llama (#36958)\n\n* bug: fully remove legacy cache from Llama\n\n* bug: fix CI issues\n\n* bug: update jetmoe model\n\n* bug: apply =check_modular_conversion.py= fix\n\n* bug: apply make fix-copies\n\n* bug: fix ruff\n\n* PR suggestions\n\n* Remove trailing commas in auto-gen files\n\n* Trivial new line removal",
    "sha": "348f3285c5114159d2ff4933b4b8ae36866d01a7",
    "files": [
        {
            "sha": "5e202645174fd5a372fdfca1cc2331dd6a6a9b46",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -828,20 +828,12 @@ def forward(self, x, position_ids):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -933,6 +925,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -1193,7 +1189,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "24fae66f058a766f9520c5ba6450c69590bf8989",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 6,
            "deletions": 10,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -478,20 +478,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -583,6 +575,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n "
        },
        {
            "sha": "0f21f7045bda56649ce1def06c36e13109c6d2cb",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -486,20 +486,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "8d13b178724e88de8cadd77a3a5056d59b72b971",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -22,7 +22,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -717,20 +717,12 @@ def forward(self, x, position_ids):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -822,6 +814,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -1070,7 +1066,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1192,7 +1188,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1292,7 +1288,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1389,7 +1385,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "82dfc23daf944f7a763773f29d4f72460ef49e28",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1401,6 +1401,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -1650,7 +1654,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9fcd9b27da7b40b49306276cbedfd31a277ff1ae",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -444,20 +444,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -803,7 +795,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -925,7 +917,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1036,7 +1028,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "0c6b8188fb6d533498fc527bc42fa89fbe186e8c",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -488,20 +488,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -1018,7 +1010,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1129,7 +1121,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "92d2d36caa73a2030288d94925550bff677f5609",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -564,20 +564,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "28156d404c981e0d669aa2c8ab9553c7756811f1",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -459,20 +459,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -564,6 +556,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -812,7 +808,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -934,7 +930,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1045,7 +1041,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "4c2d8d575556a41168cdd227baf013ea271d4e03",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -437,20 +437,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "d564e085802151c414ee80bdf5a10b2894b05a4b",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -459,20 +459,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "31649866423c8bc5d54f479c5a397931ddea019e",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -20,7 +20,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -446,20 +446,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -551,6 +543,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -799,7 +795,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -921,7 +917,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1032,7 +1028,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "de8780fe09e063fd97e58c617882f0fa73264ba6",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -20,7 +20,7 @@\n \"\"\"PyTorch Jamba model.\"\"\"\n \n import math\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Dict, Optional, Tuple, Union\n \n import torch\n import torch.nn.functional as F\n@@ -1645,7 +1645,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "88a003a44cf809337ef7ffc25a929cb9baf1ce89",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1436,7 +1436,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "513e65204f669724fd1cf5e0adb0c0e5e07f859d",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -17,7 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -448,20 +448,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -553,6 +545,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -801,7 +797,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -923,7 +919,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1024,7 +1020,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1121,7 +1117,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "bcb294712c9fd67560da74637a994d4b65a962d2",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -413,20 +413,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -518,6 +510,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -790,7 +786,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -916,7 +912,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1000,7 +996,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6b00960f38541812bf0e520bde221f37c5c0d37b",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -535,20 +535,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -1153,7 +1145,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1264,7 +1256,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "04cf4d5a2c1642c82c2f419eb7321dde1ca2a3ab",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -767,20 +767,12 @@ def forward(\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`"
        },
        {
            "sha": "0ff0544debcee9b8c1fee367f0b9f12649051dc1",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1169,7 +1169,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1271,7 +1271,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,\n@@ -1369,7 +1369,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "bd8a88af33d2aa5b5898c309b4c26e3de9bd6790",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -424,20 +424,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -529,6 +521,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -777,7 +773,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "dfdaab9a2b03ae5dab0bad672e3f5b83f1c4f363",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 12,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_olmo2.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -425,20 +425,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -530,6 +522,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -778,7 +774,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "995770b35c291d7bad090c5f19d195a74fcf8478",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -980,7 +980,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1092,7 +1092,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "f071ad043ee64c89eeadf895797254f8d742be0e",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 14,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_phi.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n@@ -420,20 +420,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -775,7 +767,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -897,7 +889,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1008,7 +1000,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "8cfd65a6f2bd2017a705fe5efeae68cd71e82a59",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 14,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -20,7 +20,7 @@\n # limitations under the License.\n \n \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -488,20 +488,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -593,6 +585,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -865,7 +861,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1026,7 +1022,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1137,7 +1133,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "8fe0137057dd0d2106d78ca17a53fbcc297a2530",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1560,7 +1560,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c266ec374c24824c798a7015dfe971586de5ea66",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -4,7 +4,7 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_qwen2.py file directly. One of our CI enforces this.\n #                \n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n@@ -426,20 +426,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -531,6 +523,10 @@ def forward(\n             )\n             use_cache = False\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n@@ -803,7 +799,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -925,7 +921,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1036,7 +1032,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1113,7 +1109,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "5c6fc1c7155987219422bb341c15af0f4b65613e",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1402,7 +1402,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1514,7 +1514,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "84f4b7f82d758d125d98386d787e1e1fbef0c42d",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -1236,7 +1236,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1348,7 +1348,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "362856fe04902543463272077c4bc2351255414a",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/348f3285c5114159d2ff4933b4b8ae36866d01a7/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=348f3285c5114159d2ff4933b4b8ae36866d01a7",
            "patch": "@@ -418,20 +418,12 @@ def _init_weights(self, module):\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache` or `tuple(tuple(torch.FloatTensor))`, *optional*):\n+        past_key_values (`Cache`, *optional*):\n             Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n             blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n             returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n \n-            Two formats are allowed:\n-            - a [`~cache_utils.Cache`] instance, see our\n-            [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache);\n-            - Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n-            shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`). This is also known as the legacy\n-            cache format.\n-\n-            The model will output the same cache format that is fed as input. If no `past_key_values` are passed, the\n-            legacy cache format will be returned.\n+            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n             have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n@@ -786,7 +778,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -908,7 +900,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1019,7 +1011,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        }
    ],
    "stats": {
        "total": 428,
        "additions": 154,
        "deletions": 274
    }
}