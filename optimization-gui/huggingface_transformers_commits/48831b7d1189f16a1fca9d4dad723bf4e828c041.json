{
    "author": "JacobLinCool",
    "message": "Add text support to the Trainer's TensorBoard integration (#34418)\n\n* feat: add text support to TensorBoardCallback\r\n\r\n* feat: ignore long strings in trainer progress\r\n\r\n* docs: add docstring for max_str_len\r\n\r\n* style: remove trailing whitespace\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "48831b7d1189f16a1fca9d4dad723bf4e828c041",
    "files": [
        {
            "sha": "be9a4aff3c7e7feaa92dad59cba4157e0fc9cd37",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/48831b7d1189f16a1fca9d4dad723bf4e828c041/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48831b7d1189f16a1fca9d4dad723bf4e828c041/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=48831b7d1189f16a1fca9d4dad723bf4e828c041",
            "patch": "@@ -697,6 +697,8 @@ def on_log(self, args, state, control, logs=None, **kwargs):\n             for k, v in logs.items():\n                 if isinstance(v, (int, float)):\n                     self.tb_writer.add_scalar(k, v, state.global_step)\n+                elif isinstance(v, str):\n+                    self.tb_writer.add_text(k, v, state.global_step)\n                 else:\n                     logger.warning(\n                         \"Trainer is attempting to log a value of \""
        },
        {
            "sha": "cf9a83aa188a30409848880e20d54376079652dd",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 18,
            "deletions": 2,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/48831b7d1189f16a1fca9d4dad723bf4e828c041/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48831b7d1189f16a1fca9d4dad723bf4e828c041/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=48831b7d1189f16a1fca9d4dad723bf4e828c041",
            "patch": "@@ -589,11 +589,21 @@ def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: Tr\n class ProgressCallback(TrainerCallback):\n     \"\"\"\n     A [`TrainerCallback`] that displays the progress of training or evaluation.\n+    You can modify `max_str_len` to control how long strings are truncated when logging.\n     \"\"\"\n \n-    def __init__(self):\n+    def __init__(self, max_str_len: int = 100):\n+        \"\"\"\n+        Initialize the callback with optional max_str_len parameter to control string truncation length.\n+\n+        Args:\n+            max_str_len (`int`):\n+                Maximum length of strings to display in logs.\n+                Longer strings will be truncated with a message.\n+        \"\"\"\n         self.training_bar = None\n         self.prediction_bar = None\n+        self.max_str_len = max_str_len\n \n     def on_train_begin(self, args, state, control, **kwargs):\n         if state.is_world_process_zero:\n@@ -631,7 +641,13 @@ def on_log(self, args, state, control, logs=None, **kwargs):\n             # but avoid doing any value pickling.\n             shallow_logs = {}\n             for k, v in logs.items():\n-                shallow_logs[k] = v\n+                if isinstance(v, str) and len(v) > self.max_str_len:\n+                    shallow_logs[k] = (\n+                        f\"[String too long to display, length: {len(v)} > {self.max_str_len}. \"\n+                        \"Consider increasing `max_str_len` if needed.]\"\n+                    )\n+                else:\n+                    shallow_logs[k] = v\n             _ = shallow_logs.pop(\"total_flos\", None)\n             # round numbers so that it looks better in console\n             if \"epoch\" in shallow_logs:"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 20,
        "deletions": 2
    }
}