{
    "author": "martin0258",
    "message": "Update torchao.md: use auto-compilation (#35490)\n\n* Update torchao.md: use auto-compilation\r\n\r\n* Update torchao.md: indicate updating transformers to the latest\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "715fdd6459b37336ba79f90ffdd6f9adb8ee22a5",
    "files": [
        {
            "sha": "46fb0f8cbb9a88fc9a3f4f9606952e68f4f7d859",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/715fdd6459b37336ba79f90ffdd6f9adb8ee22a5/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/715fdd6459b37336ba79f90ffdd6f9adb8ee22a5/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=715fdd6459b37336ba79f90ffdd6f9adb8ee22a5",
            "patch": "@@ -16,7 +16,8 @@ rendered properly in your Markdown viewer.\n Before you begin, make sure the following libraries are installed with their latest version:\n \n ```bash\n-pip install --upgrade torch torchao\n+# Updating ðŸ¤— Transformers to the latest version, as the example script below uses the new auto compilation\n+pip install --upgrade torch torchao transformers\n ```\n \n By default, the weights are loaded in full precision (torch.float32) regardless of the actual data type the weights are stored in such as torch.float16. Set `torch_dtype=\"auto\"` to load the weights in the data type defined in a model's `config.json` file to automatically load the most memory-optimal data type.\n@@ -35,12 +36,8 @@ tokenizer = AutoTokenizer.from_pretrained(model_name)\n input_text = \"What are we having for dinner?\"\n input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-# compile the quantized model to get speedup\n-import torchao\n-torchao.quantization.utils.recommended_inductor_config_setter()\n-quantized_model = torch.compile(quantized_model, mode=\"max-autotune\")\n-\n-output = quantized_model.generate(**input_ids, max_new_tokens=10)\n+# auto-compile the quantized model with `cache_implementation=\"static\"` to get speedup\n+output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n # benchmark the performance\n@@ -59,11 +56,11 @@ def benchmark_fn(f, *args, **kwargs):\n     return f\"{(t0.blocked_autorange().mean):.3f}\"\n \n MAX_NEW_TOKENS = 1000\n-print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS))\n+print(\"int4wo-128 model:\", benchmark_fn(quantized_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n bf16_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n-bf16_model = torch.compile(bf16_model, mode=\"max-autotune\")\n-print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS))\n+output = bf16_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\") # auto-compile\n+print(\"bf16 model:\", benchmark_fn(bf16_model.generate, **input_ids, max_new_tokens=MAX_NEW_TOKENS, cache_implementation=\"static\"))\n \n ```\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 7,
        "deletions": 10
    }
}