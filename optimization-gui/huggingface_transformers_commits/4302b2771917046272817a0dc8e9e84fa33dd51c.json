{
    "author": "jla524",
    "message": "Fix typos in translated quicktour docs (#35302)\n\n* fix: quicktour typos\n\n* fix: one more",
    "sha": "4302b2771917046272817a0dc8e9e84fa33dd51c",
    "files": [
        {
            "sha": "1795c3a5d74fcc36ce97b05fcf46b004ebc1202f",
            "filename": "docs/source/ar/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Far%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Far%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -347,17 +347,17 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "c01609207fec2a1da1c643bdd38f8146a0308657",
            "filename": "docs/source/de/quicktour.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fde%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fde%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -109,7 +109,7 @@ label: NEGATIVE, with score: 0.5309\n Die [`pipeline`] kann auch √ºber einen ganzen Datensatz iterieren. Starten wir mit der Installation der [ü§ó Datasets](https://huggingface.co/docs/datasets/) Bibliothek:\n \n ```bash\n-pip install datasets \n+pip install datasets\n ```\n \n Erstellen wir eine [`pipeline`] mit der Aufgabe die wir l√∂sen und dem Modell welches wir nutzen m√∂chten.\n@@ -191,7 +191,7 @@ Wenn Sie kein Modell f√ºr Ihren Anwendungsfall finden k√∂nnen, m√ºssen Sie ein v\n \n <Youtube id=\"AhChOFRegn4\"/>\n \n-Unter der Haube arbeiten die Klassen [`AutoModelForSequenceClassification`] und [`AutoTokenizer`] zusammen, um die [`pipeline`] zu betreiben. Eine [`AutoClass`](./model_doc/auto) ist eine Abk√ºrzung, die automatisch die Architektur eines trainierten Modells aus dessen Namen oder Pfad abruft. Sie m√ºssen nur die passende `AutoClass` f√ºr Ihre Aufgabe und den zugeh√∂rigen Tokenizer mit [`AutoTokenizer`] ausw√§hlen. \n+Unter der Haube arbeiten die Klassen [`AutoModelForSequenceClassification`] und [`AutoTokenizer`] zusammen, um die [`pipeline`] zu betreiben. Eine [`AutoClass`](./model_doc/auto) ist eine Abk√ºrzung, die automatisch die Architektur eines trainierten Modells aus dessen Namen oder Pfad abruft. Sie m√ºssen nur die passende `AutoClass` f√ºr Ihre Aufgabe und den zugeh√∂rigen Tokenizer mit [`AutoTokenizer`] ausw√§hlen.\n \n Kehren wir zu unserem Beispiel zur√ºck und sehen wir uns an, wie Sie die `AutoClass` verwenden k√∂nnen, um die Ergebnisse der [`pipeline`] zu replizieren.\n \n@@ -281,7 +281,7 @@ Jetzt k√∂nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Model\n ```\n \n Das Modell gibt die endg√ºltigen Aktivierungen in dem Attribut \"logits\" aus. Wenden Sie die Softmax-Funktion auf die \"logits\" an, um die Wahrscheinlichkeiten zu erhalten:\n-  \n+\n ```py\n >>> from torch import nn\n \n@@ -308,7 +308,7 @@ In der [Aufgabenzusammenfassung](./task_summary) steht, welche [AutoModel]-Klass\n </Tip>\n \n Jetzt k√∂nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell √ºbergeben, indem Sie die W√∂rterbuchschl√ºssel direkt an die Tensoren √ºbergeben:\n-  \n+\n ```py\n >>> tf_outputs = tf_model(tf_batch)\n ```\n@@ -383,17 +383,17 @@ Ein besonders cooles ü§ó Transformers-Feature ist die M√∂glichkeit, ein Modell\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "c4babab09f023df427320dbd9c6ce26ff0eb24da",
            "filename": "docs/source/es/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fes%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fes%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -385,17 +385,17 @@ Una caracter√≠stica particularmente interesante de ü§ó Transformers es la habil\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "dcf21562316d5d1d1991a098f0ef039b5b62372d",
            "filename": "docs/source/fr/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Ffr%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Ffr%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -354,17 +354,17 @@ Une fonctionnalit√© particuli√®rement cool ü§ó Transformers est la possibilit√©\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "f0291a6167715a860af1dad07d41ada4d667a405",
            "filename": "docs/source/it/quicktour.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fit%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fit%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -111,7 +111,7 @@ etichetta: negative, con punteggio: 0.9998\n La [`pipeline`] pu√≤ anche iterare su un dataset intero. Inizia installando la libreria [ü§ó Datasets](https://huggingface.co/docs/datasets/):\n \n ```bash\n-pip install datasets \n+pip install datasets\n ```\n \n Crea una [`pipeline`] con il compito che vuoi risolvere e con il modello che vuoi utilizzare.\n@@ -385,17 +385,17 @@ Una caratteristica particolarmente interessante di ü§ó Transformers √® la sua a\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "0eb00cf220b54ab6c9f407564d5f1053461cdf51",
            "filename": "docs/source/ja/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fja%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fja%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -386,8 +386,8 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n \n </pt>\n@@ -396,8 +396,8 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "4c3b137aa00ff9d5221258a4e788ccdfd4841fd6",
            "filename": "docs/source/ko/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fko%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fko%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -361,17 +361,17 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        },
        {
            "sha": "cc583697b9a65880c13b4aa9604eb3a3c4e3287c",
            "filename": "docs/source/pt/quicktour.md",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fpt%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fpt%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -37,7 +37,7 @@ A [`pipeline`] apoia diversas tarefas fora da caixa:\n **Texto**:\n * An√°lise sentimental: classifica a polaridade de um texto.\n * Gera√ß√£o de texto (em Ingl√™s): gera texto a partir de uma entrada.\n-* Reconhecimento de entidade mencionada: legenda cada palavra com uma classe que a representa (pessoa, data, local, etc...) \n+* Reconhecimento de entidade mencionada: legenda cada palavra com uma classe que a representa (pessoa, data, local, etc...)\n * Respostas: extrai uma resposta dado algum contexto e uma quest√£o\n * M√°scara de preenchimento: preenche o espa√ßo, dado um texto com m√°scaras de palavras.\n * Sumariza√ß√£o: gera o resumo de um texto longo ou documento.\n@@ -87,7 +87,7 @@ Importe [`pipeline`] e especifique a tarefa que deseja completar:\n >>> classifier = pipeline(\"sentiment-analysis\")\n ```\n \n-A pipeline baixa and armazena um [modelo pr√©-treinado](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) padr√£o e tokenizer para an√°lise sentimental. Agora voc√™ pode usar `classifier` no texto alvo: \n+A pipeline baixa and armazena um [modelo pr√©-treinado](https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english) padr√£o e tokenizer para an√°lise sentimental. Agora voc√™ pode usar `classifier` no texto alvo:\n \n ```py\n >>> classifier(\"We are very happy to show you the ü§ó Transformers library.\")\n@@ -107,7 +107,7 @@ label: NEGATIVE, with score: 0.5309\n A [`pipeline`] tamb√©m pode iterar sobre um Dataset inteiro. Comece instalando a biblioteca de [ü§ó Datasets](https://huggingface.co/docs/datasets/):\n \n ```bash\n-pip install datasets \n+pip install datasets\n ```\n \n Crie uma [`pipeline`] com a tarefa que deseja resolver e o modelo que deseja usar.\n@@ -133,7 +133,7 @@ Precisamos garantir que a taxa de amostragem do conjunto de dados corresponda √†\n >>> dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))\n ```\n \n-Os arquivos de √°udio s√£o carregados e re-amostrados automaticamente ao chamar a coluna `\"audio\"`. \n+Os arquivos de √°udio s√£o carregados e re-amostrados automaticamente ao chamar a coluna `\"audio\"`.\n Vamos extrair as arrays de formas de onda originais das primeiras 4 amostras e pass√°-las como uma lista para o pipeline:\n \n ```py\n@@ -176,7 +176,7 @@ Use o [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] para carreg\n </tf>\n </frameworkcontent>\n \n-Ent√£o voc√™ pode especificar o modelo e o tokenizador na [`pipeline`] e aplicar o `classifier` no seu texto alvo: \n+Ent√£o voc√™ pode especificar o modelo e o tokenizador na [`pipeline`] e aplicar o `classifier` no seu texto alvo:\n \n ```py\n >>> classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n@@ -190,7 +190,7 @@ Se voc√™ n√£o conseguir achar um modelo para o seu caso de uso, precisar√° usar\n \n <Youtube id=\"AhChOFRegn4\"/>\n \n-Por baixo dos panos, as classes [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] trabalham juntas para fortificar o [`pipeline`]. Um [AutoClass](./model_doc/auto) √© um atalho que automaticamente recupera a arquitetura de um modelo pr√©-treinado a partir de seu nome ou caminho. Basta selecionar a `AutoClass` apropriada para sua tarefa e seu tokenizer associado com [`AutoTokenizer`]. \n+Por baixo dos panos, as classes [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] trabalham juntas para fortificar o [`pipeline`]. Um [AutoClass](./model_doc/auto) √© um atalho que automaticamente recupera a arquitetura de um modelo pr√©-treinado a partir de seu nome ou caminho. Basta selecionar a `AutoClass` apropriada para sua tarefa e seu tokenizer associado com [`AutoTokenizer`].\n \n Vamos voltar ao nosso exemplo e ver como voc√™ pode usar a `AutoClass` para replicar os resultados do [`pipeline`].\n \n@@ -383,17 +383,17 @@ Um recurso particularmente interessante dos ü§ó Transformers √© a capacidade de\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "6045b673d2d3d0e71f097ff6666f3dd9bb9a5f06",
            "filename": "docs/source/te/quicktour.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fte%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4302b2771917046272817a0dc8e9e84fa33dd51c/docs%2Fsource%2Fte%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fte%2Fquicktour.md?ref=4302b2771917046272817a0dc8e9e84fa33dd51c",
            "patch": "@@ -366,17 +366,17 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```py\n >>> from transformers import AutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n+>>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n <tf>\n \n ```py\n >>> from transformers import TFAutoModel\n \n->>> tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n+>>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n+>>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n ```\n </tf>\n </frameworkcontent>"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 47,
        "deletions": 47
    }
}