{
    "author": "molbap",
    "message": "fix tests with main revision and read token (#33560)\n\n* fix tests with main revision and read token\r\n\r\n* [run-slow]mamba2\r\n\r\n* test previously skipped tests\r\n\r\n* [run-slow]mamba2\r\n\r\n* skip some tests\r\n\r\n* [run-slow]mamba2\r\n\r\n* finalize tests\r\n\r\n* [run-slow]mamba2",
    "sha": "4f0246e5359d2d9b7c9f1e5b7c3d7dc79fed0e99",
    "files": [
        {
            "sha": "a1e2138d4d6d78218f7c3a7bc23fff92e2a6fc84",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 25,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f0246e5359d2d9b7c9f1e5b7c3d7dc79fed0e99/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f0246e5359d2d9b7c9f1e5b7c3d7dc79fed0e99/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=4f0246e5359d2d9b7c9f1e5b7c3d7dc79fed0e99",
            "patch": "@@ -20,7 +20,7 @@\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, Mamba2Config, is_torch_available\n-from transformers.testing_utils import require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import require_read_token, require_torch, require_torch_gpu, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -96,7 +96,7 @@ def __init__(\n         self.tie_word_embeddings = tie_word_embeddings\n \n     def get_large_model_config(self):\n-        return Mamba2Config.from_pretrained(\"revision='refs/pr/9'\")\n+        return Mamba2Config.from_pretrained(\"mistralai/Mamba-Codestral-7B-v0.1\")\n \n     def prepare_config_and_inputs(\n         self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n@@ -199,34 +199,26 @@ def test_initialization(self):\n     def test_tied_weights_keys(self):\n         pass\n \n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing is interacting with beam search\")\n-    def test_beam_search_generate_dict_outputs_use_cache(self):\n-        pass\n-\n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing is interacting with beam search\")\n-    def test_beam_sample_generate(self):\n+    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n+    def test_generate_without_input_ids(self):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    def test_generate_without_input_ids(self):\n+    def test_generate_from_inputs_embeds_decoder_only(self):\n         pass\n \n     @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"Initialization of mamba2 fails this\")\n-    def test_save_load_fast_init_from_base(self):\n+    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing is interacting with beam search\")\n+    def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass\n \n     @unittest.skip(reason=\"A large mamba2 would be necessary (and costly) for that\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n \n-    @unittest.skip(reason=\"To fix, Mamba 2 cache slicing test case is an edge case\")\n-    def test_generate_from_inputs_embeds_decoder_only(self):\n-        pass\n-\n     def test_model_outputs_equivalence(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -292,12 +284,11 @@ def test_inputs_embeds_matches_input_ids_with_generate(self):\n \n @require_torch\n @slow\n+@require_read_token\n class Mamba2IntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.model_id = \"mistralai/Mamba-Codestral-7B-v0.1\"\n-        self.tokenizer = AutoTokenizer.from_pretrained(\n-            self.model_id, revision=\"refs/pr/9\", from_slow=True, legacy=False\n-        )\n+        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id, from_slow=True, legacy=False)\n         self.prompt = (\"[INST]Write a hello world program in C++.\",)\n \n     @parameterized.expand(\n@@ -317,7 +308,7 @@ def test_simple_generate(self, device):\n         tokenizer = self.tokenizer\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n \n-        model = Mamba2ForCausalLM.from_pretrained(self.model_id, revision=\"refs/pr/9\", torch_dtype=torch.bfloat16)\n+        model = Mamba2ForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16)\n         model.to(device)\n         input_ids = tokenizer(\"[INST]Write a hello world program in C++.[/INST]\", return_tensors=\"pt\")[\"input_ids\"].to(\n             device\n@@ -343,9 +334,7 @@ def test_batched_equivalence_with_cache(self):\n             \"[INST] Write a simple Fibonacci number computation function in Rust that does memoization, with comments, in safe Rust.[/INST]\",\n         ]\n \n-        model = Mamba2ForCausalLM.from_pretrained(self.model_id, revision=\"refs/pr/9\", torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = Mamba2ForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n         # batched generation\n         tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)\n@@ -375,9 +364,7 @@ def test_batched_equivalence_without_cache(self):\n             \"[INST] Write a simple Fibonacci number computation function in Rust that does memoization, with comments, in safe Rust.[/INST]\",\n         ]\n \n-        model = Mamba2ForCausalLM.from_pretrained(self.model_id, revision=\"refs/pr/9\", torch_dtype=torch.bfloat16).to(\n-            torch_device\n-        )\n+        model = Mamba2ForCausalLM.from_pretrained(self.model_id, torch_dtype=torch.bfloat16).to(torch_device)\n         tokenizer.pad_token_id = tokenizer.eos_token_id\n         # batched generation\n         tokenized_prompts = tokenizer(prompt, return_tensors=\"pt\", padding=\"longest\").to(torch_device)"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 12,
        "deletions": 25
    }
}