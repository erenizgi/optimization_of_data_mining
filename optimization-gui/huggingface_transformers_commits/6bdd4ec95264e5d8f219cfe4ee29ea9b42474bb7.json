{
    "author": "eustlb",
    "message": "Add kyutai stt (#38909)\n\n* first draft\n\n* cleaner version\n\n* udpate tests + modeling\n\n* add tests\n\n* init\n\n* udpate test_modeling_common\n\n* fix tests\n\n* csm Processor draft\n\n* convertion update\n\n* mimi cache padding convolutions draft\n\n* mimi streaming udpates\n\n* update mimi padding cache test\n\n* udpate cache padding mimi test\n\n* make style mimi\n\n* updates generate moshi asr\n\n* moshi asr integration tests (single + batched)\n\n* update tests\n\n* update conversion script\n\n* good default sliding window value\n\n* udpdate generate\n\n* update test checkpoint\n\n* nit\n\n* fix mimi\n\n* fix codec prefix\n\n* revert\n\n* revert\n\n* update config\n\n* update config\n\n* unnecessary mimi input restriction\n\n* remove delay in tokens\n\n* remove _prepare_4d_causal_attention_mask_with_cache_position and _update_causal_mask\n\n* test update\n\n* modular update\n\n* make style\n\n* nit\n\n* rename\n\n* create codec model generation config at init\n\n* remove delay\n\n* max_new_tokens/length warning\n\n* correct conv1 padding cache import for modular\n\n* nit\n\n* fix on encoder_past_key_values\n\n* convert modular\n\n* move frame_size to config\n\n* move frame_size to config\n\n* update test name\n\n* handle first token is bos\n\n* better handling of max_new_tokens\n\n* fix\n\n* fix batch size in test input prep\n\n* update docstring\n\n* convert modular\n\n* make style\n\n* make style\n\n* add feature extractor\n\n* correct modular convention name for feature_extraction file\n\n* update convertion script\n\n* doc processor\n\n* update doc\n\n* udpate init\n\n* update model type\n\n* fixes\n\n* update tests\n\n* fix\n\n* make\n\n* add doc\n\n* nit\n\n* fix\n\n* doc\n\n* auto mappings\n\n* doc\n\n* nit\n\n* convert modular\n\n* doc\n\n* nit\n\n* extend _keep_in_fp32_modules to enforce fp32\n\n* renaming to stt\n\n* doc update + test update\n\n* doc fixes\n\n* doc fix\n\n* doc fix\n\n* fix musicgen tests\n\n* fix musicgen tests\n\n* make style\n\n* fix musicgen tests\n\n* correct frame_rate config param for mimi\n\n* update mimi test\n\n* revert update mimi test\n\n* enforce cpu test\n\n* move cache init in cache class\n\n* convert modular\n\n* docstring update\n\n* update model id\n\n* feature_extractor -> feature_extraction (SEW)\n\n* convert modular\n\n* update model id",
    "sha": "6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
    "files": [
        {
            "sha": "d8438a41655f93192e27cbac91ac67d5166a923b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -843,6 +843,8 @@\n         title: GraniteSpeech\n       - local: model_doc/hubert\n         title: Hubert\n+      - local: model_doc/stt\n+        title: Kyutai Speech-To-Text\n       - local: model_doc/mctct\n         title: MCTCT\n       - local: model_doc/mimi"
        },
        {
            "sha": "02428899df3f13f6ddd0e09415dd02a28237d06a",
            "filename": "docs/source/en/model_doc/stt.md",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/docs%2Fsource%2Fen%2Fmodel_doc%2Fstt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/docs%2Fsource%2Fen%2Fmodel_doc%2Fstt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fstt.md?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,122 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Kyutai Speech-To-Text \n+## Overview\n+\n+Kyutai STT is a speech-to-text model architecture based on the [Mimi codec](https://huggingface.co/docs/transformers/en/model_doc/mimi), which encodes audio into discrete tokens in a streaming fashion, and a [Moshi-like](https://huggingface.co/docs/transformers/en/model_doc/moshi) autoregressive decoder. Kyutai’s lab has released two model checkpoints:\n+- [kyutai/stt-1b-en_fr](https://huggingface.co/kyutai/stt-1b-en_fr): a 1B-parameter model capable of transcribing both English and French\n+- [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en): a 2.6B-parameter model focused solely on English, optimized for maximum transcription accuracy\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/eustlb/documentation-images/resolve/main/kyutai_stt.png\"/>\n+</div>\n+\n+## Usage Tips\n+\n+### Inference\n+\n+```python\n+import torch\n+from datasets import load_dataset, Audio\n+from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+\n+# 1. load the model and the processor\n+torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+model_id = \"kyutai/stt-2.6b-en\"\n+\n+processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n+model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+\n+# 2. load audio samples\n+ds = load_dataset(\n+    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+)\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+\n+# 3. prepare the model inputs\n+inputs = processor(\n+    ds[0][\"audio\"][\"array\"],\n+)\n+inputs.to(torch_device)\n+\n+# 4. infer the model\n+output_tokens = model.generate(**inputs)\n+\n+# 5. decode the generated tokens\n+print(processor.batch_decode(output_tokens, skip_special_tokens=True))\n+```\n+\n+### Batched Inference\n+\n+```python\n+import torch\n+from datasets import load_dataset, Audio\n+from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+\n+# 1. load the model and the processor\n+torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+model_id = \"kyutai/stt-2.6b-en\"\n+\n+processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n+model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+\n+# 2. load audio samples\n+ds = load_dataset(\n+    \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+)\n+ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+\n+# 3. prepare the model inputs\n+audio_arrays = [ds[i][\"audio\"][\"array\"] for i in range(4)]\n+inputs = processor(audio_arrays, return_tensors=\"pt\", padding=True)\n+inputs = inputs.to(torch_device)\n+\n+# 4. infer the model\n+output_tokens = model.generate(**inputs)\n+\n+# 5. decode the generated tokens\n+decoded_outputs = processor.batch_decode(output_tokens, skip_special_tokens=True)\n+for output in decoded_outputs:\n+    print(output)\n+```\n+\n+This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb).\n+The original code can be found [here](https://github.com/kyutai-labs/moshi).\n+\n+\n+## KyutaiSpeechToTextConfig\n+\n+[[autodoc]] KyutaiSpeechToTextConfig\n+\n+## KyutaiSpeechToTextProcessor\n+\n+[[autodoc]] KyutaiSpeechToTextProcessor\n+    - __call__\n+\n+## KyutaiSpeechToTextFeatureExtractor\n+\n+[[autodoc]] KyutaiSpeechToTextFeatureExtractor\n+\n+## KyutaiSpeechToTextForConditionalGeneration\n+\n+[[autodoc]] KyutaiSpeechToTextForConditionalGeneration\n+    - forward\n+    - generate\n+\n+## KyutaiSpeechToTextModel\n+\n+[[autodoc]] KyutaiSpeechToTextModel"
        },
        {
            "sha": "4f6095a3eddcfd45a2a0176ec2e29cda8ee0c592",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -4658,8 +4658,11 @@ def from_pretrained(\n         # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n         # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n         # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n+        # Update: to extend _keep_in_fp32_modules flag feature, it can also be used to force modules that should stay in fp32\n         if model._keep_in_fp32_modules is not None and (\n-            torch_dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+            torch_dtype == torch.float16\n+            or torch_dtype == torch.bfloat16\n+            or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n         ):\n             # We need to match exact layers, so we add either `.` on each side, or start/end of string\n             keep_in_fp32_regex = re.compile("
        },
        {
            "sha": "8d360683531c1c0fa2b98a56007585cf41621ed4",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -285,6 +285,7 @@\n     from .squeezebert import *\n     from .stablelm import *\n     from .starcoder2 import *\n+    from .stt import *\n     from .superglue import *\n     from .superpoint import *\n     from .swiftformer import *"
        },
        {
            "sha": "54a285e3c65f8f6770cbb85467526e0adc4b09f3",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -322,6 +322,7 @@\n         (\"squeezebert\", \"SqueezeBertConfig\"),\n         (\"stablelm\", \"StableLmConfig\"),\n         (\"starcoder2\", \"Starcoder2Config\"),\n+        (\"stt\", \"KyutaiSpeechToTextConfig\"),\n         (\"superglue\", \"SuperGlueConfig\"),\n         (\"superpoint\", \"SuperPointConfig\"),\n         (\"swiftformer\", \"SwiftFormerConfig\"),\n@@ -707,6 +708,7 @@\n         (\"squeezebert\", \"SqueezeBERT\"),\n         (\"stablelm\", \"StableLm\"),\n         (\"starcoder2\", \"Starcoder2\"),\n+        (\"stt\", \"KyutaiSpeechToText\"),\n         (\"superglue\", \"SuperGlue\"),\n         (\"superpoint\", \"SuperPoint\"),\n         (\"swiftformer\", \"SwiftFormer\"),"
        },
        {
            "sha": "5754b3bc1bb6792e960296da9451dab9d36121b7",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -91,6 +91,7 @@\n         (\"sew-d\", \"Wav2Vec2FeatureExtractor\"),\n         (\"speech_to_text\", \"Speech2TextFeatureExtractor\"),\n         (\"speecht5\", \"SpeechT5FeatureExtractor\"),\n+        (\"stt\", \"KyutaiSpeechToTextFeatureExtractor\"),\n         (\"swiftformer\", \"ViTFeatureExtractor\"),\n         (\"swin\", \"ViTFeatureExtractor\"),\n         (\"swinv2\", \"ViTFeatureExtractor\"),"
        },
        {
            "sha": "cbfc0f7647f03969ba6ba364e494fb62f2fab543",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -300,6 +300,7 @@\n         (\"squeezebert\", \"SqueezeBertModel\"),\n         (\"stablelm\", \"StableLmModel\"),\n         (\"starcoder2\", \"Starcoder2Model\"),\n+        (\"stt\", \"KyutaiSpeechToTextModel\"),\n         (\"superglue\", \"SuperGlueForKeypointMatching\"),\n         (\"swiftformer\", \"SwiftFormerModel\"),\n         (\"swin\", \"SwinModel\"),\n@@ -1055,6 +1056,7 @@\n         (\"speech-encoder-decoder\", \"SpeechEncoderDecoderModel\"),\n         (\"speech_to_text\", \"Speech2TextForConditionalGeneration\"),\n         (\"speecht5\", \"SpeechT5ForSpeechToText\"),\n+        (\"stt\", \"KyutaiSpeechToTextForConditionalGeneration\"),\n         (\"whisper\", \"WhisperForConditionalGeneration\"),\n     ]\n )"
        },
        {
            "sha": "478766e6eea85f6c022e2551404ed266c57fb3ef",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -116,6 +116,7 @@\n         (\"speech_to_text\", \"Speech2TextProcessor\"),\n         (\"speech_to_text_2\", \"Speech2Text2Processor\"),\n         (\"speecht5\", \"SpeechT5Processor\"),\n+        (\"stt\", \"KyutaiSpeechToTextProcessor\"),\n         (\"trocr\", \"TrOCRProcessor\"),\n         (\"tvlt\", \"TvltProcessor\"),\n         (\"tvp\", \"TvpProcessor\"),"
        },
        {
            "sha": "b213359886dc9805b8fae61158488ed39d8164fc",
            "filename": "src/transformers/models/mimi/configuration_mimi.py",
            "status": "modified",
            "additions": 46,
            "deletions": 4,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fconfiguration_mimi.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -38,8 +38,8 @@ class MimiConfig(PretrainedConfig):\n     Args:\n         sampling_rate (`int`, *optional*, defaults to 24000):\n             The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).\n-        frame_rate (`float`, *optional*, defaults to 12.5):\n-            Framerate of the model.\n+        frame_rate (`float`, *optional*):\n+            Should be computed from the other parameters, yet kept for backward compatibility.\n         audio_channels (`int`, *optional*, defaults to 1):\n             Number of channels in the audio data. Either 1 for mono or 2 for stereo.\n         hidden_size (`int`, *optional*, defaults to 512):\n@@ -111,6 +111,8 @@ class MimiConfig(PretrainedConfig):\n         use_cache (`bool`, *optional*, defaults to `False`):\n             Whether or not the model should return the last key/values attentions (not used by all models). Only\n             relevant if `config.is_decoder=True`.\n+        use_streaming (`bool`, *optional*, defaults to `False`):\n+            Whether to use streaming mode. If `True`, the model encode method will return the padding cache that can be used in a subsequent call to the encode method.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n         sliding_window (`int`, *optional*, defaults to 250):\n@@ -141,7 +143,7 @@ class MimiConfig(PretrainedConfig):\n     def __init__(\n         self,\n         sampling_rate=24_000,\n-        frame_rate=12.5,\n+        frame_rate=None,\n         audio_channels=1,\n         hidden_size=512,\n         num_filters=64,\n@@ -172,6 +174,7 @@ def __init__(\n         initializer_range=0.02,\n         norm_eps=1e-5,\n         use_cache=False,\n+        use_streaming=False,\n         rope_theta=10000.0,\n         sliding_window=250,\n         attention_dropout=0.0,\n@@ -180,7 +183,6 @@ def __init__(\n         **kwargs,\n     ):\n         self.sampling_rate = sampling_rate\n-        self.frame_rate = frame_rate\n         self.audio_channels = audio_channels\n         self.hidden_size = hidden_size\n         self.num_filters = num_filters\n@@ -209,13 +211,22 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.norm_eps = norm_eps\n         self.use_cache = use_cache\n+        self.use_streaming = use_streaming\n         self.rope_theta = rope_theta\n         self.sliding_window = sliding_window\n         self.attention_dropout = attention_dropout\n         self.head_dim = head_dim or hidden_size // num_attention_heads\n         self.layer_scale_initial_scale = layer_scale_initial_scale\n         self.attention_bias = attention_bias\n \n+        # Handle backward compatibility for frame_rate:\n+        # If frame_rate is explicitly provided, use it (backward compatibility)\n+        # Otherwise, compute it from other parameters (correctly)\n+        if frame_rate is not None:\n+            self._frame_rate = frame_rate\n+        else:\n+            self._frame_rate = None\n+\n         if num_semantic_quantizers >= self.num_quantizers:\n             raise ValueError(\n                 f\"The number of semantic quantizers should be lower than the total number of quantizers {self.num_quantizers}, but is currently {num_semantic_quantizers}.\"\n@@ -233,5 +244,36 @@ def num_codebooks(self) -> int:\n         # alias to num_quantizers\n         return self.num_quantizers\n \n+    @property\n+    def frame_size(self) -> int:\n+        # 1. we need each encoder conv stride\n+        # first conv\n+        strides = [1]\n+\n+        # layer convs\n+        for ratio in reversed(self.upsampling_ratios):\n+            for j in range(self.num_residual_layers):\n+                len_kernel_sizes = len(self.residual_kernel_size) if isinstance(self.residual_kernel_size, list) else 1\n+                strides.extend([1] * (len_kernel_sizes + 1))\n+                if self.use_conv_shortcut:  # skip connection\n+                    strides.append(1)\n+\n+            strides.append(ratio)\n+\n+        # last conv\n+        strides.append(1)\n+\n+        # downsampling layer\n+        strides.append(2)\n+\n+        return math.prod(strides)\n+\n+    @property\n+    def frame_rate(self) -> float:\n+        # handle backward compatibility\n+        if self._frame_rate is not None:\n+            return self._frame_rate\n+        return self.sampling_rate / self.frame_size\n+\n \n __all__ = [\"MimiConfig\"]"
        },
        {
            "sha": "221388f858a317d5880102d5d9f944fddc0becf6",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 168,
            "deletions": 186,
            "changes": 354,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -23,25 +23,20 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, is_torch_flex_attn_available, logging\n+from ...utils import ModelOutput, auto_docstring, logging\n from .configuration_mimi import MimiConfig\n \n \n if is_flash_attn_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -78,6 +73,91 @@ class MimiOutput(ModelOutput):\n     decoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n \n \n+class MimiConv1dPaddingCache:\n+    \"\"\"\n+    Padding cache for MimiConv1d causal convolutions in order to support streaming via cache padding.\n+    See: https://arxiv.org/pdf/2005.06720 & https://arxiv.org/pdf/2204.07064\n+\n+    A padding cache is a list of cached partial hidden states for each convolution layer.\n+    Hidden states are cached from the previous call to the MimiConv1d forward pass, given the padding size.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_layers: int,\n+        per_layer_padding: list[int],\n+        per_layer_padding_mode: list[str],\n+        per_layer_in_channels: list[int],\n+    ):\n+        # ensure correct number of layers for each arg\n+        from_args_num_layers = {len(per_layer_padding), len(per_layer_padding_mode), len(per_layer_in_channels)}\n+\n+        if len(from_args_num_layers) != 1 or from_args_num_layers.pop() != num_layers:\n+            raise ValueError(\n+                f\"Expected `num_layers` ({num_layers}) values in `per_layer_padding`, `per_layer_padding_mode` and `per_layer_in_channels`\"\n+            )\n+        elif not all(mode in [\"constant\", \"replicate\"] for mode in per_layer_padding_mode):\n+            raise NotImplementedError(\n+                \"`padding_cache` is not supported for convolutions using other than `constant` or `replicate` padding mode\"\n+            )\n+\n+        self.per_layer_padding = per_layer_padding\n+        self.per_layer_padding_mode = per_layer_padding_mode\n+        self.per_layer_in_channels = per_layer_in_channels\n+        self.per_layer_is_init = [True] * num_layers\n+\n+        self.padding_cache = [None] * num_layers\n+\n+    def update(self, hidden_states: torch.Tensor, layer_idx: int):\n+        \"\"\"\n+        Updates the padding cache with the new padding states for the layer `layer_idx` and returns the current cache.\n+\n+        Parameters:\n+            hidden_states (`torch.Tensor`):\n+                The hidden states to be partially cached.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+        Returns:\n+            `torch.Tensor` or `None`, the current padding cache.\n+        \"\"\"\n+        batch_size, dtype, device = hidden_states.shape[0], hidden_states.dtype, hidden_states.device\n+        padding = self.per_layer_padding[layer_idx]\n+        padding_mode = self.per_layer_padding_mode[layer_idx]\n+        in_channels = self.per_layer_in_channels[layer_idx]\n+\n+        if self.padding_cache[layer_idx] is None:\n+            if padding_mode == \"constant\":\n+                current_cache = torch.zeros(\n+                    batch_size,\n+                    in_channels,\n+                    padding,\n+                    device=device,\n+                    dtype=dtype,\n+                )\n+            elif padding_mode == \"replicate\":\n+                current_cache = (\n+                    torch.ones(\n+                        batch_size,\n+                        in_channels,\n+                        padding,\n+                        device=device,\n+                        dtype=dtype,\n+                    )\n+                    * hidden_states[..., :1]\n+                )\n+        else:\n+            current_cache = self.padding_cache[layer_idx]\n+\n+        # update the cache\n+        if padding > 0:\n+            padding_states = hidden_states[:, :, -padding:]\n+        else:\n+            padding_states = torch.empty(batch_size, in_channels, padding, dtype=dtype, device=device)\n+        self.padding_cache[layer_idx] = padding_states\n+\n+        return current_cache\n+\n+\n @dataclass\n @auto_docstring\n class MimiEncoderOutput(ModelOutput):\n@@ -96,6 +176,7 @@ class MimiEncoderOutput(ModelOutput):\n \n     audio_codes: Optional[torch.LongTensor] = None\n     encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None\n+    padding_cache: Optional[MimiConv1dPaddingCache] = None\n \n \n @dataclass\n@@ -130,12 +211,15 @@ def __init__(\n         stride: int = 1,\n         dilation: int = 1,\n         groups: int = 1,\n-        pad_mode=None,\n+        pad_mode: Optional[str] = None,\n         bias: bool = True,\n+        layer_idx: Optional[int] = None,\n     ):\n         super().__init__()\n         self.causal = config.use_causal_conv\n         self.pad_mode = config.pad_mode if pad_mode is None else pad_mode\n+        self.layer_idx = layer_idx\n+        self.in_channels = in_channels\n \n         # warn user on unusual setup between dilation and stride\n         if stride > 1 and dilation > 1:\n@@ -232,12 +316,20 @@ def _get_output_length(self, input_length: torch.LongTensor) -> torch.LongTensor\n         ) // self.conv.stride[0] + 1\n         return output_lenght\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states, padding_cache=None):\n         extra_padding = self._get_extra_padding_for_conv1d(hidden_states)\n \n-        if self.causal:\n+        if not self.causal and padding_cache is not None:\n+            raise ValueError(\"`padding_cache` is not supported for non-causal convolutions.\")\n+\n+        if self.causal and padding_cache is not None:\n+            layer_padding_cache = padding_cache.update(hidden_states, self.layer_idx)\n+            hidden_states = torch.cat([layer_padding_cache, hidden_states], dim=2)\n+\n+        elif self.causal:\n             # Left padding for causal\n             hidden_states = self._pad1d(hidden_states, (self.padding_total, extra_padding), mode=self.pad_mode)\n+\n         else:\n             hidden_states = self._pad1d(\n                 hidden_states, (self.padding_left, self.padding_right + extra_padding), mode=self.pad_mode\n@@ -305,7 +397,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.encodec.modeling_encodec.EncodecResnetBlock with Encodec->Mimi,EnCodec->Mimi\n class MimiResnetBlock(nn.Module):\n     \"\"\"\n     Residual block from SEANet model as used by Mimi.\n@@ -331,12 +422,21 @@ def __init__(self, config: MimiConfig, dim: int, dilations: list[int]):\n         else:\n             self.shortcut = nn.Identity()\n \n-    def forward(self, hidden_states):\n+    def forward(self, hidden_states, padding_cache=None):\n         residual = hidden_states\n+\n         for layer in self.block:\n-            hidden_states = layer(hidden_states)\n+            if isinstance(layer, MimiConv1d):\n+                hidden_states = layer(hidden_states, padding_cache=padding_cache)\n+            else:\n+                hidden_states = layer(hidden_states)\n \n-        return self.shortcut(residual) + hidden_states\n+        if isinstance(self.shortcut, MimiConv1d):\n+            residual = self.shortcut(residual, padding_cache=padding_cache)\n+        else:\n+            residual = self.shortcut(residual)\n+\n+        return residual + hidden_states\n \n \n class MimiEncoder(nn.Module):\n@@ -370,10 +470,17 @@ def __init__(self, config: MimiConfig):\n         self.layers = nn.ModuleList(model)\n         self._mimiconv1d_layer_names = mimiconv1d_layer_names\n \n-    # Copied from transformers.models.encodec.modeling_encodec.EncodecEncoder.forward\n-    def forward(self, hidden_states):\n+        # initialize layer_idx for MimiConv1d submodules, necessary for padding_cache\n+        for layer_idx, layername in enumerate(self._mimiconv1d_layer_names):\n+            conv_layer = self.get_submodule(layername)\n+            setattr(conv_layer, \"layer_idx\", layer_idx)\n+\n+    def forward(self, hidden_states, padding_cache=None):\n         for layer in self.layers:\n-            hidden_states = layer(hidden_states)\n+            if isinstance(layer, (MimiConv1d, MimiResnetBlock)):\n+                hidden_states = layer(hidden_states, padding_cache=padding_cache)\n+            else:\n+                hidden_states = layer(hidden_states)\n         return hidden_states\n \n \n@@ -1005,11 +1112,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = None\n-        if attention_mask is not None:\n-            causal_mask = self._update_causal_mask(\n-                attention_mask, hidden_states, cache_position, past_key_values, output_attentions\n-            )\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+        )\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n@@ -1054,163 +1163,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Mimi\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Mimi. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Mimi\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: MimiConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`MimiConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class MimiDecoder(nn.Module):\n     \"\"\"SEANet decoder as used by Mimi.\"\"\"\n@@ -1269,7 +1221,7 @@ def embed(self) -> torch.Tensor:\n     def quantize(self, hidden_states):\n         # Projects each vector in `hidden_states` over the nearest centroid and return its index.\n         # `hidden_states` should be `[N, D]` with `N` the number of input vectors and `D` the dimension.\n-        dists = torch.cdist(hidden_states[None], self.embed[None], p=2)[0]\n+        dists = torch.cdist(hidden_states[None].float(), self.embed[None].float(), p=2)[0]\n         embed_ind = dists.argmin(dim=-1)\n         return embed_ind\n \n@@ -1476,6 +1428,7 @@ def __init__(self, config: MimiConfig):\n                 stride=2,\n                 bias=False,\n                 pad_mode=\"replicate\",\n+                layer_idx=len(self.encoder._mimiconv1d_layer_names),\n             )\n \n             self.upsample = MimiConvTranspose1d(\n@@ -1512,12 +1465,17 @@ def _encode_frame(\n         num_quantizers: int,\n         padding_mask: int,\n         past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        padding_cache: Optional[MimiConv1dPaddingCache] = None,\n         return_dict: Optional[bool] = None,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"\n         Encodes the given input using the underlying VQVAE. The padding mask is required to compute the correct scale.\n         \"\"\"\n-        embeddings = self.encoder(input_values)\n+\n+        # TODO: @eustlb, let's make the encoder support padding_mask so that batched inputs are supported.\n+        embeddings = self.encoder(input_values, padding_cache=padding_cache)\n+\n+        # TODO: @eustlb, convert the padding mask to attention mask.\n         encoder_outputs = self.encoder_transformer(\n             embeddings.transpose(1, 2), past_key_values=past_key_values, return_dict=return_dict\n         )\n@@ -1526,11 +1484,11 @@ def _encode_frame(\n         elif len(encoder_outputs) > 1:\n             past_key_values = encoder_outputs[1]\n         embeddings = encoder_outputs[0].transpose(1, 2)\n-        embeddings = self.downsample(embeddings)\n+        embeddings = self.downsample(embeddings, padding_cache=padding_cache)\n \n         codes = self.quantizer.encode(embeddings, num_quantizers)\n         codes = codes.transpose(0, 1)\n-        return codes, past_key_values\n+        return codes, past_key_values, padding_cache\n \n     def get_encoded_length(self, input_length: torch.LongTensor) -> torch.LongTensor:\n         \"\"\"\n@@ -1570,6 +1528,8 @@ def encode(\n         padding_mask: Optional[torch.Tensor] = None,\n         num_quantizers: Optional[float] = None,\n         encoder_past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        padding_cache: Optional[MimiConv1dPaddingCache] = None,\n+        use_streaming: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor, Optional[torch.Tensor]], MimiEncoderOutput]:\n         \"\"\"\n@@ -1598,6 +1558,7 @@ def encode(\n             `codebook` of shape `[batch_size, num_codebooks, frames]`, the discrete encoded codes for the input audio waveform.\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.return_dict\n+        use_streaming = use_streaming if use_streaming is not None else self.config.use_streaming\n \n         num_quantizers = self.config.num_quantizers if num_quantizers is None else num_quantizers\n \n@@ -1614,21 +1575,42 @@ def encode(\n         if padding_mask is None:\n             padding_mask = torch.ones_like(input_values).bool()\n \n-        encoded_frames, encoder_past_key_values = self._encode_frame(\n+        if use_streaming and padding_cache is None:\n+            per_layer_padding, per_layer_padding_mode, per_layer_in_channels = [], [], []\n+            for layer_name in self.encoder._mimiconv1d_layer_names:\n+                per_layer_padding.append(self.encoder.get_submodule(layer_name).padding_total)\n+                per_layer_padding_mode.append(self.encoder.get_submodule(layer_name).pad_mode)\n+                per_layer_in_channels.append(self.encoder.get_submodule(layer_name).in_channels)\n+\n+            # downsample layer\n+            per_layer_padding.append(self.downsample.padding_total)\n+            per_layer_padding_mode.append(self.downsample.pad_mode)\n+            per_layer_in_channels.append(self.downsample.in_channels)\n+\n+            padding_cache = MimiConv1dPaddingCache(\n+                num_layers=len(self.encoder._mimiconv1d_layer_names) + 1,\n+                per_layer_padding=per_layer_padding,\n+                per_layer_padding_mode=per_layer_padding_mode,\n+                per_layer_in_channels=per_layer_in_channels,\n+            )\n+\n+        encoded_frames, encoder_past_key_values, padding_cache = self._encode_frame(\n             input_values,\n             num_quantizers,\n             padding_mask.bool(),\n             past_key_values=encoder_past_key_values,\n+            padding_cache=padding_cache,\n             return_dict=return_dict,\n         )\n \n         if not return_dict:\n             return (\n                 encoded_frames,\n                 encoder_past_key_values,\n+                padding_cache,\n             )\n \n-        return MimiEncoderOutput(encoded_frames, encoder_past_key_values)\n+        return MimiEncoderOutput(encoded_frames, encoder_past_key_values, padding_cache)\n \n     def _decode_frame(\n         self,"
        },
        {
            "sha": "c58812b58d6bc1ddde5c4049b2f03c1fdbf76223",
            "filename": "src/transformers/models/sew/feature_extraction_sew.py",
            "status": "renamed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extraction_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extraction_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Ffeature_extraction_sew.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "previous_filename": "src/transformers/models/sew/feature_extractor_sew.py"
        },
        {
            "sha": "5823883c6cb8ba35c0964d62d4f32e5ca3d24d64",
            "filename": "src/transformers/models/stt/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2F__init__.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_kyutai_speech_to_text import *\n+    from .feature_extraction_kyutai_speech_to_text import *\n+    from .modeling_kyutai_speech_to_text import *\n+    from .processing_kyutai_speech_to_text import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "f9ea11a5f47cfa725c9b97062b740dad2d5205c4",
            "filename": "src/transformers/models/stt/configuration_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 188,
            "deletions": 0,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fconfiguration_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fconfiguration_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Fconfiguration_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,188 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.s\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto.configuration_auto import AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class KyutaiSpeechToTextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`KyutaiSpeechToTextForConditionalGeneration`].\n+    It is used to instantiate a Kyutai Speech-to-Text model according to the specified arguments, defining the model\n+    architecture. Instantiating a configuration with the defaults will yield a similar configuration to that of the\n+    2.6b-en model.\n+\n+    e.g. [kyutai/stt-2.6b-en](https://huggingface.co/kyutai/stt-2.6b-en)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        codebook_vocab_size (`int`, *optional*, defaults to 2049):\n+            Vocabulary size of the codebook. Defines the number of different audio tokens that can be represented by each codebook.\n+        vocab_size (`int`, *optional*, defaults to 4001):\n+            Vocabulary size of the model. Defines the number of different tokens that can be represented by the\n+            `input_ids` passed when calling the model.\n+        hidden_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the layers and the pooler layer of the main decoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 48):\n+            Number of decoder layers.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the main decoder block.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to `num_attention_heads`.\n+        max_position_embeddings (`int`, *optional*, defaults to 750):\n+            The maximum sequence length that this model might ever be used with. Typically, set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        rope_theta (`float`, *optional*, defaults to 100000.0):\n+            The base period of the RoPE embeddings.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n+            The attention head dimension.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        sliding_window (`int`, *optional*, defaults to 375):\n+            Sliding window attention window size. If not specified, will default to `3000`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        ffn_dim (`int`, *optional*, defaults to 11264):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in the main decoder block. Must be even.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-08):\n+            The epsilon used by the rms normalization layers.\n+        num_codebooks (`int`, *optional*, defaults to 32):\n+            The number of audio codebooks for each audio channels.\n+        audio_bos_token_id (`int`, *optional*, defaults to 2048):\n+            Beginning of stream token id for codebook tokens.\n+        audio_pad_token_id (`int`, *optional*, defaults to 69569):\n+            Padding token id for codebook tokens.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings.\n+        pad_token_id (`int`, *optional*, defaults to 3):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 48000):\n+            Beginning of stream token id for text tokens.\n+        codec_config (`PretrainedConfig`, *optional*):\n+            Configuration for the codec.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments. Notably:\n+                - **audio_encoder_config** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that\n+                  defines the audio encoder config.\n+                - **depth__config** ([`PretrainedConfig`], *optional*) -- An instance of a configuration object that\n+                  defines the depth decoder config.\n+\n+\n+    Example:\n+    ```python\n+    >>> from transformers import KyutaiSpeechToTextConfig, KyutaiSpeechToTextForConditionalGeneration\n+\n+    >>> # Initializing a KyutaiSpeechToTextConfig\n+    >>> configuration = KyutaiSpeechToTextConfig()\n+\n+    >>> # Initializing a model\n+    >>> model = KyutaiSpeechToTextForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    # not the best naming here for `model_type`, but original codebase already uses model type:`stt` for in the config so we keep it to simplify\n+    model_type = \"stt\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    sub_configs = {\"codec_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        codebook_vocab_size=2049,\n+        vocab_size=4001,\n+        hidden_size=2048,\n+        num_hidden_layers=48,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        max_position_embeddings=750,\n+        rope_theta=100000.0,\n+        hidden_act=\"silu\",\n+        head_dim=None,\n+        initializer_range=0.02,\n+        use_cache=True,\n+        sliding_window=375,\n+        attention_dropout=0.0,\n+        ffn_dim=11264,\n+        rms_norm_eps=1e-8,\n+        num_codebooks=32,\n+        audio_bos_token_id=2048,\n+        audio_pad_token_id=69569,\n+        tie_word_embeddings=False,\n+        pad_token_id=3,\n+        bos_token_id=48000,\n+        codec_config=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id, bos_token_id=bos_token_id, tie_word_embeddings=tie_word_embeddings, **kwargs\n+        )\n+\n+        if codec_config is None:\n+            self.codec_config = AutoConfig.for_model(\"mimi\")\n+            logger.info(\"codec_config is None, using default audio encoder config.\")\n+        elif isinstance(codec_config, dict):\n+            self.codec_config = AutoConfig.for_model(**codec_config)\n+        elif isinstance(codec_config, PretrainedConfig):\n+            self.codec_config = codec_config\n+\n+        self.num_codebooks = num_codebooks\n+        self.frame_size = self.codec_config.frame_size\n+\n+        self.audio_bos_token_id = audio_bos_token_id\n+        self.audio_pad_token_id = audio_pad_token_id\n+        self.codebook_vocab_size = codebook_vocab_size\n+\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        if ffn_dim % 2 == 1:\n+            raise ValueError(f\"`ffn_dim={ffn_dim}` must be even.\")\n+        self.ffn_dim = ffn_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_dropout = attention_dropout\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        self.sliding_window = sliding_window\n+\n+\n+__all__ = [\"KyutaiSpeechToTextConfig\"]"
        },
        {
            "sha": "fe4a5a6bc6f10ba3a9226187ebedcfb27ffd1709",
            "filename": "src/transformers/models/stt/convert_kyutai_speech_to_text_to_hf.py",
            "status": "added",
            "additions": 377,
            "deletions": 0,
            "changes": 377,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fconvert_kyutai_speech_to_text_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fconvert_kyutai_speech_to_text_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Fconvert_kyutai_speech_to_text_to_hf.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,377 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import os\n+import re\n+\n+import safetensors.torch\n+import sentencepiece\n+import torch\n+\n+from transformers import (\n+    KyutaiSpeechToTextConfig,\n+    KyutaiSpeechToTextFeatureExtractor,\n+    KyutaiSpeechToTextForConditionalGeneration,\n+    KyutaiSpeechToTextProcessor,\n+    PreTrainedTokenizerFast,\n+)\n+from transformers.convert_slow_tokenizer import MoshiConverter\n+from transformers.utils.hub import cached_file\n+\n+\n+# fmt: off\n+MOSHI_ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"out_norm\":                                                r\"norm\",\n+    r\"gating\\.linear_in\":                              r\"mlp.fc1\",\n+    r\"gating\\.linear_out\":                             r\"mlp.fc2\",\n+    r\"self_attn\\.out_proj\":                r\"self_attn.o_proj.linear\",\n+    r\"norm1\":                                      r\"input_layernorm\",\n+    r\"norm2\":                              r\"post_attention_layernorm\",\n+    r\"layer_scale_1\":                          r\"self_attn_layer_scale\",\n+    r\"layer_scale_2\":                             r\"mlp_layer_scale\",\n+    r\"alpha\":                                              r\"weight\",\n+}\n+# fmt: on\n+\n+\n+# fmt: off\n+MIMI_ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"conv\\.conv\\.conv\": \"conv\",\n+    r\"convtr\\.convtr\\.convtr\": \"conv\",\n+    r\"conv\\.conv\": \"conv\",\n+    r\"convtr\\.convtr\": \"conv\",\n+    r\"quantizer\\.rvq_first\\.vq\": \"quantizer.semantic_residual_vector_quantizer\",\n+    r\"quantizer\\.rvq_first\": \"quantizer.semantic_residual_vector_quantizer\",\n+    r\"quantizer\\.rvq_rest\\.vq\": \"quantizer.acoustic_residual_vector_quantizer\",\n+    r\"quantizer\\.rvq_rest\": \"quantizer.acoustic_residual_vector_quantizer\",\n+    r\"_codebook\": \"codebook\",\n+    r\"_initialized\": \"initialized\",\n+    r\"embedding_sum\": \"embed_sum\",\n+    r\"encoder\\.model\": \"encoder.layers\",\n+    r\"decoder\\.model\": \"decoder.layers\",\n+    r\"encoder_transformer\\.transformer\": \"encoder_transformer\",\n+    r\"decoder_transformer\\.transformer\": \"decoder_transformer\",\n+    r\"linear1\": \"mlp.fc1\",\n+    r\"linear2\": \"mlp.fc2\",\n+    r\"self_attn\\.out_proj\": \"self_attn.o_proj\",\n+    r\"norm1\": \"input_layernorm\",\n+    r\"norm2\": \"post_attention_layernorm\",\n+    r\"layer_scale_1\": \"self_attn_layer_scale\",\n+    r\"layer_scale_2\": \"mlp_layer_scale\",\n+}\n+# fmt: on\n+\n+\n+def permute_for_rope(input_tensor, n_heads, dim1, dim2):\n+    \"\"\"\n+    When you go from the complex ROPE formulation to sin and cos one, you need\n+    to permute the query and key weights (to avoid doing it on the fly)\n+    \"\"\"\n+    return input_tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n+\n+\n+def convert_key(key, mapping):\n+    for pattern, replacement in mapping.items():\n+        key = re.sub(pattern, replacement, key)\n+    return key\n+\n+\n+def convert_kyutai_speech_to_text_state_dict(state_dict, config, unwanted_prefix=\"transformer.\"):\n+    hidden_size = config.hidden_size\n+    head_dim = config.head_dim\n+    num_heads = int(config.hidden_size // config.head_dim)\n+    num_key_value_heads = config.num_key_value_heads\n+    key_value_head_dim = config.num_key_value_heads * head_dim\n+\n+    # concat embeddings\n+    embed_tokens_weight = []\n+    for i in range(32):\n+        embed_tokens_weight.append(state_dict.pop(f\"emb.{i}.weight\"))\n+\n+    embed_tokens_weight = torch.cat(embed_tokens_weight, dim=0)\n+    embed_tokens_weight = torch.cat([state_dict.pop(\"text_emb.weight\"), embed_tokens_weight])\n+    embed_tokens_weight = torch.cat([embed_tokens_weight, torch.zeros(1, config.hidden_size)], dim=0)\n+    state_dict[\"embed_tokens.embed_tokens.weight\"] = embed_tokens_weight\n+\n+    for key, value in list(state_dict.items()):\n+        if unwanted_prefix is not None and unwanted_prefix in key:\n+            new_key = key[len(unwanted_prefix) :]\n+        else:\n+            new_key = key\n+\n+        new_key = convert_key(new_key, MOSHI_ORIGINAL_TO_CONVERTED_KEY_MAPPING)\n+\n+        # Post-process the current_parameter.\n+        if \"alpha\" in key:\n+            state_dict[key] = state_dict[key].squeeze()\n+\n+        if \"in_proj_weight\" in new_key:\n+            # split qkv into query key and value\n+            mixed_qkv = state_dict.pop(key)\n+            qkv_dim = mixed_qkv.size(0) // 3\n+\n+            query_layer = mixed_qkv[:qkv_dim]\n+            key_layer = mixed_qkv[qkv_dim : qkv_dim * 2]\n+            value_layer = mixed_qkv[qkv_dim * 2 :]\n+            state_dict[new_key.replace(\"in_proj_weight\", \"q_proj.linear.weight\")] = permute_for_rope(\n+                query_layer, num_heads, hidden_size, hidden_size\n+            )\n+            state_dict[new_key.replace(\"in_proj_weight\", \"k_proj.linear.weight\")] = permute_for_rope(\n+                key_layer, num_key_value_heads, key_value_head_dim, hidden_size\n+            )\n+\n+            state_dict[new_key.replace(\"in_proj_weight\", \"v_proj.linear.weight\")] = value_layer\n+        else:\n+            state_dict[new_key] = state_dict.pop(key)\n+\n+    return state_dict\n+\n+\n+def convert_mimi_state_dict(state_dict, config, unwanted_prefix=None):\n+    hidden_size = config.hidden_size\n+    head_dim = config.head_dim\n+    num_heads = int(config.hidden_size // config.head_dim)\n+    num_key_value_heads = config.num_key_value_heads\n+    key_value_head_dim = config.num_key_value_heads * head_dim\n+\n+    for key, value in list(state_dict.items()):\n+        if unwanted_prefix is not None and unwanted_prefix in key:\n+            new_key = key[len(unwanted_prefix) :]\n+        else:\n+            new_key = key\n+\n+        new_key = convert_key(new_key, MIMI_ORIGINAL_TO_CONVERTED_KEY_MAPPING)\n+\n+        if \"in_proj_weight\" in new_key:\n+            # split qkv into query key and value\n+            mixed_qkv = state_dict.pop(key)\n+            qkv_dim = mixed_qkv.size(0) // 3\n+\n+            query_layer = mixed_qkv[:qkv_dim]\n+            key_layer = mixed_qkv[qkv_dim : qkv_dim * 2]\n+            value_layer = mixed_qkv[qkv_dim * 2 :]\n+\n+            state_dict[new_key.replace(\"in_proj_weight\", \"q_proj.weight\")] = permute_for_rope(\n+                query_layer, num_heads, hidden_size, hidden_size\n+            )\n+            state_dict[new_key.replace(\"in_proj_weight\", \"k_proj.weight\")] = permute_for_rope(\n+                key_layer, num_key_value_heads, key_value_head_dim, hidden_size\n+            )\n+            state_dict[new_key.replace(\"in_proj_weight\", \"v_proj.weight\")] = value_layer\n+        else:\n+            state_dict[new_key] = state_dict.pop(key)\n+\n+    return state_dict\n+\n+\n+def write_model(\n+    input_path_or_repo,\n+    model_name,\n+    codec_model_path_or_repo,\n+    codec_model_name,\n+    output_dir,\n+    safe_serialization=True,\n+    unwanted_prefix=\"transformer.\",\n+):\n+    print(\"Converting the model.\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    config = KyutaiSpeechToTextConfig()\n+    config.use_cache = True\n+    config.codec_config.sliding_window = 250\n+\n+    model_path = cached_file(\n+        input_path_or_repo,\n+        model_name,\n+    )\n+\n+    codec_path = cached_file(\n+        codec_model_path_or_repo,\n+        codec_model_name,\n+    )\n+\n+    print(f\"Fetching all parameters from the checkpoint at {model_path}...\")\n+    state_dict = safetensors.torch.load_file(model_path)\n+\n+    print(f\"Fetching all parameters from the checkpoint at {codec_path}...\")\n+    codec_state_dict = safetensors.torch.load_file(codec_path)\n+\n+    print(\"Converting model...\")\n+    # -----------------------\n+    # convert parameter names\n+    # -----------------------\n+    state_dict = convert_kyutai_speech_to_text_state_dict(state_dict, config, unwanted_prefix=unwanted_prefix)\n+    codec_state_dict = convert_mimi_state_dict(codec_state_dict, config.codec_config, unwanted_prefix=None)\n+\n+    # -------------------------\n+    # load the weights and save\n+    # -------------------------\n+    print(\"Loading the checkpoint in a Moshi ASR model.\")\n+    with torch.device(\"meta\"):\n+        model = KyutaiSpeechToTextForConditionalGeneration(config)\n+\n+    linear_weight = state_dict.pop(\"text_linear.weight\")\n+    model.model.load_state_dict(state_dict, strict=True, assign=True)\n+\n+    linear_weight = torch.cat([linear_weight, torch.zeros(1, config.hidden_size)])\n+    model.lm_head.load_state_dict({\"weight\": linear_weight}, strict=True, assign=True)\n+\n+    model.codec_model.load_state_dict(codec_state_dict, strict=True, assign=True)\n+\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+    del model.config.codec_config._name_or_path\n+\n+    # default generation config\n+    model.generation_config._from_model_config = False\n+    model.generation_config.audio_window_size = 1\n+    model.generation_config.cache_implementation = \"sliding_window\"\n+\n+    model.codec_model.generation_config._from_model_config = False\n+    model.codec_model.generation_config.cache_implementation = \"sliding_window\"\n+    model.codec_model.generation_config.use_cache = True\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+        output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\"\n+    )\n+    print(\"Model reloaded successfully.\")\n+\n+\n+def write_processor(\n+    input_path_or_repo,\n+    tokenizer_model_name,\n+    codec_model_path_or_repo,\n+    output_dir,\n+    audio_delay_seconds,\n+    audio_silence_prefix_seconds,\n+):\n+    tokenizer_path = cached_file(\n+        input_path_or_repo,\n+        tokenizer_model_name,\n+    )\n+\n+    tokenizer = MoshiConverter(tokenizer_path).converted()\n+    original_tokenizer = sentencepiece.SentencePieceProcessor(tokenizer_path)\n+\n+    tokenizer = PreTrainedTokenizerFast(\n+        tokenizer_object=tokenizer,\n+        chat_template=None,\n+        unk_token=\"<unk>\",\n+        model_input_names=[\"input_ids\", \"attention_mask\"],\n+        clean_up_tokenization_spaces=False,\n+        bos_token_id=original_tokenizer.bos_id(),\n+        eos_token_id=original_tokenizer.eos_id(),\n+        pad_token_id=original_tokenizer.pad_id(),\n+    )\n+\n+    feature_extractor = KyutaiSpeechToTextFeatureExtractor(\n+        audio_delay_seconds=audio_delay_seconds,\n+        audio_silence_prefix_seconds=audio_silence_prefix_seconds,\n+    )\n+\n+    processor = KyutaiSpeechToTextProcessor(feature_extractor, tokenizer)\n+    processor.save_pretrained(output_dir)\n+    print(f\"Processor saved successfully to {output_dir}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert Moshi ASR weights to HuggingFace format\")\n+    parser.add_argument(\n+        \"--input_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing Moshi ASR weights\",\n+    )\n+    parser.add_argument(\n+        \"--model_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the model in input_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--tokenizer_model_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the tokenizer model in input_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--codec_model_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing the Mimi weights\",\n+    )\n+    parser.add_argument(\n+        \"--mimi_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the Mimi model in codec_model_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--preprocessor_model_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing the preprocessor config\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--audio_delay_seconds\",\n+        type=float,\n+        required=True,\n+        help=\"Audio delay in seconds to add to the right of the input\",\n+    )\n+    parser.add_argument(\n+        \"--audio_silence_prefix_seconds\",\n+        type=float,\n+        required=True,\n+        help=\"Audio silence prefix in seconds to add to the left of the input\",\n+    )\n+    args = parser.parse_args()\n+\n+    write_model(\n+        args.input_path_or_repo,\n+        args.model_name,\n+        args.codec_model_path_or_repo,\n+        args.mimi_name,\n+        args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    write_processor(\n+        args.input_path_or_repo,\n+        args.tokenizer_model_name,\n+        args.preprocessor_model_path_or_repo,\n+        args.output_dir,\n+        args.audio_delay_seconds,\n+        args.audio_silence_prefix_seconds,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "94ddb15daa6dcb411a924958c896f78707f9b417",
            "filename": "src/transformers/models/stt/feature_extraction_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 237,
            "deletions": 0,
            "changes": 237,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Ffeature_extraction_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Ffeature_extraction_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Ffeature_extraction_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,237 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/stt/modular_kyutai_speech_to_text.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_kyutai_speech_to_text.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Kyutai and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...feature_extraction_sequence_utils import SequenceFeatureExtractor\n+from ...feature_extraction_utils import BatchFeature\n+from ...utils import PaddingStrategy, TensorType, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class KyutaiSpeechToTextFeatureExtractor(SequenceFeatureExtractor):\n+    r\"\"\"\n+    Constructs an KyutaiSpeechToText feature extractor.\n+\n+    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n+    most of the main methods. Users should refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        feature_size (`int`, *optional*, defaults to 1):\n+            The feature dimension of the extracted features. Use 1 for mono, 2 for stereo.\n+        sampling_rate (`int`, *optional*, defaults to 24000):\n+            The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).\n+        padding_value (`float`, *optional*, defaults to 0.0):\n+            The value that is used to fill the padding values.\n+        chunk_length_s (`float`, *optional*):\n+            If defined the audio is pre-processed into chunks of lengths `chunk_length_s` and then encoded.\n+        overlap (`float`, *optional*):\n+            Defines the overlap between each chunk. It is used to compute the `chunk_stride` using the following\n+            formulae : `int((1.0 - self.overlap) * self.chunk_length)`.\n+        audio_delay_seconds (`float`, *optional*, defaults to 0.0):\n+            The delay in seconds to add after the audio (right padding).\n+        audio_silence_prefix_seconds (`float`, *optional*, defaults to 0.0):\n+            The silence prefix in seconds to add before the audio (left padding).\n+    \"\"\"\n+\n+    model_input_names = [\"input_values\", \"padding_mask\"]\n+\n+    def __init__(\n+        self,\n+        feature_size: int = 1,\n+        sampling_rate: int = 24000,\n+        padding_value: float = 0.0,\n+        chunk_length_s: Optional[float] = None,\n+        overlap: Optional[float] = None,\n+        audio_delay_seconds: Optional[float] = 0.0,\n+        audio_silence_prefix_seconds: Optional[float] = 0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)\n+        self.chunk_length_s = chunk_length_s\n+        self.overlap = overlap\n+        self.audio_delay_seconds = audio_delay_seconds\n+        self.audio_silence_prefix_seconds = audio_silence_prefix_seconds\n+\n+    # This is a property because you might want to change the chunk_length_s on the fly\n+    @property\n+    def chunk_length(self) -> Optional[int]:\n+        if self.chunk_length_s is None:\n+            return None\n+        else:\n+            return int(self.chunk_length_s * self.sampling_rate)\n+\n+    # This is a property because you might want to change the chunk_length_s on the fly\n+    @property\n+    def chunk_stride(self) -> Optional[int]:\n+        if self.chunk_length_s is None or self.overlap is None:\n+            return None\n+        else:\n+            return max(1, int((1.0 - self.overlap) * self.chunk_length))\n+\n+    def __call__(\n+        self,\n+        raw_audio: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        padding: Optional[Union[bool, str, PaddingStrategy]] = None,\n+        truncation: Optional[bool] = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        sampling_rate: Optional[int] = None,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several sequence(s).\n+\n+        Args:\n+            raw_audio (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n+                The sequence or batch of sequences to be processed. Each sequence can be a numpy array, a list of float\n+                values, a list of numpy arrays or a list of list of float values. The numpy array must be of shape\n+                `(num_samples,)` for mono audio (`feature_size = 1`), or `(2, num_samples)` for stereo audio\n+                (`feature_size = 2`).\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n+                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                index) among:\n+\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            truncation (`bool`, *optional*, defaults to `False`):\n+                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `audio` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self} was trained using a sampling rate of\"\n+                    f\" {self.sampling_rate}. Please make sure that the provided audio input was sampled with\"\n+                    f\" {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        if padding and truncation:\n+            raise ValueError(\"Both padding and truncation were set. Make sure you only set one.\")\n+        elif padding is None:\n+            # by default let's pad the inputs\n+            padding = True\n+\n+        is_batched = bool(\n+            isinstance(raw_audio, (list, tuple)) and (isinstance(raw_audio[0], (np.ndarray, tuple, list)))\n+        )\n+\n+        if is_batched:\n+            raw_audio = [np.asarray(audio, dtype=np.float32).T for audio in raw_audio]\n+        elif not is_batched and not isinstance(raw_audio, np.ndarray):\n+            raw_audio = np.asarray(raw_audio, dtype=np.float32)\n+        elif isinstance(raw_audio, np.ndarray) and raw_audio.dtype is np.dtype(np.float64):\n+            raw_audio = raw_audio.astype(np.float32)\n+\n+        # always return batch\n+        if not is_batched:\n+            raw_audio = [np.asarray(raw_audio).T]\n+\n+        # verify inputs are valid\n+        for idx, example in enumerate(raw_audio):\n+            if example.ndim > 2:\n+                raise ValueError(f\"Expected input shape (channels, length) but got shape {example.shape}\")\n+            if self.feature_size == 1 and example.ndim != 1:\n+                raise ValueError(f\"Expected mono audio but example has {example.shape[-1]} channels\")\n+            if self.feature_size == 2 and example.shape[-1] != 2:\n+                raise ValueError(f\"Expected stereo audio but example has {example.shape[-1]} channels\")\n+\n+        padded_inputs = None\n+        input_values = BatchFeature({\"input_values\": raw_audio})\n+        if self.chunk_stride is not None and self.chunk_length is not None and max_length is None:\n+            if truncation:\n+                max_length = min(array.shape[0] for array in raw_audio)\n+                nb_step = int(np.floor(max_length / self.chunk_stride))\n+                max_length = (nb_step - 1) * self.chunk_stride + self.chunk_length\n+            elif padding:\n+                max_length = max(array.shape[0] for array in raw_audio)\n+                nb_step = int(np.ceil(max_length / self.chunk_stride))\n+                max_length = (nb_step - 1) * self.chunk_stride + self.chunk_length\n+                padding = \"max_length\"\n+            else:\n+                padded_inputs = input_values\n+\n+        # normal padding on batch\n+        if padded_inputs is None:\n+            padded_inputs = self.pad(\n+                input_values,\n+                max_length=max_length,\n+                truncation=truncation,\n+                padding=padding,\n+                return_attention_mask=padding,\n+            )\n+\n+            if padding:\n+                padded_inputs[\"padding_mask\"] = padded_inputs.pop(\"attention_mask\")\n+\n+        # now let's padd left and right\n+        pad_left = int(self.audio_silence_prefix_seconds * self.sampling_rate)\n+        pad_right = int((self.audio_delay_seconds + 1.0) * self.sampling_rate)\n+        padded_inputs[\"input_values\"] = np.pad(\n+            padded_inputs[\"input_values\"],\n+            ((0, 0), (pad_left, pad_right)),\n+            mode=\"constant\",\n+            constant_values=0.0,\n+        )\n+        if padding:\n+            padded_inputs[\"padding_mask\"] = np.pad(\n+                padded_inputs[\"padding_mask\"],\n+                ((0, 0), (pad_left, pad_right)),\n+                mode=\"constant\",\n+                constant_values=0,\n+            )\n+\n+        input_values = []\n+        for example in padded_inputs.pop(\"input_values\"):\n+            if self.feature_size == 1:\n+                example = example[..., None]\n+            input_values.append(example.T)\n+\n+        padded_inputs[\"input_values\"] = input_values\n+        if return_tensors is not None:\n+            padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n+\n+        return padded_inputs\n+\n+\n+__all__ = [\"KyutaiSpeechToTextFeatureExtractor\"]"
        },
        {
            "sha": "7a86cd440c0f47e3ef823d1c800e316c70049649",
            "filename": "src/transformers/models/stt/modeling_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 1434,
            "deletions": 0,
            "changes": 1434,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Fmodeling_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,1434 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/stt/modular_kyutai_speech_to_text.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_kyutai_speech_to_text.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Kyutai and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+import types\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...generation import GenerationConfig, GenerationMixin\n+from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...modeling_flash_attention_utils import (\n+    FlashAttentionKwargs,\n+    flash_attn_supports_top_left_mask,\n+    is_flash_attn_available,\n+)\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ..auto import AutoModel\n+from .configuration_kyutai_speech_to_text import KyutaiSpeechToTextConfig\n+\n+\n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class KyutaiSpeechToTextRMSNorm(nn.Module):\n+    def __init__(self, dim: int, eps: float = 1e-6):\n+        super().__init__()\n+        self.eps = eps\n+        self.weight = nn.Parameter(torch.ones(dim))  # Ignore copy\n+\n+    def _norm(self, x):\n+        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n+\n+    # Ignore copy\n+    def forward(self, x):\n+        output = self._norm(x.float())\n+        output = output * self.weight.float()\n+        return output.type_as(x)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.eps}\"\n+\n+\n+class KyutaiSpeechToTextFlexibleLinear(nn.Module):\n+    def __init__(self, input_size, output_size, num_layers):\n+        super().__init__()\n+        # Stack the weights for N layers into a single tensor (num_layers, output_size, input_size)\n+        self.weight = nn.Parameter(torch.randn(num_layers, output_size, input_size))\n+\n+    def forward(self, x, layer_idx=None):\n+        \"\"\"\n+        `KyutaiSpeechToTextFlexibleLinear` creates one linear layer per codebook. There's multiple ways to use it.\n+        In the default case, `sequence_length=num_layers`, so each element of the sequence will be matmul to the weights corresponding to its index on the sequence.\n+\n+        For more advanced cases, one can specify which codebook's layer(s) to use with `layer_idx`.\n+        If `layer_idx` indicates a single integer, all of the element of the sequence will be matmul to this single codebook's layer.\n+        But if `layer_idx` is a tensor of shape `(seq_length,)`, it will matmul each i-th element of the input sequence to the corresponding layer `weight[i]`.\n+\n+\n+        Args:\n+            x (`torch.FloatTensor): input to the layer of shape `(batch, num_layers, embed_dim)` or of shape `(batch, seq_length, embed_dim)`\n+            layer_idx (`torch.Tensor`, *optional*):\n+                Can be used to specify which codebook's layers(s) to use.\n+                If it's a tensor of shape `(seq_length,)`, will matmul each element of the sequence to the corresponding weights.\n+                But if `layer_idx` is a tensor of shape `(seq_length,)`, it will matmul each i-th element of the input sequence to the corresponding layer `weight[i]`.\n+        \"\"\"\n+\n+        # Use torch.gather to select the corresponding weights for each sample\n+        # (codebooks, output_size, hidden_size)\n+        selected_weights = torch.index_select(self.weight, 0, layer_idx) if layer_idx is not None else self.weight\n+\n+        # (1, codebooks, hidden_size, output_size)\n+        selected_weights = selected_weights.transpose(1, 2)[None, :, :, :]\n+\n+        # (batch_size, codebooks, 1, hidden_size) x (1, codebooks, hidden_size, output_size)\n+        # -> (batch_size, codebooks, 1, output_size)\n+        x = torch.matmul(x[:, :, None, :], selected_weights)\n+\n+        # (batch_size, codebooks, output_size)\n+        return x.squeeze(2)\n+\n+\n+@auto_docstring\n+class KyutaiSpeechToTextPreTrainedModel(PreTrainedModel):\n+    config_class = KyutaiSpeechToTextConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"KyutaiSpeechToTextDecoderLayer\", \"MimiTransformerLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_cache_class = True\n+    main_input_name = \"input_ids\"\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, KyutaiSpeechToTextFlexibleLinear):\n+            module.weight.data.normal_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, KyutaiSpeechToTextRMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+class KyutaiSpeechToTextConv1dPaddingCache:\n+    \"\"\"\n+    Padding cache for KyutaiSpeechToTextConv1d causal convolutions in order to support streaming via cache padding.\n+    See: https://arxiv.org/pdf/2005.06720 & https://arxiv.org/pdf/2204.07064\n+\n+    A padding cache is a list of cached partial hidden states for each convolution layer.\n+    Hidden states are cached from the previous call to the KyutaiSpeechToTextConv1d forward pass, given the padding size.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        num_layers: int,\n+        per_layer_padding: list[int],\n+        per_layer_padding_mode: list[str],\n+        per_layer_in_channels: list[int],\n+    ):\n+        # ensure correct number of layers for each arg\n+        from_args_num_layers = {len(per_layer_padding), len(per_layer_padding_mode), len(per_layer_in_channels)}\n+\n+        if len(from_args_num_layers) != 1 or from_args_num_layers.pop() != num_layers:\n+            raise ValueError(\n+                f\"Expected `num_layers` ({num_layers}) values in `per_layer_padding`, `per_layer_padding_mode` and `per_layer_in_channels`\"\n+            )\n+        elif not all(mode in [\"constant\", \"replicate\"] for mode in per_layer_padding_mode):\n+            raise NotImplementedError(\n+                \"`padding_cache` is not supported for convolutions using other than `constant` or `replicate` padding mode\"\n+            )\n+\n+        self.per_layer_padding = per_layer_padding\n+        self.per_layer_padding_mode = per_layer_padding_mode\n+        self.per_layer_in_channels = per_layer_in_channels\n+        self.per_layer_is_init = [True] * num_layers\n+\n+        self.padding_cache = [None] * num_layers\n+\n+    def update(self, hidden_states: torch.Tensor, layer_idx: int):\n+        \"\"\"\n+        Updates the padding cache with the new padding states for the layer `layer_idx` and returns the current cache.\n+\n+        Parameters:\n+            hidden_states (`torch.Tensor`):\n+                The hidden states to be partially cached.\n+            layer_idx (`int`):\n+                The index of the layer to cache the states for.\n+        Returns:\n+            `torch.Tensor` or `None`, the current padding cache.\n+        \"\"\"\n+        batch_size, dtype, device = hidden_states.shape[0], hidden_states.dtype, hidden_states.device\n+        padding = self.per_layer_padding[layer_idx]\n+        padding_mode = self.per_layer_padding_mode[layer_idx]\n+        in_channels = self.per_layer_in_channels[layer_idx]\n+\n+        if self.padding_cache[layer_idx] is None:\n+            if padding_mode == \"constant\":\n+                current_cache = torch.zeros(\n+                    batch_size,\n+                    in_channels,\n+                    padding,\n+                    device=device,\n+                    dtype=dtype,\n+                )\n+            elif padding_mode == \"replicate\":\n+                current_cache = (\n+                    torch.ones(\n+                        batch_size,\n+                        in_channels,\n+                        padding,\n+                        device=device,\n+                        dtype=dtype,\n+                    )\n+                    * hidden_states[..., :1]\n+                )\n+        else:\n+            current_cache = self.padding_cache[layer_idx]\n+\n+        # update the cache\n+        if padding > 0:\n+            padding_states = hidden_states[:, :, -padding:]\n+        else:\n+            padding_states = torch.empty(batch_size, in_channels, padding, dtype=dtype, device=device)\n+        self.padding_cache[layer_idx] = padding_states\n+\n+        return current_cache\n+\n+\n+class KyutaiSpeechToTextEmbeddings(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.embed_tokens = nn.Embedding(\n+            config.vocab_size + (config.num_codebooks * config.codebook_vocab_size) + 1,\n+            config.hidden_size,\n+            padding_idx=config.audio_pad_token_id,\n+        )\n+        audio_tokens_offsets = torch.arange(config.num_codebooks) * config.codebook_vocab_size\n+        audio_tokens_offsets += config.vocab_size\n+        audio_tokens_offsets = nn.functional.pad(\n+            audio_tokens_offsets, (1, 0)\n+        )  # pad one 0 to the left for the text token\n+        self.register_buffer(\"audio_tokens_offsets\", audio_tokens_offsets, persistent=False)\n+\n+    def forward(self, input_ids):\n+        input_ids = torch.where(\n+            input_ids == self.embed_tokens.padding_idx, input_ids, input_ids + self.audio_tokens_offsets\n+        )\n+        inputs_embeds = self.embed_tokens(input_ids)\n+        inputs_embeds = inputs_embeds.sum(dim=2)\n+        return inputs_embeds\n+\n+\n+class KyutaiSpeechToTextLinear(nn.Module):\n+    def __init__(self, input_dim, output_dim, num_codebooks, use_flexible_linear=False):\n+        super().__init__()\n+\n+        self.use_flexible_linear = use_flexible_linear\n+\n+        if not use_flexible_linear:\n+            self.linear = nn.Linear(input_dim, output_dim, bias=False)\n+        else:\n+            self.linear = KyutaiSpeechToTextFlexibleLinear(input_dim, output_dim, num_layers=num_codebooks)\n+\n+    def forward(self, x, layer_idx=None):\n+        if self.use_flexible_linear:\n+            return self.linear(x, layer_idx)\n+        else:\n+            return self.linear(x)\n+\n+\n+class KyutaiSpeechToTextRotaryEmbedding(nn.Module):\n+    def __init__(self, config: KyutaiSpeechToTextConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class KyutaiSpeechToTextGatingMLP(nn.Module):\n+    def __init__(self, config, use_flexible_linear=False):\n+        super().__init__()\n+\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        ffn_dim = config.ffn_dim\n+        hidden_size = config.hidden_size\n+        num_layers = config.num_codebooks if use_flexible_linear else 1\n+        if num_layers == 1:\n+            self.fc1 = nn.Linear(hidden_size, ffn_dim, bias=False)\n+            self.fc2 = nn.Linear(ffn_dim // 2, hidden_size, bias=False)\n+        else:\n+            self.fc1 = KyutaiSpeechToTextFlexibleLinear(hidden_size, ffn_dim, num_layers)\n+            self.fc2 = KyutaiSpeechToTextFlexibleLinear(ffn_dim // 2, hidden_size, num_layers)\n+\n+    def forward(self, hidden_states: torch.Tensor, layer_idx: Optional[int] = None) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states) if layer_idx is None else self.fc1(hidden_states, layer_idx)\n+\n+        batch_size, sequence_length, _ = hidden_states.shape\n+        hidden_states = hidden_states.view(batch_size, sequence_length, 2, -1)\n+        hidden_states = self.activation_fn(hidden_states[..., 0, :]) * hidden_states[..., 1, :]\n+        hidden_states = self.fc2(hidden_states) if layer_idx is None else self.fc2(hidden_states, layer_idx)\n+        return hidden_states\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+class KyutaiSpeechToTextAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(\n+        self,\n+        config: KyutaiSpeechToTextConfig,\n+        layer_idx: Optional[int] = None,\n+        use_flexible_linear=False,\n+        use_rope=True,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        if layer_idx is None:\n+            logger.warning_once(\n+                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n+                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+\n+        self.attention_dropout = config.attention_dropout\n+        self.hidden_size = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = config.head_dim\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n+        self.is_causal = True\n+        self.scaling = 1 / math.sqrt(self.head_dim)\n+\n+        if self.hidden_size % self.num_heads != 0:\n+            raise ValueError(\n+                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n+                f\" and `num_heads`: {self.num_heads}).\"\n+            )\n+\n+        self.q_proj = KyutaiSpeechToTextLinear(\n+            self.hidden_size, self.num_heads * self.head_dim, config.num_codebooks, use_flexible_linear\n+        )\n+        self.k_proj = KyutaiSpeechToTextLinear(\n+            self.hidden_size, self.num_key_value_heads * self.head_dim, config.num_codebooks, use_flexible_linear\n+        )\n+        self.v_proj = KyutaiSpeechToTextLinear(\n+            self.hidden_size, self.num_key_value_heads * self.head_dim, config.num_codebooks, use_flexible_linear\n+        )\n+        self.o_proj = KyutaiSpeechToTextLinear(\n+            self.num_heads * self.head_dim, self.hidden_size, config.num_codebooks, use_flexible_linear\n+        )\n+\n+        # rotary embeddings are not used in the depth decoder\n+        self.rotary_emb = None\n+        if use_rope:\n+            self.rope_theta = config.rope_theta\n+            self.rotary_emb = KyutaiSpeechToTextRotaryEmbedding(config)\n+\n+    # copied from transformers.models.gemma.modeling_gemma.GemmaAttention.forward\n+    # no longer copied after attention refactors\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states, cache_position)  # Ignore copy\n+        key_states = self.k_proj(hidden_states, cache_position)  # Ignore copy\n+        value_states = self.v_proj(hidden_states, cache_position)  # Ignore copy\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if self.rotary_emb is not None:  # Ignore copy\n+            cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = (\n+                {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+                if self.rotary_emb is not None\n+                else {\"cache_position\": cache_position}\n+            )  # Ignore copy\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scaling\n+\n+        if attention_mask is not None:  # no matter the length, we just slice it\n+            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+            attn_weights = attn_weights + causal_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+        attn_output = self.o_proj(attn_output, cache_position)  # Ignore copy\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->KyutaiSpeechToText\n+# TODO cyril: modular\n+class KyutaiSpeechToTextFlashAttention2(KyutaiSpeechToTextAttention):\n+    \"\"\"\n+    KyutaiSpeechToText flash attention module. This module inherits from `KyutaiSpeechToTextAttention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignment, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = flash_attn_supports_top_left_mask()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        if isinstance(past_key_value, StaticCache):\n+            raise ValueError(\n+                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n+                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n+            )\n+\n+        output_attentions = False\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states, cache_position)  # Ignore copy\n+        key_states = self.k_proj(hidden_states, cache_position)  # Ignore copy\n+        value_states = self.v_proj(hidden_states, cache_position)  # Ignore copy\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if self.rotary_emb is not None:  # Ignore copy\n+            cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = (\n+                {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+                if self.rotary_emb is not None\n+                else {\"cache_position\": cache_position}\n+            )  # Ignore copy\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n+        # to be able to avoid many of these transpose/reshape/view.\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        dropout_rate = self.attention_dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32. (KyutaiSpeechToTextRMSNorm handles it correctly)\n+\n+        input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            position_ids=position_ids,\n+            dropout=dropout_rate,\n+            sliding_window=getattr(self, \"sliding_window\", None),\n+            is_causal=self.is_causal,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n+        attn_output = self.o_proj(attn_output, cache_position)  # Ignore copy\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights, past_key_value\n+\n+\n+# NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->KyutaiSpeechToText\n+# TODO cyril: modular\n+class KyutaiSpeechToTextSdpaAttention(KyutaiSpeechToTextAttention):\n+    \"\"\"\n+    KyutaiSpeechToText attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `KyutaiSpeechToTextAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    # Adapted from KyutaiSpeechToTextAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: bool = False,\n+        use_cache: bool = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"KyutaiSpeechToTextModel is using KyutaiSpeechToTextSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+        bsz, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states, cache_position)  # Ignore copy\n+        key_states = self.k_proj(hidden_states, cache_position)  # Ignore copy\n+        value_states = self.v_proj(hidden_states, cache_position)  # Ignore copy\n+\n+        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+\n+        if self.rotary_emb is not None:  # Ignore copy\n+            cos, sin = self.rotary_emb(value_states, position_ids)  # Ignore copy\n+            query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)  # Ignore copy\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = (\n+                {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+                if self.rotary_emb is not None\n+                else {\"cache_position\": cache_position}\n+            )  # Ignore copy\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        key_states = repeat_kv(key_states, self.num_key_value_groups)\n+        value_states = repeat_kv(value_states, self.num_key_value_groups)\n+\n+        causal_mask = attention_mask\n+        if attention_mask is not None:\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and causal_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if causal_mask is None and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=causal_mask,\n+            dropout_p=self.attention_dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(bsz, q_len, -1)\n+\n+        attn_output = self.o_proj(attn_output, cache_position)  # Ignore copy\n+\n+        return attn_output, None, past_key_value\n+\n+\n+STT_ATTENTION_CLASSES = {\n+    \"eager\": KyutaiSpeechToTextAttention,\n+    \"flash_attention_2\": KyutaiSpeechToTextFlashAttention2,\n+    \"sdpa\": KyutaiSpeechToTextSdpaAttention,\n+}\n+\n+\n+class KyutaiSpeechToTextDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: KyutaiSpeechToTextConfig, layer_idx: int, use_flexible_linear: bool, use_rope=True):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.use_flexible_linear = use_flexible_linear\n+\n+        self.self_attn = STT_ATTENTION_CLASSES[config._attn_implementation](\n+            config=config, layer_idx=layer_idx, use_flexible_linear=use_flexible_linear, use_rope=use_rope\n+        )\n+\n+        self.mlp = KyutaiSpeechToTextGatingMLP(config, use_flexible_linear)\n+        self.input_layernorm = KyutaiSpeechToTextRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = KyutaiSpeechToTextRMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n+        self.sliding_window = config.sliding_window\n+\n+        self._attn_implementation = config._attn_implementation\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = (\n+            self.mlp(hidden_states) if not self.use_flexible_linear else self.mlp(hidden_states, cache_position)\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        if use_cache:\n+            outputs += (present_key_value,)\n+\n+        return outputs\n+\n+\n+@auto_docstring\n+class KyutaiSpeechToTextModel(KyutaiSpeechToTextPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+        self.embed_tokens = KyutaiSpeechToTextEmbeddings(config)\n+        self.layers = nn.ModuleList(\n+            [\n+                KyutaiSpeechToTextDecoderLayer(config, layer_idx, use_flexible_linear=False)\n+                for layer_idx in range(config.num_hidden_layers)\n+            ]\n+        )\n+        self.norm = KyutaiSpeechToTextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[tuple, BaseModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        return_legacy_cache = False  # noqa: F841\n+        if (\n+            use_cache and not isinstance(past_key_values, Cache) and not self.training\n+        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+            return_legacy_cache = True  # noqa: F841\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = None\n+        if attention_mask is not None:\n+            causal_mask = self._update_causal_mask(\n+                attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+            )\n+\n+        # embed positions\n+        hidden_states = inputs_embeds\n+\n+        if (\n+            use_cache and not isinstance(past_key_values, Cache) and not self.training\n+        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n+            return_legacy_cache = True\n+            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n+            logger.warning_once(\n+                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n+                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n+            )\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        next_decoder_cache = None\n+\n+        for decoder_layer in self.layers:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if use_cache:\n+                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        next_cache = next_decoder_cache if use_cache else None\n+        if return_legacy_cache:\n+            next_cache = next_cache.to_legacy_cache()\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=next_cache,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n+\n+    def _update_causal_mask(\n+        self,\n+        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n+        input_tensor: torch.Tensor,\n+        cache_position: torch.Tensor,\n+        past_key_values: Cache,\n+        output_attentions: bool = False,\n+    ):\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of KyutaiSpeechToText. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n+            if attention_mask is not None and 0.0 in attention_mask:\n+                return attention_mask\n+            return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            return attention_mask\n+\n+        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n+        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n+        # to infer the attention mask.\n+        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n+\n+        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and not (using_static_cache or using_sliding_window_cache)\n+            and not output_attentions\n+        ):\n+            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n+                attention_mask,\n+                inputs_embeds=input_tensor,\n+                past_key_values_length=past_seen_tokens,\n+                sliding_window=self.config.sliding_window,\n+                is_training=self.training,\n+            ):\n+                return None\n+\n+        dtype = input_tensor.dtype\n+        min_dtype = torch.finfo(dtype).min\n+        sequence_length = input_tensor.shape[1]\n+        # SlidingWindowCache or StaticCache\n+        if using_sliding_window_cache or using_static_cache:\n+            target_length = past_key_values.get_max_cache_shape()\n+        # DynamicCache or no cache\n+        else:\n+            target_length = (\n+                attention_mask.shape[-1]\n+                if isinstance(attention_mask, torch.Tensor)\n+                else past_seen_tokens + sequence_length + 1\n+            )\n+\n+        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n+        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n+            attention_mask,\n+            sequence_length=sequence_length,\n+            target_length=target_length,\n+            dtype=dtype,\n+            cache_position=cache_position,\n+            batch_size=input_tensor.shape[0],\n+            config=self.config,\n+            past_key_values=past_key_values,\n+        )\n+\n+        if (\n+            self.config._attn_implementation == \"sdpa\"\n+            and attention_mask is not None\n+            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n+            and not output_attentions\n+        ):\n+            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n+            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n+            # Details: https://github.com/pytorch/pytorch/issues/110213\n+            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n+\n+        return causal_mask\n+\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        config: KyutaiSpeechToTextConfig,\n+        past_key_values: Cache,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+            config (`KyutaiSpeechToTextConfig`):\n+                The model's configuration class\n+            past_key_values (`Cache`):\n+                The cache class that is being used currently to generate\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n+                -1, 1\n+            )\n+            text_config = config.get_text_config()\n+            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n+                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n+                # the check is needed to verify is current checkpoint was trained with sliding window or not\n+                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n+                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n+                        cache_position.reshape(-1, 1) - text_config.sliding_window\n+                    )\n+                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n+            causal_mask *= diagonal_attend_mask\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                if attention_mask.shape[-1] > target_length:\n+                    attention_mask = attention_mask[:, :target_length]\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+        return causal_mask\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+@auto_docstring\n+class KyutaiSpeechToTextForConditionalGeneration(KyutaiSpeechToTextPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+    _keep_in_fp32_modules = [\"codec_model\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = KyutaiSpeechToTextModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+        self.codec_model = AutoModel.from_config(config.codec_config)\n+\n+        # we are in an edge case where for the codec_model self.can_generate is False, setting self.codec_model.generation_config to None\n+        # yet the codec_model needs a generation config to initalize it's cache for streaming inference\n+        # we therefore initialize a generation config for the codec model\n+        self.codec_model.generation_config = GenerationConfig.from_model_config(config.codec_config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from datasets import load_dataset, Audio\n+        >>> from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+\n+        >>> torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        >>> model_id = \"kyutai/stt-2.6b-en\"\n+\n+        >>> processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n+        >>> model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+\n+        >>> ds = load_dataset(\n+        ...     \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+        ... )\n+\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+        >>> inputs = processor(\n+        ...     ds[0][\"audio\"][\"array\"],\n+        ... )\n+        >>> inputs.to(torch_device)\n+\n+        >>> output_tokens = model.generate(**inputs)\n+        >>> print(processor.batch_decode(output_tokens, skip_special_tokens=True))\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+    def _prepare_generation_config(self, *args, **kwargs):\n+        generation_config, model_kwargs = super()._prepare_generation_config(*args, **kwargs)\n+        # this should be passed to the model kwargs for the input preparation\n+        model_kwargs[\"audio_window_size\"] = (\n+            generation_config.audio_window_size if hasattr(generation_config, \"audio_window_size\") else None\n+        )\n+        return generation_config, model_kwargs\n+\n+    def _prepare_model_inputs(\n+        self,\n+        inputs: Optional[torch.Tensor] = None,\n+        bos_token_id: Optional[torch.Tensor] = None,\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n+        inputs, input_name, model_kwargs = super()._prepare_model_inputs(\n+            inputs=inputs,\n+            bos_token_id=bos_token_id,\n+            model_kwargs=model_kwargs,\n+        )\n+\n+        audio_window_size = model_kwargs.get(\"audio_window_size\", None)\n+        if audio_window_size is None:\n+            audio_window_size = self.codec_model.get_encoded_length(model_kwargs[\"input_values\"].shape[-1]).item()\n+            model_kwargs[\"audio_window_size\"] = audio_window_size\n+\n+        batch_size = inputs.shape[0]\n+        device = inputs.device\n+\n+        # initialize audio tokens\n+        model_kwargs[\"audio_tokens\"] = torch.zeros(\n+            (batch_size, audio_window_size, self.config.num_codebooks),\n+            device=device,\n+            dtype=torch.long,\n+        )\n+        model_kwargs[\"current_window\"] = (\n+            torch.tensor([0, 0], device=device, dtype=torch.long).expand(batch_size, -1).contiguous()\n+        )\n+\n+        # let's use generate's cache preparation to prepare the cache for the codec model\n+        temporary_model_kwargs = {}\n+\n+        # monkey patching the codec model with cache preparation methods since we don't want it to inherit fully from GenerationMixin\n+        # Add cache-related methods from GenerationMixin to codec model\n+        cache_methods = [\n+            \"_prepare_cache_for_generation\",\n+            \"_get_cache\",\n+            \"_supports_default_dynamic_cache\",\n+            \"_get_layer_device_map_for_cache_init\",\n+        ]\n+        for method in cache_methods:\n+            setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n+\n+        self.codec_model._prepare_cache_for_generation(\n+            generation_config=self.codec_model.generation_config,\n+            model_kwargs=temporary_model_kwargs,\n+            assistant_model=None,\n+            batch_size=batch_size,\n+            max_cache_length=self.config.codec_config.sliding_window,\n+            device=device,\n+        )\n+\n+        if \"past_key_values\" in temporary_model_kwargs:\n+            model_kwargs[\"encoder_past_key_values\"] = temporary_model_kwargs[\"past_key_values\"]\n+\n+        # initialize the padding cache for the codec model\n+        per_layer_padding, per_layer_padding_mode, per_layer_in_channels = [], [], []\n+        for layer_name in self.codec_model.encoder._mimiconv1d_layer_names:\n+            per_layer_padding.append(self.codec_model.encoder.get_submodule(layer_name).padding_total)\n+            per_layer_padding_mode.append(self.codec_model.encoder.get_submodule(layer_name).pad_mode)\n+            per_layer_in_channels.append(self.codec_model.encoder.get_submodule(layer_name).in_channels)\n+\n+        # downsample layer\n+        per_layer_padding.append(self.codec_model.downsample.padding_total)\n+        per_layer_padding_mode.append(self.codec_model.downsample.pad_mode)\n+        per_layer_in_channels.append(self.codec_model.downsample.in_channels)\n+\n+        model_kwargs[\"padding_cache\"] = KyutaiSpeechToTextConv1dPaddingCache(\n+            num_layers=len(self.codec_model.encoder._mimiconv1d_layer_names) + 1,\n+            per_layer_padding=per_layer_padding,\n+            per_layer_padding_mode=per_layer_padding_mode,\n+            per_layer_in_channels=per_layer_in_channels,\n+        )\n+\n+        return inputs, input_name, model_kwargs\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        *args,\n+        audio_tokens: Optional[torch.LongTensor] = None,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        padding_mask: Optional[torch.Tensor] = None,\n+        audio_window_size: Optional[int] = None,\n+        current_window: Optional[tuple[int, int]] = None,\n+        encoder_past_key_values: Optional[Cache] = None,\n+        padding_cache: Optional[KyutaiSpeechToTextConv1dPaddingCache] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if input_values is not None:\n+            cache_position = model_inputs[\"cache_position\"]\n+            start, end = current_window[0]\n+\n+            # first cache position is for bos token, so we need to offset by -1\n+            if cache_position[-1] - 1 >= end:\n+                # we need to encode the new audio tokens\n+                with torch.no_grad():\n+                    input_values_start_idx = start * self.config.frame_size\n+                    input_values_end_idx = (start + audio_window_size) * self.config.frame_size\n+                    current_input_values = input_values[..., input_values_start_idx:input_values_end_idx]\n+                    codec_model_output = self.codec_model.encode(\n+                        current_input_values,\n+                        encoder_past_key_values=encoder_past_key_values,\n+                        padding_cache=padding_cache,\n+                    )\n+                    new_audio_tokens = codec_model_output.audio_codes.transpose(1, 2)\n+\n+                audio_tokens.copy_(new_audio_tokens)\n+\n+                start = end.clone()\n+                end = end + audio_window_size\n+                current_window.copy_(\n+                    torch.tensor([start, end], device=current_window.device).expand(current_window.shape[0], -1)\n+                )\n+\n+            # first cache position is for bos token, so we need to offset by -1\n+            current_audio_tokens_idxs = (cache_position - start - 1).clamp(min=0)\n+            current_audio_tokens = audio_tokens[:, current_audio_tokens_idxs, :]\n+\n+            current_audio_tokens[:, cache_position == 0, :] = self.config.audio_bos_token_id\n+\n+            input_ids = model_inputs.pop(\"input_ids\")\n+            input_ids = torch.cat(\n+                [input_ids.unsqueeze(2), current_audio_tokens],\n+                dim=2,\n+            )\n+            model_inputs[\"input_ids\"] = input_ids\n+\n+        return model_inputs\n+\n+    # TODO: @eustlb, this should be standardized\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        if kwargs.get(\"output_loading_info\", False):\n+            model, loading_info = super().from_pretrained(*args, **kwargs)\n+        else:\n+            model = super().from_pretrained(*args, **kwargs)\n+\n+        # copy depth decoder generation conf attr to the depth decoder generation config\n+        prefix = \"codec_\"\n+        prefix_len = len(prefix)\n+        codec_model_attrs = {\n+            attr[prefix_len:]: value\n+            for attr, value in vars(model.generation_config).items()\n+            if attr.startswith(prefix)\n+        }\n+\n+        vars(model.codec_model.generation_config).update({\"_from_model_config\": False, **codec_model_attrs})\n+\n+        # remove the depth decoder generation conf attr from the model generation config\n+        for attr in codec_model_attrs:\n+            delattr(model.generation_config, prefix + attr)\n+\n+        if \"output_loading_info\" in kwargs:\n+            return model, loading_info\n+        else:\n+            return model\n+\n+    # TODO: @eustlb, this should be standardized\n+    def save_pretrained(self, *args, **kwargs):\n+        prefix = \"codec_\"\n+        codec_model_attrs = self.codec_model.generation_config.to_diff_dict()\n+        codec_model_attrs.pop(\"transformers_version\", None)\n+        for attr, value in codec_model_attrs.items():\n+            setattr(self.generation_config, prefix + attr, value)\n+\n+        super().save_pretrained(*args, **kwargs)\n+\n+    def generate(self, *args, **kwargs):\n+        r\"\"\"\n+        This method forwards all its arguments to GenerationMixin's [`~GenerationMixin.generate`]. Please refer to the docstring of this method for more information.\n+        \"\"\"\n+        max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n+        input_values = kwargs.get(\"input_values\")\n+\n+        # TODO: @eustlb, we should have per-batch-idx values\n+        # here we do not use padding_mask to be aligned to what's done in the original codebase\n+        max_audio_frames = input_values.shape[-1] // self.config.codec_config.frame_size\n+\n+        if max_new_tokens is None or max_new_tokens > max_audio_frames:\n+            if max_new_tokens is not None:\n+                logger.warning(\n+                    f\"`max_new_tokens` ({max_new_tokens}) is greater than the maximum number of audio frames ({max_audio_frames}).\"\n+                    f\"Setting `max_new_tokens` to {max_audio_frames}.\"\n+                )\n+            max_new_tokens = max_audio_frames\n+\n+        return super().generate(\n+            *args,\n+            max_new_tokens=max_new_tokens,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\n+    \"KyutaiSpeechToTextPreTrainedModel\",\n+    \"KyutaiSpeechToTextModel\",\n+    \"KyutaiSpeechToTextForConditionalGeneration\",\n+]"
        },
        {
            "sha": "8cc0c9d2a7a606fe2312cf6d53cbd4dbb5c0df36",
            "filename": "src/transformers/models/stt/modular_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 510,
            "deletions": 0,
            "changes": 510,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Fmodular_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,510 @@\n+# coding=utf-8\n+# Copyright 2025 Kyutai and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import types\n+from typing import Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+\n+from ...cache_utils import Cache\n+from ...feature_extraction_utils import BatchFeature\n+from ...generation import GenerationConfig, GenerationMixin\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import PaddingStrategy, TensorType, logging\n+from ..auto import AutoModel\n+from ..encodec.feature_extraction_encodec import EncodecFeatureExtractor\n+from ..llama.modeling_llama import LlamaForCausalLM\n+from ..mimi.modeling_mimi import MimiConv1dPaddingCache\n+from ..moshi.modeling_moshi import MoshiModel, MoshiPreTrainedModel\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class KyutaiSpeechToTextFeatureExtractor(EncodecFeatureExtractor):\n+    r\"\"\"\n+    Constructs an KyutaiSpeechToText feature extractor.\n+\n+    This feature extractor inherits from [`~feature_extraction_sequence_utils.SequenceFeatureExtractor`] which contains\n+    most of the main methods. Users should refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        feature_size (`int`, *optional*, defaults to 1):\n+            The feature dimension of the extracted features. Use 1 for mono, 2 for stereo.\n+        sampling_rate (`int`, *optional*, defaults to 24000):\n+            The sampling rate at which the audio waveform should be digitalized expressed in hertz (Hz).\n+        padding_value (`float`, *optional*, defaults to 0.0):\n+            The value that is used to fill the padding values.\n+        chunk_length_s (`float`, *optional*):\n+            If defined the audio is pre-processed into chunks of lengths `chunk_length_s` and then encoded.\n+        overlap (`float`, *optional*):\n+            Defines the overlap between each chunk. It is used to compute the `chunk_stride` using the following\n+            formulae : `int((1.0 - self.overlap) * self.chunk_length)`.\n+        audio_delay_seconds (`float`, *optional*, defaults to 0.0):\n+            The delay in seconds to add after the audio (right padding).\n+        audio_silence_prefix_seconds (`float`, *optional*, defaults to 0.0):\n+            The silence prefix in seconds to add before the audio (left padding).\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        audio_delay_seconds: Optional[float] = 0.0,\n+        audio_silence_prefix_seconds: Optional[float] = 0.0,\n+        **super_kwargs,\n+    ):\n+        super().__init__(**super_kwargs)\n+        self.audio_delay_seconds = audio_delay_seconds\n+        self.audio_silence_prefix_seconds = audio_silence_prefix_seconds\n+\n+    def __call__(\n+        self,\n+        raw_audio: Union[np.ndarray, list[float], list[np.ndarray], list[list[float]]],\n+        padding: Optional[Union[bool, str, PaddingStrategy]] = None,\n+        truncation: Optional[bool] = False,\n+        max_length: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        sampling_rate: Optional[int] = None,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to featurize and prepare for the model one or several sequence(s).\n+\n+        Args:\n+            raw_audio (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n+                The sequence or batch of sequences to be processed. Each sequence can be a numpy array, a list of float\n+                values, a list of numpy arrays or a list of list of float values. The numpy array must be of shape\n+                `(num_samples,)` for mono audio (`feature_size = 1`), or `(2, num_samples)` for stereo audio\n+                (`feature_size = 2`).\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n+                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                index) among:\n+\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            truncation (`bool`, *optional*, defaults to `False`):\n+                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors instead of list of python integers. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return Numpy `np.ndarray` objects.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate at which the `audio` input was sampled. It is strongly recommended to pass\n+                `sampling_rate` at the forward call to prevent silent errors.\n+        \"\"\"\n+        if sampling_rate is not None:\n+            if sampling_rate != self.sampling_rate:\n+                raise ValueError(\n+                    f\"The model corresponding to this feature extractor: {self} was trained using a sampling rate of\"\n+                    f\" {self.sampling_rate}. Please make sure that the provided audio input was sampled with\"\n+                    f\" {self.sampling_rate} and not {sampling_rate}.\"\n+                )\n+        else:\n+            logger.warning(\n+                f\"It is strongly recommended to pass the `sampling_rate` argument to `{self.__class__.__name__}()`. \"\n+                \"Failing to do so can result in silent errors that might be hard to debug.\"\n+            )\n+\n+        if padding and truncation:\n+            raise ValueError(\"Both padding and truncation were set. Make sure you only set one.\")\n+        elif padding is None:\n+            # by default let's pad the inputs\n+            padding = True\n+\n+        is_batched = bool(\n+            isinstance(raw_audio, (list, tuple)) and (isinstance(raw_audio[0], (np.ndarray, tuple, list)))\n+        )\n+\n+        if is_batched:\n+            raw_audio = [np.asarray(audio, dtype=np.float32).T for audio in raw_audio]\n+        elif not is_batched and not isinstance(raw_audio, np.ndarray):\n+            raw_audio = np.asarray(raw_audio, dtype=np.float32)\n+        elif isinstance(raw_audio, np.ndarray) and raw_audio.dtype is np.dtype(np.float64):\n+            raw_audio = raw_audio.astype(np.float32)\n+\n+        # always return batch\n+        if not is_batched:\n+            raw_audio = [np.asarray(raw_audio).T]\n+\n+        # verify inputs are valid\n+        for idx, example in enumerate(raw_audio):\n+            if example.ndim > 2:\n+                raise ValueError(f\"Expected input shape (channels, length) but got shape {example.shape}\")\n+            if self.feature_size == 1 and example.ndim != 1:\n+                raise ValueError(f\"Expected mono audio but example has {example.shape[-1]} channels\")\n+            if self.feature_size == 2 and example.shape[-1] != 2:\n+                raise ValueError(f\"Expected stereo audio but example has {example.shape[-1]} channels\")\n+\n+        padded_inputs = None\n+        input_values = BatchFeature({\"input_values\": raw_audio})\n+        if self.chunk_stride is not None and self.chunk_length is not None and max_length is None:\n+            if truncation:\n+                max_length = min(array.shape[0] for array in raw_audio)\n+                nb_step = int(np.floor(max_length / self.chunk_stride))\n+                max_length = (nb_step - 1) * self.chunk_stride + self.chunk_length\n+            elif padding:\n+                max_length = max(array.shape[0] for array in raw_audio)\n+                nb_step = int(np.ceil(max_length / self.chunk_stride))\n+                max_length = (nb_step - 1) * self.chunk_stride + self.chunk_length\n+                padding = \"max_length\"\n+            else:\n+                padded_inputs = input_values\n+\n+        # normal padding on batch\n+        if padded_inputs is None:\n+            padded_inputs = self.pad(\n+                input_values,\n+                max_length=max_length,\n+                truncation=truncation,\n+                padding=padding,\n+                return_attention_mask=padding,\n+            )\n+\n+            if padding:\n+                padded_inputs[\"padding_mask\"] = padded_inputs.pop(\"attention_mask\")\n+\n+        # now let's padd left and right\n+        pad_left = int(self.audio_silence_prefix_seconds * self.sampling_rate)\n+        pad_right = int((self.audio_delay_seconds + 1.0) * self.sampling_rate)\n+        padded_inputs[\"input_values\"] = np.pad(\n+            padded_inputs[\"input_values\"],\n+            ((0, 0), (pad_left, pad_right)),\n+            mode=\"constant\",\n+            constant_values=0.0,\n+        )\n+        if padding:\n+            padded_inputs[\"padding_mask\"] = np.pad(\n+                padded_inputs[\"padding_mask\"],\n+                ((0, 0), (pad_left, pad_right)),\n+                mode=\"constant\",\n+                constant_values=0,\n+            )\n+\n+        input_values = []\n+        for example in padded_inputs.pop(\"input_values\"):\n+            if self.feature_size == 1:\n+                example = example[..., None]\n+            input_values.append(example.T)\n+\n+        padded_inputs[\"input_values\"] = input_values\n+        if return_tensors is not None:\n+            padded_inputs = padded_inputs.convert_to_tensors(return_tensors)\n+\n+        return padded_inputs\n+\n+\n+class KyutaiSpeechToTextPreTrainedModel(MoshiPreTrainedModel):\n+    pass\n+\n+\n+class KyutaiSpeechToTextConv1dPaddingCache(MimiConv1dPaddingCache):\n+    pass\n+\n+\n+class KyutaiSpeechToTextEmbeddings(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.embed_tokens = nn.Embedding(\n+            config.vocab_size + (config.num_codebooks * config.codebook_vocab_size) + 1,\n+            config.hidden_size,\n+            padding_idx=config.audio_pad_token_id,\n+        )\n+        audio_tokens_offsets = torch.arange(config.num_codebooks) * config.codebook_vocab_size\n+        audio_tokens_offsets += config.vocab_size\n+        audio_tokens_offsets = nn.functional.pad(\n+            audio_tokens_offsets, (1, 0)\n+        )  # pad one 0 to the left for the text token\n+        self.register_buffer(\"audio_tokens_offsets\", audio_tokens_offsets, persistent=False)\n+\n+    def forward(self, input_ids):\n+        input_ids = torch.where(\n+            input_ids == self.embed_tokens.padding_idx, input_ids, input_ids + self.audio_tokens_offsets\n+        )\n+        inputs_embeds = self.embed_tokens(input_ids)\n+        inputs_embeds = inputs_embeds.sum(dim=2)\n+        return inputs_embeds\n+\n+\n+class KyutaiSpeechToTextModel(MoshiModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.embed_tokens = KyutaiSpeechToTextEmbeddings(config)\n+\n+\n+class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin, PreTrainedModel):\n+    _keep_in_fp32_modules = [\"codec_model\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.codec_model = AutoModel.from_config(config.codec_config)\n+\n+        # we are in an edge case where for the codec_model self.can_generate is False, setting self.codec_model.generation_config to None\n+        # yet the codec_model needs a generation config to initalize it's cache for streaming inference\n+        # we therefore initialize a generation config for the codec model\n+        self.codec_model.generation_config = GenerationConfig.from_model_config(config.codec_config)\n+\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from datasets import load_dataset, Audio\n+        >>> from transformers import KyutaiSpeechToTextProcessor, KyutaiSpeechToTextForConditionalGeneration\n+\n+        >>> torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        >>> model_id = \"kyutai/stt-2.6b-en\"\n+\n+        >>> processor = KyutaiSpeechToTextProcessor.from_pretrained(model_id)\n+        >>> model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(model_id, device_map=torch_device)\n+\n+        >>> ds = load_dataset(\n+        ...     \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+        ... )\n+\n+        >>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+        >>> inputs = processor(\n+        ...     ds[0][\"audio\"][\"array\"],\n+        ... )\n+        >>> inputs.to(torch_device)\n+\n+        >>> output_tokens = model.generate(**inputs)\n+        >>> print(processor.batch_decode(output_tokens, skip_special_tokens=True))\n+        ```\"\"\"\n+        super().forward(**super_kwargs)\n+\n+    def _prepare_generation_config(self, *args, **kwargs):\n+        generation_config, model_kwargs = GenerationMixin._prepare_generation_config(*args, **kwargs)\n+        # this should be passed to the model kwargs for the input preparation\n+        model_kwargs[\"audio_window_size\"] = (\n+            generation_config.audio_window_size if hasattr(generation_config, \"audio_window_size\") else None\n+        )\n+        return generation_config, model_kwargs\n+\n+    def _prepare_model_inputs(\n+        self,\n+        inputs: Optional[torch.Tensor] = None,\n+        bos_token_id: Optional[torch.Tensor] = None,\n+        model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n+        inputs, input_name, model_kwargs = GenerationMixin._prepare_model_inputs(\n+            inputs=inputs,\n+            bos_token_id=bos_token_id,\n+            model_kwargs=model_kwargs,\n+        )\n+\n+        audio_window_size = model_kwargs.get(\"audio_window_size\", None)\n+        if audio_window_size is None:\n+            audio_window_size = self.codec_model.get_encoded_length(model_kwargs[\"input_values\"].shape[-1]).item()\n+            model_kwargs[\"audio_window_size\"] = audio_window_size\n+\n+        batch_size = inputs.shape[0]\n+        device = inputs.device\n+\n+        # initialize audio tokens\n+        model_kwargs[\"audio_tokens\"] = torch.zeros(\n+            (batch_size, audio_window_size, self.config.num_codebooks),\n+            device=device,\n+            dtype=torch.long,\n+        )\n+        model_kwargs[\"current_window\"] = (\n+            torch.tensor([0, 0], device=device, dtype=torch.long).expand(batch_size, -1).contiguous()\n+        )\n+\n+        # let's use generate's cache preparation to prepare the cache for the codec model\n+        temporary_model_kwargs = {}\n+\n+        # monkey patching the codec model with cache preparation methods since we don't want it to inherit fully from GenerationMixin\n+        # Add cache-related methods from GenerationMixin to codec model\n+        cache_methods = [\n+            \"_prepare_cache_for_generation\",\n+            \"_get_cache\",\n+            \"_supports_default_dynamic_cache\",\n+            \"_get_layer_device_map_for_cache_init\",\n+        ]\n+        for method in cache_methods:\n+            setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n+\n+        self.codec_model._prepare_cache_for_generation(\n+            generation_config=self.codec_model.generation_config,\n+            model_kwargs=temporary_model_kwargs,\n+            assistant_model=None,\n+            batch_size=batch_size,\n+            max_cache_length=self.config.codec_config.sliding_window,\n+            device=device,\n+        )\n+\n+        if \"past_key_values\" in temporary_model_kwargs:\n+            model_kwargs[\"encoder_past_key_values\"] = temporary_model_kwargs[\"past_key_values\"]\n+\n+        # initialize the padding cache for the codec model\n+        per_layer_padding, per_layer_padding_mode, per_layer_in_channels = [], [], []\n+        for layer_name in self.codec_model.encoder._mimiconv1d_layer_names:\n+            per_layer_padding.append(self.codec_model.encoder.get_submodule(layer_name).padding_total)\n+            per_layer_padding_mode.append(self.codec_model.encoder.get_submodule(layer_name).pad_mode)\n+            per_layer_in_channels.append(self.codec_model.encoder.get_submodule(layer_name).in_channels)\n+\n+        # downsample layer\n+        per_layer_padding.append(self.codec_model.downsample.padding_total)\n+        per_layer_padding_mode.append(self.codec_model.downsample.pad_mode)\n+        per_layer_in_channels.append(self.codec_model.downsample.in_channels)\n+\n+        model_kwargs[\"padding_cache\"] = KyutaiSpeechToTextConv1dPaddingCache(\n+            num_layers=len(self.codec_model.encoder._mimiconv1d_layer_names) + 1,\n+            per_layer_padding=per_layer_padding,\n+            per_layer_padding_mode=per_layer_padding_mode,\n+            per_layer_in_channels=per_layer_in_channels,\n+        )\n+\n+        return inputs, input_name, model_kwargs\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        *args,\n+        audio_tokens: Optional[torch.LongTensor] = None,\n+        input_values: Optional[torch.FloatTensor] = None,\n+        padding_mask: Optional[torch.Tensor] = None,\n+        audio_window_size: Optional[int] = None,\n+        current_window: Optional[tuple[int, int]] = None,\n+        encoder_past_key_values: Optional[Cache] = None,\n+        padding_cache: Optional[KyutaiSpeechToTextConv1dPaddingCache] = None,\n+        **kwargs,\n+    ):\n+        model_inputs = GenerationMixin.prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if input_values is not None:\n+            cache_position = model_inputs[\"cache_position\"]\n+            start, end = current_window[0]\n+\n+            # first cache position is for bos token, so we need to offset by -1\n+            if cache_position[-1] - 1 >= end:\n+                # we need to encode the new audio tokens\n+                with torch.no_grad():\n+                    input_values_start_idx = start * self.config.frame_size\n+                    input_values_end_idx = (start + audio_window_size) * self.config.frame_size\n+                    current_input_values = input_values[..., input_values_start_idx:input_values_end_idx]\n+                    codec_model_output = self.codec_model.encode(\n+                        current_input_values,\n+                        encoder_past_key_values=encoder_past_key_values,\n+                        padding_cache=padding_cache,\n+                    )\n+                    new_audio_tokens = codec_model_output.audio_codes.transpose(1, 2)\n+\n+                audio_tokens.copy_(new_audio_tokens)\n+\n+                start = end.clone()\n+                end = end + audio_window_size\n+                current_window.copy_(\n+                    torch.tensor([start, end], device=current_window.device).expand(current_window.shape[0], -1)\n+                )\n+\n+            # first cache position is for bos token, so we need to offset by -1\n+            current_audio_tokens_idxs = (cache_position - start - 1).clamp(min=0)\n+            current_audio_tokens = audio_tokens[:, current_audio_tokens_idxs, :]\n+\n+            current_audio_tokens[:, cache_position == 0, :] = self.config.audio_bos_token_id\n+\n+            input_ids = model_inputs.pop(\"input_ids\")\n+            input_ids = torch.cat(\n+                [input_ids.unsqueeze(2), current_audio_tokens],\n+                dim=2,\n+            )\n+            model_inputs[\"input_ids\"] = input_ids\n+\n+        return model_inputs\n+\n+    # TODO: @eustlb, this should be standardized\n+    @classmethod\n+    def from_pretrained(cls, *args, **kwargs):\n+        if kwargs.get(\"output_loading_info\", False):\n+            model, loading_info = PreTrainedModel.from_pretrained(*args, **kwargs)\n+        else:\n+            model = PreTrainedModel.from_pretrained(*args, **kwargs)\n+\n+        # copy depth decoder generation conf attr to the depth decoder generation config\n+        prefix = \"codec_\"\n+        prefix_len = len(prefix)\n+        codec_model_attrs = {\n+            attr[prefix_len:]: value\n+            for attr, value in vars(model.generation_config).items()\n+            if attr.startswith(prefix)\n+        }\n+\n+        vars(model.codec_model.generation_config).update({\"_from_model_config\": False, **codec_model_attrs})\n+\n+        # remove the depth decoder generation conf attr from the model generation config\n+        for attr in codec_model_attrs:\n+            delattr(model.generation_config, prefix + attr)\n+\n+        if \"output_loading_info\" in kwargs:\n+            return model, loading_info\n+        else:\n+            return model\n+\n+    # TODO: @eustlb, this should be standardized\n+    def save_pretrained(self, *args, **kwargs):\n+        prefix = \"codec_\"\n+        codec_model_attrs = self.codec_model.generation_config.to_diff_dict()\n+        codec_model_attrs.pop(\"transformers_version\", None)\n+        for attr, value in codec_model_attrs.items():\n+            setattr(self.generation_config, prefix + attr, value)\n+\n+        PreTrainedModel.save_pretrained(self, *args, **kwargs)\n+\n+    def generate(self, *args, **kwargs):\n+        r\"\"\"\n+        This method forwards all its arguments to GenerationMixin's [`~GenerationMixin.generate`]. Please refer to the docstring of this method for more information.\n+        \"\"\"\n+        max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n+        input_values = kwargs.get(\"input_values\")\n+\n+        # TODO: @eustlb, we should have per-batch-idx values\n+        # here we do not use padding_mask to be aligned to what's done in the original codebase\n+        max_audio_frames = input_values.shape[-1] // self.config.codec_config.frame_size\n+\n+        if max_new_tokens is None or max_new_tokens > max_audio_frames:\n+            if max_new_tokens is not None:\n+                logger.warning(\n+                    f\"`max_new_tokens` ({max_new_tokens}) is greater than the maximum number of audio frames ({max_audio_frames}).\"\n+                    f\"Setting `max_new_tokens` to {max_audio_frames}.\"\n+                )\n+            max_new_tokens = max_audio_frames\n+\n+        return GenerationMixin.generate(\n+            *args,\n+            max_new_tokens=max_new_tokens,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\n+    \"KyutaiSpeechToTextPreTrainedModel\",\n+    \"KyutaiSpeechToTextModel\",\n+    \"KyutaiSpeechToTextForConditionalGeneration\",\n+    \"KyutaiSpeechToTextFeatureExtractor\",\n+]"
        },
        {
            "sha": "0b3a021712362a25c231d2a22724cd9545f8196a",
            "filename": "src/transformers/models/stt/processing_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fprocessing_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/src%2Ftransformers%2Fmodels%2Fstt%2Fprocessing_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstt%2Fprocessing_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,104 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+\n+\n+class KyutaiSpeechToTextProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 24000,\n+        },\n+        \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+    }\n+\n+\n+class KyutaiSpeechToTextProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Moshi ASR processor which wraps [`EncodecFeatureExtractor`] and\n+    [`PreTrainedTokenizerFast`] into a single processor that inherits both the audio feature extraction and\n+    tokenizer functionalities. See the [`~KyutaiSpeechToTextProcessor.__call__`] for more\n+    information.\n+    \"\"\"\n+\n+    feature_extractor_class = \"KyutaiSpeechToTextFeatureExtractor\"\n+    tokenizer_class = \"PreTrainedTokenizerFast\"\n+\n+    def __call__(\n+        self,\n+        audio: Optional[AudioInput] = None,\n+        **kwargs: Unpack[KyutaiSpeechToTextProcessorKwargs],\n+    ):\n+        r\"\"\"\n+        Main method to prepare audio to be fed as input to the model. This method forwards the `audio`\n+        arguments to KyutaiSpeechToTextFeatureExtractor's [`~KyutaiSpeechToTextFeatureExtractor.__call__`]. Please refer\n+        to the docstring of the above method for more information.\n+\n+        Args:\n+            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The audio or batch of audio to be prepared. Each audio can be a NumPy array or PyTorch\n+                tensor.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_values** -- List of audio values to be fed to a model. Returned when `audio` is not `None`.\n+            - **padding_mask** -- List of indices specifying which input values should be ignored by the model.\n+        \"\"\"\n+\n+        if audio is None:\n+            raise ValueError(\"`audio` is required.\")\n+\n+        output_kwargs = self._merge_kwargs(\n+            KyutaiSpeechToTextProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+\n+        # ensure audio in correct format\n+        audio = make_list_of_audio(audio)\n+\n+        inputs = self.feature_extractor(\n+            audio,\n+            **audio_kwargs,\n+        )\n+\n+        return inputs\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to KyutaiSpeechToTextTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to KyutaiSpeechToTextTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+\n+__all__ = [\"KyutaiSpeechToTextProcessor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/kyutai_speech_to_text/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fkyutai_speech_to_text%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fkyutai_speech_to_text%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2F__init__.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7"
        },
        {
            "sha": "a6e08f714f9656dc78118045f89d16f815727ad5",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "added",
            "additions": 704,
            "deletions": 0,
            "changes": 704,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -0,0 +1,704 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Moshi ASR model.\"\"\"\n+\n+import gc\n+import inspect\n+import tempfile\n+import unittest\n+\n+import datasets\n+import pytest\n+from parameterized import parameterized\n+\n+from transformers import (\n+    KyutaiSpeechToTextConfig,\n+    KyutaiSpeechToTextForConditionalGeneration,\n+    KyutaiSpeechToTextProcessor,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    require_torch_accelerator,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    floats_tensor,\n+    ids_tensor,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        KyutaiSpeechToTextForConditionalGeneration,\n+        KyutaiSpeechToTextModel,\n+    )\n+\n+\n+class KyutaiSpeechToTextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=13,\n+        seq_length=7,\n+        text_seq_length=1,\n+        input_values_length=192,  # gives 3 audio tokens, corresponding to the default in GenerationTesterMixin\n+        is_training=False,\n+        use_input_mask=True,\n+        use_token_type_ids=False,\n+        use_labels=True,\n+        codebook_vocab_size=2049,\n+        vocab_size=99,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        num_key_value_heads=None,\n+        max_position_embeddings=512,\n+        rope_theta=10000.0,\n+        hidden_act=\"silu\",\n+        head_dim=None,\n+        initializer_range=0.02,\n+        use_cache=True,\n+        sliding_window=512,\n+        attention_dropout=0.1,\n+        ffn_dim=38,\n+        rms_norm_eps=1e-6,\n+        num_codebooks=8,\n+        frame_size=64,\n+        delay_in_tokens=5,\n+        audio_bos_token_id=2048,\n+        audio_pad_token_id=2048,\n+        tie_word_embeddings=False,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        codec_config={\n+            \"model_type\": \"mimi\",\n+            \"num_quantizers\": 8,\n+            \"audio_channels\": 1,\n+            \"chunk_in_sec\": None,\n+            \"hidden_size\": 16,\n+            \"num_filters\": 8,\n+            \"num_residual_layers\": 1,\n+            \"upsampling_ratios\": [8, 4],\n+            \"codebook_size\": 16,\n+            \"vector_quantization_hidden_dimension\": 16,\n+            \"upsample_groups\": 16,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 2,\n+            \"num_key_value_heads\": 2,\n+            \"sliding_window\": 4,\n+            \"codebook_dim\": 16,\n+            \"use_cache\": False,\n+        },\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.text_seq_length = text_seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_token_type_ids = use_token_type_ids\n+        self.use_labels = use_labels\n+        self.codebook_vocab_size = codebook_vocab_size\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.rope_theta = rope_theta\n+        self.hidden_act = hidden_act\n+        self.head_dim = head_dim\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+        self.sliding_window = sliding_window\n+        self.attention_dropout = attention_dropout\n+        self.ffn_dim = ffn_dim\n+        self.rms_norm_eps = rms_norm_eps\n+        self.num_codebooks = num_codebooks\n+        self.frame_size = frame_size\n+        self.delay_in_tokens = delay_in_tokens\n+        self.audio_bos_token_id = audio_bos_token_id\n+        self.audio_pad_token_id = audio_pad_token_id\n+        self.tie_word_embeddings = tie_word_embeddings\n+        self.pad_token_id = pad_token_id\n+        self.bos_token_id = bos_token_id\n+        self.codec_config = codec_config\n+        self.scope = scope\n+        self.input_values_length = input_values_length\n+\n+    def get_config(self):\n+        return KyutaiSpeechToTextConfig(\n+            codebook_vocab_size=self.codebook_vocab_size,\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            num_key_value_heads=self.num_key_value_heads,\n+            max_position_embeddings=self.max_position_embeddings,\n+            rope_theta=self.rope_theta,\n+            hidden_act=self.hidden_act,\n+            head_dim=self.head_dim,\n+            initializer_range=self.initializer_range,\n+            use_cache=self.use_cache,\n+            sliding_window=self.sliding_window,\n+            attention_dropout=self.attention_dropout,\n+            ffn_dim=self.ffn_dim,\n+            rms_norm_eps=self.rms_norm_eps,\n+            num_codebooks=self.num_codebooks,\n+            frame_size=self.frame_size,\n+            delay_in_tokens=self.delay_in_tokens,\n+            audio_bos_token_id=self.audio_bos_token_id,\n+            audio_pad_token_id=self.audio_pad_token_id,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+            pad_token_id=self.pad_token_id,\n+            bos_token_id=self.bos_token_id,\n+            codec_config=self.codec_config,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, input_mask):\n+        model = KyutaiSpeechToTextModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=input_mask)\n+        result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def prepare_config_and_inputs(self):\n+        config = self.get_config()\n+\n+        text_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size - 1) + 1\n+        codebook_input_ids = (\n+            ids_tensor([self.batch_size, self.seq_length, self.num_codebooks], self.codebook_vocab_size - 1) + 1\n+        )\n+\n+        input_ids = torch.cat([text_input_ids.unsqueeze(2), codebook_input_ids], dim=2)\n+        attention_mask = text_input_ids.ne(1).to(torch_device)\n+\n+        return config, input_ids, attention_mask\n+\n+    def prepare_config_and_inputs_generate(self):\n+        config = self.get_config()\n+\n+        input_ids = torch.ones([self.batch_size, 1], dtype=torch.long, device=torch_device)\n+        input_values = floats_tensor([self.batch_size, 1, self.input_values_length])\n+        padding_mask = torch.ones_like(input_values, dtype=torch.int32, device=torch_device)\n+\n+        return config, input_ids, input_values, padding_mask\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        (\n+            config,\n+            input_ids,\n+            attention_mask,\n+        ) = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_common_generate(self):\n+        config_and_inputs = self.prepare_config_and_inputs_generate()\n+        (\n+            config,\n+            input_ids,\n+            input_values,\n+            padding_mask,\n+        ) = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"input_values\": input_values,\n+            \"padding_mask\": padding_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class KyutaiSpeechToTextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            KyutaiSpeechToTextModel,\n+            KyutaiSpeechToTextForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": KyutaiSpeechToTextModel,\n+            \"automatic-speech-recognition\": KyutaiSpeechToTextForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    def setUp(self):\n+        self.model_tester = KyutaiSpeechToTextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=KyutaiSpeechToTextConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels)\n+\n+        return inputs_dict\n+\n+    def prepare_config_and_inputs_for_generate(self, batch_size=2):\n+        # monkey patch prepare_config_and_inputs_for_common\n+\n+        prepare_config_and_inputs_for_common = self.model_tester.prepare_config_and_inputs_for_common\n+        original_batch_size = self.model_tester.batch_size\n+\n+        self.model_tester.prepare_config_and_inputs_for_common = (\n+            self.model_tester.prepare_config_and_inputs_for_common_generate\n+        )\n+        self.model_tester.batch_size = batch_size\n+\n+        config, filtered_inputs_dict = super().prepare_config_and_inputs_for_generate()\n+        self.model_tester.prepare_config_and_inputs_for_common = prepare_config_and_inputs_for_common\n+\n+        self.model_tester.batch_size = original_batch_size\n+        return config, filtered_inputs_dict\n+\n+    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n+    def test_tie_model_weights(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n+    def test_resize_embeddings_untied(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n+    def test_tied_weights_keys(self):\n+        pass\n+\n+    @pytest.mark.skip(reason=\"Does not apply to Moshi ASR that requires input_values.\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    def test_initialization(self):\n+        \"\"\"\n+        Overrides [ModelTesterMixin.test_initialization] because of specificities of Mimi codec model.\n+        See https://github.com/huggingface/transformers/blob/1077603410cd73ba71d64a522033574d66d64b55/tests/models/mimi/test_modeling_mimi.py#L384-L397\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                uniform_init_parms = [\"conv\", \"input_proj\", \"output_proj\"]\n+                if param.requires_grad:\n+                    if any(x in name for x in uniform_init_parms):\n+                        self.assertTrue(\n+                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        if use_attention_mask or (not use_attention_mask and torch_dtype == \"fp32\" and not output_attentions):\n+            self.skipTest(\"Test is failing, fix me :) \")\n+        parent_parameterized_test = getattr(ModelTesterMixin, self._testMethodName)\n+        parent_parameterized_test(self)\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # NOTE: left-padding results in small numerical differences. This is expected.\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        # First, filter out models that don't support left padding\n+        # - The model must have generative capabilities\n+        if len(self.all_generative_model_classes) == 0:\n+            self.skipTest(reason=\"No generative architecture available for this model.\")\n+\n+        # - The model must support padding\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"This model doesn't support padding.\")\n+\n+        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n+        decoder_only_classes = []\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n+            if config.is_encoder_decoder:\n+                continue\n+            else:\n+                decoder_only_classes.append(model_class)\n+        if len(decoder_only_classes) == 0:\n+            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n+\n+        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n+        #   added support for it yet. We skip these models for now.\n+        has_encoder_attributes = any(\n+            attr_name\n+            for attr_name in config.to_dict().keys()\n+            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n+        )\n+        if has_encoder_attributes:\n+            self.skipTest(\n+                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n+            )\n+\n+        # Then, test left-padding\n+        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n+            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            return model_kwargs\n+\n+        for model_class in decoder_only_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n+            next_logits_wo_padding = model(**model_kwargs).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32, *input_ids.shape[2:])\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat(\n+                (torch.zeros(pad_size[:2], dtype=input_ids.dtype, device=torch_device), attention_mask), dim=1\n+            )\n+            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n+            next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n+\n+    def test_generate_continue_from_past_key_values(self):\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"imagegpt\", \"mllama\"]):\n+                self.skipTest(reason=\"Won't fix: old model with unique inputs/caches/other\")\n+            if any(model_name in model_class.__name__.lower() for model_name in [\"umt5\"]):\n+                self.skipTest(reason=\"TODO: needs modeling or test input preparation fixes for compatibility\")\n+\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(reason=f\"{model_class.__name__} doesn't support caching\")\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            if \"token_type_ids\" in inputs:\n+                del inputs[\"token_type_ids\"]\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)\n+            outputs = model(**inputs)\n+            if \"past_key_values\" not in outputs:\n+                self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n+\n+            generate_kwargs = {\n+                \"pad_token_id\": -1,\n+                \"eos_token_id\": -1,\n+                \"forced_eos_token_id\": None,\n+                \"encoder_no_repeat_ngram_size\": 0,\n+                \"use_cache\": True,\n+                \"do_sample\": False,\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+            }\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            _, inputs = self.prepare_config_and_inputs_for_generate()\n+            outputs = model.generate(**inputs, **generate_kwargs, max_new_tokens=3)\n+\n+            # Let's generate again, but passing the past key values in between (2 + 1 = 3 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=2)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+            if config.is_encoder_decoder:\n+                inputs[\"decoder_input_ids\"] = outputs_cached.sequences\n+                if \"decoder_attention_mask\" in inputs:\n+                    inputs[\"decoder_attention_mask\"] = torch.nn.functional.pad(\n+                        inputs[\"decoder_attention_mask\"],\n+                        (0, new_attention_len - inputs[\"decoder_attention_mask\"].shape[1]),\n+                        mode=\"constant\",\n+                        value=1,\n+                    )\n+            else:\n+                inputs[\"input_ids\"] = outputs_cached.sequences\n+                if \"attention_mask\" in inputs:\n+                    inputs[\"attention_mask\"] = torch.nn.functional.pad(\n+                        inputs[\"attention_mask\"],\n+                        (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n+                        mode=\"constant\",\n+                        value=1,\n+                    )\n+            first_caches_scores = outputs_cached.scores\n+            outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n+            full_cached_scores = first_caches_scores + outputs_cached.scores\n+            outputs_cached.scores = full_cached_scores\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self._check_similar_generate_outputs(outputs, outputs_cached)\n+            for layer_idx in range(len(outputs_cached.past_key_values)):\n+                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            outputs_cached.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n+    # needs to be overridden to avoid to avoid casting of input_values to float16\n+    # indeed, the codec model is kept in fp32, so we need to avoid casting input_values to float16\n+    def _test_attention_implementation(self, attn_implementation):\n+        \"\"\"\n+        Compares the output of generate with the eager attention implementation against other implementations.\n+        NOTE: despite the test logic being the same, different implementations actually need different decorators, hence\n+        this separate function.\n+        \"\"\"\n+        max_new_tokens = 30\n+        support_flag = {\n+            \"sdpa\": \"_supports_sdpa\",\n+            \"flash_attention_2\": \"_supports_flash_attn_2\",\n+        }\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not getattr(model_class, support_flag[attn_implementation]):\n+                self.skipTest(f\"{model_class.__name__} does not support `attn_implementation={attn_implementation}`\")\n+\n+            config, original_inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            inputs_dict = {}\n+            for input_name, input_data in original_inputs_dict.items():\n+                if (\n+                    isinstance(input_data, torch.Tensor)\n+                    and input_data.dtype in [torch.float32, torch.bfloat16]\n+                    and input_name != \"input_values\"\n+                ):\n+                    inputs_dict[input_name] = input_data.to(torch.float16)\n+                else:\n+                    inputs_dict[input_name] = input_data\n+            main_input = inputs_dict[model_class.main_input_name]\n+\n+            # FA2 doesn't accept masking in the middle of the sequence for now. We usually generate right-padded\n+            # attention masks at test time and, with generate, the mask will be appended with 1s on the right,\n+            # resulting in a mask with holes (not supported properly by FA2).\n+            if attn_implementation == \"flash_attention_2\":\n+                for input_name in (\"attention_mask\", \"decoder_attention_mask\", \"encoder_attention_mask\"):\n+                    if input_name in inputs_dict:\n+                        inputs_dict[input_name] = torch.ones_like(inputs_dict[input_name])\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + main_input.shape[1] + 1\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                del model\n+                gc.collect()\n+\n+                generate_kwargs = {\n+                    \"max_new_tokens\": max_new_tokens,\n+                    \"do_sample\": False,\n+                    \"return_dict_in_generate\": True,\n+                    \"output_scores\": True,\n+                    \"use_cache\": True,\n+                }\n+\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation=\"eager\",\n+                ).to(torch_device)\n+                res_eager = model_eager.generate(**inputs_dict, **generate_kwargs)\n+                del model_eager\n+                gc.collect()\n+\n+                model_attn = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch.float16,\n+                    attn_implementation=attn_implementation,\n+                ).to(torch_device)\n+                res_attn = model_attn.generate(**inputs_dict, **generate_kwargs)\n+                del model_attn\n+                gc.collect()\n+\n+                self._check_similar_generate_outputs(res_eager, res_attn, atol=1e-3, rtol=1e-3)\n+\n+\n+class KyutaiSpeechToTextForConditionalGenerationIntegrationTests(unittest.TestCase):\n+    _dataset = None\n+\n+    def setUp(self):\n+        self.model_checkpoint = \"kyutai/stt-2.6b-en\"\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+            )\n+            # using 24000 here for simplicity, should rather be processor.feature_extractor.sampling_rate\n+            cls._dataset = cls._dataset.cast_column(\"audio\", datasets.Audio(sampling_rate=24000))\n+\n+    def _load_datasamples(self, num_samples):\n+        self._load_dataset()\n+        ds = self._dataset\n+        speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n+        return [x[\"array\"] for x in speech_samples]\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_generation(self):\n+        \"\"\"\n+        reproduce test expected outputs using original codebase: https://gist.github.com/eustlb/7a9aa6139d11e0103c6b65bac103da52\n+\n+        DISCLAIMER: we are testing for pretty short inputs. Indeed, reproducing correct expected outputs for longer is not possible\n+        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context lenght,\n+        ultimately giving different outputs.\n+        \"\"\"\n+        processor = KyutaiSpeechToTextProcessor.from_pretrained(self.model_checkpoint)\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, device_map=torch_device\n+        )\n+\n+        samples = self._load_datasamples(1)\n+        inputs = processor(\n+            samples,\n+        ).to(torch_device)\n+\n+        out = model.generate(**inputs)\n+\n+        # fmt: off\n+        EXPECTED_TOKENS = torch.tensor([\n+            [48000, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1519, 263, 3, 3, 0, 3635, 428, 641, 0, 277, 3, 0, 265, 0, 267, 1162, 261, 274, 410, 0,  272, 3, 0, 265, 0, 260, 1621, 0, 1174, 371, 262, 3, 3, 3, 0, 269, 0, 281, 0, 304, 0, 2433, 3, 0, 266, 3, 0, 281, 1661, 3, 0, 376, 3, 3, 0, 350, 261, 401, 516, 263, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]],\n+        )\n+        # fmt: on\n+\n+        torch.testing.assert_close(out.cpu(), EXPECTED_TOKENS)\n+\n+    @slow\n+    @require_torch_accelerator\n+    def test_generation_batched(self):\n+        \"\"\"\n+        reproduce test expected outputs using original codebase: https://gist.github.com/eustlb/b58c217c75124d405ec1c13877c7ece8\n+\n+        DISCLAIMER: we are testing for pretty short inputs. Indeed, reproducing correct expected outputs for longer is not possible\n+        as implementation choices (qkv matrix in one linear for original code vs three for hf) create growing divergence with context lenght,\n+        ultimately giving different outputs.\n+        \"\"\"\n+        processor = KyutaiSpeechToTextProcessor.from_pretrained(self.model_checkpoint)\n+        model = KyutaiSpeechToTextForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint, device_map=torch_device\n+        )\n+\n+        samples = self._load_datasamples(4)\n+        inputs = processor(\n+            samples,\n+        ).to(torch_device)\n+\n+        out = model.generate(**inputs)\n+\n+        # fmt: off\n+        EXPECTED_TOKENS = torch.tensor([\n+            [48000, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1519, 263, 3, 3, 0, 3635, 428, 641, 0, 277, 3, 0, 265, 0, 267, 1162, 261, 274, 410, 0, 272, 3, 0, 265, 0, 260, 1621, 0, 1174, 371, 262, 3, 3, 3, 0, 269, 0, 281, 0, 304, 0, 2433, 3, 0, 266, 3, 0, 281, 1661, 3, 0, 376, 3, 3, 0, 350, 261, 401, 516, 263, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n+            [48000, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 500, 334, 0, 277, 3, 0, 1519, 263, 3, 3, 0, 3635, 428, 641, 264, 261, 0, 511, 1109, 3, 0, 1138, 3, 3, 3, 0, 508, 827, 3, 3, 3, 3, 0, 468, 3, 3, 0, 376, 3, 3, 3, 0, 260, 978, 263, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n+            [48000, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 414, 0, 527, 261, 3, 0, 409, 3, 3, 3, 0, 271, 3, 0, 309, 3, 0, 285, 3, 0, 521, 371, 609, 3, 3, 0, 260, 959, 3, 3, 3, 0, 272, 3, 0, 265, 0, 546, 262, 3, 3, 3, 3, 3, 3, 0, 291, 3, 0, 975, 2203, 3, 3, 3, 3, 0, 269, 3, 0, 260, 489, 651, 274, 279, 1870, 3, 0, 1084, 873, 273, 3, 0, 260, 531, 3, 3, 0, 409, 262, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 1502, 1005, 836, 3, 3, 0, 1666, 306, 3, 0, 340, 3, 0, 260, 3232, 3, 0, 269, 3, 3, 0, 275, 261, 0, 260, 1379, 261, 0, 3324, 3, 3, 3, 3, 0, 549, 3, 3, 0, 693, 405, 323, 3, 0, 266, 3, 3, 0, 265, 0, 699, 263, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n+            [48000, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 414, 0, 392, 3, 3, 0, 1269, 314, 0, 2607, 261, 3, 3, 3, 0, 1098, 295, 3, 3, 3, 0, 446, 625, 3, 0, 496, 280, 1205, 485, 1071, 1627, 449, 264, 261, 3, 0, 400, 0, 277, 3, 3, 3, 0, 260, 342, 3, 0, 618, 280, 1866, 3, 3, 0, 554, 3, 3, 3, 3, 0, 317, 262, 3, 3, 3, 3, 3, 3, 3, 3, 0, 269, 0, 303, 3, 0, 573, 2615, 3, 3, 0, 276, 3, 0, 275, 0, 305, 3, 0, 260, 415, 3, 3, 0, 272, 3, 3, 3, 3, 0, 1631, 327, 3, 3, 0, 333, 739, 841, 263, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n+        ])\n+        # fmt: on\n+\n+        torch.testing.assert_close(out.cpu(), EXPECTED_TOKENS)"
        },
        {
            "sha": "d9b0216b159ba90b23fef8f04de817d8a7099a5b",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 59,
            "deletions": 4,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -107,14 +107,21 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.use_cache = use_cache\n \n-    def prepare_config_and_inputs(self):\n-        input_values = floats_tensor([self.batch_size, self.num_channels, self.intermediate_size], scale=1.0)\n+    def prepare_config_and_inputs(self, input_values_length=None):\n+        input_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_channels,\n+                self.intermediate_size if input_values_length is None else input_values_length,\n+            ],\n+            scale=1.0,\n+        )\n         config = self.get_config()\n         inputs_dict = {\"input_values\": input_values}\n         return config, inputs_dict\n \n-    def prepare_config_and_inputs_for_common(self):\n-        config, inputs_dict = self.prepare_config_and_inputs()\n+    def prepare_config_and_inputs_for_common(self, input_values_length=None):\n+        config, inputs_dict = self.prepare_config_and_inputs(input_values_length=input_values_length)\n         return config, inputs_dict\n \n     def prepare_config_and_inputs_for_model_class(self, model_class):\n@@ -508,6 +515,54 @@ def test_integration_using_cache_decode(self):\n             )\n             self.assertTrue(rmse < 1e-3)\n \n+    def test_integration_encode_with_padding_cache(self):\n+        \"\"\"\n+        We test here the possibility to run Mimi in a streaming manner, i.e. chunk by chunk.\n+        1. we encode a first time the entire audio\n+        2. we encode the audio chunk by chunk, each chunk being the smallest size possible for the model (i.e. the frame size)\n+\n+        This test must be run on CPU since GPU floating point operations accumulate rounding errors that cause test failures.\n+        \"\"\"\n+        librispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n+\n+        model_id = \"kyutai/mimi\"\n+\n+        model = MimiModel.from_pretrained(model_id, use_cache=True).to(\"cpu\")\n+        processor = AutoFeatureExtractor.from_pretrained(model_id)\n+\n+        librispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\n+        audio_sample = librispeech_dummy[-1][\"audio\"][\"array\"]\n+\n+        inputs = processor(\n+            raw_audio=audio_sample,\n+            sampling_rate=processor.sampling_rate,\n+            return_tensors=\"pt\",\n+        ).to(\"cpu\")\n+\n+        frame_size = model.config.frame_size\n+        audio_codes = model.encode(inputs[\"input_values\"]).audio_codes\n+\n+        # streaming chunk by chunk\n+        encoder_past_key_values = None\n+        padding_cache = None\n+        encoded_frames_list = []\n+\n+        for start in range(0, inputs[\"input_values\"].shape[-1], frame_size):\n+            input_values_chunk = inputs[\"input_values\"][:, :, start : start + frame_size]\n+            encoder_outputs = model.encode(\n+                input_values_chunk,\n+                padding_cache=padding_cache,\n+                encoder_past_key_values=encoder_past_key_values,\n+                use_streaming=True,\n+            )\n+            encoder_past_key_values = encoder_outputs.encoder_past_key_values\n+            padding_cache = encoder_outputs.padding_cache\n+            encoded_frames_list.append(encoder_outputs.audio_codes)\n+\n+        streamed_audio_codes = torch.cat(encoded_frames_list, dim=-1)\n+\n+        torch.testing.assert_close(streamed_audio_codes, audio_codes)\n+\n     def test_integration(self):\n         expected_rmses = {\n             \"8\": 0.0018785292,"
        },
        {
            "sha": "f404f99628346ec8451f47394172ce41dfb9820d",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -3566,7 +3566,11 @@ def test_eager_matches_sdpa_inference(\n             # TODO: if we can also check with `batch_size=1` without being flaky?\n             for batch_size in [7]:\n                 # musicgen decoder models; TODO: find better abstraction\n-                if hasattr(self.model_tester, \"num_codebooks\") and not hasattr(model_eager, \"text_encoder\"):\n+                if (\n+                    model.__class__.__name__.startswith(\"Musicgen\")\n+                    and hasattr(self.model_tester, \"num_codebooks\")\n+                    and not hasattr(model_eager, \"text_encoder\")\n+                ):\n                     input_data_batch_size = batch_size * self.model_tester.num_codebooks\n                 else:\n                     input_data_batch_size = batch_size\n@@ -3626,7 +3630,7 @@ def test_eager_matches_sdpa_inference(\n \n                 if is_encoder_decoder:\n                     # musicgen encoder-decoder models; TODO: find better abstraction\n-                    if hasattr(self.model_tester, \"num_codebooks\"):\n+                    if model.__class__.__name__.startswith(\"Musicgen\") and hasattr(self.model_tester, \"num_codebooks\"):\n                         input_data_batch_size = batch_size * self.model_tester.num_codebooks\n                     else:\n                         input_data_batch_size = batch_size"
        },
        {
            "sha": "a930e63e99f230a76480426b02599885d035d7d3",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=6bdd4ec95264e5d8f219cfe4ee29ea9b42474bb7",
            "patch": "@@ -619,7 +619,7 @@ def augmented_dependencies_for_class_node(\n     \"processing\",\n     \"image_processing\",\n     \"video_processing\",\n-    \"feature_extractor\",\n+    \"feature_extraction\",\n )\n \n \n@@ -1137,7 +1137,7 @@ def replace_class_node(\n     \"VideoProcessor\": \"video_processing\",\n     \"VideoProcessorInitKwargs\": \"video_processing\",\n     \"FastImageProcessorKwargs\": \"image_processing*_fast\",\n-    \"FeatureExtractor\": \"feature_extractor\",\n+    \"FeatureExtractor\": \"feature_extraction\",\n     \"ProcessorKwargs\": \"processing\",\n     \"VideosKwargs\": \"processing\",\n     \"ImagesKwargs\": \"processing\","
        }
    ],
    "stats": {
        "total": 4198,
        "additions": 3999,
        "deletions": 199
    }
}