{
    "author": "VladOS95-cyber",
    "message": "change sequence_bias type of SequenceBiasLogitsProcessor to list, addâ€¦ (#33375)\n\n* change sequence_bias type of SequenceBiasLogitsProcessor tp list, add config tests for all processors\r\n\r\n* fix format\r\n\r\n* small fix for all_token_bias_pairs_are_valid internal func\r\n\r\n* small typo fix in description\r\n\r\n* improve test impl, some SequenceBiasLogitsProcessor refactoring",
    "sha": "162056a3f433132fa6ce32c87663742803036b59",
    "files": [
        {
            "sha": "d88c7a17d892d43cbbbc16733edd6ae23a21b64b",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 40,
            "deletions": 13,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/162056a3f433132fa6ce32c87663742803036b59/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/162056a3f433132fa6ce32c87663742803036b59/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=162056a3f433132fa6ce32c87663742803036b59",
            "patch": "@@ -15,7 +15,7 @@\n \n import inspect\n import math\n-from typing import Callable, Dict, Iterable, List, Optional, Tuple, Union\n+from typing import Callable, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n@@ -1064,8 +1064,9 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     </Tip>\n \n     Args:\n-        sequence_bias (`Dict[Tuple[int], float]`):\n-            Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n+        sequence_bias (`List[List[Union[List[int], float]]]`):\n+            List of lists that maps a sequence of tokens to its bias term (e.g. `[[[10, 45], -2.0],\n+            [[64], -7.5]]`). Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. If a sequence has a length of 1, its bias\n             will always be applied. Otherwise, the bias will only be applied if the sequence in question is about to be\n             completed (in the token selection step after this processor is applied).\n@@ -1087,12 +1088,12 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     >>> tokenizer_with_prefix_space = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", add_prefix_space=True)\n \n \n-    >>> def get_tokens_as_tuple(word):\n-    ...     return tuple(tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0])\n+    >>> def get_tokens(word):\n+    ...     return tokenizer_with_prefix_space([word], add_special_tokens=False).input_ids[0]\n \n \n     >>> # If we add a negative bias without beam search, it may become \"stuck\" in a prefix without good continuations\n-    >>> sequence_bias = {get_tokens_as_tuple(\"Trump\"): -10.0}\n+    >>> sequence_bias = [get_tokens(\"Trump\"), -10.0]\n     >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, sequence_bias=sequence_bias)\n     >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n     The full name of Donald is Donald J. Donald,\n@@ -1102,16 +1103,17 @@ class SequenceBiasLogitsProcessor(LogitsProcessor):\n     The full name of Donald is Donald Rumsfeld,\n \n     >>> # We can also add a positive bias to nudge the model towards specific tokens or continuations\n-    >>> sequence_bias = {get_tokens_as_tuple(\"Donald Duck\"): 10.0}\n+    >>> sequence_bias = [get_tokens(\"Donald Duck\"), 10.0]\n     >>> biased_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, sequence_bias=sequence_bias)\n     >>> print(tokenizer.batch_decode(biased_ids, skip_special_tokens=True)[0])\n     The full name of Donald is Donald Duck.\n     ```\n     \"\"\"\n \n-    def __init__(self, sequence_bias: Dict[Tuple[int], float]):\n+    def __init__(self, sequence_bias: List[List[Union[List[int], float]]]):\n         self.sequence_bias = sequence_bias\n         self._validate_arguments()\n+        self._convert_list_arguments_into_dict()\n \n         # Bias variables that will be populated on the first call (for retrocompatibility purposes, the vocabulary size\n         # is infered in the first usage, which inhibits initializing here)\n@@ -1178,11 +1180,15 @@ def _prepare_bias_variables(self, scores: torch.FloatTensor):\n \n     def _validate_arguments(self):\n         sequence_bias = self.sequence_bias\n-        if not isinstance(sequence_bias, dict) or len(sequence_bias) == 0:\n-            raise ValueError(f\"`sequence_bias` has to be a non-empty dictionary, but is {sequence_bias}.\")\n-        if any(not isinstance(sequence_ids, tuple) for sequence_ids in sequence_bias.keys()):\n+        if not isinstance(sequence_bias, dict) and not isinstance(sequence_bias, list) or len(sequence_bias) == 0:\n+            raise ValueError(\n+                f\"`sequence_bias` has to be a non-empty dictionary, or non-empty list of lists but is {sequence_bias}.\"\n+            )\n+        if isinstance(sequence_bias, dict) and any(\n+            not isinstance(sequence_ids, tuple) for sequence_ids in sequence_bias.keys()\n+        ):\n             raise ValueError(f\"`sequence_bias` has to be a dict with tuples as keys, but is {sequence_bias}.\")\n-        if any(\n+        if isinstance(sequence_bias, dict) and any(\n             any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in sequence_ids)\n             or len(sequence_ids) == 0\n             for sequence_ids in sequence_bias.keys()\n@@ -1191,9 +1197,30 @@ def _validate_arguments(self):\n                 f\"Each key in `sequence_bias` has to be a non-empty tuple of positive integers, but is \"\n                 f\"{sequence_bias}.\"\n             )\n-        if any(not isinstance(bias, float) for bias in sequence_bias.values()):\n+\n+        def all_token_bias_pairs_are_valid(sequence):\n+            return (\n+                isinstance(sequence[0], list)\n+                and all(isinstance(token_id, (int, np.integer)) and token_id > 0 for token_id in sequence[0])\n+                and isinstance(sequence[1], float)\n+            )\n+\n+        if isinstance(sequence_bias, list) and any(\n+            (not all_token_bias_pairs_are_valid(sequence)) or len(sequence) == 0 for sequence in sequence_bias\n+        ):\n+            raise ValueError(\n+                f\"Each element in `sequence_bias` has to be a non-empty list of lists of positive integers and float, but is \"\n+                f\"{sequence_bias}.\"\n+            )\n+        if isinstance(sequence_bias, dict) and any(not isinstance(bias, float) for bias in sequence_bias.values()):\n             raise ValueError(f\"`sequence_bias` has to be a dict with floats as values, but is {sequence_bias}.\")\n \n+    def _convert_list_arguments_into_dict(self):\n+        \"\"\"BC: we used to accept `dict{tuple of tokens: float}` directly, now we expect a list\"\"\"\n+        if isinstance(self.sequence_bias, list):\n+            temp_sequence = self.sequence_bias\n+            self.sequence_bias = {tuple(sublist[0]): sublist[1] for sublist in temp_sequence}\n+\n \n class NoBadWordsLogitsProcessor(SequenceBiasLogitsProcessor):\n     \"\"\""
        },
        {
            "sha": "1e11a9679b258792bbd094bdac786f31ee31b87d",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 446,
            "deletions": 3,
            "changes": 449,
            "blob_url": "https://github.com/huggingface/transformers/blob/162056a3f433132fa6ce32c87663742803036b59/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/162056a3f433132fa6ce32c87663742803036b59/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=162056a3f433132fa6ce32c87663742803036b59",
            "patch": "@@ -23,9 +23,41 @@\n from huggingface_hub import HfFolder, delete_repo\n from parameterized import parameterized\n \n-from transformers import AutoConfig, GenerationConfig\n-from transformers.generation import GenerationMode\n-from transformers.testing_utils import TOKEN, USER, is_staging_test\n+from transformers import AutoConfig, GenerationConfig, WatermarkingConfig, is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+from transformers.generation import (\n+    ClassifierFreeGuidanceLogitsProcessor,\n+    EncoderNoRepeatNGramLogitsProcessor,\n+    EncoderRepetitionPenaltyLogitsProcessor,\n+    EpsilonLogitsWarper,\n+    EtaLogitsWarper,\n+    ExponentialDecayLengthPenalty,\n+    ForcedBOSTokenLogitsProcessor,\n+    ForcedEOSTokenLogitsProcessor,\n+    GenerationMode,\n+    HammingDiversityLogitsProcessor,\n+    MinLengthLogitsProcessor,\n+    MinNewTokensLengthLogitsProcessor,\n+    MinPLogitsWarper,\n+    NoBadWordsLogitsProcessor,\n+    NoRepeatNGramLogitsProcessor,\n+    PrefixConstrainedLogitsProcessor,\n+    RepetitionPenaltyLogitsProcessor,\n+    SequenceBiasLogitsProcessor,\n+    SuppressTokensAtBeginLogitsProcessor,\n+    SuppressTokensLogitsProcessor,\n+    TemperatureLogitsWarper,\n+    TopKLogitsWarper,\n+    TopPLogitsWarper,\n+    TypicalLogitsWarper,\n+    UnbatchedClassifierFreeGuidanceLogitsProcessor,\n+    WatermarkLogitsProcessor,\n+)\n+from transformers.testing_utils import TOKEN, USER, is_staging_test, torch_device\n \n \n class GenerationConfigTest(unittest.TestCase):\n@@ -225,6 +257,417 @@ def test_generation_mode(self):\n         self.assertEqual(config.get_generation_mode(assistant_model=\"foo\"), GenerationMode.ASSISTED_GENERATION)\n \n \n+class GenerationConfigSerializationTest(unittest.TestCase):\n+    def test_serialize_generation_sequence_bias(self):\n+        \"\"\"Tests that GenerationConfig is serialized and SequenceBiasLogitsProcessor is initialized with sequence_bias parameter\"\"\"\n+        generation_config = GenerationConfig()\n+        sequence_bias = [[[45, 67], -0.6], [[89], 1.2]]\n+        generation_config.sequence_bias = sequence_bias\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertSequenceEqual(new_config.sequence_bias, sequence_bias)\n+\n+        expected_sequence_bias = {(45, 67): -0.6, (89,): 1.2}\n+        bias_logits_processor = SequenceBiasLogitsProcessor(new_config.sequence_bias)\n+        self.assertDictEqual(bias_logits_processor.sequence_bias, expected_sequence_bias)\n+\n+    def test_serialize_generation_min_length_eos_token(self):\n+        \"\"\"Tests that GenerationConfig is serialized and MinLengthLogitsProcessor is initialized with min_length and eos_token_id\"\"\"\n+        eos_token_id = 0\n+        min_length = 10\n+\n+        generation_config = GenerationConfig(min_length=min_length, eos_token_id=eos_token_id)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.min_length, min_length)\n+        self.assertEqual(new_config.eos_token_id, eos_token_id)\n+\n+        min_dist_processor = MinLengthLogitsProcessor(\n+            min_length=new_config.min_length, eos_token_id=new_config.eos_token_id\n+        )\n+        self.assertEqual(min_dist_processor.min_length, min_length)\n+        self.assertEqual(min_dist_processor.eos_token_id, eos_token_id)\n+\n+    def test_serialize_generation_min_new_tokens(self):\n+        \"\"\"Tests that GenerationConfig is serialized and MinNewTokensLengthLogitsProcessor is initialized with min_new_tokens\"\"\"\n+        eos_token_id = 0\n+        min_new_tokens = 5\n+        prompt_length_to_skip = 2\n+\n+        generation_config = GenerationConfig(min_new_tokens=min_new_tokens)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.min_new_tokens, min_new_tokens)\n+\n+        min_new_tokens_processor = MinNewTokensLengthLogitsProcessor(\n+            prompt_length_to_skip=prompt_length_to_skip,\n+            min_new_tokens=new_config.min_new_tokens,\n+            eos_token_id=eos_token_id,\n+        )\n+        self.assertEqual(min_new_tokens_processor.min_new_tokens, min_new_tokens)\n+\n+    def test_serialize_generation_temperature(self):\n+        \"\"\"Tests that GenerationConfig is serialized and TemperatureLogitsWarper is initialized with temperature\"\"\"\n+        temperature = 2.0\n+\n+        generation_config = GenerationConfig(temperature=temperature, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.temperature, temperature)\n+\n+        temperature_logits_warper = TemperatureLogitsWarper(temperature=new_config.temperature)\n+        self.assertEqual(temperature_logits_warper.temperature, temperature)\n+\n+    def test_serialize_generation_repetition_penalty(self):\n+        \"\"\"Tests that GenerationConfig is serialized and RepetitionPenaltyLogitsProcessor is initialized with repetition_penalty\"\"\"\n+        penalty = 2.0\n+\n+        generation_config = GenerationConfig(repetition_penalty=penalty)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.repetition_penalty, penalty)\n+\n+        rep_penalty_proc = RepetitionPenaltyLogitsProcessor(penalty=new_config.repetition_penalty)\n+        self.assertEqual(rep_penalty_proc.penalty, penalty)\n+\n+    def test_serialize_generation_encoder_repetition_penalty(self):\n+        \"\"\"Tests that GenerationConfig is serialized and EncoderRepetitionPenaltyLogitsProcessor is initialized with penalty and input_ids\"\"\"\n+        penalty = 2.0\n+        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n+\n+        generation_config = GenerationConfig(encoder_repetition_penalty=penalty)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.encoder_repetition_penalty, penalty)\n+\n+        rep_penalty_proc = EncoderRepetitionPenaltyLogitsProcessor(\n+            penalty=new_config.encoder_repetition_penalty, encoder_input_ids=input_ids\n+        )\n+        self.assertEqual(rep_penalty_proc.penalty, 1 / penalty)\n+        torch.testing.assert_close(rep_penalty_proc.encoder_input_ids, input_ids)\n+\n+    def test_serialize_generation_top_p(self):\n+        \"\"\"Tests that GenerationConfig is serialized and TopPLogitsWarper is initialized with top_p\"\"\"\n+        top_p = 0.8\n+\n+        generation_config = GenerationConfig(top_p=top_p, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.top_p, top_p)\n+\n+        rep_penalty_proc = TopPLogitsWarper(top_p=new_config.top_p)\n+        self.assertEqual(rep_penalty_proc.top_p, top_p)\n+\n+    def test_serialize_generation_top_k(self):\n+        \"\"\"Tests that GenerationConfig is serialized and TopKLogitsWarper is initialized with top_k\"\"\"\n+        top_k = 2\n+\n+        generation_config = GenerationConfig(top_k=top_k, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.top_k, top_k)\n+\n+        top_k_logits_wrap = TopKLogitsWarper(top_k=new_config.top_k)\n+        self.assertEqual(top_k_logits_wrap.top_k, top_k)\n+\n+    def test_serialize_generation_min_p(self):\n+        \"\"\"Tests that GenerationConfig is serialized and MinPLogitsWarper is initialized with min_p\"\"\"\n+        min_p = 0.8\n+\n+        generation_config = GenerationConfig(min_p=min_p, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.min_p, min_p)\n+\n+        min_k_logits_wrap = MinPLogitsWarper(min_p=new_config.min_p)\n+        self.assertEqual(min_k_logits_wrap.min_p, min_p)\n+\n+    def test_serialize_generation_typical_p(self):\n+        \"\"\"Tests that GenerationConfig is serialized and TypicalLogitsWarper is initialized with mass\"\"\"\n+        mass = 0.8\n+\n+        generation_config = GenerationConfig(typical_p=mass, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.typical_p, mass)\n+\n+        typical_p_logits_wrap = TypicalLogitsWarper(mass=new_config.typical_p)\n+        self.assertEqual(typical_p_logits_wrap.mass, mass)\n+\n+    def test_serialize_generation_epsilon_cutoff(self):\n+        \"\"\"Tests that GenerationConfig is serialized and EpsilonLogitsWarper is initialized with epsilon\"\"\"\n+        epsilon = 0.8\n+\n+        generation_config = GenerationConfig(epsilon_cutoff=epsilon, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.epsilon_cutoff, epsilon)\n+\n+        epsilon_logits_wrap = EpsilonLogitsWarper(epsilon=new_config.epsilon_cutoff)\n+        self.assertEqual(epsilon_logits_wrap.epsilon, epsilon)\n+\n+    def test_serialize_generation_eta_cutoff(self):\n+        \"\"\"Tests that GenerationConfig is serialized and EtaLogitsWarper is initialized with epsilon\"\"\"\n+        epsilon = 0.8\n+\n+        generation_config = GenerationConfig(eta_cutoff=epsilon, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.eta_cutoff, epsilon)\n+\n+        eta_logits_wrap = EtaLogitsWarper(epsilon=new_config.eta_cutoff)\n+        self.assertEqual(eta_logits_wrap.epsilon, epsilon)\n+\n+    def test_serialize_generation_ngram_size(self):\n+        \"\"\"Tests that GenerationConfig is serialized and NoRepeatNGramLogitsProcessor is initialized with ngram_size\"\"\"\n+        ngram_size = 2\n+\n+        generation_config = GenerationConfig(no_repeat_ngram_size=ngram_size, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.no_repeat_ngram_size, ngram_size)\n+\n+        no_repeat_ngram_proc = NoRepeatNGramLogitsProcessor(ngram_size=new_config.no_repeat_ngram_size)\n+        self.assertEqual(no_repeat_ngram_proc.ngram_size, ngram_size)\n+\n+    def test_serialize_generation_encoder_ngram_size(self):\n+        \"\"\"Tests that GenerationConfig is serialized and EncoderNoRepeatNGramLogitsProcessor is initialized with ngram_size\"\"\"\n+        ngram_size = 2\n+        input_ids = torch.tensor([[0, 1], [5, 0]], device=torch_device, dtype=torch.long)\n+\n+        generation_config = GenerationConfig(encoder_no_repeat_ngram_size=ngram_size, do_sample=True)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.encoder_no_repeat_ngram_size, ngram_size)\n+\n+        encoder_no_repeat_ngram_proc = EncoderNoRepeatNGramLogitsProcessor(\n+            encoder_ngram_size=new_config.encoder_no_repeat_ngram_size, encoder_input_ids=input_ids\n+        )\n+        self.assertEqual(encoder_no_repeat_ngram_proc.ngram_size, ngram_size)\n+\n+    def test_serialize_generation_bad_words_ids(self):\n+        \"\"\"Tests that GenerationConfig is serialized and NoBadWordsLogitsProcessor is initialized with bad_words_ids\"\"\"\n+        bad_word_tokens = [[1], [4], [1, 0], [0, 1, 2], [1, 3, 1, 3]]\n+\n+        generation_config = GenerationConfig(bad_words_ids=bad_word_tokens)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertSequenceEqual(new_config.bad_words_ids, bad_word_tokens)\n+\n+        no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=new_config.bad_words_ids)\n+        self.assertSequenceEqual(no_bad_words_dist_proc.bad_word_ids, bad_word_tokens)\n+\n+    def test_serialize_generation_num_beams(self):\n+        \"\"\"Tests that GenerationConfig is serialized and PrefixConstrainedLogitsProcessor is initialized with num_beams\"\"\"\n+        num_beams = 1\n+\n+        def prefix_allowed_tokens_fn(batch_id, inputs_ids):\n+            return [[0, 1], [2, 3]][batch_id]\n+\n+        generation_config = GenerationConfig(num_beams=num_beams)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.num_beams, num_beams)\n+\n+        prefix_constrained_logits_proc = PrefixConstrainedLogitsProcessor(\n+            prefix_allowed_tokens_fn, num_beams=new_config.num_beams\n+        )\n+        self.assertEqual(prefix_constrained_logits_proc._num_beams, num_beams)\n+\n+    def test_serialize_generation_diversity_penalty_and_num_bean_groups(self):\n+        \"\"\"Tests that GenerationConfig is serialized and HammingDiversityLogitsProcessor is initialized with diversity_penalty_and_num_bean_groups\"\"\"\n+        num_beams = 2\n+        num_beam_groups = 2\n+        diversity_penalty = 1.0\n+\n+        generation_config = GenerationConfig(\n+            num_beams=num_beams, diversity_penalty=diversity_penalty, num_beam_groups=num_beam_groups\n+        )\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.num_beams, num_beams)\n+        self.assertEqual(new_config.diversity_penalty, diversity_penalty)\n+        self.assertEqual(new_config.num_beam_groups, num_beam_groups)\n+\n+        diversity_logits_processor = HammingDiversityLogitsProcessor(\n+            diversity_penalty=new_config.diversity_penalty,\n+            num_beams=new_config.num_beams,\n+            num_beam_groups=new_config.num_beam_groups,\n+        )\n+        self.assertEqual(diversity_logits_processor._num_beams, num_beams)\n+        self.assertEqual(diversity_logits_processor._diversity_penalty, diversity_penalty)\n+        self.assertEqual(diversity_logits_processor._num_sub_beams, num_beams // num_beam_groups)\n+\n+    def test_serialize_generation_bos_token_id(self):\n+        \"\"\"Tests that GenerationConfig is serialized and ForcedBOSTokenLogitsProcessor is initialized with bos_token_id\"\"\"\n+        bos_token_id = 0\n+\n+        generation_config = GenerationConfig(bos_token_id=bos_token_id)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.bos_token_id, bos_token_id)\n+\n+        logits_processor = ForcedBOSTokenLogitsProcessor(bos_token_id=new_config.bos_token_id)\n+        self.assertEqual(logits_processor.bos_token_id, bos_token_id)\n+\n+    def test_serialize_generation_eos_token_id(self):\n+        \"\"\"Tests that GenerationConfig is serialized and ForcedEOSTokenLogitsProcessor is initialized with eos_token_id\"\"\"\n+        eos_token_id = 0\n+        max_length = 5\n+\n+        generation_config = GenerationConfig(eos_token_id=eos_token_id)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.eos_token_id, eos_token_id)\n+\n+        logits_processor = ForcedEOSTokenLogitsProcessor(\n+            max_length=max_length, eos_token_id=new_config.eos_token_id, device=torch_device\n+        )\n+        self.assertEqual(logits_processor.eos_token_id, eos_token_id)\n+\n+    def test_serialize_generation_exponential_decay_length_penalty(self):\n+        \"\"\"Tests that GenerationConfig is serialized and ExponentialDecayLengthPenalty is initialized with regulation_start and regulation_factor\"\"\"\n+        eos_token_id = 0\n+        penalty_start = 5\n+        penalty_factor = 1.1\n+        input_ids_seq_length = 10\n+        exponential_decay_length_penalty = (penalty_start, penalty_factor)\n+\n+        generation_config = GenerationConfig(exponential_decay_length_penalty=exponential_decay_length_penalty)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.exponential_decay_length_penalty, [penalty_start, penalty_factor])\n+\n+        exponential_decay_processor = ExponentialDecayLengthPenalty(\n+            exponential_decay_length_penalty=new_config.exponential_decay_length_penalty,\n+            eos_token_id=eos_token_id,\n+            input_ids_seq_length=input_ids_seq_length,\n+        )\n+        self.assertEqual(\n+            exponential_decay_processor.regulation_start, exponential_decay_length_penalty[0] + input_ids_seq_length\n+        )\n+        self.assertEqual(exponential_decay_processor.regulation_factor, exponential_decay_length_penalty[1])\n+\n+    def test_serialize_generation_begin_suppress_tokens(self):\n+        \"\"\"Tests that GenerationConfig is serialized and SuppressTokensAtBeginLogitsProcessor is initialized with begin_suppress_token and begin_index\"\"\"\n+\n+        begin_suppress_tokens = [220, 50256]\n+        begin_index = 0\n+        generation_config = GenerationConfig(begin_suppress_tokens=begin_suppress_tokens)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertSequenceEqual(new_config.begin_suppress_tokens, begin_suppress_tokens)\n+\n+        suppress_processor = SuppressTokensAtBeginLogitsProcessor(\n+            begin_suppress_tokens=new_config.begin_suppress_tokens, begin_index=begin_index\n+        )\n+        self.assertSequenceEqual(suppress_processor.begin_suppress_tokens, begin_suppress_tokens)\n+        self.assertEqual(suppress_processor.begin_index, begin_index)\n+\n+    def test_serialize_generation_suppress_tokens(self):\n+        \"\"\"Tests that GenerationConfig is serialized and SuppressTokensLogitsProcessor is initialized with suppress_token\"\"\"\n+        suppress_tokens = [220, 50256]\n+\n+        generation_config = GenerationConfig(suppress_tokens=suppress_tokens)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertSequenceEqual(new_config.suppress_tokens, suppress_tokens)\n+\n+        suppress_processor = SuppressTokensLogitsProcessor(suppress_tokens=new_config.suppress_tokens)\n+        self.assertSequenceEqual(suppress_processor.suppress_tokens, suppress_tokens)\n+\n+    def test_serialize_generation_guidance_scale(self):\n+        \"\"\"Tests that GenerationConfig is serialized and ClassifierFreeGuidanceLogitsProcessor is initialized with guidance_scale\"\"\"\n+        guidance_scale = 2.0\n+        generation_config = GenerationConfig(guidance_scale=guidance_scale)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.guidance_scale, guidance_scale)\n+\n+        classifier_processor = ClassifierFreeGuidanceLogitsProcessor(guidance_scale=new_config.guidance_scale)\n+        self.assertEqual(classifier_processor.guidance_scale, guidance_scale)\n+\n+    def test_serialize_generation_guidance_scale_unbatched(self):\n+        \"\"\"Tests that GenerationConfig is serialized and UnbatchedClassifierFreeGuidanceLogitsProcessor is initialized with guidance_scale\"\"\"\n+        guidance_scale = 2.0\n+\n+        input_ids = torch.LongTensor([[0]])\n+\n+        generation_config = GenerationConfig(guidance_scale=guidance_scale)\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.guidance_scale, guidance_scale)\n+\n+        cfg = UnbatchedClassifierFreeGuidanceLogitsProcessor(new_config.guidance_scale, {}, input_ids)\n+        self.assertEqual(cfg.guidance_scale, guidance_scale)\n+\n+    def test_serialize_generation_watermarking_config(self):\n+        \"\"\"Tests that GenerationConfig is serialized and WatermarkLogitsProcessor is initialized with WatermarkingConfig parameters\"\"\"\n+\n+        vocab_size = 20\n+        bias = 2.0\n+        greenlist_ratio = 0.5\n+        hashing_key = 10\n+        seeding_scheme = \"lefthash\"\n+        context_width = 10\n+        watermarking_config = WatermarkingConfig(\n+            bias=bias,\n+            greenlist_ratio=greenlist_ratio,\n+            hashing_key=hashing_key,\n+            seeding_scheme=seeding_scheme,\n+            context_width=context_width,\n+        )\n+        generation_config = GenerationConfig(watermarking_config=watermarking_config)\n+\n+        with tempfile.TemporaryDirectory(\"test-generation-config\") as tmp_dir:\n+            generation_config.save_pretrained(tmp_dir)\n+            new_config = GenerationConfig.from_pretrained(tmp_dir)\n+        self.assertEqual(new_config.watermarking_config.bias, bias)\n+        self.assertEqual(new_config.watermarking_config.greenlist_ratio, greenlist_ratio)\n+        self.assertEqual(new_config.watermarking_config.hashing_key, hashing_key)\n+        self.assertEqual(new_config.watermarking_config.seeding_scheme, seeding_scheme)\n+        self.assertEqual(new_config.watermarking_config.context_width, context_width)\n+\n+        watermark = WatermarkLogitsProcessor(\n+            vocab_size=vocab_size,\n+            device=torch_device,\n+            greenlist_ratio=new_config.watermarking_config.greenlist_ratio,\n+            bias=new_config.watermarking_config.bias,\n+            hashing_key=new_config.watermarking_config.hashing_key,\n+            seeding_scheme=new_config.watermarking_config.seeding_scheme,\n+            context_width=new_config.watermarking_config.context_width,\n+        )\n+        self.assertEqual(watermark.bias, bias)\n+        self.assertEqual(watermark.greenlist_size, int(vocab_size * greenlist_ratio))\n+        self.assertEqual(watermark.hash_key, hashing_key)\n+        self.assertEqual(watermark.seeding_scheme, seeding_scheme)\n+        self.assertEqual(watermark.context_width, context_width)\n+\n+\n @is_staging_test\n class ConfigPushToHubTester(unittest.TestCase):\n     @classmethod"
        }
    ],
    "stats": {
        "total": 502,
        "additions": 486,
        "deletions": 16
    }
}