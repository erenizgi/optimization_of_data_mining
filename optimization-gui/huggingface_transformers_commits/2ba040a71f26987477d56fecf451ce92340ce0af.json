{
    "author": "mrsndmn",
    "message": "apply_chat_template: consistent behaviour for return_assistant_tokens_mask=True return_tensors=True (#35582)\n\n* apply_chat_template: consistent return_tensors behaviour with return_assistant_tokens_mask flag\n\n* test_chat_template_return_assistant_tokens_mask: support tokenizers with no attention mask\n\n* test_chat_template_return_assistant_tokens_mask: skip tokenizers with no padding token\n\n* test_chat_template_return_assistant_tokens_mask: force tokenizer padding_side=right\n\n---------\n\nCo-authored-by: Eduard Allakhverdov <goncharova@airi.net>\nCo-authored-by: d.tarasov <d.tarasov@airi.net>",
    "sha": "2ba040a71f26987477d56fecf451ce92340ce0af",
    "files": [
        {
            "sha": "7ad36ab017f713cdfc971d01c54173c67450dee2",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba040a71f26987477d56fecf451ce92340ce0af/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba040a71f26987477d56fecf451ce92340ce0af/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=2ba040a71f26987477d56fecf451ce92340ce0af",
            "patch": "@@ -1742,7 +1742,15 @@ def apply_chat_template(\n                             for token_id in range(start_token, end_token + 1 if end_token else len(input_ids[i])):\n                                 current_mask[token_id] = 1\n                         assistant_masks.append(current_mask)\n-                    out[\"assistant_masks\"] = assistant_masks if is_batched else assistant_masks[0]\n+\n+                    if not is_batched and not return_tensors:\n+                        assistant_masks = assistant_masks[0]\n+\n+                    out[\"assistant_masks\"] = assistant_masks\n+\n+                    if return_tensors:\n+                        out.convert_to_tensors(tensor_type=return_tensors)\n+\n                 return out\n             else:\n                 return out[\"input_ids\"]"
        },
        {
            "sha": "d1dc9cc20243d1fde5385b4a1a3abb20cb85fe71",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 52,
            "deletions": 0,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ba040a71f26987477d56fecf451ce92340ce0af/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ba040a71f26987477d56fecf451ce92340ce0af/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=2ba040a71f26987477d56fecf451ce92340ce0af",
            "patch": "@@ -62,6 +62,7 @@\n \n \n if is_torch_available():\n+    import torch\n     import torch.nn as nn\n \n \n@@ -1219,6 +1220,7 @@ def test_jinja_strftime(self):\n                 self.assertEqual(len(strftime_output), 10)\n                 self.assertEqual(len(strftime_output.split(\"-\")), 3)\n \n+    @require_torch\n     @require_jinja\n     def test_chat_template_return_assistant_tokens_mask(self):\n         dummy_template = (\n@@ -1263,6 +1265,9 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     self.skipTest(reason=\"No fast tokenizer defined\")\n \n                 tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name)\n+                self._check_no_pad_token_padding(tokenizer_r, conversations)\n+\n+                tokenizer_r.padding_side = \"right\"\n \n                 # check batched\n                 output = tokenizer_r.apply_chat_template(\n@@ -1272,6 +1277,20 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     return_assistant_tokens_mask=True,\n                     return_dict=True,\n                 )\n+\n+                output_pt = tokenizer_r.apply_chat_template(\n+                    conversations,\n+                    chat_template=dummy_template,\n+                    tokenize=True,\n+                    padding=True,\n+                    return_assistant_tokens_mask=True,\n+                    return_dict=True,\n+                    return_tensors=\"pt\",\n+                )\n+\n+                self.assertEqual(type(output_pt[\"assistant_masks\"]), torch.Tensor)\n+                self.assertEqual(output_pt[\"assistant_masks\"].shape, output_pt[\"input_ids\"].shape)\n+\n                 for i, conv in enumerate(conversations):\n                     chat_string = tokenizer_r.apply_chat_template(\n                         conversations[i], tokenize=False, chat_template=dummy_template\n@@ -1297,18 +1316,30 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                         output[\"assistant_masks\"][i][assistant_start : assistant_end + 1],\n                         [1] * (assistant_end - assistant_start + 1),\n                     )\n+                    self.assertTrue(\n+                        (output_pt[\"assistant_masks\"][i, assistant_start : assistant_end + 1] == 1).all(),\n+                    )\n+\n                     # assert 1 second assistant message\n                     self.assertEqual(\n                         output[\"assistant_masks\"][i][assistant_start2 : assistant_end2 + 1],\n                         [1] * (assistant_end2 - assistant_start2 + 1),\n                     )\n+                    self.assertTrue(\n+                        (output_pt[\"assistant_masks\"][i, assistant_start2 : assistant_end2 + 1] == 1).all(),\n+                    )\n \n                     # assert 0 in user/system indices\n                     self.assertEqual(output[\"assistant_masks\"][i][:assistant_start], [0] * assistant_start)\n+                    self.assertTrue((output_pt[\"assistant_masks\"][i, :assistant_start] == 0).all())\n+\n                     self.assertEqual(\n                         output[\"assistant_masks\"][i][assistant_end + 1 : assistant_start2],\n                         [0] * (assistant_start2 - assistant_end - 1),\n                     )\n+                    self.assertTrue(\n+                        (output_pt[\"assistant_masks\"][i, assistant_end + 1 : assistant_start2] == 0).all(),\n+                    )\n \n                 # check not batched\n                 output = tokenizer_r.apply_chat_template(\n@@ -1318,6 +1349,17 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     return_assistant_tokens_mask=True,\n                     return_dict=True,\n                 )\n+                output_pt = tokenizer_r.apply_chat_template(\n+                    conversations[0],\n+                    chat_template=dummy_template,\n+                    tokenize=True,\n+                    return_assistant_tokens_mask=True,\n+                    return_dict=True,\n+                    return_tensors=\"pt\",\n+                )\n+\n+                self.assertEqual(type(output_pt[\"assistant_masks\"]), torch.Tensor)\n+                self.assertEqual(output_pt[\"assistant_masks\"].shape, output_pt[\"input_ids\"].shape)\n \n                 chat_string = tokenizer_r.apply_chat_template(\n                     conversations[0], tokenize=False, chat_template=dummy_template\n@@ -1336,17 +1378,27 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     output[\"assistant_masks\"][assistant_start : assistant_end + 1],\n                     [1] * (assistant_end - assistant_start + 1),\n                 )\n+                self.assertTrue(\n+                    (output_pt[\"assistant_masks\"][assistant_start : assistant_end + 1] == 1).all(),\n+                )\n                 self.assertEqual(\n                     output[\"assistant_masks\"][assistant_start2 : assistant_end2 + 1],\n                     [1] * (assistant_end2 - assistant_start2 + 1),\n                 )\n+                self.assertTrue(\n+                    (output_pt[\"assistant_masks\"][assistant_start2 : assistant_end2 + 1] == 1).all(),\n+                )\n \n                 # assert 0 in user/system indices\n                 self.assertEqual(output[\"assistant_masks\"][:assistant_start], [0] * assistant_start)\n+                self.assertTrue((output_pt[\"assistant_masks\"][0, :assistant_start] == 0).all())\n                 self.assertEqual(\n                     output[\"assistant_masks\"][assistant_end + 1 : assistant_start2],\n                     [0] * (assistant_start2 - assistant_end - 1),\n                 )\n+                self.assertTrue(\n+                    (output_pt[\"assistant_masks\"][0, assistant_end + 1 : assistant_start2] == 0).all(),\n+                )\n \n     @require_jinja\n     def test_chat_template_return_assistant_tokens_mask_truncated(self):"
        }
    ],
    "stats": {
        "total": 62,
        "additions": 61,
        "deletions": 1
    }
}