{
    "author": "BenjaminBossan",
    "message": "[PEFT] Support low_cpu_mem_usage option for PEFT loading adapters (#33725)\n\n* [PEFT] Support low_cpu_mem_usage for PEFT loading\r\n\r\nPEFT added support for low_cpu_mem_usage=True when loading adapters in\r\nhttps://github.com/huggingface/peft/pull/1961. This feature is now\r\navailable when installing PEFT v0.13.0. With this PR, this option is\r\nalso supported when loading PEFT adapters directly into transformers\r\nmodels.\r\n\r\nAdditionally, with this PR,\r\nhttps://github.com/huggingface/diffusers/pull/9510 will be unblocked,\r\nwhich implements this option in diffusers.\r\n\r\n* Fix typo",
    "sha": "6500f78c86bfd4729a01a5cebc915a263f8984e6",
    "files": [
        {
            "sha": "bd0ca16f865f4c5249f88b05ca399850fedfb336",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 23,
            "deletions": 2,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/6500f78c86bfd4729a01a5cebc915a263f8984e6/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6500f78c86bfd4729a01a5cebc915a263f8984e6/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=6500f78c86bfd4729a01a5cebc915a263f8984e6",
            "patch": "@@ -11,10 +11,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import importlib\n import inspect\n import warnings\n from typing import Any, Dict, List, Optional, Union\n \n+from packaging import version\n+\n from ..utils import (\n     check_peft_version,\n     find_adapter_config_file,\n@@ -77,6 +80,7 @@ def load_adapter(\n         offload_index: Optional[int] = None,\n         peft_config: Dict[str, Any] = None,\n         adapter_state_dict: Optional[Dict[str, \"torch.Tensor\"]] = None,\n+        low_cpu_mem_usage: bool = False,\n         adapter_kwargs: Optional[Dict[str, Any]] = None,\n     ) -> None:\n         \"\"\"\n@@ -129,12 +133,27 @@ def load_adapter(\n             adapter_state_dict (`Dict[str, torch.Tensor]`, *optional*):\n                 The state dict of the adapter to load. This argument is used in case users directly pass PEFT state\n                 dicts\n+            low_cpu_mem_usage (`bool`, *optional*, defaults to `False`):\n+                Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.\n+                Requires PEFT version 0.13.0 or higher.\n             adapter_kwargs (`Dict[str, Any]`, *optional*):\n                 Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n                 `find_adapter_config_file` method.\n         \"\"\"\n         check_peft_version(min_version=MIN_PEFT_VERSION)\n \n+        # peft only supports low_cpu_mem_usage starting from v0.13.0\n+        peft_load_kwargs = {}\n+        if low_cpu_mem_usage:\n+            min_version_lcmu = \"0.13.0\"\n+            if version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu):\n+                peft_load_kwargs[\"low_cpu_mem_usage\"] = low_cpu_mem_usage\n+            else:\n+                raise ValueError(\n+                    \"The version of PEFT you are using does not support `low_cpu_mem_usage` yet, \"\n+                    f\"please install PEFT >= {min_version_lcmu}.\"\n+                )\n+\n         adapter_name = adapter_name if adapter_name is not None else \"default\"\n         if adapter_kwargs is None:\n             adapter_kwargs = {}\n@@ -192,7 +211,7 @@ def load_adapter(\n             )\n \n         # Create and add fresh new adapters into the model.\n-        inject_adapter_in_model(peft_config, self, adapter_name)\n+        inject_adapter_in_model(peft_config, self, adapter_name, **peft_load_kwargs)\n \n         if not self._hf_peft_config_loaded:\n             self._hf_peft_config_loaded = True\n@@ -211,7 +230,9 @@ def load_adapter(\n             processed_adapter_state_dict[new_key] = value\n \n         # Load state dict\n-        incompatible_keys = set_peft_model_state_dict(self, processed_adapter_state_dict, adapter_name)\n+        incompatible_keys = set_peft_model_state_dict(\n+            self, processed_adapter_state_dict, adapter_name, **peft_load_kwargs\n+        )\n \n         if incompatible_keys is not None:\n             # check only for unexpected keys"
        },
        {
            "sha": "a80919dc61cf3f2c3ec4b1433574e6851d3da55e",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/6500f78c86bfd4729a01a5cebc915a263f8984e6/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6500f78c86bfd4729a01a5cebc915a263f8984e6/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=6500f78c86bfd4729a01a5cebc915a263f8984e6",
            "patch": "@@ -12,11 +12,13 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import importlib\n import os\n import tempfile\n import unittest\n \n from huggingface_hub import hf_hub_download\n+from packaging import version\n \n from transformers import AutoModelForCausalLM, OPTForCausalLM\n from transformers.testing_utils import (\n@@ -478,6 +480,48 @@ def test_peft_add_adapter_with_state_dict(self):\n                 # dummy generation\n                 _ = model.generate(input_ids=dummy_input)\n \n+    def test_peft_add_adapter_with_state_dict_low_cpu_mem_usage(self):\n+        \"\"\"\n+        Check the usage of low_cpu_mem_usage, which is supported in PEFT >= 0.13.0\n+        \"\"\"\n+        from peft import LoraConfig\n+\n+        min_version_lcmu = \"0.13.0\"\n+        is_lcmu_supported = version.parse(importlib.metadata.version(\"peft\")) >= version.parse(min_version_lcmu)\n+\n+        for model_id, peft_model_id in zip(self.transformers_test_model_ids, self.peft_test_model_ids):\n+            for transformers_class in self.transformers_test_model_classes:\n+                model = transformers_class.from_pretrained(model_id).to(torch_device)\n+\n+                peft_config = LoraConfig()\n+                state_dict_path = hf_hub_download(peft_model_id, \"adapter_model.bin\")\n+                dummy_state_dict = torch.load(state_dict_path)\n+\n+                # this should always work\n+                model.load_adapter(\n+                    adapter_state_dict=dummy_state_dict, peft_config=peft_config, low_cpu_mem_usage=False\n+                )\n+\n+                if is_lcmu_supported:\n+                    # if supported, this should not raise an error\n+                    model.load_adapter(\n+                        adapter_state_dict=dummy_state_dict,\n+                        adapter_name=\"other\",\n+                        peft_config=peft_config,\n+                        low_cpu_mem_usage=True,\n+                    )\n+                    # after loading, no meta device should be remaining\n+                    self.assertFalse(any((p.device.type == \"meta\") for p in model.parameters()))\n+                else:\n+                    err_msg = r\"The version of PEFT you are using does not support `low_cpu_mem_usage` yet\"\n+                    with self.assertRaisesRegex(ValueError, err_msg):\n+                        model.load_adapter(\n+                            adapter_state_dict=dummy_state_dict,\n+                            adapter_name=\"other\",\n+                            peft_config=peft_config,\n+                            low_cpu_mem_usage=True,\n+                        )\n+\n     def test_peft_from_pretrained_hub_kwargs(self):\n         \"\"\"\n         Tests different combinations of PEFT model + from_pretrained + hub kwargs"
        }
    ],
    "stats": {
        "total": 69,
        "additions": 67,
        "deletions": 2
    }
}