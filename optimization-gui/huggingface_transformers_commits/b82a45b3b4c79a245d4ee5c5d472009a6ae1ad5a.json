{
    "author": "Rocketknight1",
    "message": "Refactor DBRX tests to use CausalLMModelTest base classes (#38475)\n\n* Refactor DBRX tests to use CausalLMModelTest base classes\n\n- Changed DbrxModelTester to inherit from CausalLMModelTester\n- Changed DbrxModelTest to inherit from CausalLMModelTest\n- Removed duplicate methods that are already in base classes\n- Added required class attributes for model classes\n- Updated pipeline_model_mapping to include feature-extraction\n- Kept DBRX-specific configuration and test methods\n- Disabled RoPE tests as DBRX's rotary embedding doesn't accept config parameter\n\nThis refactoring reduces code duplication and follows the pattern established\nin other causal LM model tests like Gemma.\n\n* Apply style fixes\n\n* Trigger tests\n\n* Refactor DBRX test\n\n* Make sure the DBRX-specific settings are handled\n\n* Use the attribute_map\n\n* Fix attribute map\n\n---------\n\nCo-authored-by: openhands <openhands@all-hands.dev>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a",
    "files": [
        {
            "sha": "9807c8856059565146732a1910a0a096c6ec3c2f",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a",
            "patch": "@@ -181,11 +181,18 @@ def prepare_config_and_inputs(self):\n \n         return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n \n+    @property\n+    def config_args(self):\n+        return list(signature(self.config_class.__init__).parameters.keys())\n+\n     def get_config(self):\n-        kwarg_names = list(signature(self.config_class.__init__).parameters.keys())\n-        kwargs = {\n-            k: getattr(self, k) for k in kwarg_names + self.forced_config_args if hasattr(self, k) and k != \"self\"\n-        }\n+        kwargs = {}\n+        model_name_to_common_name = {v: k for k, v in self.config_class.attribute_map.items()}\n+        for k in self.config_args + self.forced_config_args:\n+            if hasattr(self, k) and k != \"self\":\n+                kwargs[k] = getattr(self, k)\n+            elif k in model_name_to_common_name and hasattr(self, model_name_to_common_name[k]):\n+                kwargs[k] = getattr(self, model_name_to_common_name[k])\n         return self.config_class(**kwargs)\n \n     def create_and_check_model("
        },
        {
            "sha": "e89740db616c3d3b9441306a2850dbb5b9dcd74d",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 43,
            "deletions": 169,
            "changes": 212,
            "blob_url": "https://github.com/huggingface/transformers/blob/b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=b82a45b3b4c79a245d4ee5c5d472009a6ae1ad5a",
            "patch": "@@ -16,12 +16,9 @@\n import unittest\n \n from transformers import DbrxConfig, is_torch_available\n-from transformers.testing_utils import require_torch, slow, torch_device\n+from transformers.testing_utils import require_torch, slow\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n \n \n if is_torch_available():\n@@ -30,197 +27,74 @@\n     from transformers import DbrxForCausalLM, DbrxModel\n \n \n-class DbrxModelTester:\n+class DbrxModelTester(CausalLMModelTester):\n+    config_class = DbrxConfig\n+    if is_torch_available():\n+        base_model_class = DbrxModel\n+        causal_lm_class = DbrxForCausalLM\n+\n     def __init__(\n         self,\n         parent,\n-        hidden_size=32,\n-        ffn_hidden_size=32,\n-        num_attention_heads=4,\n-        kv_n_heads=4,\n-        num_hidden_layers=5,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        use_cache=True,\n-        type_sequence_label_size=2,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n         clip_qkv=8,\n         rope_theta=500000,\n         attn_config_model_type=\"\",\n-        emb_pdrop=0.0,\n         moe_jitter_eps=0,\n         moe_loss_weight=0.05,\n-        moe_num_experts=16,\n+        moe_num_experts=8,\n         moe_top_k=4,\n         ffn_config_model_type=\"\",\n-        ffn_act_fn_name=\"gelu\",\n         initializer_range=0.02,\n-        output_router_logits=False,\n         resid_pdrop=0.0,\n-        tie_word_embeddings=False,\n-        torch_dtype=\"bfloat16\",\n-        vocab_size=99,\n         is_decoder=True,\n         pad_token_id=0,\n     ):\n-        # Parameters unique to testing\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = scope\n-        self.parent = parent\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n+        # Call parent init\n+        super().__init__(\n+            parent=parent,\n+            hidden_dropout_prob=resid_pdrop,\n+            attention_probs_dropout_prob=resid_pdrop,\n+            initializer_range=initializer_range,\n+            pad_token_id=pad_token_id,\n+            is_decoder=is_decoder,\n+        )\n \n-        # attn_config params\n+        # Set DBRX's unusual params\n         self.clip_qkv = clip_qkv\n-        self.kv_n_heads = kv_n_heads\n-        self.rope_theta = rope_theta\n-        self.attn_config_model_type = attn_config_model_type\n-\n-        # ffn_config params\n-        self.ffn_hidden_size = ffn_hidden_size\n-        self.moe_jitter_eps = moe_jitter_eps\n-        self.moe_loss_weight = moe_loss_weight\n-        self.moe_num_experts = moe_num_experts\n-        self.moe_top_k = moe_top_k\n-        self.ffn_config_model_type = ffn_config_model_type\n-        self.ffn_act_fn_name = ffn_act_fn_name\n \n-        # Other model params\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.max_position_embeddings = max_position_embeddings\n-        self.vocab_size = vocab_size\n-        self.use_cache = use_cache\n-        self.initializer_range = initializer_range\n-        self.emb_pdrop = emb_pdrop\n-        self.output_router_logits = output_router_logits\n-        self.resid_pdrop = resid_pdrop\n-        self.tie_word_embeddings = tie_word_embeddings\n-        self.torch_dtype = torch_dtype\n-        self.is_decoder = is_decoder\n-        self.pad_token_id = pad_token_id\n-\n-        # Make the dictionaries\n+        # DBRX takes sub-configurations for the FFN and attention layers, so we need to set that correctly here\n         self.ffn_config = {\n-            \"ffn_hidden_size\": self.ffn_hidden_size,\n-            \"moe_jitter_eps\": self.moe_jitter_eps,\n-            \"moe_loss_weight\": self.moe_loss_weight,\n-            \"moe_num_experts\": self.moe_num_experts,\n-            \"moe_top_k\": self.moe_top_k,\n-            \"model_type\": self.ffn_config_model_type,\n-            \"ffn_act_fn\": {\"name\": self.ffn_act_fn_name},\n+            \"ffn_hidden_size\": self.hidden_size,\n+            \"moe_jitter_eps\": moe_jitter_eps,\n+            \"moe_loss_weight\": moe_loss_weight,\n+            \"moe_num_experts\": moe_num_experts,\n+            \"moe_top_k\": moe_top_k,\n+            \"model_type\": ffn_config_model_type,\n+            \"ffn_act_fn\": {\"name\": self.hidden_act},\n         }\n         self.attn_config = {\n-            \"clip_qkv\": self.clip_qkv,\n-            \"kv_n_heads\": self.kv_n_heads,\n-            \"model_type\": self.attn_config_model_type,\n-            \"rope_theta\": self.rope_theta,\n+            \"clip_qkv\": clip_qkv,\n+            \"model_type\": attn_config_model_type,\n+            \"rope_theta\": rope_theta,\n         }\n \n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def get_config(self):\n-        # Behind the scenes, `DbrxConfig` maps the parameters `hidden_size`, `num_hidden_layers`,\n-        # `num_attention_heads`, `max_position_embeddings` to the parameters `d_model`, `n_layers`,\n-        # `n_heads`, `max_seq_len` respectively. We use the first group of parameters because\n-        # other tests expect every model to have these parameters with these specific names.\n-        config = DbrxConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,  # mapped to `d_model`\n-            num_hidden_layers=self.num_hidden_layers,  # mapped to `n_layers`\n-            num_attention_heads=self.num_attention_heads,  # mapped to `n_heads`\n-            max_position_embeddings=self.max_position_embeddings,  # mapped to `max_seq_len`\n-            attn_config=self.attn_config,\n-            ffn_config=self.ffn_config,\n-            resid_pdrop=self.resid_pdrop,\n-            emb_pdrop=self.emb_pdrop,\n-            use_cache=self.use_cache,\n-            initializer_range=self.initializer_range,\n-            output_router_logits=self.output_router_logits,\n-            is_decoder=self.is_decoder,\n-            pad_token_id=self.pad_token_id,\n-        )\n-        return config\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = DbrxModel(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n+    @property\n+    def config_args(self):\n+        return super().config_args + [\"ffn_config\", \"attn_config\"]\n \n \n @require_torch\n-class DbrxModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class DbrxModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (DbrxModel, DbrxForCausalLM) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"text-generation\": DbrxForCausalLM} if is_torch_available() else {}\n-    test_headmasking = False\n-    test_pruning = False\n-\n-    def setUp(self):\n-        self.model_tester = DbrxModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DbrxConfig, d_model=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": DbrxModel,\n+            \"text-generation\": DbrxForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    model_tester_class = DbrxModelTester\n \n     def test_model_various_embeddings(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 54,
        "deletions": 173
    }
}