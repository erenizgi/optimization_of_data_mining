{
    "author": "qgallouedec",
    "message": "Fix backward compatibility with accelerate in Trainer (#40668)",
    "sha": "e39f2220969d5ee2fe5643eef4888e36b6800a3c",
    "files": [
        {
            "sha": "f808c67ac308fc6816c239c966e0ff09103f48e1",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e39f2220969d5ee2fe5643eef4888e36b6800a3c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e39f2220969d5ee2fe5643eef4888e36b6800a3c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=e39f2220969d5ee2fe5643eef4888e36b6800a3c",
            "patch": "@@ -5626,7 +5626,7 @@ def get_batch_samples(\n                     # In the DataParallel case, convert the scalar tensor into a 1-dim tensor\n                     num_items_in_batch = num_items_in_batch.unsqueeze(0)\n                 # Divide by number of devices with the same batch\n-                if pc := self.accelerator.parallelism_config:\n+                if pc := getattr(self.accelerator, \"parallelism_config\", None):\n                     num_items_in_batch = num_items_in_batch // pc.non_data_parallel_size\n \n         return batch_samples, num_items_in_batch"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}