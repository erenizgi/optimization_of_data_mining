{
    "author": "daskol",
    "message": "Fix binding of video frames to video placeholder in `InternVL` model (#41237)\n\n* Fix binding video frames to video placeholder in prompt\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>\n\n* Add test on binding video frames to prompt\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>\n\n* Fix code style issues\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>\n\n* Fix broken tests on `InternVLProcessor`\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>\n\n* Add `return_tensors` to video processor defaults\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>\n\n---------\n\nSigned-off-by: Daniel Bershatsky <daniel.bershatsky@gmail.com>",
    "sha": "e6a8e7debed69d4cf5d493158f3f347931c24c9a",
    "files": [
        {
            "sha": "f68d48502b6c3e004a67827a11785a26960f3bc7",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 25,
            "deletions": 16,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6a8e7debed69d4cf5d493158f3f347931c24c9a/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6a8e7debed69d4cf5d493158f3f347931c24c9a/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=e6a8e7debed69d4cf5d493158f3f347931c24c9a",
            "patch": "@@ -40,7 +40,9 @@ class InternVLProcessorKwargs(ProcessingKwargs, total=False):\n         \"images_kwargs\": {\n             \"crop_to_patches\": True,\n         },\n-        \"videos_kwargs\": {},\n+        \"videos_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+        },\n     }\n \n \n@@ -132,10 +134,10 @@ def _insert_media_placeholders(\n                     # Get the slice of patches corresponding to the current video\n                     # Here we need to account for both the multiple video frames and the potential multiple patches per frame\n                     # As of now, InternVL only supports one patch per frame, but we keep the code flexible for future updates\n-                    current_patch_index = video_patch_indices[video_index - 1] if video_index > 0 else 0\n-                    end_patch_index = video_patch_indices[video_index]\n-                    start_index = video_num_patches_indices[current_patch_index] if video_index > 0 else 0\n-                    end_index = video_num_patches_indices[end_patch_index - 1]\n+                    current_patch_index = video_patch_indices[video_index]\n+                    end_patch_index = video_patch_indices[video_index + 1]\n+                    start_index = video_num_patches_indices[current_patch_index]\n+                    end_index = video_num_patches_indices[end_patch_index]\n                     image_video_patches.append(video_pixel_values[start_index:end_index])\n                     # Get the number of patches per frame and replace the video placeholder with the correct number of image tokens\n                     num_patches = list(video_num_patches[current_patch_index:end_patch_index])\n@@ -206,31 +208,38 @@ def __call__(\n \n         # Process images and videos separately, as videos don't support crop_to_patches\n         image_num_patches = []\n-        video_num_patches = []\n-        image_videos_inputs = {}\n         image_pixel_values = None\n-        video_pixel_values = None\n         image_num_patches_indices = np.array([0])\n-        video_patch_indices = np.array([0])\n-        video_num_patches_indices = np.array([0])\n         if images is not None:\n             images = self.image_processor.fetch_images(images)\n             images = make_flat_list_of_images(images)\n             image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             image_num_patches = image_inputs.pop(\"num_patches\")\n             image_pixel_values = image_inputs.pop(\"pixel_values\")\n             image_num_patches_indices = np.cumsum(image_num_patches)\n+\n+        video_num_patches = []  # per frame\n+        video_pixel_values = None\n+        video_patch_indices = np.array([0])\n+        video_num_patches_indices = np.array([0])\n         if videos is not None:\n-            video_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            video_kwargs = output_kwargs[\"videos_kwargs\"]\n+            video_inputs = self.video_processor(videos=videos, **video_kwargs)\n             video_pixel_values = video_inputs.pop(\"pixel_values_videos\")\n \n-            # Obtain per frame information first and then flatten to (BS * T, ...)\n-            num_frames_per_video = [len(video) for video in video_pixel_values]\n-            video_num_patches = [1 for frames in num_frames_per_video for _ in range(frames)]\n-            video_patch_indices = np.cumsum(num_frames_per_video)\n-            video_num_patches_indices = np.cumsum(video_num_patches)\n+            batch_size, num_frames, *_ = video_pixel_values.shape\n+            num_frames_per_video = np.full(batch_size, num_frames)\n+            num_frames = sum(num_frames_per_video)  # total\n+            video_patch_indices = np.empty(batch_size + 1, int)\n+            video_patch_indices[0] = 0\n+            video_patch_indices[1:] = np.cumsum(num_frames_per_video)\n+            video_num_patches = [1] * num_frames\n+            video_num_patches_indices = np.empty(num_frames + 1, int)\n+            video_num_patches_indices[0] = 0\n+            video_num_patches_indices[1:] = np.cumsum(video_num_patches)\n             video_pixel_values = video_pixel_values.flatten(0, 1)\n \n+        image_videos_inputs = {}\n         if images is not None or videos is not None:\n             text, image_video_patches, image_index, video_index = self._insert_media_placeholders(\n                 text,"
        },
        {
            "sha": "bbb4df973da65a25e378d94e384797fd85ee7fd7",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "modified",
            "additions": 35,
            "deletions": 6,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6a8e7debed69d4cf5d493158f3f347931c24c9a/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6a8e7debed69d4cf5d493158f3f347931c24c9a/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=e6a8e7debed69d4cf5d493158f3f347931c24c9a",
            "patch": "@@ -17,6 +17,8 @@\n import tempfile\n import unittest\n \n+from parameterized import parameterized\n+\n from transformers import AutoProcessor, AutoTokenizer, InternVLProcessor\n from transformers.testing_utils import require_av, require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n@@ -345,25 +347,30 @@ def _test_apply_chat_template(\n         for idx, url in enumerate(input_data[:batch_size]):\n             batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n \n+        num_frames = 2  # by default no more than 2 frames, otherwise too slow\n         out_dict = processor.apply_chat_template(\n             batch_messages,\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n             return_tensors=\"pt\",\n-            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n+            num_frames=num_frames,\n         )\n         self.assertTrue(self.videos_input_name in out_dict)\n         self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n         self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n \n         # InternVL internally collects frames from all the videos in a batch and flattens the batch dimension (B T C H W) -> (B*T C H W) then patches and removes the frames\n         # hence output length does not equal batch size\n-        # removed hardcoded video length check video_len = 2 if batch_size == 1 else 3\n-        # from experiment video_len looks like batch_size + 1\n-        # TODO: update expected video_len calculation based on the internal processing logic of InternVLProcessor\n-        output_len = batch_size + 1 if modality == \"video\" else batch_size\n-        self.assertEqual(len(out_dict[self.videos_input_name]), output_len)\n+        num_pixel_planes = 0  # i.e. images + video frames\n+        for message_thread in batch_messages:\n+            for message in message_thread:\n+                for content in message.get(\"content\", []):\n+                    if (content_type := content.get(\"type\")) == \"image\":\n+                        num_pixel_planes += 1\n+                    elif content_type == \"video\":\n+                        num_pixel_planes += num_frames\n+        self.assertEqual(len(out_dict[self.videos_input_name]), num_pixel_planes)\n         for k in out_dict:\n             self.assertIsInstance(out_dict[k], torch.Tensor)\n \n@@ -377,3 +384,25 @@ def _test_apply_chat_template(\n         continue_prompt = processor.apply_chat_template(batch_messages, continue_final_message=True, tokenize=False)\n         for prompt in continue_prompt:\n             self.assertTrue(prompt.endswith(\"It is the sound of\"))  # no `eos` token at the end\n+\n+    @parameterized.expand([(1,), (2,)])\n+    @require_torch\n+    def test_frames_binding(self, batch_size: int):\n+        texts = [\n+            \"<video>\\nAre there any cyan objects that enter the scene?\\nno\",\n+            \"<video>\\nAre there any red spheres that enter the scene?\\nno\",\n+        ]\n+        frames = torch.ones((4, 448, 448, 3), dtype=torch.float32)\n+        videos = [frames, frames]\n+\n+        processor = self.get_processor()\n+        inputs = processor(\n+            text=texts[:batch_size],\n+            return_tensors=\"pt\",\n+            videos=videos[:batch_size],\n+            videos_kwargs={\"size\": (448, 448)},\n+        )\n+\n+        actual_num_frames = inputs.pixel_values.shape[0]\n+        expected_num_frames = sum(x.shape[0] for x in videos[:batch_size])\n+        assert actual_num_frames == expected_num_frames"
        }
    ],
    "stats": {
        "total": 82,
        "additions": 60,
        "deletions": 22
    }
}