{
    "author": "stevhliu",
    "message": "[docs] TP blog post (#42637)\n\nblog link",
    "sha": "fccb04998af44166524c2a7cfe0d946b99aef497",
    "files": [
        {
            "sha": "7f835509ce758db4f1b9674896865efee6bdccfd",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fccb04998af44166524c2a7cfe0d946b99aef497/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fccb04998af44166524c2a7cfe0d946b99aef497/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=fccb04998af44166524c2a7cfe0d946b99aef497",
            "patch": "@@ -306,3 +306,7 @@ The most important part of DTensor is the `placement` attribute because it tells\n     ```\n \n - `Partial()` - Indicates a tensor is pending a reduction operation (not typically relevant for usage in Transformers).\n+\n+## Resources\n+\n+Read the [Tensor Parallelism (TP) in Transformers: 5 Minutes to Understand](https://huggingface.co/blog/qgallouedec/tp) blog post for a quick overview of tensor parallelism and learn how column and row parallel setups differ.\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}