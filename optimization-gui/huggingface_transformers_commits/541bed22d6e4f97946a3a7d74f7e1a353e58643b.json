{
    "author": "yonigozlan",
    "message": "Improve @auto_docstring doc and rename `args_doc.py` to `auto_docstring.py` (#39439)\n\n* rename `args_doc.py` to `auto_docstring.py` and improve doc\n\n* modifs after review",
    "sha": "541bed22d6e4f97946a3a7d74f7e1a353e58643b",
    "files": [
        {
            "sha": "0938d89ee495e906d9ffaee70ebb8ecab551e9fb",
            "filename": "docs/source/en/auto_docstring.md",
            "status": "modified",
            "additions": 34,
            "deletions": 7,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/541bed22d6e4f97946a3a7d74f7e1a353e58643b/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/541bed22d6e4f97946a3a7d74f7e1a353e58643b/docs%2Fsource%2Fen%2Fauto_docstring.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fauto_docstring.md?ref=541bed22d6e4f97946a3a7d74f7e1a353e58643b",
            "patch": "@@ -64,9 +64,9 @@ Arguments can also be passed directly to `@auto_docstring` for more control. Use\n     It builds upon the standard Transformer architecture with unique modifications.\"\"\",\n     custom_args=\"\"\"\n     custom_parameter (`type`, *optional*, defaults to `default_value`):\n-        A concise description for custom_parameter if not defined or overriding the description in `args_doc.py`.\n+        A concise description for custom_parameter if not defined or overriding the description in `auto_docstring.py`.\n     internal_helper_arg (`type`, *optional*, defaults to `default_value`):\n-        A concise description for internal_helper_arg if not defined or overriding the description in `args_doc.py`.\n+        A concise description for internal_helper_arg if not defined or overriding the description in `auto_docstring.py`.\n     \"\"\"\n )\n class MySpecialModel(PreTrainedModel):\n@@ -85,13 +85,40 @@ class MySpecialModel(PreTrainedModel):\n     def __init__(self, config: ConfigType, custom_parameter: \"type\" = \"default_value\", internal_helper_arg=None):\n         r\"\"\"\n         custom_parameter (`type`, *optional*, defaults to `default_value`):\n-            A concise description for custom_parameter if not defined or overriding the description in `args_doc.py`.\n+            A concise description for custom_parameter if not defined or overriding the description in `auto_docstring.py`.\n         internal_helper_arg (`type`, *optional*, defaults to `default_value`):\n-            A concise description for internal_helper_arg if not defined or overriding the description in `args_doc.py`.\n+            A concise description for internal_helper_arg if not defined or overriding the description in `auto_docstring.py`.\n         \"\"\"\n         # ...\n ```\n \n+You should also use the `@auto_docstring` decorator for classes that inherit from [`~utils.ModelOutput`].\n+\n+```python\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Custom model outputs with additional fields.\n+    \"\"\"\n+)\n+class MyModelOutput(ImageClassifierOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor`, *optional*):\n+        The loss of the model.\n+    custom_field (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*):\n+        A custom output field specific to this model.\n+    \"\"\"\n+\n+    # Standard fields like hidden_states, logits, attentions etc. can be automatically documented if the description is the same as the standard arguments.\n+    # However, given that the loss docstring is often different per model, you should document it in the docstring above.\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+    # Custom fields need to be documented in the docstring above\n+    custom_field: Optional[torch.FloatTensor] = None\n+```\n+\n </hfoption>\n <hfoption id=\"functions\">\n \n@@ -171,7 +198,7 @@ class MyModel(PreTrainedModel):\n \n There are some rules for documenting different types of arguments and they're listed below.\n \n-- Standard arguments (`input_ids`, `attention_mask`, `pixel_values`, etc.) are defined and retrieved from `args_doc.py`. It is the single source of truth for standard arguments and should not be redefined locally if an argument's description and shape is the same as an argument in `args_doc.py`.\n+- Standard arguments (`input_ids`, `attention_mask`, `pixel_values`, etc.) are defined and retrieved from `auto_docstring.py`. It is the single source of truth for standard arguments and should not be redefined locally if an argument's description and shape is the same as an argument in `auto_docstring.py`.\n \n     If a standard argument behaves differently in your model, then you can override it locally in a `r\"\"\" \"\"\"` block. This local definition has a higher priority. For example, the `labels` argument is often customized per model and typically requires overriding.\n \n@@ -245,15 +272,15 @@ When working with modular files (`modular_model.py`), follow the guidelines belo\n The `@auto_docstring` decorator automatically generates docstrings by:\n \n 1. Inspecting the signature (arguments, types, defaults) of the decorated class' `__init__` method or the decorated function.\n-2. Retrieving the predefined docstrings for common arguments (`input_ids`, `attention_mask`, etc.) from internal library sources like [`ModelArgs`], [`ImageProcessorArgs`], and the `args_doc.py` file.\n+2. Retrieving the predefined docstrings for common arguments (`input_ids`, `attention_mask`, etc.) from internal library sources like [`ModelArgs`], [`ImageProcessorArgs`], and the `auto_docstring.py` file.\n 3. Adding argument descriptions in one of two ways as shown below.\n \n     | method | description | usage |\n     |---|---|---|\n     | `r\"\"\" \"\"\"` | add custom docstring content directly to a method signature or within the `__init__` docstring | document new arguments or override standard descriptions |\n     | `custom_args` | add custom docstrings for specific arguments directly in `@auto_docstring` | define docstring for new arguments once if they're repeated in multiple places in the modeling file |\n \n-4. Adding class and function descriptions. For model classes with standard naming patterns, like `ModelForCausalLM`, or if it belongs to a pipeline, `@auto_docstring` automatically generates the appropriate descriptions with `ClassDocstring` from `args_doc.py`.\n+4. Adding class and function descriptions. For model classes with standard naming patterns, like `ModelForCausalLM`, or if it belongs to a pipeline, `@auto_docstring` automatically generates the appropriate descriptions with `ClassDocstring` from `auto_docstring.py`.\n \n     `@auto_docstring` also accepts the `custom_intro` argument to describe a class or function.\n "
        },
        {
            "sha": "1e212b5fa4d8411f5c61f601eaf668e76ea309ba",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/541bed22d6e4f97946a3a7d74f7e1a353e58643b/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/541bed22d6e4f97946a3a7d74f7e1a353e58643b/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=541bed22d6e4f97946a3a7d74f7e1a353e58643b",
            "patch": "@@ -21,7 +21,7 @@\n from packaging import version\n \n from .. import __version__\n-from .args_doc import (\n+from .auto_docstring import (\n     ClassAttrs,\n     ClassDocstring,\n     ImageProcessorArgs,"
        },
        {
            "sha": "c0f154baae06396609bae666cf37688b06586be0",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "renamed",
            "additions": 143,
            "deletions": 20,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/541bed22d6e4f97946a3a7d74f7e1a353e58643b/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/541bed22d6e4f97946a3a7d74f7e1a353e58643b/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=541bed22d6e4f97946a3a7d74f7e1a353e58643b",
            "patch": "@@ -1271,7 +1271,7 @@ def _get_model_info(func, parent_class):\n             else:\n                 config_class = \"ModelConfig\"\n                 print(\n-                    f\"ðŸš¨ Config not found for {model_name_lowercase}. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/args_doc.py\"\n+                    f\"ðŸš¨ Config not found for {model_name_lowercase}. You can manually add it to HARDCODED_CONFIG_FOR_MODELS in utils/auto_docstring.py\"\n                 )\n \n     return model_name_lowercase, class_name, config_class\n@@ -1893,27 +1893,150 @@ def auto_class_docstring(cls, custom_intro=None, custom_args=None, checkpoint=No\n \n \n def auto_docstring(obj=None, *, custom_intro=None, custom_args=None, checkpoint=None):\n-    \"\"\"\n-    Automatically generates docstrings for classes and methods in the Transformers library.\n-\n-    This decorator can be used in the following forms:\n-    @auto_docstring\n-    def my_function(...):\n-        ...\n-    or\n-    @auto_docstring()\n-    def my_function(...):\n-        ...\n-    or\n-    @auto_docstring(custom_intro=\"Custom intro\", ...)\n-    def my_function(...):\n-        ...\n+    r\"\"\"\n+    Automatically generates comprehensive docstrings for model classes and methods in the Transformers library.\n+\n+    This decorator reduces boilerplate by automatically including standard argument descriptions while allowing\n+    overrides to add new or custom arguments. It inspects function signatures, retrieves predefined docstrings\n+    for common arguments (like `input_ids`, `attention_mask`, etc.), and generates complete documentation\n+    including examples and return value descriptions.\n+\n+    For complete documentation and examples, read this [guide](https://huggingface.co/docs/transformers/auto_docstring).\n+\n+    Examples of usage:\n+\n+        Basic usage (no parameters):\n+        ```python\n+        @auto_docstring\n+        class MyAwesomeModel(PreTrainedModel):\n+            def __init__(self, config, custom_parameter: int = 10):\n+                r'''\n+                custom_parameter (`int`, *optional*, defaults to 10):\n+                    Description of the custom parameter for MyAwesomeModel.\n+                '''\n+                super().__init__(config)\n+                self.custom_parameter = custom_parameter\n+        ```\n+\n+        Using `custom_intro` with a class:\n+        ```python\n+        @auto_docstring(\n+            custom_intro=\"This model implements a novel attention mechanism for improved performance.\"\n+        )\n+        class MySpecialModel(PreTrainedModel):\n+            def __init__(self, config, attention_type: str = \"standard\"):\n+                r'''\n+                attention_type (`str`, *optional*, defaults to \"standard\"):\n+                    Type of attention mechanism to use.\n+                '''\n+                super().__init__(config)\n+        ```\n+\n+        Using `custom_intro` with a method, and specify custom arguments and example directly in the docstring:\n+        ```python\n+        @auto_docstring(\n+            custom_intro=\"Performs forward pass with enhanced attention computation.\"\n+        )\n+        def forward(\n+            self,\n+            input_ids: Optional[torch.Tensor] = None,\n+            attention_mask: Optional[torch.Tensor] = None,\n+        ):\n+            r'''\n+            custom_parameter (`int`, *optional*, defaults to 10):\n+                Description of the custom parameter for MyAwesomeModel.\n+\n+            Example:\n+\n+            ```python\n+            >>> model = MyAwesomeModel(config)\n+            >>> model.forward(input_ids=torch.tensor([1, 2, 3]), attention_mask=torch.tensor([1, 1, 1]))\n+            ```\n+            '''\n+        ```\n+\n+        Using `custom_args` to define reusable arguments:\n+        ```python\n+        VISION_ARGS = r'''\n+        pixel_values (`torch.FloatTensor`, *optional*):\n+            Pixel values of the input images.\n+        image_features (`torch.FloatTensor`, *optional*):\n+            Pre-computed image features for efficient processing.\n+        '''\n+\n+        @auto_docstring(custom_args=VISION_ARGS)\n+        def encode_images(self, pixel_values=None, image_features=None):\n+            # ... method implementation\n+        ```\n+\n+        Combining `custom_intro` and `custom_args`:\n+        ```python\n+        MULTIMODAL_ARGS = r'''\n+        vision_features (`torch.FloatTensor`, *optional*):\n+            Pre-extracted vision features from the vision encoder.\n+        fusion_strategy (`str`, *optional*, defaults to \"concat\"):\n+            Strategy for fusing text and vision modalities.\n+        '''\n+\n+        @auto_docstring(\n+            custom_intro=\"Processes multimodal inputs combining text and vision.\",\n+            custom_args=MULTIMODAL_ARGS\n+        )\n+        def forward(\n+            self,\n+            input_ids,\n+            attention_mask=None,\n+            vision_features=None,\n+            fusion_strategy=\"concat\"\n+        ):\n+            # ... multimodal processing\n+        ```\n+\n+        Using with ModelOutput classes:\n+        ```python\n+        @dataclass\n+        @auto_docstring(\n+            custom_intro=\"Custom model outputs with additional fields.\"\n+        )\n+        class MyModelOutput(ImageClassifierOutput):\n+            r'''\n+            loss (`torch.FloatTensor`, *optional*):\n+                The loss of the model.\n+            custom_field (`torch.FloatTensor` of shape `(batch_size, hidden_size)`, *optional*):\n+                A custom output field specific to this model.\n+            '''\n+\n+            # Standard fields like hidden_states, logits, attentions etc. can be automatically documented\n+            # However, given that the loss docstring is often different per model, you should document it above\n+            loss: Optional[torch.FloatTensor] = None\n+            logits: Optional[torch.FloatTensor] = None\n+            hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+            attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+            custom_field: Optional[torch.FloatTensor] = None\n+        ```\n \n     Args:\n-        custom_intro (str, optional): Custom introduction text to add to the docstring. This will replace the default\n-            introduction text generated by the decorator before the Args section.\n-        checkpoint (str, optional): Checkpoint name to use in the docstring. This should be automatically inferred from the\n-            model configuration class, but can be overridden if needed.\n+        custom_intro (`str`, *optional*):\n+            Custom introduction text to add to the docstring. This replaces the default\n+            introduction text generated by the decorator before the Args section. Use this to describe what\n+            makes your model or method special.\n+        custom_args (`str`, *optional*):\n+            Custom argument documentation in docstring format. This allows you to define\n+            argument descriptions once and reuse them across multiple methods. The format should follow the\n+            standard docstring convention: `arg_name (`type`, *optional*, defaults to `value`): Description.`\n+        checkpoint (`str`, *optional*):\n+            Checkpoint name to use in examples within the docstring. This is typically\n+            automatically inferred from the model configuration class, but can be overridden if needed for\n+            custom examples.\n+\n+    Note:\n+        - Standard arguments (`input_ids`, `attention_mask`, `pixel_values`, etc.) are automatically documented\n+          from predefined descriptions and should not be redefined unless their behavior differs in your model.\n+        - New or custom arguments should be documented in the method's docstring using the `r''' '''` block\n+          or passed via the `custom_args` parameter.\n+        - For model classes, the decorator derives parameter descriptions from the `__init__` method's signature\n+          and docstring.\n+        - Return value documentation is automatically generated for methods that return ModelOutput subclasses.\n     \"\"\"\n \n     def auto_docstring_decorator(obj):",
            "previous_filename": "src/transformers/utils/args_doc.py"
        },
        {
            "sha": "cd5f4953552ff858c88b84d2501d7cff523792d4",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/541bed22d6e4f97946a3a7d74f7e1a353e58643b/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/541bed22d6e4f97946a3a7d74f7e1a353e58643b/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=541bed22d6e4f97946a3a7d74f7e1a353e58643b",
            "patch": "@@ -49,7 +49,7 @@\n from git import Repo\n \n from transformers.utils import direct_transformers_import\n-from transformers.utils.args_doc import (\n+from transformers.utils.auto_docstring import (\n     ImageProcessorArgs,\n     ModelArgs,\n     ModelOutputArgs,\n@@ -1487,7 +1487,7 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n         if docstring_args_ro_remove_warnings:\n             if not overwrite:\n                 print(\n-                    \"Some docstrings are redundant with the ones in `args_doc.py` and will be removed. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to remove the redundant docstrings.\"\n+                    \"Some docstrings are redundant with the ones in `auto_docstring.py` and will be removed. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to remove the redundant docstrings.\"\n                 )\n             print(f\"ðŸš¨ Redundant docstring for the following arguments in {candidate_file}:\")\n             for warning in docstring_args_ro_remove_warnings:"
        }
    ],
    "stats": {
        "total": 210,
        "additions": 180,
        "deletions": 30
    }
}