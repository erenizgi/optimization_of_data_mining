{
    "author": "eustlb",
    "message": "[Moonshine] compute head_dim_padding at init (#35984)\n\ncompute head_dim_padding at init",
    "sha": "e6f4a4ebbf970c12fe475be79a039f943c28f975",
    "files": [
        {
            "sha": "d82f715fbd54b4e7151a7a7aa4caa39e38525014",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 14,
            "deletions": 19,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f4a4ebbf970c12fe475be79a039f943c28f975/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f4a4ebbf970c12fe475be79a039f943c28f975/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=e6f4a4ebbf970c12fe475be79a039f943c28f975",
            "patch": "@@ -18,7 +18,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import math\n from typing import Callable, Optional, Tuple, Union\n \n import numpy as np\n@@ -207,6 +206,14 @@ def __init__(\n         )\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n+        # Pad head dimension to the next specified multiple.\n+        if self.config.pad_head_dim_to_multiple_of is not None:\n+            target_multiple = self.config.pad_head_dim_to_multiple_of\n+            target_head_dim = target_multiple * ((self.head_dim + target_multiple - 1) // target_multiple)\n+            self.head_dim_padding = target_head_dim - self.head_dim\n+        else:\n+            self.head_dim_padding = 0\n+\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -276,21 +283,10 @@ def forward(\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n \n-        # Pad head size dimension to next specified multiple. Q K and V always have equal head sizes.\n-        head_dim_padding = 0\n-        if self.config.pad_head_dim_to_multiple_of is not None:\n-            head_dim = query_states.shape[-1]\n-            target_multiple = self.config.pad_head_dim_to_multiple_of\n-            target_head_dim = target_multiple * ((head_dim + target_multiple - 1) // target_multiple)\n-            head_dim_padding = target_head_dim - head_dim\n-            if head_dim_padding > 0:\n-                # Ensure scaling is correct even with padding.\n-                if self.scaling is None:\n-                    self.scaling = 1.0 / math.sqrt(query_states.shape[-1])\n-\n-                query_states = torch.nn.functional.pad(query_states, (0, head_dim_padding))\n-                key_states = torch.nn.functional.pad(key_states, (0, head_dim_padding))\n-                value_states = torch.nn.functional.pad(value_states, (0, head_dim_padding))\n+        if self.head_dim_padding > 0:\n+            query_states = torch.nn.functional.pad(query_states, (0, self.head_dim_padding))\n+            key_states = torch.nn.functional.pad(key_states, (0, self.head_dim_padding))\n+            value_states = torch.nn.functional.pad(value_states, (0, self.head_dim_padding))\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -304,9 +300,8 @@ def forward(\n             **kwargs,\n         )\n \n-        # Remove head size padding.\n-        if head_dim_padding > 0:\n-            attn_output = attn_output[:, :, :, :-head_dim_padding]\n+        if self.head_dim_padding > 0:\n+            attn_output = attn_output[..., : -self.head_dim_padding]\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)"
        },
        {
            "sha": "24fa4f0a1ef82f001d3068e8f7d11a03959e836b",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 15,
            "deletions": 19,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6f4a4ebbf970c12fe475be79a039f943c28f975/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6f4a4ebbf970c12fe475be79a039f943c28f975/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=e6f4a4ebbf970c12fe475be79a039f943c28f975",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import math\n from typing import Callable, Optional, Tuple, Union\n \n import torch\n@@ -302,6 +301,15 @@ def __init__(\n         config.update({\"num_attention_heads\": num_attention_heads, \"num_key_value_heads\": num_key_value_heads})\n         super().__init__(config, layer_idx)\n         self.is_causal = is_causal\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+\n+        # Pad head dimension to the next specified multiple.\n+        if self.config.pad_head_dim_to_multiple_of is not None:\n+            target_multiple = self.config.pad_head_dim_to_multiple_of\n+            target_head_dim = target_multiple * ((self.head_dim + target_multiple - 1) // target_multiple)\n+            self.head_dim_padding = target_head_dim - self.head_dim\n+        else:\n+            self.head_dim_padding = 0\n \n     def forward(\n         self,\n@@ -372,21 +380,10 @@ def forward(\n \n         is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n \n-        # Pad head size dimension to next specified multiple. Q K and V always have equal head sizes.\n-        head_dim_padding = 0\n-        if self.config.pad_head_dim_to_multiple_of is not None:\n-            head_dim = query_states.shape[-1]\n-            target_multiple = self.config.pad_head_dim_to_multiple_of\n-            target_head_dim = target_multiple * ((head_dim + target_multiple - 1) // target_multiple)\n-            head_dim_padding = target_head_dim - head_dim\n-            if head_dim_padding > 0:\n-                # Ensure scaling is correct even with padding.\n-                if self.scaling is None:\n-                    self.scaling = 1.0 / math.sqrt(query_states.shape[-1])\n-\n-                query_states = torch.nn.functional.pad(query_states, (0, head_dim_padding))\n-                key_states = torch.nn.functional.pad(key_states, (0, head_dim_padding))\n-                value_states = torch.nn.functional.pad(value_states, (0, head_dim_padding))\n+        if self.head_dim_padding > 0:\n+            query_states = torch.nn.functional.pad(query_states, (0, self.head_dim_padding))\n+            key_states = torch.nn.functional.pad(key_states, (0, self.head_dim_padding))\n+            value_states = torch.nn.functional.pad(value_states, (0, self.head_dim_padding))\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -400,9 +397,8 @@ def forward(\n             **kwargs,\n         )\n \n-        # Remove head size padding.\n-        if head_dim_padding > 0:\n-            attn_output = attn_output[:, :, :, :-head_dim_padding]\n+        if self.head_dim_padding > 0:\n+            attn_output = attn_output[..., : -self.head_dim_padding]\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)"
        }
    ],
    "stats": {
        "total": 67,
        "additions": 29,
        "deletions": 38
    }
}