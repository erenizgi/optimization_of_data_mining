{
    "author": "zucchini-nlp",
    "message": "Fix generation config validation (#43175)\n\n* fix\n\n* extend the test case by trying to save the model",
    "sha": "e5a860837f8a4f577b93e35677584906860bb6eb",
    "files": [
        {
            "sha": "f72788c5e1450e8b3a1defc88be596a36e25474b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a860837f8a4f577b93e35677584906860bb6eb/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a860837f8a4f577b93e35677584906860bb6eb/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=e5a860837f8a4f577b93e35677584906860bb6eb",
            "patch": "@@ -687,7 +687,7 @@ def validate(self, strict=False):\n                 )\n \n         # 2.5. check cache-related arguments\n-        if self.use_cache is not True:\n+        if self.use_cache is False:\n             # In this case, all cache-related arguments should be unset. However, since `use_cache=False` is often used\n             # passed to `generate` directly to hot-fix cache issues, let's raise a warning instead of an error\n             # (otherwise a user might need to overwrite several parameters)."
        },
        {
            "sha": "57abb5ce01d75096fd19a0b8b039bf6e6ccfa5bb",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e5a860837f8a4f577b93e35677584906860bb6eb/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e5a860837f8a4f577b93e35677584906860bb6eb/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e5a860837f8a4f577b93e35677584906860bb6eb",
            "patch": "@@ -2686,6 +2686,13 @@ def test_generation_config_defaults(self):\n         out = model.generate(input_ids, generation_config=generation_config)\n         self.assertTrue(len(out[0]) == 20)  # generated max_length=20 tokens, not 50!\n \n+        # Lastly try saving to make sure no errors are raised about\n+        # \"generation params in config\" or during config validation (#43175)\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            model.generation_config.cache_implementation = \"dynamic\"\n+            model.generation_config.use_cache = None\n+            model.save_pretrained(tmpdirname)\n+\n     # TODO joao, manuel: remove in v4.62.0\n     @slow\n     def test_diverse_beam_search(self):"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}