{
    "author": "ydshieh",
    "message": "Update cohere2 vision test (#39888)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "ee7eb2d0b11a7db5fb9003ba410aa14c7c791923",
    "files": [
        {
            "sha": "73f7c10dee9381b82be3d3bab2397be0cc2e8358",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 30,
            "deletions": 51,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee7eb2d0b11a7db5fb9003ba410aa14c7c791923/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee7eb2d0b11a7db5fb9003ba410aa14c7c791923/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=ee7eb2d0b11a7db5fb9003ba410aa14c7c791923",
            "patch": "@@ -180,40 +180,35 @@ def test_initialization(self):\n @require_read_token\n @require_torch\n class Cohere2IntegrationTest(unittest.TestCase):\n-    @classmethod\n-    def setUpClass(cls):\n-        cls.model_checkpoint = \"CohereLabs/command-a-vision-07-2025\"\n-        cls.model = None\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-        del cls.model\n-        cleanup(torch_device, gc_collect=True)\n+    def setUp(self):\n+        self.model_checkpoint = \"CohereLabs/command-a-vision-07-2025\"\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n-    @classmethod\n-    def get_model(cls):\n-        # Use 4-bit on T4\n+    def get_model(self, dummy=True):\n         device_type, major, _ = get_device_properties()\n-        load_in_4bit = (device_type == \"cuda\") and (major < 8)\n-        torch_dtype = None if load_in_4bit else torch.float16\n-\n-        if cls.model is None:\n-            cls.model = Cohere2VisionForConditionalGeneration.from_pretrained(\n-                cls.model_checkpoint,\n-                device_map=\"auto\",\n-                torch_dtype=torch_dtype,\n-                load_in_4bit=load_in_4bit,\n-            )\n-        return cls.model\n+        torch_dtype = torch.float16\n+\n+        # too large to fit into A10\n+        config = Cohere2VisionConfig.from_pretrained(self.model_checkpoint)\n+        if dummy:\n+            config.text_config.num_hidden_layers = 4\n+            config.text_config.layer_types = config.text_config.layer_types[:4]\n+\n+        model = Cohere2VisionForConditionalGeneration.from_pretrained(\n+            self.model_checkpoint,\n+            config=config,\n+            torch_dtype=torch_dtype,\n+            device_map=\"auto\",\n+        )\n+        return model\n \n     @slow\n     @require_torch_accelerator\n     def test_model_integration_forward(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = self.get_model()\n+        model = self.get_model(dummy=False)\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -269,17 +264,14 @@ def test_model_integration_generate_text_only(self):\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         ).to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n-            generate_ids = model.generate(**inputs, max_new_tokens=25, do_sample=False)\n+            generate_ids = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Whispers on the breeze,\\nLeaves dance under moonlit skies,\\nNature's quiet song.\",\n-                # 4-bit\n-                (\"cuda\", 7): \"Sure, here's a haiku for you:\\n\\nMorning dew sparkles,\\nPetals unfold in sunlight,\\n\",\n-                (\"cuda\", 8): \"**Haiku**\\n\\n*Softly falls the snow*\\n*Blanketing the earth in white*\\n*\",\n+                (\"cuda\", 8): \"<|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|>\",\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -306,17 +298,14 @@ def test_model_integration_generate_chat_template(self):\n             messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         ).to(torch_device, dtype=torch.float16)\n         with torch.no_grad():\n-            generate_ids = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+            generate_ids = model.generate(**inputs, max_new_tokens=10, do_sample=False)\n             decoded_output = processor.decode(\n                 generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True\n             )\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): 'The image depicts a cozy scene of two cats resting on a bright pink blanket. The cats,',\n-                # 4-bit\n-                (\"cuda\", 7): 'The image depicts two cats comfortably resting on a pink blanket spread across a sofa. The cats,',\n-                (\"cuda\", 8): 'The image depicts two cats lying on a bright pink blanket that covers a red couch. The cat',\n+                (\"cuda\", 8): '<|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|>',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -327,7 +316,7 @@ def test_model_integration_generate_chat_template(self):\n     @require_torch_accelerator\n     def test_model_integration_batched_generate(self):\n         processor = AutoProcessor.from_pretrained(self.model_checkpoint)\n-        model = self.get_model()\n+        model = self.get_model(dummy=False)\n         # Prepare inputs\n         messages = [\n             [\n@@ -353,16 +342,13 @@ def test_model_integration_batched_generate(self):\n             messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         ).to(model.device, dtype=torch.float16)\n \n-        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=5)\n \n         # Check first output\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n-                # 4-bit\n-                (\"cuda\", 7): \"Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene\",\n-                (\"cuda\", 8): 'Dock stretches to calm,  \\nMountains whisper through the trees,  \\nLake mirrors the sky.',\n+                (\"cuda\", 8): 'Dock stretches to calm',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -378,10 +364,7 @@ def test_model_integration_batched_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',\n-                # 4-bit\n-                (\"cuda\", 7): 'This vibrant image captures a bustling street scene in a multicultural urban area, featuring a traditional Chinese gate adorned with intricate red and',\n-                (\"cuda\", 8): 'The image depicts a vibrant street scene in what appears to be a Chinatown district, likely in an urban area. The focal',\n+                (\"cuda\", 8): 'The image depicts a',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -432,16 +415,14 @@ def test_model_integration_batched_generate_multi_image(self):\n         inputs = processor.apply_chat_template(\n             messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\"\n         ).to(model.device, dtype=torch.float16)\n-        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n \n         # Check first output\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         # Batching seems to alter the output slightly, but it is also the case in the original implementation. This seems to be expected: https://github.com/huggingface/transformers/issues/23017#issuecomment-1649630232\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n-                (\"cuda\", 7): 'Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene',\n-                (\"cuda\", 8): 'Dock stretches to calm,  \\nMountains whisper through the trees,  \\nLake mirrors the sky.',\n+                (\"cuda\", 8): '<|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|>',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -456,9 +437,7 @@ def test_model_integration_batched_generate_multi_image(self):\n         decoded_output = processor.decode(output[1, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"The first image showcases the Statue of Liberty, a colossal neoclassical sculpture on Liberty Island in New York Harbor. Standing at \",\n-                (\"cuda\", 7): 'The first image showcases the Statue of Liberty, a monumental sculpture located on Liberty Island in New York Harbor. Standing atop a',\n-                (\"cuda\", 8): 'The two landmarks depicted in the images are the Statue of Liberty and the Golden Gate Bridge. \\n\\n1. **Statue',\n+                (\"cuda\", 8): '<|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|><|CHATBOT_TOKEN|>',\n             }\n         )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 30,
        "deletions": 51
    }
}