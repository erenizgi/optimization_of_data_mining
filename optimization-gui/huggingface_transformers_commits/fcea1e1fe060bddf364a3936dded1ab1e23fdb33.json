{
    "author": "i3hz",
    "message": "Fixes Flash Attention implementation for models  (#42149)\n\n* flash-att3 fix for smolvlm2\n\n* flash-att3 fix for idefics2\n\n* idefics2 changes\n\n* reset idefics2",
    "sha": "fcea1e1fe060bddf364a3936dded1ab1e23fdb33",
    "files": [
        {
            "sha": "49b4404feb3cae2a9fcf9f23dc36a81c4d9b848e",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcea1e1fe060bddf364a3936dded1ab1e23fdb33/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcea1e1fe060bddf364a3936dded1ab1e23fdb33/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=fcea1e1fe060bddf364a3936dded1ab1e23fdb33",
            "patch": "@@ -24,7 +24,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n@@ -506,13 +506,12 @@ def forward(\n         hidden_states = self.embeddings(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n \n         patch_attention_mask = patch_attention_mask.view(batch_size, -1)\n-        # The call to `_upad_input` in `_flash_attention_forward` is expensive\n-        # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n-        # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if self.config._attn_implementation != \"flash_attention_2\":\n-            patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n-        elif not torch.any(~patch_attention_mask):\n-            patch_attention_mask = None\n+        # Create the correct attention mask based on the attention implementation\n+        patch_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=patch_attention_mask,\n+        )\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,"
        },
        {
            "sha": "d6b6c79eda6a445eaa95ca29b1fd1cf5856eb780",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcea1e1fe060bddf364a3936dded1ab1e23fdb33/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcea1e1fe060bddf364a3936dded1ab1e23fdb33/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=fcea1e1fe060bddf364a3936dded1ab1e23fdb33",
            "patch": "@@ -29,7 +29,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationConfig, GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...masking_utils import create_bidirectional_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n@@ -396,13 +396,12 @@ def forward(\n         hidden_states = self.embeddings(pixel_values=pixel_values, patch_attention_mask=patch_attention_mask)\n \n         patch_attention_mask = patch_attention_mask.view(batch_size, -1)\n-        # The call to `_upad_input` in `_flash_attention_forward` is expensive\n-        # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n-        # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if self.config._attn_implementation != \"flash_attention_2\":\n-            patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n-        elif not torch.any(~patch_attention_mask):\n-            patch_attention_mask = None\n+        # Create the correct attention mask based on the attention implementation\n+        patch_attention_mask = create_bidirectional_mask(\n+            config=self.config,\n+            input_embeds=hidden_states,\n+            attention_mask=patch_attention_mask,\n+        )\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 14,
        "deletions": 16
    }
}