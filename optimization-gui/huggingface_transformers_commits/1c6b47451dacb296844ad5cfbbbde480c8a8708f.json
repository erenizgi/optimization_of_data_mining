{
    "author": "zucchini-nlp",
    "message": "Fix cache-related tests (#39676)\n\n* fix\n\n* fix kyutai at last\n\n* fix unrelated tests and copies\n\n* update musicgen as well\n\n* revert tensor\n\n* fix old test failures\n\n* why it wasn't added?",
    "sha": "1c6b47451dacb296844ad5cfbbbde480c8a8708f",
    "files": [
        {
            "sha": "509adaa8e2fec94cc696685f4388e8b3e66160c2",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -2055,7 +2055,7 @@ def _prepare_cache_for_generation(\n             generation_config.cache_implementation = None\n \n         generation_config.cache_implementation = generation_config.cache_implementation or getattr(\n-            self.config.get_text_config(), \"cache_implementation\", None\n+            self.config.get_text_config(decoder=True), \"cache_implementation\", None\n         )\n         if generation_config.cache_implementation is not None:\n             if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:"
        },
        {
            "sha": "bf0b55d5a19cf7fbb0cf4f9c4905e89998e34bf4",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -1215,12 +1215,15 @@ def _prepare_model_inputs(\n         cache_methods = [\n             \"_prepare_cache_for_generation\",\n             \"_get_cache\",\n-            \"_supports_default_dynamic_cache\",\n             \"_get_layer_device_map_for_cache_init\",\n         ]\n         for method in cache_methods:\n             setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n \n+        setattr(\n+            self.codec_model, \"_supports_default_dynamic_cache\", types.MethodType(lambda x: True, self.codec_model)\n+        )\n+\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,"
        },
        {
            "sha": "e0e424ac605e8ef18d9527f69eb1521f5605842a",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -344,12 +344,15 @@ def _prepare_model_inputs(\n         cache_methods = [\n             \"_prepare_cache_for_generation\",\n             \"_get_cache\",\n-            \"_supports_default_dynamic_cache\",\n             \"_get_layer_device_map_for_cache_init\",\n         ]\n         for method in cache_methods:\n             setattr(self.codec_model, method, types.MethodType(getattr(self, method).__func__, self.codec_model))\n \n+        setattr(\n+            self.codec_model, \"_supports_default_dynamic_cache\", types.MethodType(lambda x: True, self.codec_model)\n+        )\n+\n         self.codec_model._prepare_cache_for_generation(\n             generation_config=self.codec_model.generation_config,\n             model_kwargs=temporary_model_kwargs,"
        },
        {
            "sha": "91c505c636d3dbd85670e3a0f6daa16980ae11cc",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -1246,7 +1246,29 @@ def generate(\n             input_ids_length=input_ids_length,\n         )\n \n-        # 6. Prepare `input_ids` which will be used for auto-regressive generation\n+        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n+\n+        # 6. Prepare the cache.\n+        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n+        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n+        # - `max_length`, prepared above, is used to determine the maximum cache length\n+        max_cache_length = generation_config.max_length - 1\n+        if (\n+            input_ids_length.shape[1] != input_ids_length\n+            and model_input_name == \"inputs_embeds\"\n+            and not self.config.is_encoder_decoder\n+        ):\n+            max_cache_length += input_ids_length.shape[1]\n+        self._prepare_cache_for_generation(\n+            generation_config,\n+            model_kwargs,\n+            assistant_model=None,\n+            batch_size=batch_size,\n+            max_cache_length=max_cache_length,\n+            device=input_ids_length.device,\n+        )\n+\n+        # 7. Prepare `input_ids` which will be used for auto-regressive generation\n         # Build the delay pattern mask for offsetting each codebook prediction by 1 (this behaviour is specific to MusicGen)\n         input_ids, delay_pattern_mask = self.build_delay_pattern_mask(\n             input_ids,\n@@ -1260,15 +1282,15 @@ def generate(\n         # stash the delay mask so that we don't have to recompute it in each forward pass\n         model_kwargs[\"delay_pattern_mask\"] = delay_pattern_mask\n \n-        # 7. determine generation mode\n+        # 8. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n+        # 9. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None\n \n-        # 9. prepare distribution pre_processing samplers\n+        # 10. prepare distribution pre_processing samplers\n         logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,"
        },
        {
            "sha": "23c5314c544746734f411db3f80a01ba87a43441",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 25,
            "deletions": 3,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -2162,6 +2162,28 @@ def generate(\n             input_ids_length=input_ids_length,\n         )\n \n+        self._validate_generated_length(generation_config, input_ids_length, has_default_max_length)\n+\n+        # 7. Prepare the cache.\n+        # - `model_kwargs` may be updated in place with a cache as defined by the parameters in `generation_config`.\n+        # - different models have a different cache name expected by the model (default = \"past_key_values\")\n+        # - `max_length`, prepared above, is used to determine the maximum cache length\n+        max_cache_length = generation_config.max_length - 1\n+        if (\n+            inputs_tensor.shape[1] != input_ids_length\n+            and model_input_name == \"inputs_embeds\"\n+            and not self.config.is_encoder_decoder\n+        ):\n+            max_cache_length += inputs_tensor.shape[1]\n+        self._prepare_cache_for_generation(\n+            generation_config,\n+            model_kwargs,\n+            assistant_model=None,\n+            batch_size=batch_size,\n+            max_cache_length=max_cache_length,\n+            device=inputs_tensor.device,\n+        )\n+\n         # build the delay pattern mask for offsetting each codebook prediction by 1 (this behaviour is specific to MusicGen)\n         input_ids, decoder_delay_pattern_mask = self.decoder.build_delay_pattern_mask(\n             input_ids,\n@@ -2175,15 +2197,15 @@ def generate(\n         if streamer is not None:\n             streamer.put(input_ids.cpu())\n \n-        # 7. determine generation mode\n+        # 8. determine generation mode\n         generation_mode = generation_config.get_generation_mode()\n \n-        # 8. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n+        # 9. prepare batched CFG externally (to enable coexistence with the unbatched CFG)\n         if generation_config.guidance_scale is not None and generation_config.guidance_scale > 1:\n             logits_processor.append(ClassifierFreeGuidanceLogitsProcessor(generation_config.guidance_scale))\n             generation_config.guidance_scale = None\n \n-        # 9. prepare distribution pre_processing samplers\n+        # 10. prepare distribution pre_processing samplers\n         logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,"
        },
        {
            "sha": "ffc1f60c8fe12b7d201103f3bd6648f0dc3e4cd1",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -1204,8 +1204,6 @@ def _reorder_stacked(hidden_states, new_order):\n         if isinstance(past_key_values, EncoderDecoderCache):\n             reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n \n-        if isinstance(past_key_values, EncoderDecoderCache):\n-            reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n         return reordered_past\n \n     def marginalize(self, seq_logits, doc_scores, n_docs=None):\n@@ -1593,13 +1591,6 @@ def extend_enc_output(tensor, num_beams=None):\n             if generation_config.num_return_sequences > generation_config.num_beams:\n                 raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n \n-            # 11. interleave input_ids with `num_beams` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_beams,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n             return self._beam_search(\n                 input_ids,\n                 logits_processor=pre_processor,"
        },
        {
            "sha": "b880496aa0cf772e9881d7ef1059a9b0b7265b5d",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -261,6 +261,17 @@ def forward(\n                 .transpose(1, 2)\n             )\n \n+            # Apply RoPE if self attention\n+            if not is_cross_attention and sinusoidal_pos is not None:\n+                if self.rotary_value:\n+                    query_layer, key_layer, value_layer = self.apply_rotary_position_embeddings(\n+                        sinusoidal_pos, query_layer, key_layer, value_layer\n+                    )\n+                else:\n+                    query_layer, key_layer = self.apply_rotary_position_embeddings(\n+                        sinusoidal_pos, query_layer, key_layer\n+                    )\n+\n             if past_key_value is not None:\n                 # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n                 cache_position = cache_position if not is_cross_attention else None\n@@ -381,13 +392,13 @@ def forward(\n     ):\n         self_outputs = self.self(\n             hidden_states,\n-            attention_mask,\n-            sinusoidal_pos,\n-            head_mask,\n-            encoder_hidden_states,\n-            past_key_value,\n-            output_attentions,\n-            cache_position,\n+            attention_mask=attention_mask,\n+            sinusoidal_pos=sinusoidal_pos,\n+            head_mask=head_mask,\n+            encoder_hidden_states=encoder_hidden_states,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(self_outputs[0], hidden_states)\n         outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them"
        },
        {
            "sha": "3286b8912cd93ab8eee86f9d8df4d3c1f9c89a47",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -274,7 +274,7 @@ def forward(\n         # such that the encoder's padding tokens are not attended to.\n         is_cross_attention = encoder_hidden_states is not None\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n-        attention_mask = encoder_attention_mask if is_cross_attention else encoder_attention_mask\n+        attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n \n         batch_size = hidden_states.shape[0]\n         key_layer = ("
        },
        {
            "sha": "0c5c771b55c90f95693bb379bb03dd630ca232c5",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -515,7 +515,7 @@ def test_small_model_integration_test_full_vision_state_selection(self):\n         # test that changing `strategy` won't error out\n         model.vision_feature_select_strategy = \"full\"\n \n-        inputs = self.processor(self.prompt, self.image, return_tensors=\"pt\").to(model.device)\n+        inputs = self.processor(text=self.prompt, images=self.image, return_tensors=\"pt\").to(model.device)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=30)\n@@ -536,7 +536,7 @@ def test_granite_vision(self):\n         model = LlavaNextForConditionalGeneration.from_pretrained(granite_model_path)\n         self.processor = AutoProcessor.from_pretrained(granite_model_path)\n         prompt = \"<|user|>\\n<image>\\nWhat is shown in this image?\\n<|assistant|>\\n\"\n-        inputs = self.processor(prompt, self.image, return_tensors=\"pt\").to(model.device)\n+        inputs = self.processor(text=prompt, images=self.image, return_tensors=\"pt\").to(model.device)\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=30)"
        },
        {
            "sha": "3230b50e7299d090b85c2c2230bc42f04272a46c",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -467,7 +467,9 @@ def test_small_model_integration_test_batch_matches_single(self):\n             padding=True,\n         ).to(torch_device)\n \n-        inputs_single = self.processor(self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(torch_device)\n+        inputs_single = self.processor(text=self.prompt_video, videos=[self.video], return_tensors=\"pt\").to(\n+            torch_device\n+        )\n \n         # verify generation\n         output_batched = model.generate(**inputs_batched, do_sample=False, max_new_tokens=50)"
        },
        {
            "sha": "28be4eba3f853bf4c6fa75b5720d7418c734f6fb",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -413,7 +413,6 @@ def attention_mask_padding_matches_padding_free_with_position_ids(\n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n \n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n@@ -698,7 +697,7 @@ def test_small_model_integration_test_batch(self):\n         )\n         text = self.processor.apply_chat_template(self.messages, tokenize=False, add_generation_prompt=True)\n         inputs = self.processor(\n-            text=text * 2,\n+            text=[text] * 2,\n             audio=[self.raw_audio, self.raw_audio],\n             images=[self.raw_image, self.raw_image],\n             return_tensors=\"pt\","
        },
        {
            "sha": "1b73dd624d22c0b40c950049de2342fe756ea0f0",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -403,7 +403,6 @@ def attention_mask_padding_matches_padding_free_with_position_ids(\n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n \n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)"
        },
        {
            "sha": "bbe615d5095406d8ae829c797cb1a6ec5a286715",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -362,7 +362,6 @@ def attention_mask_padding_matches_padding_free_with_position_ids(\n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n \n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n                 tol = torch.finfo(torch.bfloat16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)"
        },
        {
            "sha": "eea5449362687d27219b34de2266f6daa7880b8e",
            "filename": "tests/pipelines/test_pipelines_image_text_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1c6b47451dacb296844ad5cfbbbde480c8a8708f/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_image_text_to_text.py?ref=1c6b47451dacb296844ad5cfbbbde480c8a8708f",
            "patch": "@@ -119,7 +119,7 @@ def test_small_model_pt_token_text_only(self):\n                             },\n                             {\n                                 \"role\": \"assistant\",\n-                                \"content\": \"Hugging Face, a company of minds\\nWith tools and services that make our lives easier\\nFrom\",\n+                                \"content\": \"Hugging Face, a company of minds\\nWith tools and services that make our lives easier\\nFrom natural language processing\\nTo machine learning and more, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and services\\nFrom image and speech recognition\\nTo text and language translation, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and services\\nFrom image and speech recognition\\nTo text and language translation, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and services\\nFrom image and speech recognition\\nTo text and language translation, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and services\\nFrom image and speech recognition\\nTo text and language translation, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and services\\nFrom image and speech recognition\\nTo text and language translation, they've got it all\\n\\nThey've made it possible for us to be more\\nInformed and efficient, with their tools and\",\n                             },\n                         ],\n                     }\n@@ -150,7 +150,7 @@ def test_small_model_pt_token(self):\n             [\n                 {\n                     \"input_text\": \"<image> What this is? Assistant: This is\",\n-                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are sleeping and appear to be comfortable. The photo captures a moment of tranquility and companionship between the two feline friends.\",\n                 }\n             ],\n         )\n@@ -161,11 +161,11 @@ def test_small_model_pt_token(self):\n             [\n                 {\n                     \"input_text\": \"<image> What this is? Assistant: This is\",\n-                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they appear to be sleeping or resting. The blanket is placed on a couch, and the cats are positioned in such a way that they are facing the camera. The image captures a peaceful moment between the two cats, and it's a great way to showcase their cuteness and relaxed demeanor.\",\n                 },\n                 {\n                     \"input_text\": \"<image> What this is? Assistant: This is\",\n-                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they\",\n+                    \"generated_text\": \"<image> What this is? Assistant: This is a photo of two cats lying on a pink blanket. The cats are facing the camera, and they appear to be sleeping or resting. The blanket is placed on a couch, and the cats are positioned in such a way that they are facing the camera. The image captures a peaceful moment between the two cats, and it's a great way to showcase their cuteness and relaxed demeanor.\",\n                 },\n             ],\n         )"
        }
    ],
    "stats": {
        "total": 127,
        "additions": 89,
        "deletions": 38
    }
}