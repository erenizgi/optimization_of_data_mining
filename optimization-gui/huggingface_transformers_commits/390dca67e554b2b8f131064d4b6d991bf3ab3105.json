{
    "author": "Sai-Suraj-27",
    "message": "Fixed convert_batch_to_list_format staticmethod function call (#42476)\n\nFixed TypeError: TokenizerTesterMixin.convert_batch_to_list_format() takes 1 positional argument but 2 were given",
    "sha": "390dca67e554b2b8f131064d4b6d991bf3ab3105",
    "files": [
        {
            "sha": "a521f51196973682bfd041f66d7d628e4338494e",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/390dca67e554b2b8f131064d4b6d991bf3ab3105/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/390dca67e554b2b8f131064d4b6d991bf3ab3105/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=390dca67e554b2b8f131064d4b6d991bf3ab3105",
            "patch": "@@ -2302,7 +2302,9 @@ def test_batch_encode_plus_batch_sequence_length(self):\n \n         encoded_sequences = [tokenizer(sequence) for sequence in sequences]\n         encoded_sequences_batch = tokenizer(sequences, padding=False)\n-        self.assertListEqual(encoded_sequences, self.convert_batch_to_list_format(encoded_sequences_batch))\n+        self.assertListEqual(\n+            encoded_sequences, TokenizerTesterMixin.convert_batch_to_list_format(encoded_sequences_batch)\n+        )\n \n         maximum_length = len(max([encoded_sequence[\"input_ids\"] for encoded_sequence in encoded_sequences], key=len))\n \n@@ -2316,7 +2318,7 @@ def test_batch_encode_plus_batch_sequence_length(self):\n         encoded_sequences_batch_padded = tokenizer(sequences, padding=True)\n         self.assertListEqual(\n             encoded_sequences_padded,\n-            self.convert_batch_to_list_format(encoded_sequences_batch_padded),\n+            TokenizerTesterMixin.convert_batch_to_list_format(encoded_sequences_batch_padded),\n         )\n \n         # check 'longest' is unsensitive to a max length\n@@ -2357,7 +2359,9 @@ def test_batch_encode_plus_padding(self):\n             tokenizer(sequence, max_length=max_length, padding=\"max_length\") for sequence in sequences\n         ]\n         encoded_sequences_batch = tokenizer(sequences, max_length=max_length, padding=\"max_length\")\n-        self.assertListEqual(encoded_sequences, self.convert_batch_to_list_format(encoded_sequences_batch))\n+        self.assertListEqual(\n+            encoded_sequences, TokenizerTesterMixin.convert_batch_to_list_format(encoded_sequences_batch)\n+        )\n \n         # Left padding tests\n         tokenizer = self.get_tokenizer(do_lower_case=False)\n@@ -2377,7 +2381,9 @@ def test_batch_encode_plus_padding(self):\n             tokenizer(sequence, max_length=max_length, padding=\"max_length\") for sequence in sequences\n         ]\n         encoded_sequences_batch = tokenizer(sequences, max_length=max_length, padding=\"max_length\")\n-        self.assertListEqual(encoded_sequences, self.convert_batch_to_list_format(encoded_sequences_batch))\n+        self.assertListEqual(\n+            encoded_sequences, TokenizerTesterMixin.convert_batch_to_list_format(encoded_sequences_batch)\n+        )\n \n     def test_pretokenized_inputs(self):\n         # Test when inputs are pretokenized"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}