{
    "author": "zucchini-nlp",
    "message": "[v5] Delete `videos` from image processing classes  (#41607)\n\n* delete\n\n* why there were video tests in image file\n\n* fix tests and copies\n\n* docs and autto class",
    "sha": "ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
    "files": [
        {
            "sha": "3a36d5366b66443524eead829d1cc862bc8f7f64",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -63,11 +63,6 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipVideoVideoProcessor\n     - preprocess\n \n-## InstructBlipVideoImageProcessor\n-\n-[[autodoc]] InstructBlipVideoImageProcessor\n-    - preprocess\n-\n ## InstructBlipVideoVisionModel\n \n [[autodoc]] InstructBlipVideoVisionModel"
        },
        {
            "sha": "d7c6b82104d50bd0de167ca8a4ab20d1e86232bd",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -247,10 +247,6 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaNextVideoProcessor\n \n-## LlavaNextVideoImageProcessor\n-\n-[[autodoc]] LlavaNextVideoImageProcessor\n-\n ## LlavaNextVideoVideoProcessor\n \n [[autodoc]] LlavaNextVideoVideoProcessor"
        },
        {
            "sha": "b3d618e6b7f5e998a68124860fe510005b42ac28",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -114,7 +114,6 @@\n             (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"imagegpt\", (\"ImageGPTImageProcessor\", \"ImageGPTImageProcessorFast\")),\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n-            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\", None)),\n             (\"janus\", (\"JanusImageProcessor\", \"JanusImageProcessorFast\")),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"kosmos-2.5\", (\"Kosmos2_5ImageProcessor\", \"Kosmos2_5ImageProcessorFast\")),\n@@ -126,7 +125,7 @@\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n             (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n             (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n-            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\", None)),\n+            (\"llava_next_video\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),"
        },
        {
            "sha": "1fbaaff42aa5ba6e2f0aaab6286f29dcb2de9ba1",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -313,7 +313,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -335,9 +334,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):"
        },
        {
            "sha": "ccd0d701738cc6f7c91500669479e4251ec60ced",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "removed",
            "additions": 0,
            "deletions": 327,
            "changes": 327,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e50b8459d981ddcbc9438e85cff8d83fe40a500/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e50b8459d981ddcbc9438e85cff8d83fe40a500/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=4e50b8459d981ddcbc9438e85cff8d83fe40a500",
            "patch": "@@ -1,327 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"\n-Image processor class for InstructBLIPVideo. Largely copy of Blip2Processor with addition of a video processing abilities\n-\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    to_numpy_array,\n-    valid_images,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-# TODO (raushan): processor can be removed after v5 release. Kept for backwards compatibility\n-# Copied from transformers.models.blip.image_processing_blip.BlipImageProcessor with Blip->InstructBlipVideo, BLIP->InstructBLIPVideo\n-class InstructBlipVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a InstructBLIPVideo image processor.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by the\n-            `do_resize` parameter in the `preprocess` method.\n-        size (`dict`, *optional*, defaults to `{\"height\": 384, \"width\": 384}`):\n-            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n-            method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`. Can be\n-            overridden by the `resample` parameter in the `preprocess` method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the\n-            `do_rescale` parameter in the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Only has an effect if `do_rescale` is set to `True`. Can be\n-            overridden by the `rescale_factor` parameter in the `preprocess` method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n-            method. Can be overridden by the `do_normalize` parameter in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method. Can be\n-            overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"height\": 384, \"width\": 384}\n-        size = get_size_dict(size, default_to_square=True)\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.resample = resample\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.vit.image_processing_vit.ViTImageProcessor.resize with PILImageResampling.BILINEAR->PILImageResampling.BICUBIC\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image to `(size[\"height\"], size[\"width\"])`.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n-            data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the output image. If unset, the channel dimension format of the input\n-                image is used. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-\n-        Returns:\n-            `np.ndarray`: The resized image.\n-        \"\"\"\n-        size = get_size_dict(size)\n-        if \"height\" not in size or \"width\" not in size:\n-            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n-\n-        output_size = (size[\"height\"], size[\"width\"])\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    # Ignore copy\n-    @filter_out_non_signature_kwargs()\n-    def preprocess(\n-        self,\n-        images: Optional[VideoInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> BatchFeature:\n-        \"\"\"\n-        Preprocess a video or batch of images/videos.\n-\n-        Args:\n-            videos (`VideoInput`):\n-                Video frames to preprocess. Expects a single or batch of videos as a list of frames with pixel values\n-                ranging from 0 to 255. If passing in video with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Controls the size of the video after `resize`. The shortest edge of the image is resized to\n-                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n-                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n-                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n-            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. Only has an effect if `do_resize` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video values between [0 - 1].\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to normalize the video by if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to normalize the video by if `do_normalize` is set to `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, default_to_square=False)\n-\n-        videos = make_batched_videos(images)\n-        logger.warning(\n-            \"`InstructBlipVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `InstructBlipVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        if not valid_images(videos):\n-            raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n-\n-        pixel_values = [\n-            [\n-                self._preprocess_image(\n-                    image=frame,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for frame in video\n-            ]\n-            for video in videos\n-        ]\n-\n-        encoded_outputs = BatchFeature(data={\"pixel_values\": pixel_values}, tensor_type=return_tensors)\n-        return encoded_outputs\n-\n-    # Ignore copy\n-    def _preprocess_image(\n-        self,\n-        image: Optional[ImageInput] = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: ChannelDimension = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> np.ndarray:\n-        # PIL RGBA images are converted to RGB\n-        if do_convert_rgb:\n-            image = convert_to_rgb(image)\n-\n-        # All transformations expect numpy arrays.\n-        image = to_numpy_array(image)\n-\n-        if do_rescale and is_scaled_image(image):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled video frames. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(image)\n-\n-        if do_resize:\n-            image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-        if do_rescale:\n-            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-        if do_normalize:\n-            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-\n-        image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-\n-        return image\n-\n-\n-__all__ = [\"InstructBlipVideoImageProcessor\"]"
        },
        {
            "sha": "f813faba7b8920ef2b8e7809f69f3a247935ba58",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -41,7 +41,7 @@ class InstructBlipVideoProcessor(ProcessorMixin):\n     Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single\n     processor.\n \n-    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoImageProcessor`] and [`AutoTokenizer`]. See the\n+    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoVideoProcessor`] and [`AutoTokenizer`]. See the\n     docstring of [`~InstructBlipVideoProcessor.__call__`] and [`~InstructBlipVideoProcessor.decode`] for more information.\n \n     Args:"
        },
        {
            "sha": "633f5ab35e4ebe0538fcd12f811edf414b67c69e",
            "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -34,8 +34,8 @@\n     LlavaNextImageProcessor,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n-    LlavaNextVideoImageProcessor,\n     LlavaNextVideoProcessor,\n+    LlavaNextVideoVideoProcessor,\n )\n \n \n@@ -187,7 +187,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n     tokenizer.add_tokens(AddedToken(\"<image>\", special=True, normalized=False), special_tokens=True)\n \n     image_processor = LlavaNextImageProcessor.from_pretrained(vision_model_id)\n-    video_processor = LlavaNextVideoImageProcessor.from_pretrained(vision_model_id)\n+    video_processor = LlavaNextVideoVideoProcessor.from_pretrained(vision_model_id)\n     processor = LlavaNextVideoProcessor(\n         tokenizer=tokenizer,\n         video_processor=video_processor,"
        },
        {
            "sha": "8468c20afa4e2479a380583c9322b5493e0f39be",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "removed",
            "additions": 0,
            "deletions": 401,
            "changes": 401,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e50b8459d981ddcbc9438e85cff8d83fe40a500/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e50b8459d981ddcbc9438e85cff8d83fe40a500/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=4e50b8459d981ddcbc9438e85cff8d83fe40a500",
            "patch": "@@ -1,401 +0,0 @@\n-# coding=utf-8\n-# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Image processor class for LLaVa-NeXT-Video.\"\"\"\n-\n-from typing import Optional, Union\n-\n-import numpy as np\n-\n-from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n-from ...image_transforms import (\n-    convert_to_rgb,\n-    get_resize_output_image_size,\n-    resize,\n-    to_channel_dimension_format,\n-)\n-from ...image_utils import (\n-    OPENAI_CLIP_MEAN,\n-    OPENAI_CLIP_STD,\n-    ChannelDimension,\n-    ImageInput,\n-    PILImageResampling,\n-    infer_channel_dimension_format,\n-    is_scaled_image,\n-    make_flat_list_of_images,\n-    to_numpy_array,\n-    validate_preprocess_arguments,\n-)\n-from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class LlavaNextVideoImageProcessor(BaseImageProcessor):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT-Video video processor. Based on [`CLIPImageProcessor`] with incorporation of processing each video frame.\n-\n-    Args:\n-        do_resize (`bool`, *optional*, defaults to `True`):\n-            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n-            `do_resize` in the `preprocess` method.\n-        size (`dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n-            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n-            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n-            method.\n-        image_grid_pinpoints (`List` *optional*, defaults to `[[672, 336], [336, 672], [672, 672], [336, 1008], [1008, 336]]`):\n-            A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n-            based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n-            method. Not used for processing videos.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n-            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n-        do_center_crop (`bool`, *optional*, defaults to `True`):\n-            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n-            `preprocess` method.\n-        crop_size (`dict[str, int]` *optional*, defaults to 224):\n-            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n-            method.\n-        do_rescale (`bool`, *optional*, defaults to `True`):\n-            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n-            the `preprocess` method.\n-        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n-            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n-            method.\n-        do_normalize (`bool`, *optional*, defaults to `True`):\n-            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n-        image_mean (`float` or `list[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n-            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n-            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n-        image_std (`float` or `list[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n-            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n-            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n-            Can be overridden by the `image_std` parameter in the `preprocess` method.\n-        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n-            Whether to convert the image to RGB.\n-    \"\"\"\n-\n-    model_input_names = [\"pixel_values_videos\"]\n-\n-    def __init__(\n-        self,\n-        do_resize: bool = True,\n-        size: Optional[dict[str, int]] = None,\n-        image_grid_pinpoints: Optional[list] = None,\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        do_center_crop: bool = True,\n-        crop_size: Optional[dict[str, int]] = None,\n-        do_rescale: bool = True,\n-        rescale_factor: Union[int, float] = 1 / 255,\n-        do_normalize: bool = True,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: bool = True,\n-        **kwargs,\n-    ) -> None:\n-        super().__init__(**kwargs)\n-        size = size if size is not None else {\"shortest_edge\": 224}\n-        size = get_size_dict(size, default_to_square=False)\n-        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n-        crop_size = get_size_dict(crop_size, default_to_square=True, param_name=\"crop_size\")\n-\n-        self.do_resize = do_resize\n-        self.size = size\n-        self.image_grid_pinpoints = image_grid_pinpoints\n-        self.resample = resample\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n-        self.do_rescale = do_rescale\n-        self.rescale_factor = rescale_factor\n-        self.do_normalize = do_normalize\n-        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n-        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n-        self.do_convert_rgb = do_convert_rgb\n-\n-    # Copied from transformers.models.clip.image_processing_clip.CLIPImageProcessor.resize with CLIP->LLaVa\n-    def resize(\n-        self,\n-        image: np.ndarray,\n-        size: dict[str, int],\n-        resample: PILImageResampling = PILImageResampling.BICUBIC,\n-        data_format: Optional[Union[str, ChannelDimension]] = None,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        **kwargs,\n-    ) -> np.ndarray:\n-        \"\"\"\n-        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n-        resized to keep the input aspect ratio.\n-\n-        Args:\n-            image (`np.ndarray`):\n-                Image to resize.\n-            size (`dict[str, int]`):\n-                Size of the output image.\n-            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n-                Resampling filter to use when resiizing the image.\n-            data_format (`str` or `ChannelDimension`, *optional*):\n-                The channel dimension format of the image. If not provided, it will be the same as the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format of the input image. If not provided, it will be inferred.\n-        \"\"\"\n-        default_to_square = True\n-        if \"shortest_edge\" in size:\n-            size = size[\"shortest_edge\"]\n-            default_to_square = False\n-        elif \"height\" in size and \"width\" in size:\n-            size = (size[\"height\"], size[\"width\"])\n-        else:\n-            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n-\n-        output_size = get_resize_output_image_size(\n-            image,\n-            size=size,\n-            default_to_square=default_to_square,\n-            input_data_format=input_data_format,\n-        )\n-\n-        return resize(\n-            image,\n-            size=output_size,\n-            resample=resample,\n-            data_format=data_format,\n-            input_data_format=input_data_format,\n-            **kwargs,\n-        )\n-\n-    def _preprocess(\n-        self,\n-        images: ImageInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> list[np.ndarray]:\n-        \"\"\"\n-        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Batch of frames (one video) to preprocess. Expects a batch of frames with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the image.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        images = make_flat_list_of_images(images)\n-\n-        if do_convert_rgb:\n-            images = [convert_to_rgb(image) for image in images]\n-\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n-\n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n-            )\n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n-\n-        all_images = []\n-        for image in images:\n-            if do_resize:\n-                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n-\n-            if do_center_crop:\n-                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n-\n-            if do_rescale:\n-                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-\n-            if do_normalize:\n-                image = self.normalize(\n-                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n-                )\n-\n-            all_images.append(image)\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n-            for image in all_images\n-        ]\n-\n-        return images\n-\n-    def preprocess(\n-        self,\n-        images: VideoInput,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[PILImageResampling] = None,\n-        do_center_crop: Optional[bool] = None,\n-        crop_size: Optional[int] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ):\n-        \"\"\"\n-        Args:\n-            images (`VideoInput`):\n-                Videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the video.\n-            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the video after resizing. Shortest edge of the video is resized to size[\"shortest_edge\"], with\n-                the longest edge resized to keep the input aspect ratio.\n-            resample (`int`, *optional*, defaults to `self.resample`):\n-                Resampling filter to use if resizing the video. This can be one of the enum `PILImageResampling`. Only\n-                has an effect if `do_resize` is set to `True`.\n-            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n-                Whether to center crop the video.\n-            crop_size (`dict[str, int]`, *optional*, defaults to `self.crop_size`):\n-                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the video.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Rescale factor to rescale the video by if `do_rescale` is set to `True`.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the video.\n-            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n-                Frame mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n-            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n-                Frame standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n-                `True`.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the video to RGB.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                - Unset: Return a list of `np.ndarray`.\n-                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n-                The channel dimension format for the output image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - Unset: Use the channel dimension format of the input image.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n-                from the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-        \"\"\"\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n-        resample = resample if resample is not None else self.resample\n-        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n-        crop_size = crop_size if crop_size is not None else self.crop_size\n-        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        images = self.fetch_images(images)\n-        images = make_batched_videos(images)\n-        logger.warning(\n-            \"`LlavaNextVideoImageProcessor` is deprecated and will be removed in v5.0. \"\n-            \"We recommend to load an instance of `LlavaNextVideoVideoProcessor` to process videos for the model. \"\n-        )\n-\n-        validate_preprocess_arguments(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_center_crop=do_center_crop,\n-            crop_size=crop_size,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-        )\n-\n-        # preprocess each video frame by frame\n-        pixel_values = [\n-            self._preprocess(\n-                frames,\n-                do_resize=do_resize,\n-                size=size,\n-                resample=resample,\n-                do_center_crop=do_center_crop,\n-                crop_size=crop_size,\n-                do_rescale=do_rescale,\n-                rescale_factor=rescale_factor,\n-                do_normalize=do_normalize,\n-                image_mean=image_mean,\n-                image_std=image_std,\n-                data_format=data_format,\n-                input_data_format=input_data_format,\n-            )\n-            for frames in images\n-        ]\n-\n-        data = {\"pixel_values_videos\": pixel_values}\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-\n-__all__ = [\"LlavaNextVideoImageProcessor\"]"
        },
        {
            "sha": "7255212f8921b10f6645270b849f0f816dbac543",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -49,7 +49,7 @@ class LlavaNextVideoProcessor(ProcessorMixin):\n     Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n     a LLaMa tokenizer into a single processor.\n \n-    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoImageProcessor`] and\n+    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoVideoProcessor`] and\n     [`LlamaTokenizerFast`]. See the [`~LlavaNextVideoProcessor.__call__`] and [`~LlavaNextVideoProcessor.decode`] for more information.\n \n     Args:\n@@ -124,8 +124,8 @@ def __call__(\n         and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n         the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n         LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoImageProcessor's\n-        [`~LlavaNextVideoImageProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n+        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoVideoProcessor's\n+        [`~LlavaNextVideoVideoProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n         of the above two methods for more information.\n \n         Args:"
        },
        {
            "sha": "6d3d1a3ee1c37cae00104f141f5a5f09b3da612f",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -341,11 +341,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
        },
        {
            "sha": "ffda3a5c3dee33a9d3384b3d0284e937d9de385e",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -858,7 +858,10 @@ class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n     def __call__("
        },
        {
            "sha": "8d0b8ed064ac4752cbd65ec8b2240089041aad7e",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -255,7 +255,10 @@ def post_process_image_text_to_text(\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        names_from_processor = list(\n+            dict.fromkeys(tokenizer_input_names + image_processor_input_names + video_processor_input_names)\n+        )\n         return names_from_processor + [\"second_per_grid_ts\"]\n \n "
        },
        {
            "sha": "1b896091d946dfc2116db2f7297893d10e39bdfe",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 25,
            "deletions": 66,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -46,7 +46,7 @@\n )\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n@@ -137,7 +137,7 @@ class Qwen2VLImageProcessor(BaseImageProcessor):\n             The merge size of the vision encoder to llm encoder.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n \n     def __init__(\n@@ -322,7 +322,6 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         min_pixels: Optional[int] = None,\n@@ -346,9 +345,6 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`):\n-                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -442,67 +438,30 @@ def preprocess(\n         )\n \n         data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = np.array(pixel_values)\n-            vision_grid_thws = np.array(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n-\n-        # kept for BC only and should be removed after v5.0\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos, vision_grid_thws_videos = [], []\n-            for images in videos:\n-                patches, video_grid_thw = self._preprocess(\n-                    images,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    data_format=data_format,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                )\n-                pixel_values_videos.extend(patches)\n-                vision_grid_thws_videos.append(video_grid_thw)\n-            data.update(\n-                {\n-                    \"pixel_values_videos\": np.array(pixel_values_videos),\n-                    \"video_grid_thw\": np.array(vision_grid_thws_videos),\n-                }\n+        pixel_values, vision_grid_thws = [], []\n+        for image in images:\n+            patches, image_grid_thw = self._preprocess(\n+                image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                patch_size=patch_size,\n+                temporal_patch_size=temporal_patch_size,\n+                merge_size=merge_size,\n+                data_format=data_format,\n+                do_convert_rgb=do_convert_rgb,\n+                input_data_format=input_data_format,\n             )\n+            pixel_values.extend(patches)\n+            vision_grid_thws.append(image_grid_thw)\n+        pixel_values = np.array(pixel_values)\n+        vision_grid_thws = np.array(vision_grid_thws)\n+        data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n "
        },
        {
            "sha": "4647bb5bae1117fd87ff14c4efc83c22b4376874",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 26,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -44,7 +44,6 @@\n     auto_docstring,\n     logging,\n )\n-from ...video_utils import VideoInput, make_batched_videos\n from .image_processing_qwen2_vl import Qwen2VLImageProcessorKwargs, smart_resize\n \n \n@@ -67,7 +66,7 @@ class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n     min_pixels = None\n     max_pixels = None\n     valid_kwargs = Qwen2VLImageProcessorKwargs\n-    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Qwen2VLImageProcessorKwargs]):\n         size = kwargs.pop(\"size\", None)\n@@ -113,15 +112,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -134,27 +131,10 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-        if videos is not None:\n-            logger.warning(\n-                \"`Qwen2VLImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `Qwen2VLVideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n         return batch_feature\n \n     def _preprocess("
        },
        {
            "sha": "4c39512b953340e3b7f215cde4f641f66a90683e",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -340,11 +340,13 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         feature_extractor_input_names = self.feature_extractor.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n+        video_processor_input_names = self.video_processor.model_input_names\n         return list(\n             dict.fromkeys(\n                 tokenizer_input_names\n                 + feature_extractor_input_names\n                 + image_processor_input_names\n+                + video_processor_input_names\n                 + [\"feature_attention_mask\"]\n                 + [\"video_second_per_grid\"]\n             )"
        },
        {
            "sha": "0a971e129cf38e388282f9bf50c02e2ed92a4ddf",
            "filename": "src/transformers/models/video_llama_3/image_processing_video_llama_3_fast.py",
            "status": "modified",
            "additions": 13,
            "deletions": 44,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fimage_processing_video_llama_3_fast.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -34,14 +34,10 @@\n     SizeDict,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, logging\n-from ...video_utils import VideoInput, make_batched_videos\n+from ...utils import TensorType, auto_docstring\n from .image_processing_video_llama_3 import VideoLlama3ImageProcessorKwargs\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -91,9 +87,6 @@ class VideoLlama3ImageProcessorFast(BaseImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def __init__(self, **kwargs: Unpack[VideoLlama3ImageProcessorKwargs]):\n@@ -140,15 +133,13 @@ def _further_process_kwargs(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return super().preprocess(images, videos, **kwargs)\n+        return super().preprocess(images, **kwargs)\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n@@ -161,39 +152,17 @@ def _preprocess_image_like_inputs(\n         \"\"\"\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n     def _preprocess("
        },
        {
            "sha": "789248676ab750623ea074e94bf1a9fdf18781b3",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 38,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -50,7 +50,6 @@\n from ...video_utils import (\n     VideoInput,\n     group_videos_by_shape,\n-    make_batched_videos,\n     reorder_videos,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -1446,55 +1445,29 @@ class VideoLlama3ImageProcessorFast(Qwen2VLImageProcessorFast):\n         \"pixel_values\",\n         \"image_grid_thw\",\n         \"image_merge_sizes\",\n-        \"pixel_values_videos\",\n-        \"video_grid_thw\",\n-        \"video_merge_sizes\",\n     ]\n \n     def _preprocess_image_like_inputs(\n         self,\n         images: ImageInput,\n-        videos: VideoInput,\n         do_convert_rgb: bool,\n         input_data_format: ChannelDimension,\n         device: Optional[Union[str, \"torch.device\"]] = None,\n         **kwargs: Unpack[VideoLlama3ImageProcessorKwargs],\n     ) -> BatchFeature:\n         # Prepare input images\n         batch_feature = BatchFeature()\n-        if images is not None:\n-            if kwargs[\"temporal_patch_size\"] != 1:\n-                raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n-            images = self._prepare_image_like_inputs(\n-                images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n-            )\n-            batch_feature = self._preprocess(images, **kwargs)\n-            batch_feature[\"image_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n-                dtype=batch_feature.image_grid_thw.dtype,\n-                device=batch_feature.image_grid_thw.device,\n-            )\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlama3ImageProcessorFast` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlama3VideoProcessor`. \"\n-            )\n-            # Can't change _prepare_images_structure to work with videos because it also needs to work with images.\n-            videos = make_batched_videos(videos)\n-            videos = [\n-                torch.stack(self._prepare_image_like_inputs(video, do_convert_rgb, input_data_format, device))\n-                for video in videos\n-            ]\n-            video_outputs = self._preprocess(videos, **kwargs)\n-            batch_feature.update(\n-                {\"pixel_values_videos\": video_outputs.pixel_values, \"video_grid_thw\": video_outputs.image_grid_thw}\n-            )\n-            batch_feature[\"video_merge_sizes\"] = torch.tensor(\n-                [kwargs[\"merge_size\"]] * video_outputs.image_grid_thw.size(0),\n-                dtype=video_outputs.image_grid_thw.dtype,\n-                device=video_outputs.image_grid_thw.device,\n-            )\n+        if kwargs[\"temporal_patch_size\"] != 1:\n+            raise ValueError(\"`temporal_patch_size` must be 1 for VideoLLaMA3\")\n+        images = self._prepare_image_like_inputs(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        batch_feature = self._preprocess(images, **kwargs)\n+        batch_feature[\"image_merge_sizes\"] = torch.tensor(\n+            [kwargs[\"merge_size\"]] * batch_feature.image_grid_thw.size(0),\n+            dtype=batch_feature.image_grid_thw.dtype,\n+            device=batch_feature.image_grid_thw.device,\n+        )\n         return batch_feature\n \n "
        },
        {
            "sha": "fa2b1e49f288617634bb8c474c5bccced0b1db2f",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 19,
            "deletions": 58,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -39,7 +39,6 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n-from ...video_utils import VideoInput, make_batched_videos\n \n \n logger = logging.get_logger(__name__)\n@@ -172,7 +171,6 @@ def resize(\n     def preprocess(\n         self,\n         images: Optional[list[ImageInput]] = None,\n-        videos: Optional[list[VideoInput]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,\n@@ -195,9 +193,6 @@ def preprocess(\n             images (`ImageInput`, *optional*):\n                 List of images to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n-            videos (`VideoInput`, *optional*):\n-                List of videos to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n-                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -261,60 +256,26 @@ def preprocess(\n         if images is not None and not valid_images(images):\n             raise ValueError(\"Invalid input type. Must be of type PIL.Image.Image, numpy.ndarray, or torch.Tensor\")\n \n-        data = {}\n-        if videos is not None:\n-            logger.warning(\n-                \"`VideoLlavaImageProcessor` works only with image inputs and doesn't process videos anymore. \"\n-                \"This is a deprecated behavior and will be removed in v5.0. \"\n-                \"Your videos should be forwarded to `VideoLlavaVideoProcessor`. \"\n+        pixel_values_images = [\n+            self._preprocess_image(\n+                image=image,\n+                do_resize=do_resize,\n+                size=size,\n+                resample=resample,\n+                do_rescale=do_rescale,\n+                rescale_factor=rescale_factor,\n+                do_normalize=do_normalize,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                do_center_crop=do_center_crop,\n+                crop_size=crop_size,\n+                do_convert_rgb=do_convert_rgb,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n-            videos = make_batched_videos(videos)\n-            pixel_values_videos = [\n-                [\n-                    self._preprocess_image(\n-                        image=frame,\n-                        do_resize=do_resize,\n-                        size=size,\n-                        resample=resample,\n-                        do_rescale=do_rescale,\n-                        rescale_factor=rescale_factor,\n-                        do_normalize=do_normalize,\n-                        image_mean=image_mean,\n-                        image_std=image_std,\n-                        do_center_crop=do_center_crop,\n-                        crop_size=crop_size,\n-                        do_convert_rgb=do_convert_rgb,\n-                        data_format=data_format,\n-                        input_data_format=input_data_format,\n-                    )\n-                    for frame in video\n-                ]\n-                for video in videos\n-            ]\n-            data[\"pixel_values_videos\"] = pixel_values_videos\n-\n-        if images is not None:\n-            pixel_values_images = [\n-                self._preprocess_image(\n-                    image=image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    resample=resample,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    do_center_crop=do_center_crop,\n-                    crop_size=crop_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    data_format=data_format,\n-                    input_data_format=input_data_format,\n-                )\n-                for image in images\n-            ]\n-            data[\"pixel_values_images\"] = pixel_values_images\n-\n+            for image in images\n+        ]\n+        data = {\"pixel_values_images\": pixel_values_images}\n         encoded_outputs = BatchFeature(data, tensor_type=return_tensors)\n \n         return encoded_outputs"
        },
        {
            "sha": "23da5bc1af4a26ad7bc4432f5fc89fdd2786a335",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=ee3a1002e2a3a4b60b66a1f9b034fbbe00c60e87",
            "patch": "@@ -274,31 +274,6 @@ def test_nested_input(self):\n             self.assertTrue((encoded_images_nested == encoded_images).all())\n             self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n \n-    def test_video_inputs(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n-\n-            for num_frames, expected_dims in expected_dims_by_frames.items():\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (expected_dims, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n-    def test_custom_patch_size(self):\n-        for image_processing_class in self.image_processor_list:\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-\n-            for patch_size in (1, 3, 5, 7):\n-                image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n-                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-                process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-                encoded_video = process_out.pixel_values_videos\n-                expected_output_video_shape = (171500, 1176)\n-                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-\n     def test_custom_image_size(self):\n         for image_processing_class in self.image_processor_list:\n             image_processing = image_processing_class(**self.image_processor_dict)\n@@ -325,24 +300,6 @@ def test_custom_pixels(self):\n                 # Just checking that it doesn't raise an error\n                 image_processor(image_inputs, return_tensors=\"pt\")\n \n-    def test_temporal_padding(self):\n-        for image_processing_class in self.image_processor_list:\n-            # Initialize image_processing\n-            image_processing = image_processing_class(**self.image_processor_dict)\n-            # Create random video inputs with a number of frames not divisible by temporal_patch_size\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=5, temporal_patch_size=4)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-\n-            # Process the video inputs\n-            process_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = process_out.pixel_values_videos\n-\n-            # Check the shape after padding\n-            expected_output_video_shape = (102900, 1176)  # Adjusted based on padding\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n-            # Check divisibility by temporal_patch_size\n-            self.assertEqual(encoded_video.shape[0] % 4, 0)\n-\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):"
        }
    ],
    "stats": {
        "total": 1119,
        "additions": 93,
        "deletions": 1026
    }
}