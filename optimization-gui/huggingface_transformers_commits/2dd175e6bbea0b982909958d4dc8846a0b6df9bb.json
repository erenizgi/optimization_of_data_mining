{
    "author": "frozenleaves",
    "message": "Adapt to the SDPA interface to enable the NPU to call FlashAttentionScore (#41143)\n\nAdapt to the SDPA interface to enable the NPU to call FlashAttentionScore.\n\nCo-authored-by: frozenleaves <frozen@Mac.local>",
    "sha": "2dd175e6bbea0b982909958d4dc8846a0b6df9bb",
    "files": [
        {
            "sha": "8f1b9fc8ff8521434d4e7f906202874c9d347598",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/2dd175e6bbea0b982909958d4dc8846a0b6df9bb/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2dd175e6bbea0b982909958d4dc8846a0b6df9bb/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=2dd175e6bbea0b982909958d4dc8846a0b6df9bb",
            "patch": "@@ -2,7 +2,7 @@\n \n import torch\n \n-from ..utils import is_torch_xpu_available, logging\n+from ..utils import is_torch_npu_available, is_torch_xpu_available, logging\n from ..utils.import_utils import is_torch_greater_or_equal\n \n \n@@ -12,6 +12,7 @@\n _is_torch_greater_or_equal_than_2_5 = is_torch_greater_or_equal(\"2.5\", accept_dev=True)\n _is_torch_greater_or_equal_than_2_8 = is_torch_greater_or_equal(\"2.8\", accept_dev=True)\n _is_torch_xpu_available = is_torch_xpu_available()\n+_is_torch_npu_available = is_torch_npu_available()\n \n \n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n@@ -35,8 +36,12 @@ def use_gqa_in_sdpa(attention_mask: Optional[torch.Tensor], key: torch.Tensor) -\n     # 2.xpu\n     #   - torch version >= 2.8\n     #   - key is not a torch.fx.Proxy (otherwise it will fail with a tracing error)\n+    # 3.npu\n+    #   - npu is not supported gqa currently\n     if _is_torch_xpu_available:\n         return _is_torch_greater_or_equal_than_2_8 and not isinstance(key, torch.fx.Proxy)\n+    if _is_torch_npu_available:\n+        return False\n     return _is_torch_greater_or_equal_than_2_5 and attention_mask is None and not isinstance(key, torch.fx.Proxy)\n \n \n@@ -80,6 +85,14 @@ def sdpa_attention_forward(\n     if torch.jit.is_tracing() and isinstance(is_causal, torch.Tensor):\n         is_causal = is_causal.item()\n \n+    # When `is_causal = False` and the `attention_mask` is not of boolean type, the Ascend NPU's SDPA interface cannot utilize the FlashAttentionScore operatorï¼Œ\n+    # and falls back to small-operator concatenation. To invoke the FlashAttentionScore, the attention_mask must be converted to boolean type.\n+    # This adaptation ensures the `attention_mask` meets the requirement for using FlashAttentionScore.\n+    if _is_torch_npu_available:\n+        if attention_mask is not None and attention_mask.dtype != torch.bool:\n+            # Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\n+            attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n+\n     attn_output = torch.nn.functional.scaled_dot_product_attention(\n         query,\n         key,"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 14,
        "deletions": 1
    }
}