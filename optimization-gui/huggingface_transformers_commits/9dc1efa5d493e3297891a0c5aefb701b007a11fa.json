{
    "author": "stas00",
    "message": "DeepSpeed github repo move sync (#36021)\n\ndeepspeed github repo move",
    "sha": "9dc1efa5d493e3297891a0c5aefb701b007a11fa",
    "files": [
        {
            "sha": "48731f1c2ed349dd2ecd52a26399752af56b9dca",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -366,7 +366,7 @@ jobs:\n         run: |\n           python3 -m pip uninstall -y deepspeed\n           rm -rf DeepSpeed\n-          git clone https://github.com/microsoft/DeepSpeed && cd DeepSpeed && rm -rf build\n+          git clone https://github.com/deepspeedai/DeepSpeed && cd DeepSpeed && rm -rf build\n           DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 python3 -m pip install . --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check\n \n       - name: NVIDIA-SMI"
        },
        {
            "sha": "a872231d0418a9b6808c99e5c1103cedf2509d47",
            "filename": "docker/transformers-past-gpu/Dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docker%2Ftransformers-past-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docker%2Ftransformers-past-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-past-gpu%2FDockerfile?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -48,8 +48,8 @@ RUN python3 -m pip uninstall -y torch-tensorrt apex\n # Pre-build **nightly** release of DeepSpeed, so it would be ready for testing (otherwise, the 1st deepspeed test will timeout)\n RUN python3 -m pip uninstall -y deepspeed\n # This has to be run inside the GPU VMs running the tests. (So far, it fails here due to GPU checks during compilation.)\n-# Issue: https://github.com/microsoft/DeepSpeed/issues/2010\n-# RUN git clone https://github.com/microsoft/DeepSpeed && cd DeepSpeed && rm -rf build && \\\n+# Issue: https://github.com/deepspeedai/DeepSpeed/issues/2010\n+# RUN git clone https://github.com/deepspeedai/DeepSpeed && cd DeepSpeed && rm -rf build && \\\n #    DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 DS_BUILD_UTILS=1 python3 -m pip install . --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check 2>&1\n \n RUN python3 -m pip install -U \"itsdangerous<2.1.0\""
        },
        {
            "sha": "80de73e37c4addaedacea4bf18806725effacd61",
            "filename": "docker/transformers-pytorch-deepspeed-nightly-gpu/Dockerfile",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-pytorch-deepspeed-nightly-gpu%2FDockerfile?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -34,8 +34,8 @@ RUN python3 -m pip uninstall -y torch-tensorrt apex\n # Pre-build **nightly** release of DeepSpeed, so it would be ready for testing (otherwise, the 1st deepspeed test will timeout)\n RUN python3 -m pip uninstall -y deepspeed\n # This has to be run inside the GPU VMs running the tests. (So far, it fails here due to GPU checks during compilation.)\n-# Issue: https://github.com/microsoft/DeepSpeed/issues/2010\n-# RUN git clone https://github.com/microsoft/DeepSpeed && cd DeepSpeed && rm -rf build && \\\n+# Issue: https://github.com/deepspeedai/DeepSpeed/issues/2010\n+# RUN git clone https://github.com/deepspeedai/DeepSpeed && cd DeepSpeed && rm -rf build && \\\n #    DS_BUILD_CPU_ADAM=1 DS_BUILD_FUSED_ADAM=1 DS_BUILD_UTILS=1 python3 -m pip install . --global-option=\"build_ext\" --global-option=\"-j8\" --no-cache -v --disable-pip-version-check 2>&1\n \n ## For `torchdynamo` tests"
        },
        {
            "sha": "76e87f063206789e026ac8eb1d65d5ddf09f3f72",
            "filename": "docs/source/en/debugging.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fdebugging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fdebugging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdebugging.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -30,7 +30,7 @@ DeepSpeed compiles CUDA C++ code and it can be a potential source of errors when\n \n <Tip>\n \n-For any other installation issues, please [open an issue](https://github.com/microsoft/DeepSpeed/issues) with the DeepSpeed team.\n+For any other installation issues, please [open an issue](https://github.com/deepspeedai/DeepSpeed/issues) with the DeepSpeed team.\n \n </Tip>\n \n@@ -89,7 +89,7 @@ sudo ln -s /usr/bin/g++-7  /usr/local/cuda-10.2/bin/g++\n If you're still having issues with installing DeepSpeed or if you're building DeepSpeed at run time, you can try to prebuild the DeepSpeed modules before installing them. To make a local build for DeepSpeed:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n@@ -141,7 +141,7 @@ It is also possible to not specify `TORCH_CUDA_ARCH_LIST` and the build program\n For training on multiple machines with the same setup, you'll need to make a binary wheel:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\"
        },
        {
            "sha": "cb21b7e8fca89fd4c1b70ca5cabd6b5444b28dc3",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -28,7 +28,7 @@ This guide will walk you through how to deploy DeepSpeed training, the features\n \n ## Installation\n \n-DeepSpeed is available to install from PyPI or Transformers (for more detailed installation options, take a look at the DeepSpeed [installation details](https://www.deepspeed.ai/tutorials/advanced-install/) or the GitHub [README](https://github.com/microsoft/deepspeed#installation)).\n+DeepSpeed is available to install from PyPI or Transformers (for more detailed installation options, take a look at the DeepSpeed [installation details](https://www.deepspeed.ai/tutorials/advanced-install/) or the GitHub [README](https://github.com/deepspeedai/DeepSpeed#installation)).\n \n <Tip>\n \n@@ -114,10 +114,10 @@ DeepSpeed works with the [`Trainer`] class by way of a config file containing al\n \n <Tip>\n \n-Find a complete list of DeepSpeed configuration options on the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. You can also find more practical examples of various DeepSpeed configuration examples on the [DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) repository or the main [DeepSpeed](https://github.com/microsoft/DeepSpeed) repository. To quickly find specific examples, you can:\n+Find a complete list of DeepSpeed configuration options on the [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/) reference. You can also find more practical examples of various DeepSpeed configuration examples on the [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples) repository or the main [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) repository. To quickly find specific examples, you can:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeedExamples\n+git clone https://github.com/deepspeedai/DeepSpeedExamples\n cd DeepSpeedExamples\n find . -name '*json'\n # find examples with the Lamb optimizer\n@@ -303,7 +303,7 @@ For more information about initializing large models with ZeRO-3 and accessing t\n \n [ZeRO-Infinity](https://hf.co/papers/2104.07857) allows offloading model states to the CPU and/or NVMe to save even more memory. Smart partitioning and tiling algorithms allow each GPU to send and receive very small amounts of data during offloading such that a modern NVMe can fit an even larger total memory pool than is available to your training process. ZeRO-Infinity requires ZeRO-3.\n \n-Depending on the CPU and/or NVMe memory available, you can offload both the [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading), just one of them, or none. You should also make sure the `nvme_path` is pointing to an NVMe device, because while it still works with a normal hard drive or solid state drive, it'll be significantly slower. With a modern NVMe, you can expect peak transfer speeds of ~3.5GB/s for read and ~3GB/s for write operations. Lastly, [run a benchmark](https://github.com/microsoft/DeepSpeed/issues/998) on your training setup to determine the optimal `aio` configuration.\n+Depending on the CPU and/or NVMe memory available, you can offload both the [optimizer states](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading) and [parameters](https://www.deepspeed.ai/docs/config-json/#parameter-offloading), just one of them, or none. You should also make sure the `nvme_path` is pointing to an NVMe device, because while it still works with a normal hard drive or solid state drive, it'll be significantly slower. With a modern NVMe, you can expect peak transfer speeds of ~3.5GB/s for read and ~3GB/s for write operations. Lastly, [run a benchmark](https://github.com/deepspeedai/DeepSpeed/issues/998) on your training setup to determine the optimal `aio` configuration.\n \n The example ZeRO-3/Infinity configuration file below sets most of the parameter values to `auto`, but you could also manually add these values.\n \n@@ -1157,7 +1157,7 @@ For Transformers>=4.28, if `synced_gpus` is automatically set to `True` if multi\n \n ## Troubleshoot\n \n-When you encounter an issue, you should consider whether DeepSpeed is the cause of the problem because often it isn't (unless it's super obviously and you can see DeepSpeed modules in the exception)! The first step should be to retry your setup without DeepSpeed, and if the problem persists, then you can report the issue. If the issue is a core DeepSpeed problem and unrelated to the Transformers integration, open an Issue on the [DeepSpeed repository](https://github.com/microsoft/DeepSpeed).\n+When you encounter an issue, you should consider whether DeepSpeed is the cause of the problem because often it isn't (unless it's super obviously and you can see DeepSpeed modules in the exception)! The first step should be to retry your setup without DeepSpeed, and if the problem persists, then you can report the issue. If the issue is a core DeepSpeed problem and unrelated to the Transformers integration, open an Issue on the [DeepSpeed repository](https://github.com/deepspeedai/DeepSpeed).\n \n For issues related to the Transformers integration, please provide the following information:\n \n@@ -1227,7 +1227,7 @@ This means the DeepSpeed loss scaler is unable to find a scaling coefficient to\n \n ## Resources\n \n-DeepSpeed ZeRO is a powerful technology for training and loading very large models for inference with limited GPU resources, making it more accessible to everyone. To learn more about DeepSpeed, feel free to read the [blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [documentation](https://www.deepspeed.ai/getting-started/), and [GitHub repository](https://github.com/microsoft/deepspeed). \n+DeepSpeed ZeRO is a powerful technology for training and loading very large models for inference with limited GPU resources, making it more accessible to everyone. To learn more about DeepSpeed, feel free to read the [blog posts](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [documentation](https://www.deepspeed.ai/getting-started/), and [GitHub repository](https://github.com/deepspeedai/DeepSpeed). \n \n The following papers are also a great resource for learning more about ZeRO:\n "
        },
        {
            "sha": "0b9e28656c0938be4264ae945e999b728a8e0fe7",
            "filename": "docs/source/en/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdeepspeed.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeed\n \n-[DeepSpeed](https://github.com/microsoft/DeepSpeed), powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the [`Trainer`] class and most of the setup is automatically taken care of for you. \n+[DeepSpeed](https://github.com/deepspeedai/DeepSpeed), powered by Zero Redundancy Optimizer (ZeRO), is an optimization library for training and fitting very large models onto a GPU. It is available in several ZeRO stages, where each stage progressively saves more GPU memory by partitioning the optimizer state, gradients, parameters, and enabling offloading to a CPU or NVMe. DeepSpeed is integrated with the [`Trainer`] class and most of the setup is automatically taken care of for you. \n \n However, if you want to use DeepSpeed without the [`Trainer`], Transformers provides a [`HfDeepSpeedConfig`] class.\n "
        },
        {
            "sha": "d60c61020c765914858b30e8fd9e0cde3fafc77c",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -476,7 +476,7 @@ And GPU1 does the same by enlisting GPU3 to its aid.\n Since each dimension requires at least 2 GPUs, here you'd need at least 4 GPUs.\n \n Implementations:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)\n@@ -497,7 +497,7 @@ This diagram is from a blog post [3D parallelism: Scaling to trillion-parameter\n Since each dimension requires at least 2 GPUs, here you'd need at least 8 GPUs.\n \n Implementations:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed also includes an even more efficient DP, which they call ZeRO-DP.\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) - DeepSpeed also includes an even more efficient DP, which they call ZeRO-DP.\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)"
        },
        {
            "sha": "52a2cb3d32d166732447ac5bce173c3786882348",
            "filename": "docs/source/ja/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fdeepspeed.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeed Integration\n \n-[DeepSpeed](https://github.com/microsoft/DeepSpeed) ã¯ã€[ZeRO è«–æ–‡](https://arxiv.org/abs/1910.02054) ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ç¾åœ¨ã€æ¬¡ã®ã‚‚ã®ã‚’å®Œå…¨ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n+[DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ã¯ã€[ZeRO è«–æ–‡](https://arxiv.org/abs/1910.02054) ã§èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã™ã¹ã¦ã‚’å®Ÿè£…ã—ã¾ã™ã€‚ç¾åœ¨ã€æ¬¡ã®ã‚‚ã®ã‚’å®Œå…¨ã«ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n \n 1. ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹åˆ†å‰² (ZeRO ã‚¹ãƒ†ãƒ¼ã‚¸ 1)\n 2. å‹¾é…åˆ†å‰² (ZeRO ã‚¹ãƒ†ãƒ¼ã‚¸ 2)\n@@ -32,7 +32,7 @@ DeepSpeed ZeRO-2 ã¯ã€ãã®æ©Ÿèƒ½ãŒæ¨è«–ã«ã¯å½¹ã«ç«‹ãŸãªã„ãŸã‚ã€ä¸»\n DeepSpeed ZeRO-3 ã¯ã€å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã® GPU ã«ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ãŸã‚ã€æ¨è«–ã«ã‚‚ä½¿ç”¨ã§ãã¾ã™ã€‚\n å˜ä¸€ã® GPU ã§ã¯ä¸å¯èƒ½ã§ã™ã€‚\n \n-ğŸ¤— Transformers ã¯ã€2 ã¤ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»‹ã—ã¦ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ã‚’çµ±åˆã—ã¾ã™ã€‚\n+ğŸ¤— Transformers ã¯ã€2 ã¤ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä»‹ã—ã¦ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ã‚’çµ±åˆã—ã¾ã™ã€‚\n \n 1. [`Trainer`] ã«ã‚ˆã‚‹ã‚³ã‚¢ DeepSpeed æ©Ÿèƒ½ã®çµ±åˆã€‚ä½•ã§ã‚‚ã‚„ã£ã¦ãã‚Œã‚‹ã‚¿ã‚¤ãƒ—ã§ã™\n    çµ±åˆã®å ´åˆ - ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹ã‹ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã™ã‚‹ã ã‘ã§ã€ä»–ã«ä½•ã‚‚ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ãŸã„ã¦ã„ã®\n@@ -78,7 +78,7 @@ pip install deepspeed\n pip install transformers[deepspeed]\n ```\n \n-ã¾ãŸã¯ã€[DeepSpeed ã® GitHub ãƒšãƒ¼ã‚¸](https://github.com/microsoft/deepspeed#installation) ã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n+ã¾ãŸã¯ã€[DeepSpeed ã® GitHub ãƒšãƒ¼ã‚¸](https://github.com/deepspeedai/DeepSpeed#installation) ã§è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n [é«˜åº¦ãªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«](https://www.deepspeed.ai/tutorials/advanced-install/)ã€‚\n \n ãã‚Œã§ã‚‚ãƒ“ãƒ«ãƒ‰ã«è‹¦åŠ´ã™ã‚‹å ´åˆã¯ã€ã¾ãš [CUDA æ‹¡å¼µæ©Ÿèƒ½ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« ãƒãƒ¼ãƒˆ](trainer#cuda-extension-installation-notes) ã‚’å¿…ãšèª­ã‚“ã§ãã ã•ã„ã€‚\n@@ -89,7 +89,7 @@ pip install transformers[deepspeed]\n DeepSpeed ã®ãƒ­ãƒ¼ã‚«ãƒ« ãƒ“ãƒ«ãƒ‰ã‚’ä½œæˆã™ã‚‹ã«ã¯:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n@@ -113,7 +113,7 @@ CUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capa\n è¤‡æ•°ã®ãƒã‚·ãƒ³ã§åŒã˜ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€ãƒã‚¤ãƒŠãƒª ãƒ›ã‚¤ãƒ¼ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\n@@ -154,7 +154,7 @@ _CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24\n ç›®çš„ã®ã‚¢ãƒ¼ãƒã‚’æ˜ç¤ºçš„ã«æŒ‡å®šã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n \n ææ¡ˆã•ã‚ŒãŸã“ã¨ã‚’ã™ã¹ã¦è©¦ã—ã¦ã‚‚ã¾ã ãƒ“ãƒ«ãƒ‰ã®å•é¡ŒãŒç™ºç”Ÿã™ã‚‹å ´åˆã¯ã€GitHub ã®å•é¡Œã«é€²ã‚“ã§ãã ã•ã„ã€‚\n-[ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¹ãƒ”ãƒ¼ãƒ‰](https://github.com/microsoft/DeepSpeed/issues)ã€\n+[ãƒ‡ã‚£ãƒ¼ãƒ—ã‚¹ãƒ”ãƒ¼ãƒ‰](https://github.com/deepspeedai/DeepSpeed/issues)ã€\n \n <a id='deepspeed-multi-gpu'></a>\n \n@@ -481,11 +481,11 @@ deepspeed examples/pytorch/translation/run_translation.py ...\n è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ä½¿ç”¨ã§ãã‚‹ DeepSpeed è¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å®Œå…¨ãªã‚¬ã‚¤ãƒ‰ã«ã¤ã„ã¦ã¯ã€æ¬¡ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n [æ¬¡ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://www.deepspeed.ai/docs/config-json/) ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚\n \n-ã•ã¾ã–ã¾ãªå®Ÿéš›ã®ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹æ•°åã® DeepSpeed æ§‹æˆä¾‹ã‚’ [DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples)ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n+ã•ã¾ã–ã¾ãªå®Ÿéš›ã®ãƒ‹ãƒ¼ã‚ºã«å¯¾å¿œã™ã‚‹æ•°åã® DeepSpeed æ§‹æˆä¾‹ã‚’ [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples)ã§è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n ãƒªãƒã‚¸ãƒˆãƒª:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeedExamples\n+git clone https://github.com/deepspeedai/DeepSpeedExamples\n cd DeepSpeedExamples\n find . -name '*json'\n ```\n@@ -497,7 +497,7 @@ find . -name '*json'\n grep -i Lamb $(find . -name '*json')\n ```\n \n-ã•ã‚‰ã«ã„ãã¤ã‹ã®ä¾‹ãŒ [ãƒ¡ã‚¤ãƒ³ ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/microsoft/DeepSpeed) ã«ã‚‚ã‚ã‚Šã¾ã™ã€‚\n+ã•ã‚‰ã«ã„ãã¤ã‹ã®ä¾‹ãŒ [ãƒ¡ã‚¤ãƒ³ ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/deepspeedai/DeepSpeed) ã«ã‚‚ã‚ã‚Šã¾ã™ã€‚\n \n DeepSpeed ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€å¸¸ã« DeepSpeed æ§‹æˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ä¸€éƒ¨ã®æ§‹æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã¯\n ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³çµŒç”±ã§è¨­å®šã—ã¾ã™ã€‚å¾®å¦™ãªé•ã„ã«ã¤ã„ã¦ã¯ã€ã“ã®ã‚¬ã‚¤ãƒ‰ã®æ®‹ã‚Šã®éƒ¨åˆ†ã§èª¬æ˜ã—ã¾ã™ã€‚\n@@ -868,7 +868,7 @@ ZeRO-Infinity ã¯ã€GPU ã¨ CPU ãƒ¡ãƒ¢ãƒªã‚’ NVMe ãƒ¡ãƒ¢ãƒªã§æ‹¡å¼µã™ã‚‹ã“ã¨\n æ›¸ãè¾¼ã¿ã§ã¯ã€èª­ã¿å–ã‚Šæœ€å¤§ 3.5 GB/ç§’ã€æ›¸ãè¾¼ã¿æœ€å¤§ 3 GB/ç§’ã®ãƒ”ãƒ¼ã‚¯é€Ÿåº¦ãŒå¾—ã‚‰ã‚Œã¾ã™)ã€‚\n \n æœ€é©ãª`aio`æ§‹æˆãƒ–ãƒ­ãƒƒã‚¯ã‚’è¦‹ã¤ã‘ã‚‹ã«ã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆè¨­å®šã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-[ã“ã“ã§èª¬æ˜](https://github.com/microsoft/DeepSpeed/issues/998)ã€‚\n+[ã“ã“ã§èª¬æ˜](https://github.com/deepspeedai/DeepSpeed/issues/998)ã€‚\n \n <a id='deepspeed-zero2-zero3-performance'></a>\n \n@@ -1934,7 +1934,7 @@ SW: Model with 2783M total params, 65M largest layer params.\n   å•é¡ŒãŒè§£æ±ºã—ãªã„å ´åˆã«ã®ã¿ã€Deepspeed ã«ã¤ã„ã¦è¨€åŠã—ã€å¿…è¦ãªè©³ç´°ã‚’ã™ã¹ã¦æä¾›ã—ã¦ãã ã•ã„ã€‚\n \n - å•é¡ŒãŒçµ±åˆéƒ¨åˆ†ã§ã¯ãªã DeepSpeed ã‚³ã‚¢ã«ã‚ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ãªå ´åˆã¯ã€å•é¡Œã‚’æå‡ºã—ã¦ãã ã•ã„ã€‚\n-  [Deepspeed](https://github.com/microsoft/DeepSpeed/) ã‚’ç›´æ¥ä½¿ç”¨ã—ã¾ã™ã€‚ã‚ˆãã‚ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ã”å®‰å¿ƒãã ã•ã„ã€‚\n+  [Deepspeed](https://github.com/deepspeedai/DeepSpeed/) ã‚’ç›´æ¥ä½¿ç”¨ã—ã¾ã™ã€‚ã‚ˆãã‚ã‹ã‚‰ãªã„å ´åˆã§ã‚‚ã€ã”å®‰å¿ƒãã ã•ã„ã€‚\n   ã©ã¡ã‚‰ã®å•é¡Œãƒˆãƒ©ãƒƒã‚«ãƒ¼ã§ã‚‚å•é¡Œã‚ã‚Šã¾ã›ã‚“ã€‚æŠ•ç¨¿ã•ã‚ŒãŸã‚‰ãã‚Œã‚’åˆ¤æ–­ã—ã€æ¬¡ã®å ´åˆã¯åˆ¥ã®å•é¡Œãƒˆãƒ©ãƒƒã‚«ãƒ¼ã«ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆã—ã¾ã™ã€‚\n   ãã†ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚\n \n@@ -1994,7 +1994,7 @@ SW: Model with 2783M total params, 65M largest layer params.\n \n ### Notes\n \n-- DeepSpeed ã«ã¯ pip ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½ãª PyPI ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã™ãŒã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚ˆã†ã«ã€ã¾ãŸæœ‰åŠ¹ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€[ã‚½ãƒ¼ã‚¹](https://github.com/microsoft/deepspeed#installation) ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n+- DeepSpeed ã«ã¯ pip ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å¯èƒ½ãª PyPI ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã™ãŒã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã«æœ€ã‚‚é©åˆã™ã‚‹ã‚ˆã†ã«ã€ã¾ãŸæœ‰åŠ¹ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã¯ã€[ã‚½ãƒ¼ã‚¹](https://github.com/deepspeedai/DeepSpeed#installation) ã‹ã‚‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹ã“ã¨ã‚’å¼·ããŠå‹§ã‚ã—ã¾ã™ã€‚\n   1 ãƒ“ãƒƒãƒˆ Adam ãªã©ã®ç‰¹å®šã®æ©Ÿèƒ½ã¯ã€pypi ãƒ‡ã‚£ã‚¹ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚\n - ğŸ¤— Transformers ã§ DeepSpeed ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã« [`Trainer`] ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ - ä»»æ„ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã¾ã™\n   å¾Œè€…ã¯ [DeepSpeed çµ±åˆæ‰‹é †](https://www.deepspeed.ai/getting-started/#writing-deepspeed-models) ã«å¾“ã£ã¦èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n@@ -2239,7 +2239,7 @@ RUN_SLOW=1 pytest tests/deepspeed\n \n ## Main DeepSpeed Resources\n \n-- [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® github](https://github.com/microsoft/deepspeed)\n+- [ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã® github](https://github.com/deepspeedai/DeepSpeed)\n - [ä½¿ç”¨æ–¹æ³•ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://www.deepspeed.ai/getting-started/)\n - [API ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://deepspeed.readthedocs.io/en/latest/index.html)\n - [ãƒ–ãƒ­ã‚°æŠ•ç¨¿](https://www.microsoft.com/en-us/research/search/?q=deepspeed)\n@@ -2251,4 +2251,4 @@ RUN_SLOW=1 pytest tests/deepspeed\n - [ZeRO-Infinity: æ¥µé™ã‚¹ã‚±ãƒ¼ãƒ«ã®æ·±å±¤å­¦ç¿’ã®ãŸã‚ã® GPU ãƒ¡ãƒ¢ãƒªã®å£ã‚’æ‰“ã¡ç ´ã‚‹](https://arxiv.org/abs/2104.07857)\n \n æœ€å¾Œã«ã€HuggingFace [`Trainer`] ã¯ DeepSpeed ã®ã¿ã‚’çµ±åˆã—ã¦ã„ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ã€‚\n-DeepSpeed ã®ä½¿ç”¨ã«é–¢ã—ã¦å•é¡Œã‚„è³ªå•ãŒã‚ã‚‹å ´åˆã¯ã€[DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues) ã«å•é¡Œã‚’æå‡ºã—ã¦ãã ã•ã„ã€‚\n+DeepSpeed ã®ä½¿ç”¨ã«é–¢ã—ã¦å•é¡Œã‚„è³ªå•ãŒã‚ã‚‹å ´åˆã¯ã€[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues) ã«å•é¡Œã‚’æå‡ºã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "sha": "0a015e1150336e3004f76d1987ae82445742ad40",
            "filename": "docs/source/ja/main_classes/trainer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Ftrainer.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -199,7 +199,7 @@ _python_ã€_numpy_ã€ãŠã‚ˆã³ _pytorch_ ã® RNG çŠ¶æ…‹ã¯ã€ãã®ãƒã‚§ãƒƒã‚¯\n torchrun --nproc_per_node=2  trainer-program.py ...\n ```\n \n-[`accelerate`](https://github.com/huggingface/accelerate) ã¾ãŸã¯ [`deepspeed`](https://github.com/microsoft/DeepSpeed) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æ¬¡ã‚’ä½¿ç”¨ã—ã¦åŒã˜ã“ã¨ã‚’é”æˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã®ä¸€ã¤ï¼š\n+[`accelerate`](https://github.com/huggingface/accelerate) ã¾ãŸã¯ [`deepspeed`](https://github.com/deepspeedai/DeepSpeed) ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€æ¬¡ã‚’ä½¿ç”¨ã—ã¦åŒã˜ã“ã¨ã‚’é”æˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã®ä¸€ã¤ï¼š\n \n ```bash\n accelerate launch --num_processes 2 trainer-program.py ...\n@@ -291,7 +291,7 @@ export CUDA_VISIBLE_DEVICES=1,0\n [`Trainer`] ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’åŠ‡çš„ã«æ”¹å–„ã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚ˆã†ã«æ‹¡å¼µã•ã‚Œã¾ã—ãŸã€‚\n æ™‚é–“ã¨ã¯ã‚‹ã‹ã«å¤§ããªãƒ¢ãƒ‡ãƒ«ã«é©åˆã—ã¾ã™ã€‚\n \n-ç¾åœ¨ã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ãŠã‚ˆã³ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚è«–æ–‡ [ZeRO: ãƒ¡ãƒ¢ãƒªã®æœ€é©åŒ–å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å‘ã‘ã¦ã€Samyam Rajbhandariã€Jeff Rasleyã€Olatunji Ruwaseã€Yuxiong He è‘—](https://arxiv.org/abs/1910.02054)ã€‚\n+ç¾åœ¨ã€ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ã®ã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ãŠã‚ˆã³ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html) ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚è«–æ–‡ [ZeRO: ãƒ¡ãƒ¢ãƒªã®æœ€é©åŒ–å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å‘ã‘ã¦ã€Samyam Rajbhandariã€Jeff Rasleyã€Olatunji Ruwaseã€Yuxiong He è‘—](https://arxiv.org/abs/1910.02054)ã€‚\n \n ã“ã®æä¾›ã•ã‚Œã‚‹ã‚µãƒãƒ¼ãƒˆã¯ã€ã“ã®è¨˜äº‹ã®åŸ·ç­†æ™‚ç‚¹ã§ã¯æ–°ã—ãã¦å®Ÿé¨“çš„ãªã‚‚ã®ã§ã™ã€‚ DeepSpeed ã¨ PyTorch FSDP ã®ã‚µãƒãƒ¼ãƒˆã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã§ã‚ã‚Šã€ãã‚Œã«é–¢ã™ã‚‹å•é¡Œã¯æ­“è¿ã—ã¾ã™ãŒã€FairScale çµ±åˆã¯ PyTorch ãƒ¡ã‚¤ãƒ³ã«çµ±åˆã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚‚ã†ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã›ã‚“ ([PyTorch FSDP çµ±åˆ](#pytorch-fully-sharded-data-parallel))\n \n@@ -301,7 +301,7 @@ export CUDA_VISIBLE_DEVICES=1,0\n \n ã“ã®è¨˜äº‹ã®åŸ·ç­†æ™‚ç‚¹ã§ã¯ã€Deepspeed ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€CUDA C++ ã‚³ãƒ¼ãƒ‰ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n \n-ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å•é¡Œã¯ã€[Deepspeed](https://github.com/microsoft/DeepSpeed/issues) ã®å¯¾å¿œã™ã‚‹ GitHub ã®å•é¡Œã‚’é€šã˜ã¦å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ“ãƒ«ãƒ‰ä¸­ã«ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ä¸€èˆ¬çš„ãªå•é¡ŒãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚\n+ã™ã¹ã¦ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã®å•é¡Œã¯ã€[Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues) ã®å¯¾å¿œã™ã‚‹ GitHub ã®å•é¡Œã‚’é€šã˜ã¦å¯¾å‡¦ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ãŒã€ãƒ“ãƒ«ãƒ‰ä¸­ã«ç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ä¸€èˆ¬çš„ãªå•é¡ŒãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚\n CUDA æ‹¡å¼µæ©Ÿèƒ½ã‚’æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ PyTorch æ‹¡å¼µæ©Ÿèƒ½ã€‚\n \n ã—ãŸãŒã£ã¦ã€æ¬¡ã®æ“ä½œã‚’å®Ÿè¡Œä¸­ã« CUDA é–¢é€£ã®ãƒ“ãƒ«ãƒ‰ã®å•é¡ŒãŒç™ºç”Ÿã—ãŸå ´åˆã¯ã€æ¬¡ã®ã¨ãŠã‚Šã§ã™ã€‚"
        },
        {
            "sha": "613ccfa20bad228f9b46ca44dff97d18063b4201",
            "filename": "docs/source/ja/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_train_gpu_many.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -360,7 +360,7 @@ by [@anton-l](https://github.com/anton-l)ã€‚\n SageMakerã¯ã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªå‡¦ç†ã®ãŸã‚ã«TPã¨DPã‚’çµ„ã¿åˆã‚ã›ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n \n ä»£æ›¿åï¼š\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed)ã¯ã“ã‚Œã‚’ã€Œãƒ†ãƒ³ã‚½ãƒ«ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã€ã¨å‘¼ã³ã¾ã™ã€‚è©³ç´°ã¯[DeepSpeedã®ç‰¹å¾´](https://www.deepspeed.ai/training/#model-parallelism)ã‚’ã”è¦§ãã ã•ã„ã€‚\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)ã¯ã“ã‚Œã‚’ã€Œãƒ†ãƒ³ã‚½ãƒ«ã‚¹ãƒ©ã‚¤ã‚·ãƒ³ã‚°ã€ã¨å‘¼ã³ã¾ã™ã€‚è©³ç´°ã¯[DeepSpeedã®ç‰¹å¾´](https://www.deepspeed.ai/training/#model-parallelism)ã‚’ã”è¦§ãã ã•ã„ã€‚\n \n å®Ÿè£…ä¾‹:\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«å›ºæœ‰ã®å†…éƒ¨å®Ÿè£…ãŒã‚ã‚Šã¾ã™ã€‚\n@@ -384,7 +384,7 @@ DeepSpeedã®[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://www.deepspeed.ai/t\n å„æ¬¡å…ƒã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®GPUãŒå¿…è¦ã§ã™ã®ã§ã€ã“ã“ã§ã¯å°‘ãªãã¨ã‚‚4ã¤ã®GPUãŒå¿…è¦ã§ã™ã€‚\n \n å®Ÿè£…ä¾‹:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)\n@@ -403,7 +403,7 @@ DeepSpeedã®[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://www.deepspeed.ai/t\n å„æ¬¡å…ƒã«ã¯å°‘ãªãã¨ã‚‚2ã¤ã®GPUãŒå¿…è¦ã§ã™ã®ã§ã€ã“ã“ã§ã¯å°‘ãªãã¨ã‚‚8ã¤ã®GPUãŒå¿…è¦ã§ã™ã€‚\n \n å®Ÿè£…ä¾‹:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeedã«ã¯ã€ã•ã‚‰ã«åŠ¹ç‡çš„ãªDPã§ã‚ã‚‹ZeRO-DPã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) - DeepSpeedã«ã¯ã€ã•ã‚‰ã«åŠ¹ç‡çš„ãªDPã§ã‚ã‚‹ZeRO-DPã¨å‘¼ã°ã‚Œã‚‹ã‚‚ã®ã‚‚å«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)"
        },
        {
            "sha": "48758ad9d52e2ddc6d7ed8e29c1fe580bed9cd70",
            "filename": "docs/source/ko/deepspeed.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fko%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fdeepspeed.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -28,7 +28,7 @@ GPUê°€ ì œí•œëœ í™˜ê²½ì—ì„œ ZeROëŠ” ìµœì í™” ë©”ëª¨ë¦¬ì™€ ê³„ì‚°ì„ GPUì—ì„œ\n \n ## ì„¤ì¹˜[[installation]]\n \n-DeepSpeedëŠ” PyPI ë˜ëŠ” Transformersì—ì„œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ìì„¸í•œ ì„¤ì¹˜ ì˜µì…˜ì€ DeepSpeed [ì„¤ì¹˜ ìƒì„¸ì‚¬í•­](https://www.deepspeed.ai/tutorials/advanced-install/) ë˜ëŠ” GitHub [README](https://github.com/microsoft/deepspeed#installation)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”).\n+DeepSpeedëŠ” PyPI ë˜ëŠ” Transformersì—ì„œ ì„¤ì¹˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤(ìì„¸í•œ ì„¤ì¹˜ ì˜µì…˜ì€ DeepSpeed [ì„¤ì¹˜ ìƒì„¸ì‚¬í•­](https://www.deepspeed.ai/tutorials/advanced-install/) ë˜ëŠ” GitHub [README](https://github.com/deepspeedai/DeepSpeed#installation)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”).\n \n <Tip>\n \n@@ -114,10 +114,10 @@ DeepSpeedëŠ” íŠ¸ë ˆì´ë‹ ì‹¤í–‰ ë°©ë²•ì„ êµ¬ì„±í•˜ëŠ” ëª¨ë“  ë§¤ê°œë³€ìˆ˜ê°€ \n \n <Tip>\n \n-DeepSpeed êµ¬ì„± ì˜µì…˜ì˜ ì „ì²´ ëª©ë¡ì€ [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ [DeepSpeedExamples](https://github.com/microsoft/DeepSpeedExamples) ë¦¬í¬ì§€í† ë¦¬ ë˜ëŠ” ê¸°ë³¸ [DeepSpeed](https://github.com/microsoft/DeepSpeed) ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë‹¤ì–‘í•œ DeepSpeed êµ¬ì„± ì˜ˆì œì— ëŒ€í•œ ë³´ë‹¤ ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì˜ˆì œë¥¼ ë¹ ë¥´ê²Œ ì°¾ìœ¼ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ì„¸ìš”:\n+DeepSpeed êµ¬ì„± ì˜µì…˜ì˜ ì „ì²´ ëª©ë¡ì€ [DeepSpeed Configuration JSON](https://www.deepspeed.ai/docs/config-json/)ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ [DeepSpeedExamples](https://github.com/deepspeedai/DeepSpeedExamples) ë¦¬í¬ì§€í† ë¦¬ ë˜ëŠ” ê¸°ë³¸ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) ë¦¬í¬ì§€í† ë¦¬ì—ì„œ ë‹¤ì–‘í•œ DeepSpeed êµ¬ì„± ì˜ˆì œì— ëŒ€í•œ ë³´ë‹¤ ì‹¤ìš©ì ì¸ ì˜ˆì œë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì˜ˆì œë¥¼ ë¹ ë¥´ê²Œ ì°¾ìœ¼ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ì„¸ìš”:\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeedExamples\n+git clone https://github.com/deepspeedai/DeepSpeedExamples\n cd DeepSpeedExamples\n find . -name '*json'\n # Lamb ì˜µí‹°ë§ˆì´ì € ìƒ˜í”Œ ì°¾ê¸°\n@@ -303,7 +303,7 @@ ZeRO-3ë¡œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì´ˆê¸°í™”í•˜ê³  ë§¤ê°œë³€ìˆ˜ì— ì•¡ì„¸ìŠ¤í•˜ëŠ” \n \n [ZeRO-Infinity](https://hf.co/papers/2104.07857)ë¥¼ ì‚¬ìš©í•˜ë©´ ëª¨ë¸ ìƒíƒœë¥¼ CPU ë°/ë˜ëŠ” NVMeë¡œ ì˜¤í”„ë¡œë“œí•˜ì—¬ ë” ë§ì€ ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ íŒŒí‹°ì…”ë‹ ë° íƒ€ì¼ë§ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ê° GPUëŠ” ì˜¤í”„ë¡œë”© ì¤‘ì— ë§¤ìš° ì ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì‹  NVMeëŠ” í›ˆë ¨ í”„ë¡œì„¸ìŠ¤ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒë³´ë‹¤ í›¨ì”¬ ë” í° ì´ ë©”ëª¨ë¦¬ í’€ì— ë§ì¶œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ZeRO-Infinityì—ëŠ” ZeRO-3ê°€ í•„ìš”í•©ë‹ˆë‹¤.\n \n-ì‚¬ìš© ê°€ëŠ¥í•œ CPU ë°/ë˜ëŠ” NVMe ë©”ëª¨ë¦¬ì— ë”°ë¼ [ì˜µí‹°ë§ˆì´ì €](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)ì™€ [ë§¤ê°œë³€ìˆ˜](https://www.deepspeed.ai/docs/config-json/#parameter-offloading) ì¤‘ í•˜ë‚˜ë§Œ ì˜¤í”„ë¡œë“œí•˜ê±°ë‚˜ ì•„ë¬´ê²ƒë„ ì˜¤í”„ë¡œë“œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì¼ë°˜ í•˜ë“œ ë“œë¼ì´ë¸Œë‚˜ ì†”ë¦¬ë“œ ìŠ¤í…Œì´íŠ¸ ë“œë¼ì´ë¸Œì—ì„œë„ ì‘ë™í•˜ì§€ë§Œ ì†ë„ê°€ í˜„ì €íˆ ëŠë ¤ì§€ë¯€ë¡œ `nvme_path`ê°€ NVMe ì¥ì¹˜ë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ìµœì‹  NVMeë¥¼ ì‚¬ìš©í•˜ë©´ ì½ê¸° ì‘ì—…ì˜ ê²½ìš° ìµœëŒ€ 3.5GB/s, ì“°ê¸° ì‘ì—…ì˜ ê²½ìš° ìµœëŒ€ 3GB/sì˜ ì „ì†¡ ì†ë„ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, íŠ¸ë ˆì´ë‹ ì„¤ì •ì—ì„œ [ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰í•˜ê¸°](https://github.com/microsoft/DeepSpeed/issues/998)ì„ í†µí•´ ìµœì ì˜ 'aio' êµ¬ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤.\n+ì‚¬ìš© ê°€ëŠ¥í•œ CPU ë°/ë˜ëŠ” NVMe ë©”ëª¨ë¦¬ì— ë”°ë¼ [ì˜µí‹°ë§ˆì´ì €](https://www.deepspeed.ai/docs/config-json/#optimizer-offloading)ì™€ [ë§¤ê°œë³€ìˆ˜](https://www.deepspeed.ai/docs/config-json/#parameter-offloading) ì¤‘ í•˜ë‚˜ë§Œ ì˜¤í”„ë¡œë“œí•˜ê±°ë‚˜ ì•„ë¬´ê²ƒë„ ì˜¤í”„ë¡œë“œí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì¼ë°˜ í•˜ë“œ ë“œë¼ì´ë¸Œë‚˜ ì†”ë¦¬ë“œ ìŠ¤í…Œì´íŠ¸ ë“œë¼ì´ë¸Œì—ì„œë„ ì‘ë™í•˜ì§€ë§Œ ì†ë„ê°€ í˜„ì €íˆ ëŠë ¤ì§€ë¯€ë¡œ `nvme_path`ê°€ NVMe ì¥ì¹˜ë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ”ì§€ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. ìµœì‹  NVMeë¥¼ ì‚¬ìš©í•˜ë©´ ì½ê¸° ì‘ì—…ì˜ ê²½ìš° ìµœëŒ€ 3.5GB/s, ì“°ê¸° ì‘ì—…ì˜ ê²½ìš° ìµœëŒ€ 3GB/sì˜ ì „ì†¡ ì†ë„ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, íŠ¸ë ˆì´ë‹ ì„¤ì •ì—ì„œ [ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰í•˜ê¸°](https://github.com/deepspeedai/DeepSpeed/issues/998)ì„ í†µí•´ ìµœì ì˜ 'aio' êµ¬ì„±ì„ ê²°ì •í•©ë‹ˆë‹¤.\n \n ì•„ë˜ ì˜ˆì œ ZeRO-3/Infinity êµ¬ì„± íŒŒì¼ì€ ëŒ€ë¶€ë¶„ì˜ ë§¤ê°œë³€ìˆ˜ ê°’ì„ `auto`ìœ¼ë¡œ ì„¤ì •í•˜ê³  ìˆì§€ë§Œ, ìˆ˜ë™ìœ¼ë¡œ ê°’ì„ ì¶”ê°€í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n \n@@ -1141,7 +1141,7 @@ rank1:\n \n ## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…[[troubleshoot]]\n \n-ë¬¸ì œê°€ ë°œìƒí•˜ë©´ DeepSpeedê°€ ë¬¸ì œì˜ ì›ì¸ì´ ì•„ë‹Œ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ(ì•„ì£¼ ëª…ë°±í•˜ê³  ì˜ˆì™¸ì ìœ¼ë¡œ DeepSpeed ëª¨ë“ˆì„ ë³¼ ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´) DeepSpeedê°€ ë¬¸ì œì˜ ì›ì¸ì¸ì§€ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤! ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” DeepSpeed ì—†ì´ ì„¤ì •ì„ ë‹¤ì‹œ ì‹œë„í•˜ê³  ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë¬¸ì œë¥¼ ì‹ ê³ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¬¸ì œê°€ í•µì‹¬ì ì¸ DeepSpeed ë¬¸ì œì´ê³  transformersì™€ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, [DeepSpeed ë¦¬í¬ì§€í† ë¦¬](https://github.com/microsoft/DeepSpeed)ì—ì„œ ì´ìŠˆë¥¼ ê°œì„¤í•˜ì„¸ìš”.\n+ë¬¸ì œê°€ ë°œìƒí•˜ë©´ DeepSpeedê°€ ë¬¸ì œì˜ ì›ì¸ì´ ì•„ë‹Œ ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ(ì•„ì£¼ ëª…ë°±í•˜ê³  ì˜ˆì™¸ì ìœ¼ë¡œ DeepSpeed ëª¨ë“ˆì„ ë³¼ ìˆ˜ ìˆëŠ” ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´) DeepSpeedê°€ ë¬¸ì œì˜ ì›ì¸ì¸ì§€ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤! ì²« ë²ˆì§¸ ë‹¨ê³„ëŠ” DeepSpeed ì—†ì´ ì„¤ì •ì„ ë‹¤ì‹œ ì‹œë„í•˜ê³  ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ë¬¸ì œë¥¼ ì‹ ê³ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë¬¸ì œê°€ í•µì‹¬ì ì¸ DeepSpeed ë¬¸ì œì´ê³  transformersì™€ ê´€ë ¨ì´ ì—†ëŠ” ê²½ìš°, [DeepSpeed ë¦¬í¬ì§€í† ë¦¬](https://github.com/deepspeedai/DeepSpeed)ì—ì„œ ì´ìŠˆë¥¼ ê°œì„¤í•˜ì„¸ìš”.\n \n transformersì™€ ê´€ë ¨ëœ ì´ìŠˆë¥¼ ê°œì„¤í•  ë•Œì—ëŠ” ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•´ ì£¼ì„¸ìš”:\n \n@@ -1211,7 +1211,7 @@ NVMe ë° ZeRO-3ë¥¼ ì„¤ì •í•œ ê²½ìš° NVMeë¡œ ì˜¤í”„ë¡œë“œë¥¼ ì‹¤í—˜í•´ ë³´ì„¸ìš”(\n \n ## ë¦¬ì†ŒìŠ¤[[resources]]\n \n-DeepSpeed ZeROëŠ” ì œí•œëœ GPU ë¦¬ì†ŒìŠ¤ë¡œ ì¶”ë¡ ì„ ìœ„í•´ ë§¤ìš° í° ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¡œë“œí•˜ëŠ” ê°•ë ¥í•œ ê¸°ìˆ ë¡œ, ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DeepSpeedì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [ê³µì‹ ë¬¸ì„œ](https://www.deepspeed.ai/getting-started/), [ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬](https://github.com/microsoft/deepspeed)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. \n+DeepSpeed ZeROëŠ” ì œí•œëœ GPU ë¦¬ì†ŒìŠ¤ë¡œ ì¶”ë¡ ì„ ìœ„í•´ ë§¤ìš° í° ëª¨ë¸ì„ í›ˆë ¨í•˜ê³  ë¡œë“œí•˜ëŠ” ê°•ë ¥í•œ ê¸°ìˆ ë¡œ, ëˆ„êµ¬ë‚˜ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. DeepSpeedì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸](https://www.microsoft.com/en-us/research/search/?q=deepspeed), [ê³µì‹ ë¬¸ì„œ](https://www.deepspeed.ai/getting-started/), [ê¹ƒí—ˆë¸Œ ë¦¬í¬ì§€í† ë¦¬](https://github.com/deepspeedai/DeepSpeed)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. \n \n ë‹¤ìŒ ë¬¸ì„œë„ ZeROì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³¼ ìˆ˜ ìˆëŠ” í›Œë¥­í•œ ìë£Œì…ë‹ˆë‹¤:\n "
        },
        {
            "sha": "b8553ea499ef10af4746e50aac6a2362309cdb55",
            "filename": "docs/source/ko/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_train_gpu_many.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -386,7 +386,7 @@ DeepSpeed [pipeline tutorial](https://www.deepspeed.ai/tutorials/pipeline/)ì—\n ê° ì°¨ì›ë§ˆë‹¤ ì ì–´ë„ 2ê°œì˜ GPUê°€ í•„ìš”í•˜ë¯€ë¡œ ìµœì†Œí•œ 4ê°œì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.\n \n êµ¬í˜„:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)\n@@ -405,7 +405,7 @@ DeepSpeed [pipeline tutorial](https://www.deepspeed.ai/tutorials/pipeline/)ì—\n ê° ì°¨ì›ë§ˆë‹¤ ì ì–´ë„ 2ê°œì˜ GPUê°€ í•„ìš”í•˜ë¯€ë¡œ ìµœì†Œí•œ 8ê°œì˜ GPUê°€ í•„ìš”í•©ë‹ˆë‹¤.\n \n êµ¬í˜„:\n-- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeedëŠ” ë”ìš± íš¨ìœ¨ì ì¸ DPì¸ ZeRO-DPë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤.\n+- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) - DeepSpeedëŠ” ë”ìš± íš¨ìœ¨ì ì¸ DPì¸ ZeRO-DPë¼ê³ ë„ ë¶€ë¦…ë‹ˆë‹¤.\n - [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)\n - [Varuna](https://github.com/microsoft/varuna)\n - [SageMaker](https://arxiv.org/abs/2111.05972)"
        },
        {
            "sha": "ddc8096d0abbc0e461f65891ccf77dc05921427c",
            "filename": "docs/source/zh/main_classes/deepspeed.md",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fdeepspeed.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # DeepSpeedé›†æˆ\n \n-[DeepSpeed](https://github.com/microsoft/DeepSpeed)å®ç°äº†[ZeROè®ºæ–‡](https://arxiv.org/abs/1910.02054)ä¸­æè¿°çš„æ‰€æœ‰å†…å®¹ã€‚ç›®å‰ï¼Œå®ƒæä¾›å¯¹ä»¥ä¸‹åŠŸèƒ½çš„å…¨é¢æ”¯æŒï¼š\n+[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)å®ç°äº†[ZeROè®ºæ–‡](https://arxiv.org/abs/1910.02054)ä¸­æè¿°çš„æ‰€æœ‰å†…å®¹ã€‚ç›®å‰ï¼Œå®ƒæä¾›å¯¹ä»¥ä¸‹åŠŸèƒ½çš„å…¨é¢æ”¯æŒï¼š\n \n 1. ä¼˜åŒ–å™¨çŠ¶æ€åˆ†åŒºï¼ˆZeRO stage 1ï¼‰\n 2. æ¢¯åº¦åˆ†åŒºï¼ˆZeRO stage 2ï¼‰\n@@ -31,7 +31,7 @@ DeepSpeed ZeRO-2ä¸»è¦ç”¨äºè®­ç»ƒï¼Œå› ä¸ºå®ƒçš„ç‰¹æ€§å¯¹æ¨ç†æ²¡æœ‰ç”¨å¤„ã€‚\n \n DeepSpeed ZeRO-3ä¹Ÿå¯ä»¥ç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå…è®¸å°†å•ä¸ªGPUæ— æ³•åŠ è½½çš„å¤§æ¨¡å‹åŠ è½½åˆ°å¤šä¸ªGPUä¸Šã€‚\n \n-ğŸ¤— Transformersé€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼é›†æˆäº†[DeepSpeed](https://github.com/microsoft/DeepSpeed)ï¼š\n+ğŸ¤— Transformersé€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼é›†æˆäº†[DeepSpeed](https://github.com/deepspeedai/DeepSpeed)ï¼š\n \n 1. é€šè¿‡[`Trainer`]é›†æˆæ ¸å¿ƒçš„DeepSpeedåŠŸèƒ½ã€‚è¿™æ˜¯ä¸€ç§â€œä¸ºæ‚¨å®Œæˆä¸€åˆ‡â€å¼çš„é›†æˆ - æ‚¨åªéœ€æä¾›è‡ªå®šä¹‰é…ç½®æ–‡ä»¶æˆ–ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡æ¿é…ç½®æ–‡ä»¶ã€‚æœ¬æ–‡æ¡£çš„å¤§éƒ¨åˆ†å†…å®¹éƒ½é›†ä¸­åœ¨è¿™ä¸ªåŠŸèƒ½ä¸Šã€‚\n 2. å¦‚æœæ‚¨ä¸ä½¿ç”¨[`Trainer`]å¹¶å¸Œæœ›åœ¨è‡ªå·±çš„Trainerä¸­é›†æˆDeepSpeedï¼Œé‚£ä¹ˆåƒ`from_pretrained`å’Œ`from_config`è¿™æ ·çš„æ ¸å¿ƒåŠŸèƒ½å‡½æ•°å°†åŒ…æ‹¬ZeRO stage 3åŠä»¥ä¸Šçš„DeepSpeedçš„åŸºç¡€éƒ¨åˆ†ï¼Œå¦‚`zero.Init`ã€‚è¦åˆ©ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·é˜…è¯»æœ‰å…³[éTrainer DeepSpeedé›†æˆ](#nontrainer-deepspeed-integration)çš„æ–‡æ¡£ã€‚\n@@ -72,7 +72,7 @@ pip install deepspeed\n pip install transformers[deepspeed]\n ```\n \n-æˆ–åœ¨ [DeepSpeed çš„ GitHub é¡µé¢](https://github.com/microsoft/deepspeed#installation) å’Œ\n+æˆ–åœ¨ [DeepSpeed çš„ GitHub é¡µé¢](https://github.com/deepspeedai/DeepSpeed#installation) å’Œ\n [é«˜çº§å®‰è£…](https://www.deepspeed.ai/tutorials/advanced-install/) ä¸­æŸ¥æ‰¾æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚\n \n å¦‚æœæ„å»ºè¿‡ç¨‹ä¸­ä»ç„¶é‡åˆ°é—®é¢˜ï¼Œè¯·é¦–å…ˆç¡®ä¿é˜…è¯» [CUDA æ‰©å±•å®‰è£…æ³¨æ„äº‹é¡¹](trainer#cuda-extension-installation-notes)ã€‚\n@@ -83,7 +83,7 @@ pip install transformers[deepspeed]\n \n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 pip install . \\\n@@ -105,7 +105,7 @@ CUDA_VISIBLE_DEVICES=0 python -c \"import torch; print(torch.cuda.get_device_capa\n \n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeed/\n+git clone https://github.com/deepspeedai/DeepSpeed/\n cd DeepSpeed\n rm -rf build\n TORCH_CUDA_ARCH_LIST=\"8.6\" DS_BUILD_CPU_ADAM=1 DS_BUILD_UTILS=1 \\\n@@ -142,7 +142,7 @@ _CudaDeviceProperties(name='GeForce RTX 3090', major=8, minor=6, total_memory=24\n \n æ‚¨ä¹Ÿå¯ä»¥å®Œå…¨çœç•¥ `TORCH_CUDA_ARCH_LIST`ï¼Œç„¶åæ„å»ºç¨‹åºå°†è‡ªåŠ¨æŸ¥è¯¢æ„å»ºæ‰€åœ¨çš„ GPU çš„æ¶æ„ã€‚è¿™å¯èƒ½ä¸ç›®æ ‡æœºå™¨ä¸Šçš„ GPU ä¸åŒ¹é…ï¼Œå› æ­¤æœ€å¥½æ˜ç¡®æŒ‡å®šæ‰€éœ€çš„æ¶æ„ã€‚\n \n-å¦‚æœå°è¯•äº†æ‰€æœ‰å»ºè®®çš„æ–¹æ³•ä»ç„¶é‡åˆ°æ„å»ºé—®é¢˜ï¼Œè¯·ç»§ç»­åœ¨ [Deepspeed](https://github.com/microsoft/DeepSpeed/issues)çš„ GitHub Issue ä¸Šæäº¤é—®é¢˜ã€‚\n+å¦‚æœå°è¯•äº†æ‰€æœ‰å»ºè®®çš„æ–¹æ³•ä»ç„¶é‡åˆ°æ„å»ºé—®é¢˜ï¼Œè¯·ç»§ç»­åœ¨ [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues)çš„ GitHub Issue ä¸Šæäº¤é—®é¢˜ã€‚\n \n \n <a id='deepspeed-multi-gpu'></a>\n@@ -471,10 +471,10 @@ deepspeed examples/pytorch/translation/run_translation.py ...\n \n æœ‰å…³å¯ä»¥åœ¨ DeepSpeed é…ç½®æ–‡ä»¶ä¸­ä½¿ç”¨çš„å®Œæ•´é…ç½®é€‰é¡¹çš„è¯¦ç»†æŒ‡å—ï¼Œè¯·å‚é˜…[ä»¥ä¸‹æ–‡æ¡£](https://www.deepspeed.ai/docs/config-json/)ã€‚\n \n-æ‚¨å¯ä»¥åœ¨ [DeepSpeedExamples ä»“åº“](https://github.com/microsoft/DeepSpeedExamples)ä¸­æ‰¾åˆ°è§£å†³å„ç§å®é™…éœ€æ±‚çš„æ•°åä¸ª DeepSpeed é…ç½®ç¤ºä¾‹ã€‚\n+æ‚¨å¯ä»¥åœ¨ [DeepSpeedExamples ä»“åº“](https://github.com/deepspeedai/DeepSpeedExamples)ä¸­æ‰¾åˆ°è§£å†³å„ç§å®é™…éœ€æ±‚çš„æ•°åä¸ª DeepSpeed é…ç½®ç¤ºä¾‹ã€‚\n \n ```bash\n-git clone https://github.com/microsoft/DeepSpeedExamples\n+git clone https://github.com/deepspeedai/DeepSpeedExamples\n cd DeepSpeedExamples\n find . -name '*json'\n ```\n@@ -485,7 +485,7 @@ find . -name '*json'\n grep -i Lamb $(find . -name '*json')\n ```\n \n-è¿˜å¯ä»¥åœ¨[ä¸»ä»“](https://github.com/microsoft/DeepSpeed)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚\n+è¿˜å¯ä»¥åœ¨[ä¸»ä»“](https://github.com/deepspeedai/DeepSpeed)ä¸­æ‰¾åˆ°æ›´å¤šç¤ºä¾‹ã€‚\n \n åœ¨ä½¿ç”¨ DeepSpeed æ—¶ï¼Œæ‚¨æ€»æ˜¯éœ€è¦æä¾›ä¸€ä¸ª DeepSpeed é…ç½®æ–‡ä»¶ï¼Œä½†æ˜¯ä¸€äº›é…ç½®å‚æ•°å¿…é¡»é€šè¿‡å‘½ä»¤è¡Œè¿›è¡Œé…ç½®ã€‚æ‚¨å°†åœ¨æœ¬æŒ‡å—çš„å‰©ä½™ç« èŠ‚æ‰¾åˆ°è¿™äº›ç»†å¾®å·®åˆ«ã€‚\n \n@@ -797,7 +797,7 @@ ZeRO-Infinity é€šè¿‡ä½¿ç”¨ NVMe å†…å­˜æ‰©å±• GPU å’Œ CPU å†…å­˜ï¼Œä»è€Œå…è®¸\n \n ç¡®ä¿æ‚¨çš„ `nvme_path` å®é™…ä¸Šæ˜¯ä¸€ä¸ª NVMeï¼Œå› ä¸ºå®ƒä¸æ™®é€šç¡¬ç›˜æˆ– SSD ä¸€èµ·å·¥ä½œï¼Œä½†é€Ÿåº¦ä¼šæ…¢å¾—å¤šã€‚å¿«é€Ÿå¯æ‰©å±•çš„è®­ç»ƒæ˜¯æ ¹æ®ç°ä»£ NVMe ä¼ è¾“é€Ÿåº¦è®¾è®¡çš„ï¼ˆæˆªè‡³æœ¬æ–‡æ’°å†™æ—¶ï¼Œå¯ä»¥è¾¾åˆ° ~3.5GB/s è¯»å–ï¼Œ~3GB/s å†™å…¥çš„å³°å€¼é€Ÿåº¦ï¼‰ã€‚\n \n-ä¸ºäº†æ‰¾å‡ºæœ€ä½³çš„ `aio` é…ç½®å—ï¼Œæ‚¨å¿…é¡»åœ¨ç›®æ ‡è®¾ç½®ä¸Šè¿è¡Œä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå…·ä½“æ“ä½œè¯·å‚è§[è¯´æ˜](https://github.com/microsoft/DeepSpeed/issues/998)ã€‚\n+ä¸ºäº†æ‰¾å‡ºæœ€ä½³çš„ `aio` é…ç½®å—ï¼Œæ‚¨å¿…é¡»åœ¨ç›®æ ‡è®¾ç½®ä¸Šè¿è¡Œä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œå…·ä½“æ“ä½œè¯·å‚è§[è¯´æ˜](https://github.com/deepspeedai/DeepSpeed/issues/998)ã€‚\n \n \n \n@@ -1789,7 +1789,7 @@ SW: Model with 2783M total params, 65M largest layer params.\n \n   å› æ­¤ï¼Œå¦‚æœé—®é¢˜æ˜æ˜¾ä¸DeepSpeedç›¸å…³ï¼Œä¾‹å¦‚æ‚¨å¯ä»¥çœ‹åˆ°æœ‰ä¸€ä¸ªå¼‚å¸¸å¹¶ä¸”å¯ä»¥çœ‹åˆ°DeepSpeedæ¨¡å—æ¶‰åŠå…¶ä¸­ï¼Œè¯·å…ˆé‡æ–°æµ‹è¯•æ²¡æœ‰DeepSpeedçš„è®¾ç½®ã€‚åªæœ‰å½“é—®é¢˜ä»ç„¶å­˜åœ¨æ—¶ï¼Œæ‰å‘Deepspeedæä¾›æ‰€æœ‰å¿…éœ€çš„ç»†èŠ‚ã€‚\n \n-- å¦‚æœæ‚¨æ˜ç¡®é—®é¢˜æ˜¯åœ¨Deepspeedæ ¸å¿ƒä¸­è€Œä¸æ˜¯é›†æˆéƒ¨åˆ†ï¼Œè¯·ç›´æ¥å‘[Deepspeed](https://github.com/microsoft/DeepSpeed/)æäº¤é—®é¢˜ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œæ— è®ºä½¿ç”¨å“ªä¸ªissueè·Ÿè¸ªé—®é¢˜éƒ½å¯ä»¥ï¼Œä¸€æ—¦æ‚¨å‘å¸ƒé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå¼„æ¸…æ¥šå¹¶å°†å…¶é‡å®šå‘åˆ°å¦ä¸€ä¸ªissueè·Ÿè¸ªï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚\n+- å¦‚æœæ‚¨æ˜ç¡®é—®é¢˜æ˜¯åœ¨Deepspeedæ ¸å¿ƒä¸­è€Œä¸æ˜¯é›†æˆéƒ¨åˆ†ï¼Œè¯·ç›´æ¥å‘[Deepspeed](https://github.com/deepspeedai/DeepSpeed/)æäº¤é—®é¢˜ã€‚å¦‚æœæ‚¨ä¸ç¡®å®šï¼Œè¯·ä¸è¦æ‹…å¿ƒï¼Œæ— è®ºä½¿ç”¨å“ªä¸ªissueè·Ÿè¸ªé—®é¢˜éƒ½å¯ä»¥ï¼Œä¸€æ—¦æ‚¨å‘å¸ƒé—®é¢˜ï¼Œæˆ‘ä»¬ä¼šå¼„æ¸…æ¥šå¹¶å°†å…¶é‡å®šå‘åˆ°å¦ä¸€ä¸ªissueè·Ÿè¸ªï¼ˆå¦‚æœéœ€è¦çš„è¯ï¼‰ã€‚\n \n \n \n@@ -2086,7 +2086,7 @@ RUN_SLOW=1 pytest tests/deepspeed\n \n ## ä¸»è¦çš„DeepSpeedèµ„æº\n \n-- [é¡¹ç›®GitHub](https://github.com/microsoft/deepspeed)\n+- [é¡¹ç›®GitHub](https://github.com/deepspeedai/DeepSpeed)\n - [ä½¿ç”¨æ–‡æ¡£](https://www.deepspeed.ai/getting-started/)\n - [APIæ–‡æ¡£](https://deepspeed.readthedocs.io/en/latest/index.html)\n - [åšå®¢æ–‡ç« ](https://www.microsoft.com/en-us/research/search/?q=deepspeed)\n@@ -2097,4 +2097,4 @@ RUN_SLOW=1 pytest tests/deepspeed\n - [ZeRO-Offload: Democratizing Billion-Scale Model Training](https://arxiv.org/abs/2101.06840)\n - [ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning](https://arxiv.org/abs/2104.07857)\n \n-æœ€åï¼Œè¯·è®°ä½ï¼ŒHuggingFace [`Trainer`]ä»…é›†æˆäº†DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨DeepSpeedæ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨[DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed/issues)ä¸Šæäº¤ä¸€ä¸ªissueã€‚\n+æœ€åï¼Œè¯·è®°ä½ï¼ŒHuggingFace [`Trainer`]ä»…é›†æˆäº†DeepSpeedï¼Œå› æ­¤å¦‚æœæ‚¨åœ¨ä½¿ç”¨DeepSpeedæ—¶é‡åˆ°ä»»ä½•é—®é¢˜æˆ–ç–‘é—®ï¼Œè¯·åœ¨[DeepSpeed GitHub](https://github.com/deepspeedai/DeepSpeed/issues)ä¸Šæäº¤ä¸€ä¸ªissueã€‚"
        },
        {
            "sha": "16d6c6606bdc99e6d57185c69f131284c56d367d",
            "filename": "docs/source/zh/main_classes/trainer.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Ftrainer.md?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -182,7 +182,7 @@ my_app.py ... --log_level error --log_level_replica error --log_on_each_node 0\n python -m torch.distributed.launch --nproc_per_node=2  trainer-program.py ...\n ```\n \n-å¦‚æœä½ å®‰è£…äº† [`accelerate`](https://github.com/huggingface/accelerate) æˆ– [`deepspeed`](https://github.com/microsoft/DeepSpeed)ï¼Œä½ è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹æ³•å®ç°ç›¸åŒçš„æ•ˆæœï¼š\n+å¦‚æœä½ å®‰è£…äº† [`accelerate`](https://github.com/huggingface/accelerate) æˆ– [`deepspeed`](https://github.com/deepspeedai/DeepSpeed)ï¼Œä½ è¿˜å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»»ä¸€æ–¹æ³•å®ç°ç›¸åŒçš„æ•ˆæœï¼š\n \n \n ```bash\n@@ -281,7 +281,7 @@ export CUDA_VISIBLE_DEVICES=1,0\n \n [`Trainer`] å·²ç»è¢«æ‰©å±•ï¼Œä»¥æ”¯æŒå¯èƒ½æ˜¾è‘—æé«˜è®­ç»ƒæ—¶é—´å¹¶é€‚åº”æ›´å¤§æ¨¡å‹çš„åº“ã€‚\n \n-ç›®å‰ï¼Œå®ƒæ”¯æŒç¬¬ä¸‰æ–¹è§£å†³æ–¹æ¡ˆ [DeepSpeed](https://github.com/microsoft/DeepSpeed) å’Œ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)ï¼Œå®ƒä»¬å®ç°äº†è®ºæ–‡ [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He](https://arxiv.org/abs/1910.02054) çš„éƒ¨åˆ†å†…å®¹ã€‚\n+ç›®å‰ï¼Œå®ƒæ”¯æŒç¬¬ä¸‰æ–¹è§£å†³æ–¹æ¡ˆ [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) å’Œ [PyTorch FSDP](https://pytorch.org/docs/stable/fsdp.html)ï¼Œå®ƒä»¬å®ç°äº†è®ºæ–‡ [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models, by Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He](https://arxiv.org/abs/1910.02054) çš„éƒ¨åˆ†å†…å®¹ã€‚\n \n æˆªè‡³æ’°å†™æœ¬æ–‡ï¼Œæ­¤æä¾›çš„æ”¯æŒæ˜¯æ–°çš„ä¸”å®éªŒæ€§çš„ã€‚å°½ç®¡æˆ‘ä»¬æ¬¢è¿å›´ç»• DeepSpeed å’Œ PyTorch FSDP çš„issuesï¼Œä½†æˆ‘ä»¬ä¸å†æ”¯æŒ FairScale é›†æˆï¼Œå› ä¸ºå®ƒå·²ç»é›†æˆåˆ°äº† PyTorch ä¸»çº¿ï¼ˆå‚è§ [PyTorch FSDP é›†æˆ](#pytorch-fully-sharded-data-parallel)ï¼‰ã€‚\n \n@@ -293,7 +293,7 @@ export CUDA_VISIBLE_DEVICES=1,0\n \n æ’°å†™æ—¶ï¼ŒDeepspeed éœ€è¦åœ¨ä½¿ç”¨ä¹‹å‰ç¼–è¯‘ CUDA C++ ä»£ç ã€‚\n \n-è™½ç„¶æ‰€æœ‰å®‰è£…é—®é¢˜éƒ½åº”é€šè¿‡ [Deepspeed](https://github.com/microsoft/DeepSpeed/issues) çš„ GitHub Issueså¤„ç†ï¼Œä½†åœ¨æ„å»ºä¾èµ–CUDA æ‰©å±•çš„ä»»ä½• PyTorch æ‰©å±•æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›å¸¸è§é—®é¢˜ã€‚\n+è™½ç„¶æ‰€æœ‰å®‰è£…é—®é¢˜éƒ½åº”é€šè¿‡ [Deepspeed](https://github.com/deepspeedai/DeepSpeed/issues) çš„ GitHub Issueså¤„ç†ï¼Œä½†åœ¨æ„å»ºä¾èµ–CUDA æ‰©å±•çš„ä»»ä½• PyTorch æ‰©å±•æ—¶ï¼Œå¯èƒ½ä¼šé‡åˆ°ä¸€äº›å¸¸è§é—®é¢˜ã€‚\n \n å› æ­¤ï¼Œå¦‚æœåœ¨æ‰§è¡Œä»¥ä¸‹æ“ä½œæ—¶é‡åˆ°ä¸ CUDA ç›¸å…³çš„æ„å»ºé—®é¢˜ï¼š\n "
        },
        {
            "sha": "e4742ecd1bfbbc47b740ff91951ae0b3011b0308",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -383,8 +383,8 @@ def deepspeed_init(trainer, num_training_steps, inference=False):\n     Returns: optimizer, lr_scheduler\n \n     We may use `deepspeed_init` more than once during the life of Trainer, when we do - it's a temp hack based on:\n-    https://github.com/microsoft/DeepSpeed/issues/1394#issuecomment-937405374 until Deepspeed fixes a bug where it\n-    can't resume from a checkpoint after it did some stepping https://github.com/microsoft/DeepSpeed/issues/1612\n+    https://github.com/deepspeedai/DeepSpeed/issues/1394#issuecomment-937405374 until Deepspeed fixes a bug where it\n+    can't resume from a checkpoint after it did some stepping https://github.com/deepspeedai/DeepSpeed/issues/1612\n \n     \"\"\"\n     from deepspeed.utils import logger as ds_logger"
        },
        {
            "sha": "5bc31b6160038d49c13e1efec3d9da41b34e7614",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -568,7 +568,7 @@ class TrainingArguments:\n                     fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n \n         deepspeed (`str` or `dict`, *optional*):\n-            Use [Deepspeed](https://github.com/microsoft/deepspeed). This is an experimental feature and its API may\n+            Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n             evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n             `ds_config.json`) or an already loaded json file as a `dict`\"\n "
        },
        {
            "sha": "28ab700590917af95dd5d036bc90ff14a4ecba7c",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9dc1efa5d493e3297891a0c5aefb701b007a11fa/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9dc1efa5d493e3297891a0c5aefb701b007a11fa/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=9dc1efa5d493e3297891a0c5aefb701b007a11fa",
            "patch": "@@ -898,7 +898,7 @@ def test_can_resume_training_normal(self, stage, dtype, optim, scheduler):\n             self.check_trainer_state_are_the_same(state, state1)\n \n             # Finally, should be able to resume with the same trainer/same deepspeed engine instance\n-            # XXX: but currently this not possible due DS bug: https://github.com/microsoft/DeepSpeed/issues/1612\n+            # XXX: but currently this not possible due DS bug: https://github.com/deepspeedai/DeepSpeed/issues/1612\n             # trainer.train(resume_from_checkpoint=checkpoint)\n             # a workaround needs to be used that re-creates the deepspeed engine\n \n@@ -975,7 +975,7 @@ def test_ds_config_object(self):\n     def test_load_best_model(self, stage, dtype):\n         # Test that forced deepspeed reinit doesn't break the model. the forced re-init after\n         # loading the best model in Trainer is there to workaround this bug in Deepspeed\n-        # https://github.com/microsoft/DeepSpeed/issues/1612\n+        # https://github.com/deepspeedai/DeepSpeed/issues/1612\n         #\n         # The test is derived from a repro script submitted in this Issue:\n         # https://github.com/huggingface/transformers/issues/17114"
        }
    ],
    "stats": {
        "total": 132,
        "additions": 66,
        "deletions": 66
    }
}