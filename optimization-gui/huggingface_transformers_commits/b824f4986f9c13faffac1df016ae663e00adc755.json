{
    "author": "Guo-Chenxu",
    "message": "fix typo (#40484)\n\n* fix typo\n\nSigned-off-by: guochenxu <guochenxu@modelbest.cn>\n\n* csm & qwen omni\n\nSigned-off-by: guochenxu <guochenxu@modelbest.cn>\n\n* format\n\nSigned-off-by: guochenxu <guochenxu@modelbest.cn>\n\n* Apply style fixes\n\n* omni\n\nSigned-off-by: guochenxu <guochenxu@modelbest.cn>\n\n---------\n\nSigned-off-by: guochenxu <guochenxu@modelbest.cn>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "b824f4986f9c13faffac1df016ae663e00adc755",
    "files": [
        {
            "sha": "33a7ed5ebd569d68014f9fb695f3cafaf73f5c0c",
            "filename": "tests/models/csm/test_processing_csm.py",
            "status": "modified",
            "additions": 109,
            "deletions": 0,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processing_csm.py?ref=b824f4986f9c13faffac1df016ae663e00adc755",
            "patch": "@@ -34,6 +34,7 @@\n @require_torch\n class CsmProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = CsmProcessor\n+    audio_input_name = \"input_values\"\n \n     @classmethod\n     def setUpClass(cls):\n@@ -64,6 +65,114 @@ def test_chat_template_is_saved(self):\n         processor_dict = self.prepare_processor_dict()\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n \n+    @require_torch\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n+        if return_tensors != \"pt\":\n+            self.skipTest(\"CSM only supports PyTorch tensors\")\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n+\n+        # some models have only Fast image processor\n+        if getattr(processor, processor_name).__class__.__name__.endswith(\"Fast\"):\n+            return_tensors = \"pt\"\n+\n+        batch_messages = [\n+            [\n+                {\n+                    \"role\": \"0\",\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n+                },\n+            ]\n+        ] * batch_size\n+\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n+\n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n+        formatted_prompt_tokenized = processor.apply_chat_template(\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n+        )\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n+        )\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n+\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n+        )\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+        )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n+        out_dict = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            num_frames=2,  # by default no more than 2 frames, otherwise too slow\n+        )\n+        input_name = getattr(self, input_name)\n+        print(f\"================ input_name={input_name} =================\")\n+        print(f\"out_dict={out_dict.keys()}\")\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+        self.assertEqual(len(out_dict[input_name]), batch_size)\n+\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n+\n+        # Test continue from final message\n+        assistant_message = {\n+            \"role\": \"1\",\n+            \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of\"}],\n+        }\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx] = batch_messages[idx] + [assistant_message]\n+        continue_prompt = processor.apply_chat_template(batch_messages, continue_final_message=True, tokenize=False)\n+        for prompt in continue_prompt:\n+            self.assertTrue(prompt.endswith(\"It is the sound of\"))  # no `eos` token at the end\n+\n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string\n         messages = ["
        },
        {
            "sha": "89f62bde0c3768feeffa7a17f82ec4e460aeba98",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=b824f4986f9c13faffac1df016ae663e00adc755",
            "patch": "@@ -408,6 +408,8 @@ def _test_apply_chat_template(\n             for thw in out_dict[\"video_grid_thw\"]:\n                 expected_video_token_count += thw[0] * thw[1] * thw[2]\n             mm_len = expected_video_token_count\n+        elif modality == \"audio\":\n+            mm_len = batch_size\n         else:\n             mm_len = batch_size * 1564\n         self.assertEqual(len(out_dict[input_name]), mm_len)"
        },
        {
            "sha": "825613f0502bdd64d8c10ba7eece43bf9c614125",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b824f4986f9c13faffac1df016ae663e00adc755/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=b824f4986f9c13faffac1df016ae663e00adc755",
            "patch": "@@ -973,7 +973,7 @@ def _test_apply_chat_template(\n     @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n     def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n         self._test_apply_chat_template(\n-            \"audio\", batch_size, return_tensors, \"audio_input_name\", \"feature_extracttor\", MODALITY_INPUT_DATA[\"audio\"]\n+            \"audio\", batch_size, return_tensors, \"audio_input_name\", \"feature_extractor\", MODALITY_INPUT_DATA[\"audio\"]\n         )\n \n     @require_av"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 112,
        "deletions": 1
    }
}