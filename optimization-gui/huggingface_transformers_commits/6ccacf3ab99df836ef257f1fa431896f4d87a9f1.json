{
    "author": "sywangyi",
    "message": "[testing] fix some cases in xpu (#42273)\n\nfix tests/models/sam_hq/test_modeling_sam_hq.py::SamHQModelIntegrationTest::test_inference_mask_generation_no_point\nfix tests/models/mpt/test_modeling_mpt.py::MptIntegrationTests::test_model_logits\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>",
    "sha": "6ccacf3ab99df836ef257f1fa431896f4d87a9f1",
    "files": [
        {
            "sha": "875e9a2e2870155e6b6290716108203b1c70cfae",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ccacf3ab99df836ef257f1fa431896f4d87a9f1/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ccacf3ab99df836ef257f1fa431896f4d87a9f1/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=6ccacf3ab99df836ef257f1fa431896f4d87a9f1",
            "patch": "@@ -528,6 +528,7 @@ def test_generation_batched(self):\n         for i, predicted_output in enumerate(decoded_outputs):\n             self.assertEqual(predicted_output, expected_output[i])\n \n+    @require_deterministic_for_xpu\n     def test_model_logits(self):\n         model_id = \"mosaicml/mpt-7b\"\n \n@@ -546,7 +547,7 @@ def test_model_logits(self):\n         expected_slices = Expectations(\n             {\n                 (None, None): torch.Tensor([-0.2520, -0.2178, -0.1953]),\n-                (\"xpu\", 3): torch.Tensor([-0.2090, -0.2061, -0.1465]),\n+                (\"xpu\", 3): torch.Tensor([-0.2656, -0.2246, -0.2637]),\n                 (\"cuda\", 8): torch.Tensor([-0.2559, -0.2227, -0.2217]),\n                 # TODO: This is quite a bit off, check BnB\n                 (\"rocm\", (9, 5)): torch.Tensor([-0.3008, -0.1309, -0.1562]),"
        },
        {
            "sha": "54c9f6a310a11fb53808151ebf39ebd725c7188d",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ccacf3ab99df836ef257f1fa431896f4d87a9f1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ccacf3ab99df836ef257f1fa431896f4d87a9f1/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=6ccacf3ab99df836ef257f1fa431896f4d87a9f1",
            "patch": "@@ -801,6 +801,7 @@ def test_inference_mask_generation_no_point(self):\n             {\n                 (None, None): [-13.1695, -14.6201, -14.8989],\n                 (\"cuda\", 8): [-7.6769, -9.6935, -9.8773],\n+                (\"xpu\", None): [-7.6769, -9.6935, -9.8773],\n             }\n         )\n         EXPECTED_MASKS = torch.tensor(expectations.get_expectation()).to(torch_device)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}