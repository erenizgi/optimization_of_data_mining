{
    "author": "gante",
    "message": "[CI] Check test if the `GenerationTesterMixin` inheritance is correct ðŸ› ðŸ”«  (#36180)",
    "sha": "678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
    "files": [
        {
            "sha": "bd2354aec7395308ca9b641bb90d1f606b94780f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -1685,17 +1685,14 @@ def base_model(self) -> nn.Module:\n     @classmethod\n     def can_generate(cls) -> bool:\n         \"\"\"\n-        Returns whether this model can generate sequences with `.generate()`.\n+        Returns whether this model can generate sequences with `.generate()` from the `GenerationMixin`.\n \n         Returns:\n             `bool`: Whether this model can generate sequences with `.generate()`.\n         \"\"\"\n         # Directly inherits `GenerationMixin` -> can generate\n         if \"GenerationMixin\" in str(cls.__bases__):\n             return True\n-        # Model class overwrites `generate` (e.g. time series models) -> can generate\n-        if str(cls.__name__) in str(cls.generate):\n-            return True\n         # The class inherits from a class that can generate (recursive check) -> can generate\n         for base in cls.__bases__:\n             if not hasattr(base, \"can_generate\"):"
        },
        {
            "sha": "dca1fe7f6002956c912b32d43760dca5d7a0c8cb",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -24,7 +24,6 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -984,7 +983,7 @@ def forward(self, pooled_output: torch.Tensor) -> torch.Tensor:\n     \"Albert Model with a `language modeling` head on top.\",\n     ALBERT_START_DOCSTRING,\n )\n-class AlbertForMaskedLM(AlbertPreTrainedModel, GenerationMixin):\n+class AlbertForMaskedLM(AlbertPreTrainedModel):\n     _tied_weights_keys = [\"predictions.decoder.bias\", \"predictions.decoder.weight\"]\n \n     def __init__(self, config):"
        },
        {
            "sha": "04dc8ed7d106b5b53c8fd2052fe0d595fbc73abf",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -2912,7 +2912,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The speech-to-text SeamlessM4T Model transformer which can be used for S2TT.\",\n     SEAMLESS_M4T_START_DOCSTRING,\n )\n-class SeamlessM4TForSpeechToText(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TForSpeechToText(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"text_decoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3182,7 +3182,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The text-to-speech SeamlessM4T Model transformer which can be used for T2ST.\",\n     SEAMLESS_M4T_START_DOCSTRING,\n )\n-class SeamlessM4TForTextToSpeech(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TForTextToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3511,7 +3511,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The speech-to-speech SeamlessM4T Model transformer which can be used for S2ST.\",\n     SEAMLESS_M4T_START_DOCSTRING,\n )\n-class SeamlessM4TForSpeechToSpeech(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TForSpeechToSpeech(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3854,7 +3854,7 @@ def _reorder_cache(past_key_values, beam_idx):\n             Default modality. Used to initialize the model.\n     \"\"\",\n )\n-class SeamlessM4TModel(SeamlessM4TPreTrainedModel):\n+class SeamlessM4TModel(SeamlessM4TPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\n         \"lm_head.weight\",\n         \"text_encoder.embed_tokens.weight\","
        },
        {
            "sha": "3bf8edd2a68b6799225bcbd90192efe1f3aea8c5",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -3192,7 +3192,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The speech-to-text SeamlessM4Tv2 Model transformer which can be used for S2TT.\",\n     SEAMLESS_M4T_V2_START_DOCSTRING,\n )\n-class SeamlessM4Tv2ForSpeechToText(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2ForSpeechToText(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"text_decoder\", \"t2u_model\", \"vocoder\"]\n     main_input_name = \"input_features\"\n \n@@ -3473,7 +3473,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The text-to-speech SeamlessM4Tv2 Model transformer which can be used for T2ST.\",\n     SEAMLESS_M4T_V2_START_DOCSTRING,\n )\n-class SeamlessM4Tv2ForTextToSpeech(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2ForTextToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"speech_encoder\"]\n     main_input_name = \"input_ids\"\n \n@@ -3844,7 +3844,7 @@ def _reorder_cache(past_key_values, beam_idx):\n     \"The speech-to-speech SeamlessM4Tv2 Model transformer which can be used for S2ST.\",\n     SEAMLESS_M4T_V2_START_DOCSTRING,\n )\n-class SeamlessM4Tv2ForSpeechToSpeech(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2ForSpeechToSpeech(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [\"text_encoder\"]\n     main_input_name = \"input_features\"\n \n@@ -4229,7 +4229,7 @@ def _reorder_cache(past_key_values, beam_idx):\n             This will be updated automatically according to the modality passed to the forward and generate passes (`input_ids` for text and `input_features` for audio).\n     \"\"\",\n )\n-class SeamlessM4Tv2Model(SeamlessM4Tv2PreTrainedModel):\n+class SeamlessM4Tv2Model(SeamlessM4Tv2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\n         \"lm_head.weight\",\n         \"text_encoder.embed_tokens.weight\","
        },
        {
            "sha": "119c466d9ed2967492845ae169abe08cc31ac786",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 2,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -19,6 +19,7 @@\n import datetime\n import gc\n import inspect\n+import random\n import tempfile\n import unittest\n import warnings\n@@ -48,8 +49,6 @@\n )\n from transformers.utils import is_ipex_available\n \n-from ..test_modeling_common import floats_tensor, ids_tensor\n-\n \n if is_torch_available():\n     import torch\n@@ -2786,6 +2785,43 @@ def test_speculative_sampling_target_distribution(self):\n         self.assertTrue(last_token_counts[8] > last_token_counts[3])\n \n \n+global_rng = random.Random()\n+\n+\n+# Copied from tests.test_modeling_common.ids_tensor\n+def ids_tensor(shape, vocab_size, rng=None, name=None):\n+    #  Creates a random int32 tensor of the shape within the vocab size\n+    if rng is None:\n+        rng = global_rng\n+\n+    total_dims = 1\n+    for dim in shape:\n+        total_dims *= dim\n+\n+    values = []\n+    for _ in range(total_dims):\n+        values.append(rng.randint(0, vocab_size - 1))\n+\n+    return torch.tensor(data=values, dtype=torch.long, device=torch_device).view(shape).contiguous()\n+\n+\n+# Copied from tests.test_modeling_common.floats_tensor\n+def floats_tensor(shape, scale=1.0, rng=None, name=None):\n+    \"\"\"Creates a random float32 tensor\"\"\"\n+    if rng is None:\n+        rng = global_rng\n+\n+    total_dims = 1\n+    for dim in shape:\n+        total_dims *= dim\n+\n+    values = []\n+    for _ in range(total_dims):\n+        values.append(rng.random() * scale)\n+\n+    return torch.tensor(data=values, dtype=torch.float, device=torch_device).view(shape).contiguous()\n+\n+\n @pytest.mark.generate\n @require_torch\n class GenerationIntegrationTests(unittest.TestCase):"
        },
        {
            "sha": "6174c226003e841e9253c73847d153584f08e522",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -451,6 +451,8 @@ class BigBirdModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": BigBirdModel,"
        },
        {
            "sha": "529e1631111d2dfa7b7460a7d9f6cabc0ac4227a",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -799,6 +799,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_vision\n class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForQuestionAnswering,) if is_torch_available() else ()\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -1106,6 +1108,8 @@ def test_model_from_pretrained(self):\n @require_torch\n class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "03bd268e24a9fcaaef7884530ab9a8a4714e24f3",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -885,7 +885,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration, Blip2Model) if is_torch_available() else ()\n     # Doesn't run generation tests. TODO: fix generation tests for Blip2ForConditionalGeneration\n     all_generative_model_classes = ()"
        },
        {
            "sha": "0f59b91871c9c11a2a8240c825f90c78a7737490",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -408,6 +408,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class ClvpModelForConditionalGenerationTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClvpModelForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n \n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "db5afffe2131826b36cdbb54785e5b5dc0bb69a8",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -22,7 +22,6 @@\n from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n from transformers.utils import cached_property\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -173,7 +172,7 @@ def create_and_check_conditional_detr_object_detection_head_model(self, config,\n \n \n @require_torch\n-class ConditionalDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class ConditionalDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             ConditionalDetrModel,"
        },
        {
            "sha": "d3835ec2374b7543132b9818597ac1dbf0c5f40f",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -136,6 +136,8 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class CpmAntModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (CpmAntModel, CpmAntForCausalLM) if is_torch_available() else ()\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": CpmAntModel, \"text-generation\": CpmAntForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "584bd1882a82d82f22cf985dd50258b2bfa66e8f",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -23,7 +23,6 @@\n from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n from transformers.utils import cached_property\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -174,15 +173,8 @@ def create_and_check_dab_detr_object_detection_head_model(self, config, pixel_va\n \n \n @require_torch\n-class DabDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            DabDetrModel,\n-            DabDetrForObjectDetection,\n-        )\n-        if is_torch_available()\n-        else ()\n-    )\n+class DabDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (DabDetrModel, DabDetrForObjectDetection) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n             \"image-feature-extraction\": DabDetrModel,"
        },
        {
            "sha": "e004629c9c34dc0cbec935d86f94ab16160feb8a",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -20,7 +20,6 @@\n from transformers import DecisionTransformerConfig, is_torch_available\n from transformers.testing_utils import require_torch, slow, torch_device\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -125,7 +124,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class DecisionTransformerModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class DecisionTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DecisionTransformerModel,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": DecisionTransformerModel} if is_torch_available() else {}\n "
        },
        {
            "sha": "1e27aaabf8d8a5c1d468d9190ecb06a473a41807",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -31,7 +31,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -188,7 +187,7 @@ def create_and_check_deformable_detr_object_detection_head_model(self, config, p\n \n \n @require_torch\n-class DeformableDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class DeformableDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (DeformableDetrModel, DeformableDetrForObjectDetection) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\"image-feature-extraction\": DeformableDetrModel, \"object-detection\": DeformableDetrForObjectDetection}"
        },
        {
            "sha": "381fa1d7cd232ad92be8b572f409f279db138e5e",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -22,7 +22,6 @@\n from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n from transformers.utils import cached_property\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -169,7 +168,7 @@ def create_and_check_detr_object_detection_head_model(self, config, pixel_values\n \n \n @require_torch\n-class DetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class DetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             DetrModel,"
        },
        {
            "sha": "08865f22aba86a2d4ecbf7f303e1be762f546018",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -389,6 +389,8 @@ class ElectraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": ElectraModel,"
        },
        {
            "sha": "4211e92e21e092000e6fe2c421ceffbeb401417d",
            "filename": "tests/models/flaubert/test_modeling_flaubert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -377,6 +377,8 @@ class FlaubertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. Outdated custom `prepare_inputs_for_generation` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": FlaubertModel,"
        },
        {
            "sha": "151fa4a1c846dfaa6245d410183521bb9bf67e16",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -326,6 +326,8 @@ def test_eager_matches_sdpa_generate(self):\n @require_torch\n class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsModel, IdeficsForVisionText2Text) if is_torch_available() else ()\n+    # Doesn't run generation tests here -- idefics has a dedicated tester for generation tests below\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": IdeficsModel, \"image-text-to-text\": IdeficsForVisionText2Text}\n         if is_torch_available()\n@@ -868,6 +870,12 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n+    @unittest.skip(\n+        \"Idefics has a separate test runner for generation tests with complex inheritance, causing this check to fail\"\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n \n @require_torch\n @require_vision"
        },
        {
            "sha": "c24d375ccc45a95a398a384cac419f92fe78f8f2",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -19,7 +19,6 @@\n from transformers import LiltConfig, is_torch_available\n from transformers.testing_utils import require_torch, slow, torch_device\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -218,7 +217,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class LiltModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class LiltModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             LiltModel,"
        },
        {
            "sha": "09fac0e752561390a6432b549f6c7f10ea112c90",
            "filename": "tests/models/megatron_bert/test_modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmegatron_bert%2Ftest_modeling_megatron_bert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -282,6 +282,8 @@ class MegatronBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MegatronBertModel,"
        },
        {
            "sha": "238999da9190df4a038f08171a88f2ab26da13df",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -29,7 +29,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -216,7 +215,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class ModernBertModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n \n     all_model_classes = ("
        },
        {
            "sha": "65feb47c6dd79e76e40eacd89484e2a79a2ef863",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -20,7 +20,6 @@\n from transformers import MoonshineConfig, is_torch_available\n from transformers.testing_utils import cleanup, require_torch, slow, torch_device\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -168,7 +167,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class MoonshineModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class MoonshineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MoonshineModel, MoonshineForConditionalGeneration) if is_torch_available() else ()\n     # Doesn't run generation tests. TODO (eustache): remove this line and then make CI green\n     all_generative_model_classes = ()"
        },
        {
            "sha": "2e105f69fd795a2607a3201de6168e3afb12f6fb",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -672,6 +672,15 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n+    @unittest.skip(\n+        reason=(\n+            \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n+            \"`greedy_sample_model_classes`\"\n+        )\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n \n def prepare_musicgen_inputs_dict(\n     config,\n@@ -1763,6 +1772,15 @@ def test_requires_grad_with_frozen_encoders(self):\n             self.assertTrue(all(audio_encoder_grads))\n             self.assertFalse(all(text_encoder_grads))\n \n+    @unittest.skip(\n+        reason=(\n+            \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n+            \"`greedy_sample_model_classes`\"\n+        )\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n \n def get_bip_bip(bip_duration=0.125, duration=0.5, sample_rate=32000):\n     \"\"\"Produces a series of 'bip bip' sounds at a given frequency.\"\"\""
        },
        {
            "sha": "57f6757a147de6a9af16d38f1e9a46964f54bd30",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -689,6 +689,15 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n \n                 self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n \n+    @unittest.skip(\n+        reason=(\n+            \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n+            \"`greedy_sample_model_classes`\"\n+        )\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n \n def prepare_musicgen_melody_inputs_dict(\n     config,\n@@ -1741,6 +1750,15 @@ def test_requires_grad_with_frozen_encoders(self):\n             self.assertTrue(all(audio_encoder_grads))\n             self.assertFalse(all(text_encoder_grads))\n \n+    @unittest.skip(\n+        reason=(\n+            \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n+            \"`greedy_sample_model_classes`\"\n+        )\n+    )\n+    def test_generation_tester_mixin_inheritance(self):\n+        pass\n+\n \n # Copied from tests.models.musicgen.test_modeling_musicgen.get_bip_bip\n def get_bip_bip(bip_duration=0.125, duration=0.5, sample_rate=32000):"
        },
        {
            "sha": "cadacf716ac3cdc218d19ade87b883579b1f8e31",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -847,7 +847,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class PegasusXStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class PegasusXStandaloneDecoderModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (PegasusXDecoder,) if is_torch_available() else ()\n     test_pruning = False\n     is_encoder_decoder = False"
        },
        {
            "sha": "23b47e0d17c0f08beec6b7eae2689c65f612ba38",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -33,7 +33,6 @@\n )\n from transformers.utils import is_essentia_available, is_librosa_available, is_scipy_available, is_torch_available\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -504,7 +503,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Pop2PianoModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Pop2PianoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (Pop2PianoForConditionalGeneration,) if is_torch_available() else ()\n     # Doesn't run generation tests. Has custom generation method with a different interface\n     all_generative_model_classes = ()"
        },
        {
            "sha": "c5a8cc34c3571e4ed356422a62b347ddfabd94b8",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -152,6 +152,8 @@ class Qwen2AudioForConditionalGenerationModelTest(ModelTesterMixin, unittest.Tes\n     \"\"\"\n \n     all_model_classes = (Qwen2AudioForConditionalGeneration,) if is_torch_available() else ()\n+    # Doesn't run generation tests. TODO eustache/joao: some generation tests are broken, the errors seem cache-related\n+    all_generative_model_classes = ()\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True"
        },
        {
            "sha": "19179c07344966417cdd97f5392ac3762709d993",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -26,7 +26,6 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -281,7 +280,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class RecurrentGemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class RecurrentGemmaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (RecurrentGemmaForCausalLM,) if is_torch_available() else ()\n     # Doesn't run generation tests. TODO @gante not fully supported\n     all_generative_model_classes = ()"
        },
        {
            "sha": "dbacbd150b3b3b3dc11a3a455b402fedd8f791fa",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -373,6 +373,8 @@ class RemBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RemBertModel,"
        },
        {
            "sha": "16d029ea4896ed5bd2c4f536ee35b021bb4c906f",
            "filename": "tests/models/roc_bert/test_modeling_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froc_bert%2Ftest_modeling_roc_bert.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -570,6 +570,8 @@ class RoCBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RoCBertModel,"
        },
        {
            "sha": "abd8cf1dc51043bafd9415d7e124c447dcd1faa1",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -392,6 +392,8 @@ class RoFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": RoFormerModel,"
        },
        {
            "sha": "558c9f7e4a5413384991074ba3927badcc54695b",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -23,7 +23,6 @@\n from transformers.trainer_utils import set_seed\n from transformers.utils import cached_property\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -358,6 +357,8 @@ class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. Custom generation method with a different interface\n+    all_generative_model_classes = ()\n \n     def setUp(self):\n         self.model_tester = SeamlessM4TModelTester(self, input_modality=\"speech\")\n@@ -580,9 +581,7 @@ def test_retain_grad_hidden_states_attentions(self):\n \n \n @require_torch\n-class SeamlessM4TModelWithTextInputTest(\n-    ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase\n-):\n+class SeamlessM4TModelWithTextInputTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n     fx_compatible = False\n     test_missing_keys = False"
        },
        {
            "sha": "ecf5363fd8216ae147b927481fa810936c9dbb53",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -23,7 +23,6 @@\n from transformers.trainer_utils import set_seed\n from transformers.utils import cached_property\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -374,6 +373,8 @@ class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase)\n         if is_torch_available()\n         else ()\n     )\n+    # Doesn't run generation tests. Has custom generation method with a different interface\n+    all_generative_model_classes = ()\n \n     def setUp(self):\n         self.model_tester = SeamlessM4Tv2ModelTester(self, input_modality=\"speech\")\n@@ -595,7 +596,7 @@ def test_retain_grad_hidden_states_attentions(self):\n \n \n @require_torch\n-class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n     fx_compatible = False\n     test_missing_keys = False"
        },
        {
            "sha": "f133099ef70728e4071fb10f13924e165a07c3e2",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -362,6 +362,8 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n @require_torch\n class SpeechT5ForSpeechToTextTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SpeechT5ForSpeechToText,) if is_torch_available() else ()\n+    # Doesn't run generation tests. TODO eustache/joao: shape checks probably need an update\n+    all_generative_model_classes = ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_headmasking = False"
        },
        {
            "sha": "aa4b7131f9494843e93a76fe1f132641e8c3b9af",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -23,7 +23,6 @@\n from transformers import ResNetConfig, TableTransformerConfig, is_torch_available, is_vision_available\n from transformers.testing_utils import require_timm, require_torch, require_vision, slow, torch_device\n \n-from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -189,7 +188,7 @@ def create_and_check_table_transformer_no_timm_backbone(self, config, pixel_valu\n \n \n @require_torch\n-class TableTransformerModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class TableTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             TableTransformerModel,"
        },
        {
            "sha": "0c127959909f17c848592fe55034dcf8230392ee",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -30,6 +30,7 @@\n )\n from transformers.utils import cached_property\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -265,7 +266,7 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class UdopModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class UdopModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n         (\n             UdopModel,\n@@ -419,6 +420,14 @@ def test_model_from_pretrained(self):\n         model = UdopForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @unittest.skip(reason=\"TODO: Fix me @joao\")\n+    def test_generate_with_head_masking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TODO: Fix me @joao\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n \n class UdopEncoderOnlyModelTester:\n     def __init__("
        },
        {
            "sha": "9f17c026b33e6749043c654eaa0ee09f16a9a132",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -3353,7 +3353,7 @@ def create_and_check_model_forward(self, config, inputs_dict, use_weighted_layer\n \n \n @require_torch\n-class WhisperEncoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+class WhisperEncoderModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (WhisperForAudioClassification,) if is_torch_available() else ()\n     is_encoder_decoder = False\n     fx_compatible = False"
        },
        {
            "sha": "80e7bd14471461fce3ace50fe13f052ef1a2c761",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -106,6 +106,8 @@\n )\n from transformers.utils.generic import ContextManagers\n \n+from .generation.test_utils import GenerationTesterMixin\n+\n \n if is_accelerate_available():\n     from accelerate.utils import compute_module_sizes\n@@ -4417,6 +4419,33 @@ def test_flex_attention_with_grads(self):\n             # If this does not raise an error, the test passes (see https://github.com/huggingface/transformers/pull/35605)\n             _ = model(inputs_dict[\"input_ids\"].to(torch_device))\n \n+    def test_generation_tester_mixin_inheritance(self):\n+        \"\"\"\n+        Ensures that we have the generation tester mixin if the model can generate. The test will fail otherwise,\n+        forcing the mixin to be added -- and ensuring proper test coverage\n+        \"\"\"\n+        if len(self.all_generative_model_classes) > 0:\n+            self.assertTrue(\n+                issubclass(self.__class__, GenerationTesterMixin),\n+                msg=(\n+                    \"This model can call `generate` from `GenerationMixin`, so one of two things must happen: 1) the \"\n+                    \"tester must inherit from `GenerationTesterMixin` to run `generate` tests, or 2) if the model \"\n+                    \"doesn't fully support the original `generate` or has a custom `generate` with partial feature \"\n+                    \"support, the tester must overwrite `all_generative_model_classes` to skip the failing classes \"\n+                    \"(make sure to comment why). If `all_generative_model_classes` is overwritten as `()`, then we \"\n+                    \"need to remove the `GenerationTesterMixin` inheritance -- no `generate` tests are being run.\"\n+                ),\n+            )\n+        else:\n+            self.assertFalse(\n+                issubclass(self.__class__, GenerationTesterMixin),\n+                msg=(\n+                    \"This model can't call `generate`, so its tester can't inherit `GenerationTesterMixin`. (If you \"\n+                    \"think the model should be able to `generate`, the model may be missing the `GenerationMixin` \"\n+                    \"inheritance)\"\n+                ),\n+            )\n+\n \n global_rng = random.Random()\n "
        },
        {
            "sha": "72434a192226af89ffeb93e7f35f8e6264707f38",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=678885bbbd8e99b8f9576a8e50f2ba4a9f1b6cce",
            "patch": "@@ -1709,17 +1709,7 @@ class DummyBertWithMixin(BertModel, GenerationMixin):\n         self.assertTrue(\"\" == cl.out)\n         self.assertTrue(can_generate)\n \n-        # 3 - Alternatively, a model can implement a `generate` method\n-        class DummyBertWithGenerate(BertModel):\n-            def generate(self):\n-                pass\n-\n-        with CaptureLogger(logger) as cl:\n-            can_generate = DummyBertWithGenerate.can_generate()\n-        self.assertTrue(\"\" == cl.out)\n-        self.assertTrue(can_generate)\n-\n-        # 4 - Finally, it can inherit from a model that can generate\n+        # 3 - Finally, it can inherit from a model that can generate\n         class DummyBertWithParent(DummyBertWithMixin):\n             pass\n \n@@ -1728,7 +1718,7 @@ class DummyBertWithParent(DummyBertWithMixin):\n         self.assertTrue(\"\" == cl.out)\n         self.assertTrue(can_generate)\n \n-        # 5 - BC: models with a custom `prepare_inputs_for_generation` can generate (it was assumed they inherited\n+        # 4 - BC: models with a custom `prepare_inputs_for_generation` can generate (it was assumed they inherited\n         # `GenerationMixin`)\n         class DummyBertWithPrepareInputs(BertModel):\n             def prepare_inputs_for_generation(self):"
        }
    ],
    "stats": {
        "total": 248,
        "additions": 180,
        "deletions": 68
    }
}