{
    "author": "SunMarc",
    "message": "[Trainer] remove env vars  (#41697)\n\n* remove env var\n\n* style\n\n* fix value\n\n* update\n\n* fix\n\n* style\n\n* fix\n\n* maybe this time\n\n* rm tests\n\n* fix",
    "sha": "91b5a680c050d56a395e4dd96eb1b48520bf752f",
    "files": [
        {
            "sha": "22652ba7a9741ed6157dab0e83498c44dc1bd377",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 22,
            "deletions": 3,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/91b5a680c050d56a395e4dd96eb1b48520bf752f/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91b5a680c050d56a395e4dd96eb1b48520bf752f/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=91b5a680c050d56a395e4dd96eb1b48520bf752f",
            "patch": "@@ -209,7 +209,6 @@\n \n if is_accelerate_available():\n     from accelerate import Accelerator, skip_first_batches\n-    from accelerate import __version__ as accelerate_version\n     from accelerate.state import AcceleratorState\n     from accelerate.utils import (\n         DataLoaderConfiguration,\n@@ -4967,7 +4966,18 @@ def create_accelerator_and_postprocess(self):\n         # this would have been updated above, no need for it anymore\n         accelerator_config.pop(\"gradient_accumulation_kwargs\")\n \n-        args = {\"deepspeed_plugin\": self.args.deepspeed_plugin, \"dataloader_config\": dataloader_config}\n+        fsdp_plugin = None\n+        if self.args.fsdp_plugin_args is not None:\n+            from accelerate.utils import FullyShardedDataParallelPlugin\n+\n+            fsdp_plugin = FullyShardedDataParallelPlugin(**self.args.fsdp_plugin_args)\n+\n+        args = {\n+            \"mixed_precision\": self.args.mixed_precision,\n+            \"dataloader_config\": dataloader_config,\n+            \"fsdp_plugin\": fsdp_plugin,\n+            \"deepspeed_plugin\": self.args.deepspeed_plugin,\n+        }\n \n         # We defer compatibility checks to accelerator\n         if self.args.parallelism_config is not None:\n@@ -4981,14 +4991,23 @@ def create_accelerator_and_postprocess(self):\n         if getattr(self.model, \"tp_size\", None) is not None and self.model.tp_size > 1:\n             self.is_tp_enabled = True\n             if self.args.parallelism_config is not None:\n-                if version.parse(accelerate_version) > version.parse(\"1.10.1\"):\n+                if is_accelerate_available(\"1.10.1\"):\n                     if self.args.parallelism_config is not None:\n                         from accelerate import ParallelismConfig\n \n                         args[\"parallelism_config\"] = ParallelismConfig(tp_size=self.model.tp_size)\n                 else:\n                     raise ValueError(\"Requires accelerate>1.10.1 to use Tensor Parallelism.\")\n \n+        if is_accelerate_available(\"1.2.0\"):\n+            # it we don't have the correct version, we will rely on env var instead that were set in TrainingArguments\n+            from accelerate.utils import TorchDynamoPlugin\n+\n+            dynamo_plugin = TorchDynamoPlugin(\n+                backend=self.args.torch_compile_backend, mode=self.args.torch_compile_mode\n+            )\n+            args[\"dynamo_plugin\"] = dynamo_plugin\n+\n         # create accelerator object\n         self.accelerator = Accelerator(**args)\n         # some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag"
        },
        {
            "sha": "1e2ec89ba0e409f850f2f466ab9c1c61aa691b9a",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 150,
            "deletions": 148,
            "changes": 298,
            "blob_url": "https://github.com/huggingface/transformers/blob/91b5a680c050d56a395e4dd96eb1b48520bf752f/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91b5a680c050d56a395e4dd96eb1b48520bf752f/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=91b5a680c050d56a395e4dd96eb1b48520bf752f",
            "patch": "@@ -462,6 +462,8 @@ class TrainingArguments:\n             fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n \n             A List of config and its options:\n+                - fsdp_version (`int`, *optional*, defaults to `1`):\n+                    The version of FSDP to use. Defaults to 1.\n                 - min_num_params (`int`, *optional*, defaults to `0`):\n                     FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n                     passed).\n@@ -1534,6 +1536,28 @@ def __post_init__(self):\n \n         self.optim = OptimizerNames(self.optim)\n \n+        # We need to get from the env as we are setting deepspeed mixed precison afterwards\n+        self.mixed_precision = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n+        if self.fp16:\n+            self.mixed_precision = \"fp16\"\n+        elif self.bf16:\n+            self.mixed_precision = \"bf16\"\n+\n+        if (self.torch_compile_mode is not None or self.torch_compile_backend is not None) and not self.torch_compile:\n+            self.torch_compile = True\n+        if self.torch_compile and self.torch_compile_backend is None:\n+            if not self.use_cpu and is_torch_hpu_available():\n+                self.torch_compile_backend = \"hpu_backend\"\n+            else:\n+                self.torch_compile_backend = \"inductor\"\n+\n+        if self.torch_compile:\n+            # TODO: remove this once we've bumped the minimum accelerate version\n+            if not is_accelerate_available(\"1.2.0\"):\n+                os.environ[\"ACCELERATE_DYNAMO_BACKEND\"] = self.torch_compile_backend\n+                if self.torch_compile_mode is not None:\n+                    os.environ[\"ACCELERATE_DYNAMO_MODE\"] = self.torch_compile_mode\n+\n         # We need to setup the accelerator config here *before* the first call to `self.device`\n         if is_accelerate_available():\n             if not isinstance(self.accelerator_config, AcceleratorConfig):\n@@ -1560,22 +1584,6 @@ def __post_init__(self):\n         if is_torch_available():\n             self.device\n \n-        if (self.torch_compile_mode is not None or self.torch_compile_backend is not None) and not self.torch_compile:\n-            self.torch_compile = True\n-        if self.torch_compile and self.torch_compile_backend is None:\n-            if not self.use_cpu and is_torch_hpu_available():\n-                self.torch_compile_backend = \"hpu_backend\"\n-            else:\n-                self.torch_compile_backend = \"inductor\"\n-\n-        # accelerate integration for torch compile\n-        if self.torch_compile:\n-            # set env vars for accelerate\n-            prefix = \"ACCELERATE_DYNAMO_\"\n-            os.environ[prefix + \"BACKEND\"] = self.torch_compile_backend\n-            if self.torch_compile_mode is not None:\n-                os.environ[prefix + \"MODE\"] = self.torch_compile_mode\n-\n         if is_torch_available() and self.torch_compile:\n             if is_torch_tf32_available():\n                 if self.tf32 is None and not self.fp16 or self.bf16:\n@@ -1612,14 +1620,6 @@ def __post_init__(self):\n                         torch.backends.cudnn.allow_tf32 = False\n                 # no need to assert on else\n \n-        # if training args is specified, it will override the one specified in the accelerate config\n-        mixed_precision_dtype = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n-        if self.fp16:\n-            mixed_precision_dtype = \"fp16\"\n-        elif self.bf16:\n-            mixed_precision_dtype = \"bf16\"\n-        os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n-\n         if self.report_to == \"all\" or self.report_to == [\"all\"]:\n             # Import at runtime to avoid a circular import.\n             from .integrations import get_available_reporting_integrations\n@@ -1648,130 +1648,15 @@ def __post_init__(self):\n         if not isinstance(self.warmup_steps, int) or self.warmup_steps < 0:\n             raise ValueError(\"warmup_steps must be of type int and must be 0 or a positive integer.\")\n \n-        if self.fsdp is None:\n-            self.fsdp = []\n-        elif self.fsdp is True:\n-            self.fsdp = [FSDPOption.FULL_SHARD]\n-        elif isinstance(self.fsdp, str):\n-            self.fsdp = [FSDPOption(s) for s in self.fsdp.split()]\n-\n-        if self.fsdp == [FSDPOption.OFFLOAD]:\n-            raise ValueError(\n-                \"`--fsdp offload` can't work on its own. It needs to be added to `--fsdp full_shard` or \"\n-                '`--fsdp shard_grad_op`. For example, `--fsdp \"full_shard offload\"`.'\n-            )\n-        elif FSDPOption.FULL_SHARD in self.fsdp and FSDPOption.SHARD_GRAD_OP in self.fsdp:\n-            raise ValueError(\"`--fsdp full_shard` is not compatible with `--fsdp shard_grad_op`.\")\n-\n-        if self.gradient_checkpointing and (\n-            FSDPOption.FULL_SHARD in self.fsdp or FSDPOption.HYBRID_SHARD in self.fsdp\n-        ):\n-            logger.warning(\n-                \"When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please\"\n-                \" use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather\"\n-                \" operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\"\n-            )\n-\n-        if self.fsdp_config is None:\n-            self.fsdp_config = {}\n-\n-        if isinstance(self.fsdp_config, str):\n-            if len(self.fsdp) == 0:\n-                warnings.warn(\"`--fsdp_config` is useful only when `--fsdp` is specified.\")\n-            with open(self.fsdp_config, encoding=\"utf-8\") as f:\n-                self.fsdp_config = json.load(f)\n-\n-        if self.fsdp_config is not None and isinstance(self.fsdp_config, dict):\n-            for k in list(self.fsdp_config.keys()):\n-                if k.startswith(\"fsdp_\"):\n-                    v = self.fsdp_config.pop(k)\n-                    self.fsdp_config[k[5:]] = v\n-\n-        self.fsdp_config[\"min_num_params\"] = self.fsdp_config.get(\"min_num_params\", 0)\n-\n-        # if fsdp_config[\"transformer_layer_cls_to_wrap\"] is specified as a string, convert it to a list with a single object\n-        if isinstance(self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None), str):\n-            self.fsdp_config[\"transformer_layer_cls_to_wrap\"] = [self.fsdp_config[\"transformer_layer_cls_to_wrap\"]]\n-\n-        if len(self.fsdp) == 0 and self.fsdp_config[\"min_num_params\"] > 0:\n-            warnings.warn(\"`min_num_params` is useful only when `--fsdp` is specified.\")\n-\n-        if len(self.fsdp) == 0 and self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None:\n-            warnings.warn(\"`transformer_layer_cls_to_wrap` is useful only when `--fsdp` is specified.\")\n-\n-        if (\n-            len(self.fsdp) > 0\n-            and self.fsdp_config[\"min_num_params\"] > 0\n-            and self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None\n-        ):\n-            raise ValueError(\"`min_num_params` and `transformer_layer_cls_to_wrap` are mutually exclusive.\")\n-        self.fsdp_config[\"xla\"] = self.fsdp_config.get(\"xla\", False)\n-        self.fsdp_config[\"xla_fsdp_v2\"] = self.fsdp_config.get(\"xla_fsdp_v2\", False)\n-        self.fsdp_config[\"xla_fsdp_grad_ckpt\"] = self.fsdp_config.get(\"xla_fsdp_grad_ckpt\", False)\n-        if self.fsdp_config[\"xla\"]:\n-            if len(self.fsdp) > 0:\n-                # store XLA fsdp configuration parameters into a dictionary\n-                # Copy the config to avoid modifying the original config (which may be used for JSON serialization)\n-                self.xla_fsdp_config = self.fsdp_config.get(\"xla_fsdp_settings\", {}).copy()\n-                # apply appropriate string to torch.dtype conversions for parameters\n-                if \"compute_dtype\" in self.xla_fsdp_config:\n-                    self.xla_fsdp_config[\"compute_dtype\"] = getattr(torch, self.xla_fsdp_config[\"compute_dtype\"])\n-                if \"buffer_dtype\" in self.xla_fsdp_config:\n-                    self.xla_fsdp_config[\"buffer_dtype\"] = getattr(torch, self.xla_fsdp_config[\"buffer_dtype\"])\n-            else:\n-                warnings.warn(\"XLA FSDP can be used only when `--fsdp` is specified.\")\n-        else:\n-            if self.fsdp_config[\"xla_fsdp_grad_ckpt\"]:\n-                warnings.warn(\"`--xla_fsdp_grad_ckpt` is useful only when `--xla` is set to true.\")\n-\n-        # accelerate integration for FSDP\n-        if len(self.fsdp) > 0 and not self.fsdp_config[\"xla\"]:\n-            os.environ[\"ACCELERATE_USE_FSDP\"] = \"true\"\n-            from accelerate.utils.constants import (\n-                FSDP_AUTO_WRAP_POLICY,\n-                FSDP_SHARDING_STRATEGY,\n-            )\n-\n-            prefix = \"FSDP_\"\n-            for fsdp_option in self.fsdp:\n-                if fsdp_option.upper() in FSDP_SHARDING_STRATEGY:\n-                    # set environment variable for FSDP sharding strategy\n-                    os.environ[f\"{prefix}SHARDING_STRATEGY\"] = str(\n-                        FSDP_SHARDING_STRATEGY.index(fsdp_option.upper()) + 1\n-                    )\n-                elif fsdp_option == FSDPOption.OFFLOAD:\n-                    os.environ[f\"{prefix}OFFLOAD_PARAMS\"] = \"true\"\n-                elif fsdp_option == FSDPOption.AUTO_WRAP:\n-                    os.environ[f\"{prefix}AUTO_WRAP_POLICY\"] = FSDP_AUTO_WRAP_POLICY[0]\n-                    if self.fsdp_config[\"min_num_params\"] > 0:\n-                        os.environ[f\"{prefix}MIN_NUM_PARAMS\"] = str(self.fsdp_config[\"min_num_params\"])\n-                        os.environ[f\"{prefix}AUTO_WRAP_POLICY\"] = FSDP_AUTO_WRAP_POLICY[1]\n-                    elif self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None:\n-                        os.environ[f\"{prefix}TRANSFORMER_CLS_TO_WRAP\"] = \",\".join(\n-                            self.fsdp_config[\"transformer_layer_cls_to_wrap\"]\n-                        )\n-            prefetch_policy = self.fsdp_config.get(\"backward_prefetch\", \"NO_PREFETCH\")\n-            os.environ[f\"{prefix}BACKWARD_PREFETCH\"] = prefetch_policy.upper()\n-            os.environ[f\"{prefix}FORWARD_PREFETCH\"] = str(self.fsdp_config.get(\"forward_prefetch\", \"false\")).lower()\n-\n-            sync_module_states = str(self.fsdp_config.get(\"sync_module_states\", \"true\")).lower()\n-            cpu_ram_efficient_loading = str(self.fsdp_config.get(\"cpu_ram_efficient_loading\", \"false\")).lower()\n-\n-            if sync_module_states == \"false\" and cpu_ram_efficient_loading == \"true\":\n-                # In this case, all the processes except the main process would have random weights leading\n-                # to unexpected behaviour during training, thus throwing error here to prevent it.\n-                raise ValueError('`sync_module_states` must be `\"True\"` if `cpu_ram_efficient_loading` is `\"True\"`')\n-\n-            os.environ[f\"{prefix}SYNC_MODULE_STATES\"] = sync_module_states\n-            os.environ[f\"{prefix}CPU_RAM_EFFICIENT_LOADING\"] = cpu_ram_efficient_loading\n-\n-            os.environ[f\"{prefix}USE_ORIG_PARAMS\"] = str(self.fsdp_config.get(\"use_orig_params\", \"true\")).lower()\n-\n         if isinstance(self.debug, str):\n             self.debug = [DebugOption(s) for s in self.debug.split()]\n         elif self.debug is None:\n             self.debug = []\n \n+        self.fsdp_plugin_args = None\n+        # we can't save the plugin due to a pickle issue, so we do not initialize the plugin here\n+        self.fsdp_plugin_args = self._process_fsdp_args()\n+\n         self.deepspeed_plugin = None\n         if self.deepspeed:\n             # - must be run very last in arg parsing, since it will use a lot of these settings.\n@@ -1790,15 +1675,13 @@ def __post_init__(self):\n             # Accelerate DeepSpeed Plugin\n             from accelerate.utils import DeepSpeedPlugin\n \n-            os.environ[\"ACCELERATE_USE_DEEPSPEED\"] = \"true\"\n             self.deepspeed_plugin = DeepSpeedPlugin(hf_ds_config=self.hf_deepspeed_config)\n         elif strtobool(os.environ.get(\"ACCELERATE_USE_DEEPSPEED\", \"false\")):\n             # Accelerate DeepSpeed Plugin\n             from accelerate.utils import DeepSpeedPlugin\n \n             self.deepspeed_plugin = DeepSpeedPlugin()\n-            mixed_precision = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n-            self.deepspeed_plugin.set_mixed_precision(mixed_precision)\n+            self.deepspeed_plugin.set_mixed_precision(self.mixed_precision)\n             self.deepspeed_plugin.set_deepspeed_weakref()\n \n         if self.use_cpu:\n@@ -1879,8 +1762,6 @@ def _setup_devices(self) -> \"torch.device\":\n         else:\n             AcceleratorState._reset_state(reset_partial_state=True)\n             self.distributed_state = None\n-        if \"ACCELERATE_USE_IPEX\" not in os.environ:\n-            os.environ[\"ACCELERATE_USE_IPEX\"] = \"false\"\n \n         self._n_gpu = 1\n         if self.use_cpu or strtobool(os.environ.get(\"ACCELERATE_USE_CPU\", \"False\")):\n@@ -2762,6 +2643,127 @@ def set_dataloader(\n         self.data_seed = sampler_seed\n         return self\n \n+    def _process_fsdp_args(self):\n+        if self.fsdp is None:\n+            self.fsdp = []\n+        elif self.fsdp is True:\n+            self.fsdp = [FSDPOption.FULL_SHARD]\n+        elif isinstance(self.fsdp, str):\n+            self.fsdp = [FSDPOption(s) for s in self.fsdp.split()]\n+\n+        if self.fsdp == [FSDPOption.OFFLOAD]:\n+            raise ValueError(\n+                \"`--fsdp offload` can't work on its own. It needs to be added to `--fsdp full_shard` or \"\n+                '`--fsdp shard_grad_op`. For example, `--fsdp \"full_shard offload\"`.'\n+            )\n+        elif FSDPOption.FULL_SHARD in self.fsdp and FSDPOption.SHARD_GRAD_OP in self.fsdp:\n+            raise ValueError(\"`--fsdp full_shard` is not compatible with `--fsdp shard_grad_op`.\")\n+\n+        if self.gradient_checkpointing and (\n+            FSDPOption.FULL_SHARD in self.fsdp or FSDPOption.HYBRID_SHARD in self.fsdp\n+        ):\n+            logger.warning(\n+                \"When using FSDP full shard, instead of using `gradient_checkpointing` in TrainingArguments, please\"\n+                \" use `activation_checkpointing` in `fsdp_config`. The former introduces a redundant AllGather\"\n+                \" operation in backward pass. Reference: https://github.com/huggingface/transformers/issues/30404\"\n+            )\n+\n+        if self.fsdp_config is None:\n+            self.fsdp_config = {}\n+\n+        if isinstance(self.fsdp_config, str):\n+            if len(self.fsdp) == 0:\n+                warnings.warn(\"`--fsdp_config` is useful only when `--fsdp` is specified.\")\n+            with open(self.fsdp_config, encoding=\"utf-8\") as f:\n+                self.fsdp_config = json.load(f)\n+\n+        if self.fsdp_config is not None and isinstance(self.fsdp_config, dict):\n+            for k in list(self.fsdp_config.keys()):\n+                if k.startswith(\"fsdp_\"):\n+                    v = self.fsdp_config.pop(k)\n+                    self.fsdp_config[k[5:]] = v\n+\n+        self.fsdp_config[\"min_num_params\"] = self.fsdp_config.get(\"min_num_params\", 0)\n+\n+        # if fsdp_config[\"transformer_layer_cls_to_wrap\"] is specified as a string, convert it to a list with a single object\n+        if isinstance(self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None), str):\n+            self.fsdp_config[\"transformer_layer_cls_to_wrap\"] = [self.fsdp_config[\"transformer_layer_cls_to_wrap\"]]\n+\n+        if len(self.fsdp) == 0 and self.fsdp_config[\"min_num_params\"] > 0:\n+            warnings.warn(\"`min_num_params` is useful only when `--fsdp` is specified.\")\n+\n+        if len(self.fsdp) == 0 and self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None:\n+            warnings.warn(\"`transformer_layer_cls_to_wrap` is useful only when `--fsdp` is specified.\")\n+\n+        if (\n+            len(self.fsdp) > 0\n+            and self.fsdp_config[\"min_num_params\"] > 0\n+            and self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None\n+        ):\n+            raise ValueError(\"`min_num_params` and `transformer_layer_cls_to_wrap` are mutually exclusive.\")\n+        self.fsdp_config[\"xla\"] = self.fsdp_config.get(\"xla\", False)\n+        self.fsdp_config[\"xla_fsdp_v2\"] = self.fsdp_config.get(\"xla_fsdp_v2\", False)\n+        self.fsdp_config[\"xla_fsdp_grad_ckpt\"] = self.fsdp_config.get(\"xla_fsdp_grad_ckpt\", False)\n+        if self.fsdp_config[\"xla\"]:\n+            if len(self.fsdp) > 0:\n+                # store XLA fsdp configuration parameters into a dictionary\n+                # Copy the config to avoid modifying the original config (which may be used for JSON serialization)\n+                self.xla_fsdp_config = self.fsdp_config.get(\"xla_fsdp_settings\", {}).copy()\n+                # apply appropriate string to torch.dtype conversions for parameters\n+                if \"compute_dtype\" in self.xla_fsdp_config:\n+                    self.xla_fsdp_config[\"compute_dtype\"] = getattr(torch, self.xla_fsdp_config[\"compute_dtype\"])\n+                if \"buffer_dtype\" in self.xla_fsdp_config:\n+                    self.xla_fsdp_config[\"buffer_dtype\"] = getattr(torch, self.xla_fsdp_config[\"buffer_dtype\"])\n+            else:\n+                warnings.warn(\"XLA FSDP can be used only when `--fsdp` is specified.\")\n+        else:\n+            if self.fsdp_config[\"xla_fsdp_grad_ckpt\"]:\n+                warnings.warn(\"`--xla_fsdp_grad_ckpt` is useful only when `--xla` is set to true.\")\n+\n+        # accelerate integration for FSDP\n+        fsdp_plugin_args = None\n+        if len(self.fsdp) > 0 and not self.fsdp_config[\"xla\"]:\n+            from accelerate.utils.constants import (\n+                FSDP_AUTO_WRAP_POLICY,\n+                FSDP_SHARDING_STRATEGY,\n+            )\n+\n+            fsdp_plugin_args = {}\n+            for fsdp_option in self.fsdp:\n+                if fsdp_option.upper() in FSDP_SHARDING_STRATEGY:\n+                    fsdp_plugin_args[\"sharding_strategy\"] = fsdp_option\n+                elif fsdp_option == FSDPOption.OFFLOAD:\n+                    fsdp_plugin_args[\"cpu_offload\"] = True\n+                elif fsdp_option == FSDPOption.AUTO_WRAP:\n+                    fsdp_plugin_args[\"auto_wrap_policy\"] = FSDP_AUTO_WRAP_POLICY[0]\n+                    if self.fsdp_config[\"min_num_params\"] > 0:\n+                        fsdp_plugin_args[\"min_num_params\"] = self.fsdp_config[\"min_num_params\"]\n+                        fsdp_plugin_args[\"auto_wrap_policy\"] = FSDP_AUTO_WRAP_POLICY[1]\n+                    elif self.fsdp_config.get(\"transformer_layer_cls_to_wrap\", None) is not None:\n+                        fsdp_plugin_args[\"transformer_cls_names_to_wrap\"] = \",\".join(\n+                            self.fsdp_config[\"transformer_layer_cls_to_wrap\"]\n+                        )\n+            fsdp_plugin_args[\"fsdp_version\"] = self.fsdp_config.get(\"fsdp_version\", 1)\n+            prefetch_policy = self.fsdp_config.get(\"backward_prefetch\", \"NO_PREFETCH\")\n+            fsdp_plugin_args[\"backward_prefetch\"] = prefetch_policy.upper()\n+            fsdp_plugin_args[\"forward_prefetch\"] = str(self.fsdp_config.get(\"forward_prefetch\", \"false\")).lower()\n+\n+            sync_module_states = str(self.fsdp_config.get(\"sync_module_states\", \"true\")).lower()\n+            cpu_ram_efficient_loading = str(self.fsdp_config.get(\"cpu_ram_efficient_loading\", \"false\")).lower()\n+            if sync_module_states == \"false\" and cpu_ram_efficient_loading == \"true\":\n+                # In this case, all the processes except the main process would have random weights leading\n+                # to unexpected behaviour during training, thus throwing error here to prevent it.\n+                raise ValueError('`sync_module_states` must be `\"True\"` if `cpu_ram_efficient_loading` is `\"True\"`')\n+\n+            # we need to set the env here as otherwise we get a warning in accelerate + we need to set it for transformers\n+            fsdp_plugin_args[\"cpu_ram_efficient_loading\"] = cpu_ram_efficient_loading\n+            os.environ[\"FSDP_CPU_RAM_EFFICIENT_LOADING\"] = cpu_ram_efficient_loading\n+\n+            fsdp_plugin_args[\"sync_module_states\"] = sync_module_states\n+            fsdp_plugin_args[\"use_orig_params\"] = str(self.fsdp_config.get(\"use_orig_params\", \"true\")).lower()\n+\n+        return fsdp_plugin_args\n+\n \n class ParallelMode(Enum):\n     NOT_PARALLEL = \"not_parallel\""
        },
        {
            "sha": "7f0cb0482bdb3d69971d5aa350959f0938a65bc9",
            "filename": "tests/fsdp/test_fsdp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/91b5a680c050d56a395e4dd96eb1b48520bf752f/tests%2Ffsdp%2Ftest_fsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91b5a680c050d56a395e4dd96eb1b48520bf752f/tests%2Ffsdp%2Ftest_fsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffsdp%2Ftest_fsdp.py?ref=91b5a680c050d56a395e4dd96eb1b48520bf752f",
            "patch": "@@ -16,7 +16,6 @@\n import os\n import subprocess\n import unittest\n-from copy import deepcopy\n from functools import partial\n \n from parameterized import parameterized\n@@ -191,7 +190,6 @@ def test_accelerate_fsdp_config(self, sharding_strategy, dtype):\n             for k, v in trainer.args.fsdp_config.items():\n                 self.assertTrue(k in self.accelerate_fsdp_config)\n                 self.assertEqual(v, self.accelerate_fsdp_config[k])\n-            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n \n     @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n     def test_fsdp_config(self, sharding_strategy, dtype):\n@@ -212,44 +210,6 @@ def test_fsdp_config(self, sharding_strategy, dtype):\n             self.assertEqual(trainer.args.fsdp[2], FSDPOption.AUTO_WRAP)\n             for k, v in trainer.args.fsdp_config.items():\n                 self.assertEqual(v, self.fsdp_config[k])\n-            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n-\n-    @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n-    def test_fsdp_config_transformers_auto_wrap(self, sharding_strategy, dtype):\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        fsdp_config = deepcopy(self.fsdp_config)\n-        del fsdp_config[\"min_num_params\"]\n-        fsdp_config[\"transformer_layer_cls_to_wrap\"] = \"BertLayer\"\n-        kwargs = {\n-            \"output_dir\": output_dir,\n-            \"train_len\": 128,\n-            \"save_steps\": 5,\n-            \"learning_rate\": 0.1,\n-            \"fsdp\": f\"{sharding_strategy} offload auto_wrap\",\n-            \"fsdp_config\": fsdp_config,\n-        }\n-        kwargs[dtype] = True\n-        prefix = \"FSDP_\"\n-        with mockenv_context(**self.dist_env_1_gpu):\n-            trainer = get_regression_trainer(**kwargs)\n-            self.assertEqual(trainer.args.fsdp[0], sharding_strategy)\n-            self.assertEqual(trainer.args.fsdp[1], FSDPOption.OFFLOAD)\n-            self.assertEqual(trainer.args.fsdp[2], FSDPOption.AUTO_WRAP)\n-            fsdp_sharding_strategy = str(FSDP_SHARDING_STRATEGY.index(sharding_strategy.upper()) + 1)\n-            self.assertEqual(os.environ[f\"{prefix}SHARDING_STRATEGY\"], fsdp_sharding_strategy)\n-            self.assertEqual(os.environ[f\"{prefix}OFFLOAD_PARAMS\"], \"true\")\n-            self.assertEqual(os.environ[f\"{prefix}AUTO_WRAP_POLICY\"], \"TRANSFORMER_BASED_WRAP\")\n-            self.assertEqual(\n-                os.environ[f\"{prefix}TRANSFORMER_CLS_TO_WRAP\"], \",\".join(fsdp_config[\"transformer_layer_cls_to_wrap\"])\n-            )\n-            self.assertEqual(os.environ[f\"{prefix}BACKWARD_PREFETCH\"], fsdp_config[\"backward_prefetch\"])\n-            self.assertEqual(os.environ[f\"{prefix}FORWARD_PREFETCH\"], fsdp_config[\"forward_prefetch\"])\n-            self.assertEqual(os.environ[f\"{prefix}USE_ORIG_PARAMS\"], fsdp_config[\"use_orig_params\"])\n-            self.assertEqual(os.environ[f\"{prefix}SYNC_MODULE_STATES\"], fsdp_config[\"sync_module_states\"])\n-            self.assertEqual(\n-                os.environ[f\"{prefix}CPU_RAM_EFFICIENT_LOADING\"], fsdp_config[\"cpu_ram_efficient_loading\"]\n-            )\n-            self.assertEqual(os.environ.get(\"ACCELERATE_USE_FSDP\", \"false\"), \"true\")\n \n     @parameterized.expand(params, name_func=_parameterized_custom_name_func)\n     @require_torch_multi_accelerator"
        }
    ],
    "stats": {
        "total": 363,
        "additions": 172,
        "deletions": 191
    }
}