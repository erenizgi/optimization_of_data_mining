{
    "author": "regisss",
    "message": "Remove call to `.item` in `get_batch_samples` (#36861)",
    "sha": "0adbc873d0de14efdeedc9dfb8b5b619783efddb",
    "files": [
        {
            "sha": "f843d9be759126fe804ae8c8fc372bf1cbba58ea",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0adbc873d0de14efdeedc9dfb8b5b619783efddb/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0adbc873d0de14efdeedc9dfb8b5b619783efddb/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=0adbc873d0de14efdeedc9dfb8b5b619783efddb",
            "patch": "@@ -2505,7 +2505,7 @@ def _inner_training_loop(\n             for _ in range(total_updates):\n                 update_step += 1\n                 num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\n-                batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\n+                batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches, args.device)\n                 for i, inputs in enumerate(batch_samples):\n                     step += 1\n                     do_sync_step = (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == steps_in_epoch\n@@ -5216,7 +5216,7 @@ def _fsdp_qlora_plugin_updates(self):\n                     self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True\n                 )\n \n-    def get_batch_samples(self, epoch_iterator, num_batches):\n+    def get_batch_samples(self, epoch_iterator, num_batches, device):\n         batch_samples = []\n         num_items_in_batch = None\n         for _ in range(num_batches):\n@@ -5232,11 +5232,12 @@ def get_batch_samples(self, epoch_iterator, num_batches):\n             except (TypeError, AttributeError):\n                 pass\n \n-        if self.args.average_tokens_across_devices and num_items_in_batch is not None:\n-            num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()\n+        if num_items_in_batch is not None:\n+            if self.args.average_tokens_across_devices:\n+                num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum()\n \n-        if torch.is_tensor(num_items_in_batch):\n-            num_items_in_batch = num_items_in_batch.item()\n+            if torch.is_tensor(num_items_in_batch):\n+                num_items_in_batch = num_items_in_batch.to(device)\n \n         return batch_samples, num_items_in_batch\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}