{
    "author": "gante",
    "message": "[generate] move max time tests (#35962)\n\n* move max time tests to their right place\r\n\r\n* move test to the right place",
    "sha": "4d3b1076a17b72f68c7332008b667c22e81d8f94",
    "files": [
        {
            "sha": "170128ce22c0b8a57edca1e9bcf275ba6bfddaf1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4d3b1076a17b72f68c7332008b667c22e81d8f94",
            "patch": "@@ -16,6 +16,7 @@\n \n import collections\n import copy\n+import datetime\n import gc\n import inspect\n import tempfile\n@@ -4249,6 +4250,46 @@ def test_assisted_generation_early_exit(self):\n         decoded_assisted = tokenizer.batch_decode(outputs_assisted, skip_special_tokens=True)\n         self.assertEqual(decoded_assisted, [expected_output])\n \n+    @slow\n+    def test_max_time(self):\n+        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n+        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n+        model.to(torch_device)\n+\n+        torch.manual_seed(0)\n+        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n+        input_ids = tokenized.input_ids.to(torch_device)\n+\n+        MAX_TIME = 0.1\n+        MAX_LENGTH = 64\n+\n+        # sampling on\n+        start = datetime.datetime.now()\n+        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=MAX_LENGTH)\n+        duration = datetime.datetime.now() - start\n+        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n+        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n+\n+        # sampling off\n+        start = datetime.datetime.now()\n+        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=MAX_LENGTH)\n+        duration = datetime.datetime.now() - start\n+        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n+        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n+\n+        # beam search\n+        start = datetime.datetime.now()\n+        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=MAX_LENGTH)\n+        duration = datetime.datetime.now() - start\n+        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n+        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n+\n+        # sanity check: no time limit\n+        start = datetime.datetime.now()\n+        model.generate(input_ids, do_sample=False, max_time=None, max_length=MAX_LENGTH)\n+        duration = datetime.datetime.now() - start\n+        self.assertGreater(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        },
        {
            "sha": "59ed051978577d9ae2267149b76c86832aab3cb9",
            "filename": "tests/models/codegen/test_modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcodegen%2Ftest_modeling_codegen.py?ref=4d3b1076a17b72f68c7332008b667c22e81d8f94",
            "patch": "@@ -14,12 +14,11 @@\n # limitations under the License.\n \n \n-import datetime\n import unittest\n \n from transformers import CodeGenConfig, is_torch_available\n from transformers.file_utils import cached_property\n-from transformers.testing_utils import backend_manual_seed, is_flaky, require_torch, slow, torch_device\n+from transformers.testing_utils import backend_manual_seed, require_torch, slow, torch_device\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -493,45 +492,3 @@ def test_codegen_sample(self):\n         self.assertTrue(\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output\n-\n-    @is_flaky(max_attempts=3, description=\"measure of timing is somehow flaky.\")\n-    @slow\n-    def test_codegen_sample_max_time(self):\n-        tokenizer = self.cached_tokenizer\n-        model = self.cached_model\n-        model.to(torch_device)\n-\n-        torch.manual_seed(0)\n-        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n-        input_ids = tokenized.input_ids.to(torch_device)\n-\n-        MAX_TIME = 0.05\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=2 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=2 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=2 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=2 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=None, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=2 * MAX_TIME))"
        },
        {
            "sha": "b75827c21538e077a860d4cd79cbb652e60fe547",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=4d3b1076a17b72f68c7332008b667c22e81d8f94",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n \n-import datetime\n import math\n import unittest\n \n@@ -828,48 +827,6 @@ def test_gpt2_sample(self):\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output\n \n-    @slow\n-    def test_gpt2_sample_max_time(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-        model.to(torch_device)\n-\n-        torch.manual_seed(0)\n-        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n-        input_ids = tokenized.input_ids.to(torch_device)\n-\n-        MAX_TIME = 0.5\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=None, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n     @slow\n     def test_contrastive_search_gpt2(self):\n         article = ("
        },
        {
            "sha": "10d5515f4670f8d6c0e14c966b93eb117bfad1c3",
            "filename": "tests/models/gptj/test_modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_gptj.py?ref=4d3b1076a17b72f68c7332008b667c22e81d8f94",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n \n-import datetime\n import unittest\n \n from transformers import GPTJConfig, is_torch_available\n@@ -546,47 +545,6 @@ def test_gptj_sample(self):\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output\n \n-    @slow\n-    def test_gptj_sample_max_time(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"anton-l/gpt-j-tiny-random\")\n-        model = GPTJForCausalLM.from_pretrained(\"anton-l/gpt-j-tiny-random\")\n-        model.to(torch_device)\n-\n-        torch.manual_seed(0)\n-        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\", return_token_type_ids=True)\n-        input_ids = tokenized.input_ids.to(torch_device)\n-\n-        MAX_TIME = 0.5\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=None, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n     @tooslow\n     def test_contrastive_search_gptj(self):\n         article = ("
        },
        {
            "sha": "85862c7af14694e9dcc5a8bfe2177a0fed7de1b7",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d3b1076a17b72f68c7332008b667c22e81d8f94/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=4d3b1076a17b72f68c7332008b667c22e81d8f94",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import datetime\n import math\n import unittest\n \n@@ -434,47 +433,6 @@ def test_xglm_sample(self):\n         ]\n         self.assertIn(output_str, EXPECTED_OUTPUT_STRS)\n \n-    @slow\n-    def test_xglm_sample_max_time(self):\n-        tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n-        model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n-        model.to(torch_device)\n-\n-        torch.manual_seed(0)\n-        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"pt\")\n-        input_ids = tokenized.input_ids.to(torch_device)\n-\n-        MAX_TIME = 0.15\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=True, num_beams=2, max_time=MAX_TIME, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=MAX_TIME))\n-        self.assertLess(duration, datetime.timedelta(seconds=1.5 * MAX_TIME))\n-\n-        start = datetime.datetime.now()\n-        model.generate(input_ids, do_sample=False, max_time=None, max_length=256)\n-        duration = datetime.datetime.now() - start\n-        self.assertGreater(duration, datetime.timedelta(seconds=1.25 * MAX_TIME))\n-\n     @require_torch_accelerator\n     @require_torch_fp16\n     def test_batched_nan_fp16(self):"
        }
    ],
    "stats": {
        "total": 213,
        "additions": 42,
        "deletions": 171
    }
}