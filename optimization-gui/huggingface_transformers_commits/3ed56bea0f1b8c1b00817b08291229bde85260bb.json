{
    "author": "BakerBunker",
    "message": "Fix inference bugs in Qwen2.5 Omni (#37701)\n\n* Init `SinusoidsPositionEmbedding` with float to avoid precision problem\n\n* fix hidden_state for talker\n\n* Update modular_qwen2_5_omni.py\n\n* Move hidden processing out from thinker\n\n* fixup\n\n---------\n\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>",
    "sha": "3ed56bea0f1b8c1b00817b08291229bde85260bb",
    "files": [
        {
            "sha": "1b90841fc2abfe459ac60f014a8438565168ee68",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 35,
            "deletions": 3,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ed56bea0f1b8c1b00817b08291229bde85260bb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ed56bea0f1b8c1b00817b08291229bde85260bb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=3ed56bea0f1b8c1b00817b08291229bde85260bb",
            "patch": "@@ -815,7 +815,7 @@ def __init__(self, length, channels, max_timescale=10000):\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n-        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n+        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2)).float()\n         scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n         self.register_buffer(\n             \"positional_embedding\",\n@@ -4560,12 +4560,44 @@ def generate(\n             return thinker_result\n \n         # 2. Generate speech tokens from talker module\n+        embeds_to_talker = thinker_result.hidden_states[0][0].clone()\n+        if thinker_kwargs.get(\"input_features\", None) is not None:\n+            audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n+            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            audio_mask_tensor = torch.zeros(\n+                [audio_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n+        if thinker_kwargs.get(\"pixel_values\", None) is not None:\n+            image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n+            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            image_mask_tensor = torch.zeros(\n+                [image_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n+        if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n+            video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n+            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            video_mask_tensor = torch.zeros(\n+                [video_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(video_mask, video_mask_tensor)\n+\n+        processed_thinker_hidden = (\n+            (embeds_to_talker,) + thinker_result.hidden_states[0][1:],\n+        ) + thinker_result.hidden_states[1:]\n         thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(self.talker.device)\n         thinker_token_embeds = [\n-            token_hidden_states[0].to(self.talker.device) for token_hidden_states in thinker_result.hidden_states\n+            token_hidden_states[0].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n         ]\n         thinker_hidden_states = [\n-            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in thinker_result.hidden_states\n+            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n         ]\n \n         talker_text_bos_token = speaker_params[\"bos_token\"]"
        },
        {
            "sha": "c0c105032355de1795394f8890b1a6db8fb9742f",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 35,
            "deletions": 3,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/3ed56bea0f1b8c1b00817b08291229bde85260bb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3ed56bea0f1b8c1b00817b08291229bde85260bb/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=3ed56bea0f1b8c1b00817b08291229bde85260bb",
            "patch": "@@ -1711,7 +1711,7 @@ def __init__(self, length, channels, max_timescale=10000):\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n-        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2))\n+        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2)).float()\n         scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n         self.register_buffer(\n             \"positional_embedding\",\n@@ -4243,12 +4243,44 @@ def generate(\n             return thinker_result\n \n         # 2. Generate speech tokens from talker module\n+        embeds_to_talker = thinker_result.hidden_states[0][0].clone()\n+        if thinker_kwargs.get(\"input_features\", None) is not None:\n+            audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n+            audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            audio_mask_tensor = torch.zeros(\n+                [audio_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n+        if thinker_kwargs.get(\"pixel_values\", None) is not None:\n+            image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n+            image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            image_mask_tensor = torch.zeros(\n+                [image_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n+        if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n+            video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n+            video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker).to(embeds_to_talker.device)\n+            video_mask_tensor = torch.zeros(\n+                [video_ids_mask.sum(), embeds_to_talker.shape[-1]],\n+                dtype=embeds_to_talker.dtype,\n+                device=self.talker.device,\n+            )\n+            embeds_to_talker.masked_scatter_(video_mask, video_mask_tensor)\n+\n+        processed_thinker_hidden = (\n+            (embeds_to_talker,) + thinker_result.hidden_states[0][1:],\n+        ) + thinker_result.hidden_states[1:]\n         thinker_generate_ids = thinker_result.sequences[:, input_ids.size(1) :].to(self.talker.device)\n         thinker_token_embeds = [\n-            token_hidden_states[0].to(self.talker.device) for token_hidden_states in thinker_result.hidden_states\n+            token_hidden_states[0].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n         ]\n         thinker_hidden_states = [\n-            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in thinker_result.hidden_states\n+            token_hidden_states[-1].to(self.talker.device) for token_hidden_states in processed_thinker_hidden\n         ]\n \n         talker_text_bos_token = speaker_params[\"bos_token\"]"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 70,
        "deletions": 6
    }
}