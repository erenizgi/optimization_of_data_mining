{
    "author": "zucchini-nlp",
    "message": "[phi4] update conversion (#37579)\n\n* update conversion\n\n* update",
    "sha": "48dd89cf55e89129d3374af6a93f6481283d0f98",
    "files": [
        {
            "sha": "65ced8db26c9504e83f799b50f3cd7b4be45bd1c",
            "filename": "src/transformers/models/phi4_multimodal/convert_phi4_multimodal_weights_to_hf.py",
            "status": "modified",
            "additions": 35,
            "deletions": 6,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/48dd89cf55e89129d3374af6a93f6481283d0f98/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/48dd89cf55e89129d3374af6a93f6481283d0f98/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fconvert_phi4_multimodal_weights_to_hf.py?ref=48dd89cf55e89129d3374af6a93f6481283d0f98",
            "patch": "@@ -21,14 +21,19 @@\n from safetensors.torch import load_file, save_file\n \n from transformers import (\n+    AutoProcessor,\n     Phi4MultimodalAudioConfig,\n     Phi4MultimodalConfig,\n+    Phi4MultimodalFeatureExtractor,\n     Phi4MultimodalForCausalLM,\n+    Phi4MultimodalImageProcessorFast,\n     Phi4MultimodalProcessor,\n     Phi4MultimodalVisionConfig,\n )\n \n \n+CHAT_TEMPLATE = \"{% for message in messages %}{{ '<|' + message['role'] + '|>' }}{% if message['content'] is string %}{{ message['content'] }}{% else %}{% for content in message['content'] %}{% if content['type'] == 'image' %}{{ '<|image|>' }}{% elif content['type'] == 'audio' %}{{ '<|audio|>' }}{% elif content['type'] == 'text' %}{{ content['text'] }}{% endif %}{% endfor %}{% endif %}{% if message['role'] == 'system' and 'tools' in message and message['tools'] is not none %}{{ '<|tool|>' + message['tools'] + '<|/tool|>' + '<|end|>' }}{% endif %}{{ '<|end|>' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>' }}{% else %}{{ eos_token }}{% endif %}\"\n+\n # fmt: off\n STATE_DICT_MAPPING = {\n     r\"^model.embed_tokens_extend.audio_embed.encoder.encoders.(\\d+).feed_forward_(in|out).net.0.linear\": r\"model.embed_tokens_extend.audio_embed.encoder.encoders.\\1.feed_forward_\\2.gate_up_proj\",\n@@ -163,12 +168,36 @@ def convert_and_write_model(input_dir: str, output_dir: str):\n \n def convert_and_save_processor(input_dir: str, output_dir: str):\n     \"\"\"Convert the processor.\"\"\"\n-    processor = Phi4MultimodalProcessor.from_pretrained(input_dir)\n-    del processor.image_processor.auto_map\n-    del processor.audio_processor.auto_map\n-    processor.chat_template = processor.tokenizer.chat_template\n-    processor.tokenizer.extra_special_tokens = {\"image_token\": \"<|endoftext10|>\", \"audio_token\": \"<|endoftext11|>\"}\n-    processor.save_pretrained(output_dir)\n+    original_processor = AutoProcessor.from_pretrained(input_dir, trust_remote_code=True)\n+    original_processor.tokenizer.extra_special_tokens = {\"image_token\": \"<|image|>\", \"audio_token\": \"<|audio|>\"}\n+    converted_processor = Phi4MultimodalProcessor(\n+        tokenizer=original_processor.tokenizer,\n+        image_processor=Phi4MultimodalImageProcessorFast(),\n+        audio_processor=Phi4MultimodalFeatureExtractor(),\n+        chat_template=CHAT_TEMPLATE,\n+    )\n+    converted_processor.save_pretrained(output_dir)\n+\n+    # we need to rename a few tokens but tokenizers doesn't allow doing that programatically\n+    # To avoid consufion and manual renaming, the below part load and re-saved each json file\n+    vocab = json.load(open(f\"{output_dir}/vocab.json\", \"r\"))\n+    vocab[\"<|endoftext11|>\"] = \"<|audio|>\"\n+    vocab[\"<|endoftext10|>\"] = \"<|image|>\"\n+    json.dump(vocab, open(f\"{output_dir}/vocab.json\", \"w\"))\n+\n+    tokenizer = json.load(open(f\"{output_dir}/tokenizer.json\", \"r\"))\n+    tokenizer[\"added_tokens\"][1][\"content\"] = \"<|image|>\"\n+    tokenizer[\"added_tokens\"][2][\"content\"] = \"<|audio|>\"\n+    tokenizer[\"model\"][\"vocab\"][\"<|audio|>\"] = tokenizer[\"model\"][\"vocab\"][\"<|endoftext11|>\"]\n+    tokenizer[\"model\"][\"vocab\"][\"<|image|>\"] = tokenizer[\"model\"][\"vocab\"][\"<|endoftext10|>\"]\n+    del tokenizer[\"model\"][\"vocab\"][\"<|endoftext11|>\"]\n+    del tokenizer[\"model\"][\"vocab\"][\"<|endoftext10|>\"]\n+    json.dump(tokenizer, open(f\"{output_dir}/tokenizer.json\", \"w\"))\n+\n+    tokenizer_config = json.load(open(f\"{output_dir}/tokenizer_config.json\", \"r\"))\n+    tokenizer_config[\"added_tokens_decoder\"][\"200010\"][\"content\"] = \"<|image|>\"\n+    tokenizer_config[\"added_tokens_decoder\"][\"200011\"][\"content\"] = \"<|audio|>\"\n+    json.dump(tokenizer_config, open(f\"{output_dir}/tokenizer_config.json\", \"w\"))\n \n \n def extract_adapters_data(input_dir: str, output_dir: str):"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 35,
        "deletions": 6
    }
}