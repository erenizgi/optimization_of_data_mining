{
    "author": "cyyever",
    "message": "Fix Optional type annotation (#36841)\n\n* Fix annotation\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation/utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Update src/transformers/generation/utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "8084b2629433d9adab84559ff43df97040966974",
    "files": [
        {
            "sha": "f1c1d58ff217026e9e58b51d2b4cac29c92bb322",
            "filename": "src/transformers/agents/agents.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fagents%2Fagents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fagents%2Fagents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagents.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -217,7 +217,7 @@ def tools(self) -> Dict[str, Tool]:\n         \"\"\"Get all tools currently in the toolbox\"\"\"\n         return self._tools\n \n-    def show_tool_descriptions(self, tool_description_template: str = None) -> str:\n+    def show_tool_descriptions(self, tool_description_template: Optional[str] = None) -> str:\n         \"\"\"\n         Returns the description of all tools in the toolbox\n \n@@ -891,7 +891,7 @@ def direct_run(self, task: str):\n \n         return final_answer\n \n-    def planning_step(self, task, is_first_step: bool = False, iteration: int = None):\n+    def planning_step(self, task, is_first_step: bool = False, iteration: Optional[int] = None):\n         \"\"\"\n         Used periodically by the agent to plan the next steps to reach the objective.\n "
        },
        {
            "sha": "2894bc800ef3124785b4836679b57116ec302037",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -1125,7 +1125,7 @@ def fram_wave(waveform: np.array, hop_length: int = 160, fft_window_size: int =\n     return frames\n \n \n-def stft(frames: np.array, windowing_function: np.array, fft_window_size: int = None):\n+def stft(frames: np.array, windowing_function: np.array, fft_window_size: Optional[int] = None):\n     \"\"\"\n     Calculates the complex Short-Time Fourier Transform (STFT) of the given framed signal. Should give the same results\n     as `torch.stft`."
        },
        {
            "sha": "02abcfd21acdd63bd53da69981b0d20393a4a81a",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -1183,8 +1183,8 @@ class StaticCache(Cache):\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: int = None,\n-        max_cache_len: int = None,\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n         device: torch.device = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n@@ -1367,8 +1367,8 @@ class SlidingWindowCache(StaticCache):\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: int = None,\n-        max_cache_len: int = None,\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n         device: torch.device = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n@@ -1674,8 +1674,8 @@ class HybridCache(Cache):\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: int = None,\n-        max_cache_len: int = None,\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n         device: Union[torch.device, str] = None,\n         dtype: torch.dtype = torch.float32,\n         max_batch_size: Optional[int] = None,\n@@ -1877,7 +1877,7 @@ class MambaCache:\n     def __init__(\n         self,\n         config: PretrainedConfig,\n-        batch_size: int = None,\n+        batch_size: Optional[int] = None,\n         dtype: torch.dtype = torch.float16,\n         device: Optional[Union[torch.device, str]] = None,\n         max_batch_size: Optional[int] = None,"
        },
        {
            "sha": "5f3cd0fd28b88740f55fbc4a48f7086eefce4e3a",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -16,6 +16,7 @@\n import os\n from functools import partial\n from multiprocessing import Pool, cpu_count\n+from typing import Optional\n \n import numpy as np\n from tqdm import tqdm\n@@ -800,8 +801,8 @@ def __init__(\n         start_position,\n         end_position,\n         is_impossible,\n-        qas_id: str = None,\n-        encoding: BatchEncoding = None,\n+        qas_id: Optional[str] = None,\n+        encoding: Optional[BatchEncoding] = None,\n     ):\n         self.input_ids = input_ids\n         self.attention_mask = attention_mask"
        },
        {
            "sha": "c2a904238add0c3e2811a1cba6443457a0399b8c",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -914,9 +914,9 @@ class PromptLookupCandidateGenerator(CandidateGenerator):\n \n     def __init__(\n         self,\n-        eos_token_id: torch.Tensor = None,\n+        eos_token_id: Optional[torch.Tensor] = None,\n         num_output_tokens: int = 10,\n-        max_matching_ngram_size: int = None,\n+        max_matching_ngram_size: Optional[int] = None,\n         max_length: int = 20,\n     ):\n         self.num_output_tokens = num_output_tokens"
        },
        {
            "sha": "b6a02cd59b338721edd0da1ff7effe90ac271af1",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -171,8 +171,8 @@ def _prepare_encoder_decoder_kwargs_for_generation(self, input_ids, params, mode\n     def _prepare_decoder_input_ids_for_generation(\n         self,\n         batch_size: int,\n-        decoder_start_token_id: int = None,\n-        bos_token_id: int = None,\n+        decoder_start_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n         model_kwargs: Optional[Dict[str, jnp.ndarray]] = None,\n     ) -> jnp.ndarray:\n         if model_kwargs is not None and \"decoder_input_ids\" in model_kwargs:\n@@ -183,7 +183,9 @@ def _prepare_decoder_input_ids_for_generation(\n         decoder_start_token_id = self._get_decoder_start_token_id(decoder_start_token_id, bos_token_id)\n         return jnp.array(decoder_start_token_id, dtype=\"i4\").reshape(1, -1).repeat(batch_size, axis=0)\n \n-    def _get_decoder_start_token_id(self, decoder_start_token_id: int = None, bos_token_id: int = None) -> int:\n+    def _get_decoder_start_token_id(\n+        self, decoder_start_token_id: Optional[int] = None, bos_token_id: Optional[int] = None\n+    ) -> int:\n         # retrieve decoder_start_token_id for encoder-decoder models\n         # fall back to bos_token_id if necessary\n         decoder_start_token_id = ("
        },
        {
            "sha": "b715bd86ac78e6ca8cc2852132d3101bf8d1b7b6",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -1077,8 +1077,8 @@ def _prepare_decoder_input_ids_for_generation(\n         batch_size: int,\n         model_input_name: str,\n         model_kwargs: Dict[str, tf.Tensor],\n-        decoder_start_token_id: int = None,\n-        bos_token_id: int = None,\n+        decoder_start_token_id: Optional[int] = None,\n+        bos_token_id: Optional[int] = None,\n     ) -> Tuple[tf.Tensor, Dict[str, tf.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n         # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n@@ -1111,7 +1111,9 @@ def _prepare_decoder_input_ids_for_generation(\n \n         return decoder_input_ids, model_kwargs\n \n-    def _get_decoder_start_token_id(self, decoder_start_token_id: int = None, bos_token_id: int = None) -> int:\n+    def _get_decoder_start_token_id(\n+        self, decoder_start_token_id: Optional[int] = None, bos_token_id: Optional[int] = None\n+    ) -> int:\n         # retrieve decoder_start_token_id for encoder-decoder models\n         # fall back to bos_token_id if necessary\n         decoder_start_token_id = ("
        },
        {
            "sha": "e7780bfeded077a43c2f1f37a3a663eda7dfbac9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -157,7 +157,7 @@ class GenerateDecoderOnlyOutput(ModelOutput):\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n-    sequences: torch.LongTensor = None\n+    sequences: torch.LongTensor\n     scores: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n@@ -202,7 +202,7 @@ class GenerateEncoderDecoderOutput(ModelOutput):\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n-    sequences: torch.LongTensor = None\n+    sequences: torch.LongTensor\n     scores: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -247,7 +247,7 @@ class GenerateBeamDecoderOnlyOutput(ModelOutput):\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n-    sequences: torch.LongTensor = None\n+    sequences: torch.LongTensor\n     sequences_scores: Optional[torch.FloatTensor] = None\n     scores: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n@@ -301,7 +301,7 @@ class GenerateBeamEncoderDecoderOutput(ModelOutput):\n             the model's documentation. Usually, a [`~cache_utils.Cache`] instance.\n     \"\"\"\n \n-    sequences: torch.LongTensor = None\n+    sequences: torch.LongTensor\n     sequences_scores: Optional[torch.FloatTensor] = None\n     scores: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n@@ -699,7 +699,7 @@ def _prepare_decoder_input_ids_for_generation(\n         model_input_name: str,\n         model_kwargs: Dict[str, torch.Tensor],\n         decoder_start_token_id: torch.Tensor,\n-        device: torch.device = None,\n+        device: Optional[torch.device] = None,\n     ) -> Tuple[torch.LongTensor, Dict[str, torch.Tensor]]:\n         \"\"\"Prepares `decoder_input_ids` for generation with encoder-decoder models\"\"\"\n         # 1. Check whether the user has defined `decoder_input_ids` manually. To facilitate in terms of input naming,\n@@ -923,7 +923,7 @@ def _get_logits_processor(\n         encoder_input_ids: torch.LongTensor,\n         prefix_allowed_tokens_fn: Callable[[int, torch.Tensor], List[int]],\n         logits_processor: Optional[LogitsProcessorList],\n-        device: str = None,\n+        device: Optional[str] = None,\n         model_kwargs: Optional[Dict[str, Any]] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n@@ -4833,7 +4833,7 @@ def _ranking_fast(\n     return selected_idx\n \n \n-def _split(data, full_batch_size: int, split_size: int = None):\n+def _split(data, full_batch_size: int, split_size: int):\n     \"\"\"\n     Takes care of three cases:\n     1. data is a tensor: e.g. last_hidden_state, pooler_output etc. split them on the batch_size dim"
        },
        {
            "sha": "669828d2c96bc0abb51c2d001691e4884a201be2",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -257,7 +257,7 @@ class BayesianDetectorConfig(PretrainedConfig):\n             Prior probability P(w) that a text is watermarked.\n     \"\"\"\n \n-    def __init__(self, watermarking_depth: int = None, base_rate: float = 0.5, **kwargs):\n+    def __init__(self, watermarking_depth: Optional[int] = None, base_rate: float = 0.5, **kwargs):\n         self.watermarking_depth = watermarking_depth\n         self.base_rate = base_rate\n         # These can be set later to store information about this detector."
        },
        {
            "sha": "0c5610dbfdf9c764de2e9ca1bab171eeebf501bf",
            "filename": "src/transformers/hf_argparser.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fhf_argparser.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fhf_argparser.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhf_argparser.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -63,11 +63,11 @@ def make_choice_type_function(choices: list) -> Callable[[str], Any]:\n \n def HfArg(\n     *,\n-    aliases: Union[str, list[str]] = None,\n-    help: str = None,\n+    aliases: Optional[Union[str, list[str]]] = None,\n+    help: Optional[str] = None,\n     default: Any = dataclasses.MISSING,\n     default_factory: Callable[[], Any] = dataclasses.MISSING,\n-    metadata: dict = None,\n+    metadata: Optional[dict] = None,\n     **kwargs,\n ) -> dataclasses.Field:\n     \"\"\"Argument helper enabling a concise syntax to create dataclass fields for parsing with `HfArgumentParser`."
        },
        {
            "sha": "e8558ceed32f4640c727a192d3ab00ed9104986f",
            "filename": "src/transformers/hyperparameter_search.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fhyperparameter_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fhyperparameter_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fhyperparameter_search.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from typing import Optional\n \n from .integrations import (\n     is_optuna_available,\n@@ -37,7 +38,7 @@\n \n class HyperParamSearchBackendBase:\n     name: str\n-    pip_package: str = None\n+    pip_package: Optional[str] = None\n \n     @staticmethod\n     def is_available():"
        },
        {
            "sha": "f07ac1ae7d91dea63514f74ee86a46051383d0ef",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -1332,12 +1332,12 @@ class SizeDict:\n     Hashable dictionary to store image size information.\n     \"\"\"\n \n-    height: int = None\n-    width: int = None\n-    longest_edge: int = None\n-    shortest_edge: int = None\n-    max_height: int = None\n-    max_width: int = None\n+    height: Optional[int] = None\n+    width: Optional[int] = None\n+    longest_edge: Optional[int] = None\n+    shortest_edge: Optional[int] = None\n+    max_height: Optional[int] = None\n+    max_width: Optional[int] = None\n \n     def __getitem__(self, key):\n         if hasattr(self, key):"
        },
        {
            "sha": "56cc7a1ebdc0e0a5c6ba2b6d02010609bc11d1fb",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -12,6 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from typing import Optional\n+\n import torch\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, MSELoss\n@@ -22,7 +24,7 @@\n from .loss_rt_detr import RTDetrForObjectDetectionLoss\n \n \n-def fixed_cross_entropy(source, target, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs):\n+def fixed_cross_entropy(source, target, num_items_in_batch: Optional[int] = None, ignore_index: int = -100, **kwargs):\n     reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n     loss = nn.functional.cross_entropy(source, target, ignore_index=ignore_index, reduction=reduction)\n     if reduction == \"sum\":\n@@ -34,7 +36,7 @@ def ForCausalLMLoss(\n     logits,\n     labels,\n     vocab_size: int,\n-    num_items_in_batch: int = None,\n+    num_items_in_batch: Optional[int] = None,\n     ignore_index: int = -100,\n     shift_labels=None,\n     **kwargs,\n@@ -58,7 +60,7 @@ def ForCausalLMLoss(\n \n \n def ForMaskedLMLoss(\n-    logits, labels, vocab_size: int, num_items_in_batch: int = None, ignore_index: int = -100, **kwargs\n+    logits, labels, vocab_size: int, num_items_in_batch: Optional[int] = None, ignore_index: int = -100, **kwargs\n ):\n     # Upcast to float if we need to compute the loss to avoid potential precision issues\n     logits = logits.float()"
        },
        {
            "sha": "da9ca2355fef785f76f727a66b36bfb95a24ee51",
            "filename": "src/transformers/onnx/features.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fonnx%2Ffeatures.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fonnx%2Ffeatures.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Ffeatures.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -53,7 +53,7 @@\n \n \n def supported_features_mapping(\n-    *supported_features: str, onnx_config_cls: str = None\n+    *supported_features: str, onnx_config_cls: Optional[str] = None\n ) -> Dict[str, Callable[[PretrainedConfig], OnnxConfig]]:\n     \"\"\"\n     Generate the mapping between supported the features and their corresponding OnnxConfig for a given model.\n@@ -626,7 +626,7 @@ def get_model_class_for_feature(feature: str, framework: str = \"pt\") -> Type:\n         return task_to_automodel[task]\n \n     @staticmethod\n-    def determine_framework(model: str, framework: str = None) -> str:\n+    def determine_framework(model: str, framework: Optional[str] = None) -> str:\n         \"\"\"\n         Determines the framework to use for the export.\n \n@@ -677,7 +677,7 @@ def determine_framework(model: str, framework: str = None) -> str:\n \n     @staticmethod\n     def get_model_from_feature(\n-        feature: str, model: str, framework: str = None, cache_dir: str = None\n+        feature: str, model: str, framework: Optional[str] = None, cache_dir: Optional[str] = None\n     ) -> Union[\"PreTrainedModel\", \"TFPreTrainedModel\"]:\n         \"\"\"\n         Attempts to retrieve a model from a model's name and the feature to be enabled."
        },
        {
            "sha": "14e4dd9b7cd2845c67400982b479151a520bc14c",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -284,7 +284,7 @@ def get_polynomial_decay_schedule_with_warmup(\n     return LambdaLR(optimizer, lr_lambda, last_epoch)\n \n \n-def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps: int, timescale: int = None):\n+def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps: int, timescale: Optional[int] = None):\n     if current_step < num_warmup_steps:\n         return float(current_step) / float(max(1, num_warmup_steps))\n     shift = timescale - num_warmup_steps\n@@ -293,7 +293,7 @@ def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps:\n \n \n def get_inverse_sqrt_schedule(\n-    optimizer: Optimizer, num_warmup_steps: int, timescale: int = None, last_epoch: int = -1\n+    optimizer: Optimizer, num_warmup_steps: int, timescale: Optional[int] = None, last_epoch: int = -1\n ):\n     \"\"\"\n     Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a"
        },
        {
            "sha": "4da4ecc901914bc12a94c8ac3193309b85661865",
            "filename": "src/transformers/optimization_tf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Foptimization_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Foptimization_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization_tf.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -59,7 +59,7 @@ def __init__(\n         decay_schedule_fn: Callable,\n         warmup_steps: int,\n         power: float = 1.0,\n-        name: str = None,\n+        name: Optional[str] = None,\n     ):\n         super().__init__()\n         self.initial_learning_rate = initial_learning_rate"
        },
        {
            "sha": "a62b3c3eab23dabef944ace9b5d0582c92aa7c43",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -565,7 +565,7 @@ def clean_custom_task(task_info):\n \n \n def pipeline(\n-    task: str = None,\n+    task: Optional[str] = None,\n     model: Optional[Union[str, \"PreTrainedModel\", \"TFPreTrainedModel\"]] = None,\n     config: Optional[Union[str, PretrainedConfig]] = None,\n     tokenizer: Optional[Union[str, PreTrainedTokenizer, \"PreTrainedTokenizerFast\"]] = None,\n@@ -580,7 +580,7 @@ def pipeline(\n     device_map=None,\n     torch_dtype=None,\n     trust_remote_code: Optional[bool] = None,\n-    model_kwargs: Dict[str, Any] = None,\n+    model_kwargs: Optional[Dict[str, Any]] = None,\n     pipeline_class: Optional[Any] = None,\n     **kwargs,\n ) -> Pipeline:"
        },
        {
            "sha": "5875658ca5ab8deeab0fa73f1bc81da30c3bc869",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -4021,7 +4021,7 @@ def prepare_seq2seq_batch(\n         max_length: Optional[int] = None,\n         max_target_length: Optional[int] = None,\n         padding: str = \"longest\",\n-        return_tensors: str = None,\n+        return_tensors: Optional[str] = None,\n         truncation: bool = True,\n         **kwargs,\n     ) -> BatchEncoding:"
        },
        {
            "sha": "e4c465dfe558ff6a31accd5fdec2198ce32cd4a8",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -99,7 +99,7 @@ class TrainerState:\n     logging_steps: int = 500\n     eval_steps: int = 500\n     save_steps: int = 500\n-    train_batch_size: int = None\n+    train_batch_size: Optional[int] = None\n     num_train_epochs: int = 0\n     num_input_tokens_seen: int = 0\n     total_flos: float = 0\n@@ -110,7 +110,7 @@ class TrainerState:\n     is_local_process_zero: bool = True\n     is_world_process_zero: bool = True\n     is_hyper_param_search: bool = False\n-    trial_name: str = None\n+    trial_name: Optional[str] = None\n     trial_params: dict[str, Union[str, float, int, bool]] = None\n     stateful_callbacks: list[\"TrainerCallback\"] = None\n "
        },
        {
            "sha": "39fb6bcebb7dba66d13a5ef4b4531c87af47d5d1",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -784,8 +784,8 @@ def _upload_modified_files(\n         commit_message: Optional[str] = None,\n         token: Optional[Union[bool, str]] = None,\n         create_pr: bool = False,\n-        revision: str = None,\n-        commit_description: str = None,\n+        revision: Optional[str] = None,\n+        commit_description: Optional[str] = None,\n     ):\n         \"\"\"\n         Uploads all modified files in `working_dir` to `repo_id`, based on `files_timestamps`.\n@@ -865,8 +865,8 @@ def push_to_hub(\n         max_shard_size: Optional[Union[int, str]] = \"5GB\",\n         create_pr: bool = False,\n         safe_serialization: bool = True,\n-        revision: str = None,\n-        commit_description: str = None,\n+        revision: Optional[str] = None,\n+        commit_description: Optional[str] = None,\n         tags: Optional[List[str]] = None,\n         **deprecated_kwargs,\n     ) -> str:"
        },
        {
            "sha": "6eddc1cc57adfad7180e97f440301d47fd80f2a2",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8084b2629433d9adab84559ff43df97040966974/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=8084b2629433d9adab84559ff43df97040966974",
            "patch": "@@ -121,7 +121,7 @@ def __init__(\n             self.update_every = 0.5  # Adjusted for smooth updated as html rending is slow on VS Code\n             # This is the only adjustment required to optimize training html rending\n \n-    def update(self, value: int, force_update: bool = False, comment: str = None):\n+    def update(self, value: int, force_update: bool = False, comment: Optional[str] = None):\n         \"\"\"\n         The main method to update the progress bar to `value`.\n "
        }
    ],
    "stats": {
        "total": 122,
        "additions": 65,
        "deletions": 57
    }
}