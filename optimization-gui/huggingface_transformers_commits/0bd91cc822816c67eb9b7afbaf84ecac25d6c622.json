{
    "author": "netique",
    "message": "Add support for `ModernBertForMultipleChoice` (#39232)\n\n* implement ModernBertForMultipleChoice\n\n* fixup, style, repo consistency\n\n* generate modeling_modernbert\n\n* add tests + docs\n\n* fix test",
    "sha": "0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
    "files": [
        {
            "sha": "8c939adce098d8c1eab3ca3cc9257077e9698ae7",
            "filename": "docs/source/en/model_doc/modernbert.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert.md?ref=0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
            "patch": "@@ -115,6 +115,11 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n [[autodoc]] ModernBertForTokenClassification\n     - forward\n \n+## ModernBertForMultipleChoice\n+\n+[[autodoc]] ModernBertForMultipleChoice\n+    - forward\n+\n ## ModernBertForQuestionAnswering\n \n [[autodoc]] ModernBertForQuestionAnswering"
        },
        {
            "sha": "84672e80a75dd85a3b92402e9da57e25e0c41fa8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
            "patch": "@@ -1469,6 +1469,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mega\", \"MegaForMultipleChoice\"),\n         (\"megatron-bert\", \"MegatronBertForMultipleChoice\"),\n         (\"mobilebert\", \"MobileBertForMultipleChoice\"),\n+        (\"modernbert\", \"ModernBertForMultipleChoice\"),\n         (\"mpnet\", \"MPNetForMultipleChoice\"),\n         (\"mra\", \"MraForMultipleChoice\"),\n         (\"nezha\", \"NezhaForMultipleChoice\"),"
        },
        {
            "sha": "68bafabcbfed67e0b49b5794cc9554321d881410",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 125,
            "deletions": 1,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
            "patch": "@@ -35,6 +35,7 @@\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n     QuestionAnsweringModelOutput,\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n@@ -605,7 +606,12 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(\n             module,\n-            (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n+            (\n+                ModernBertForSequenceClassification,\n+                ModernBertForMultipleChoice,\n+                ModernBertForTokenClassification,\n+                ModernBertForQuestionAnswering,\n+            ),\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n         elif isinstance(module, nn.LayerNorm):\n@@ -1393,11 +1399,129 @@ def forward(\n         )\n \n \n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The ModernBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\n+    \"\"\"\n+)\n+class ModernBertForMultipleChoice(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, 1)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers\n+            perform global attention, while the rest perform local attention. This mask is used to avoid attending to\n+            far-away tokens in the local attention layers when not using Flash Attention.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors.\n+        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n+            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n+        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n+            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.\n+        max_seqlen (`int`, *optional*):\n+            Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.\n+        batch_size (`int`, *optional*):\n+            Batch size of the input sequences. Used to pad the output tensors.\n+        seq_len (`int`, *optional*):\n+            Sequence length of the input sequences including padding tokens. Used to pad the output tensors.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        if self.config.classifier_pooling == \"cls\":\n+            last_hidden_state = last_hidden_state[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        pooled_output = self.head(last_hidden_state)\n+        pooled_output = self.drop(pooled_output)\n+        logits = self.classifier(pooled_output)\n+\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        if not return_dict:\n+            output = (reshaped_logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n __all__ = [\n     \"ModernBertModel\",\n     \"ModernBertPreTrainedModel\",\n     \"ModernBertForMaskedLM\",\n     \"ModernBertForSequenceClassification\",\n     \"ModernBertForTokenClassification\",\n     \"ModernBertForQuestionAnswering\",\n+    \"ModernBertForMultipleChoice\",\n ]"
        },
        {
            "sha": "7e1d5fbfb168339d4c6d718fb8579ace087200c5",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 125,
            "deletions": 1,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_outputs import (\n     BaseModelOutput,\n     MaskedLMOutput,\n+    MultipleChoiceModelOutput,\n     QuestionAnsweringModelOutput,\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n@@ -805,7 +806,12 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(\n             module,\n-            (ModernBertForSequenceClassification, ModernBertForTokenClassification, ModernBertForQuestionAnswering),\n+            (\n+                ModernBertForSequenceClassification,\n+                ModernBertForMultipleChoice,\n+                ModernBertForTokenClassification,\n+                ModernBertForQuestionAnswering,\n+            ),\n         ):\n             init_weight(module.classifier, stds[\"final_out\"])\n         elif isinstance(module, nn.LayerNorm):\n@@ -1521,6 +1527,123 @@ def forward(\n         )\n \n \n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The ModernBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\n+    \"\"\"\n+)\n+class ModernBertForMultipleChoice(ModernBertPreTrainedModel):\n+    def __init__(self, config: ModernBertConfig):\n+        super().__init__(config)\n+        self.config = config\n+\n+        self.model = ModernBertModel(config)\n+        self.head = ModernBertPredictionHead(config)\n+        self.drop = torch.nn.Dropout(config.classifier_dropout)\n+        self.classifier = nn.Linear(config.hidden_size, 1)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        sliding_window_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        indices: Optional[torch.Tensor] = None,\n+        cu_seqlens: Optional[torch.Tensor] = None,\n+        max_seqlen: Optional[int] = None,\n+        batch_size: Optional[int] = None,\n+        seq_len: Optional[int] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **kwargs,\n+    ) -> Union[tuple[torch.Tensor], MultipleChoiceModelOutput]:\n+        r\"\"\"\n+        sliding_window_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding or far-away tokens. In ModernBert, only every few layers\n+            perform global attention, while the rest perform local attention. This mask is used to avoid attending to\n+            far-away tokens in the local attention layers when not using Flash Attention.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the multiple choice classification loss. Indices should be in `[0, ...,\n+            num_choices-1]` where `num_choices` is the size of the second dimension of the input tensors.\n+        indices (`torch.Tensor` of shape `(total_unpadded_tokens,)`, *optional*):\n+            Indices of the non-padding tokens in the input sequence. Used for unpadding the output.\n+        cu_seqlens (`torch.Tensor` of shape `(batch + 1,)`, *optional*):\n+            Cumulative sequence lengths of the input sequences. Used to index the unpadded tensors.\n+        max_seqlen (`int`, *optional*):\n+            Maximum sequence length in the batch excluding padding tokens. Used to unpad input_ids and pad output tensors.\n+        batch_size (`int`, *optional*):\n+            Batch size of the input sequences. Used to pad the output tensors.\n+        seq_len (`int`, *optional*):\n+            Sequence length of the input sequences including padding tokens. Used to pad the output tensors.\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        num_choices = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n+\n+        input_ids = input_ids.view(-1, input_ids.size(-1)) if input_ids is not None else None\n+        attention_mask = attention_mask.view(-1, attention_mask.size(-1)) if attention_mask is not None else None\n+        position_ids = position_ids.view(-1, position_ids.size(-1)) if position_ids is not None else None\n+        inputs_embeds = (\n+            inputs_embeds.view(-1, inputs_embeds.size(-2), inputs_embeds.size(-1))\n+            if inputs_embeds is not None\n+            else None\n+        )\n+\n+        self._maybe_set_compile()\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            sliding_window_mask=sliding_window_mask,\n+            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n+            indices=indices,\n+            cu_seqlens=cu_seqlens,\n+            max_seqlen=max_seqlen,\n+            batch_size=batch_size,\n+            seq_len=seq_len,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        last_hidden_state = outputs[0]\n+\n+        if self.config.classifier_pooling == \"cls\":\n+            last_hidden_state = last_hidden_state[:, 0]\n+        elif self.config.classifier_pooling == \"mean\":\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n+                dim=1, keepdim=True\n+            )\n+\n+        pooled_output = self.head(last_hidden_state)\n+        pooled_output = self.drop(pooled_output)\n+        logits = self.classifier(pooled_output)\n+\n+        reshaped_logits = logits.view(-1, num_choices)\n+\n+        loss = None\n+        if labels is not None:\n+            loss_fct = nn.CrossEntropyLoss()\n+            loss = loss_fct(reshaped_logits, labels)\n+\n+        if not return_dict:\n+            output = (reshaped_logits,) + outputs[1:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return MultipleChoiceModelOutput(\n+            loss=loss,\n+            logits=reshaped_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n __all__ = [\n     \"ModernBertConfig\",\n     \"ModernBertModel\",\n@@ -1529,4 +1652,5 @@ def forward(\n     \"ModernBertForSequenceClassification\",\n     \"ModernBertForTokenClassification\",\n     \"ModernBertForQuestionAnswering\",\n+    \"ModernBertForMultipleChoice\",\n ]"
        },
        {
            "sha": "94445d46f9283fa177dd5d72b2e86f504041c2e8",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 23,
            "deletions": 0,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0bd91cc822816c67eb9b7afbaf84ecac25d6c622/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=0bd91cc822816c67eb9b7afbaf84ecac25d6c622",
            "patch": "@@ -41,6 +41,7 @@\n     from transformers import (\n         MODEL_FOR_PRETRAINING_MAPPING,\n         ModernBertForMaskedLM,\n+        ModernBertForMultipleChoice,\n         ModernBertForQuestionAnswering,\n         ModernBertForSequenceClassification,\n         ModernBertForTokenClassification,\n@@ -202,6 +203,22 @@ def create_and_check_for_token_classification(\n         result = model(input_ids, attention_mask=input_mask, labels=token_labels)\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n \n+    def create_and_check_for_multiple_choice(\n+        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n+    ):\n+        config.num_labels = self.num_labels\n+        model = ModernBertForMultipleChoice(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        multiple_choice_inputs_ids = input_ids.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n+        multiple_choice_input_mask = input_mask.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n+        result = model(\n+            multiple_choice_inputs_ids,\n+            attention_mask=multiple_choice_input_mask,\n+            labels=choice_labels,\n+        )\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n+\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         (\n@@ -227,6 +244,7 @@ class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n             ModernBertForSequenceClassification,\n             ModernBertForTokenClassification,\n             ModernBertForQuestionAnswering,\n+            ModernBertForMultipleChoice,\n         )\n         if is_torch_available()\n         else ()\n@@ -298,6 +316,7 @@ def test_initialization(self):\n                         ModernBertForSequenceClassification,\n                         ModernBertForTokenClassification,\n                         ModernBertForQuestionAnswering,\n+                        ModernBertForMultipleChoice,\n                     ]\n                 ):\n                     self.assertIn(\n@@ -318,6 +337,10 @@ def test_for_token_classification(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n \n+    def test_for_multiple_choice(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n+\n     def test_for_warning_if_padding_and_no_attention_mask(self):\n         (\n             config,"
        }
    ],
    "stats": {
        "total": 281,
        "additions": 279,
        "deletions": 2
    }
}