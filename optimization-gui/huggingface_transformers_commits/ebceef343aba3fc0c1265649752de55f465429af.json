{
    "author": "ivarflakstad",
    "message": "Collated reports (#40080)\n\n* Add initial collated reports script and job definition\n\n* provide commit hash for this run. Also use hash in generated artifact name. Json formatting\n\n* tidy\n\n* Add option to upload collated reports to hf hub\n\n* Add glob pattern for test report folders\n\n* Fix glob\n\n* Use machine_type as path filter instead of glob. Include machine_type in collated report",
    "sha": "ebceef343aba3fc0c1265649752de55f465429af",
    "files": [
        {
            "sha": "42bb89e460992172eef21ca6efc98cef82c9e093",
            "filename": ".github/workflows/collated-reports.yml",
            "status": "added",
            "additions": 49,
            "deletions": 0,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebceef343aba3fc0c1265649752de55f465429af/.github%2Fworkflows%2Fcollated-reports.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebceef343aba3fc0c1265649752de55f465429af/.github%2Fworkflows%2Fcollated-reports.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fcollated-reports.yml?ref=ebceef343aba3fc0c1265649752de55f465429af",
            "patch": "@@ -0,0 +1,49 @@\n+name: CI collated reports\n+\n+on:\n+  workflow_call:\n+    inputs:\n+      job:\n+        required: true\n+        type: string\n+      report_repo_id:\n+        required: true\n+        type: string\n+      machine_type:\n+        required: true\n+        type: string\n+      gpu_name:\n+        description: Name of the GPU used for the job. Its enough that the value contains the name of the GPU, e.g. \"noise-h100-more-noise\". Case insensitive.\n+        required: true\n+        type: string\n+\n+jobs:\n+  collated_reports:\n+    name: Collated reports\n+    runs-on: ubuntu-22.04\n+    if: always()\n+    steps:\n+      - uses: actions/checkout@v4\n+      - uses: actions/download-artifact@v4\n+\n+      - name: Collated reports\n+        shell: bash\n+        env:\n+          ACCESS_REPO_INFO_TOKEN: ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n+          CI_SHA: ${{ github.sha }}\n+          TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN: ${{ secrets.TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN }}\n+        run: |\n+          pip install huggingface_hub\n+          python3 utils/collated_reports.py                  \\\n+            --path /transformers/reports/                    \\\n+            --machine-type ${{ inputs.machine_type }}        \\\n+            --commit-hash ${{ env.CI_SHA }}                  \\\n+            --job ${{ inputs.job }}                          \\\n+            --report-repo-id ${{ inputs.report_repo_id }}    \\\n+            --gpu-name ${{ inputs.gpu_name }}\n+\n+      - name: Upload collated reports\n+        uses: actions/upload-artifact@v4\n+        with:\n+          name: collated_reports_${{ env.CI_SHA }}.json\n+          path: collated_reports_${{ env.CI_SHA }}.json"
        },
        {
            "sha": "566cc05db7c11633ec841223a388e91304b3eb72",
            "filename": "utils/collated_reports.py",
            "status": "added",
            "additions": 219,
            "deletions": 0,
            "changes": 219,
            "blob_url": "https://github.com/huggingface/transformers/blob/ebceef343aba3fc0c1265649752de55f465429af/utils%2Fcollated_reports.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ebceef343aba3fc0c1265649752de55f465429af/utils%2Fcollated_reports.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcollated_reports.py?ref=ebceef343aba3fc0c1265649752de55f465429af",
            "patch": "@@ -0,0 +1,219 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import json\n+import subprocess\n+from dataclasses import dataclass\n+from pathlib import Path\n+\n+\n+DEFAULT_GPU_NAMES = [\"mi300\", \"mi325\", \"mi355\", \"h100\", \"a10\"]\n+\n+\n+def simplify_gpu_name(gpu_name: str, simplified_names: list[str]) -> str:\n+    matches = []\n+    for simplified_name in simplified_names:\n+        if simplified_name in gpu_name:\n+            matches.append(simplified_name)\n+    if len(matches) == 1:\n+        return matches[0]\n+    return gpu_name\n+\n+\n+def parse_short_summary_line(line: str) -> tuple[str | None, int]:\n+    if line.startswith(\"PASSED\"):\n+        return \"passed\", 1\n+    if line.startswith(\"FAILED\"):\n+        return \"failed\", 1\n+    if line.startswith(\"SKIPPED\"):\n+        line = line.split(\"[\", maxsplit=1)[1]\n+        line = line.split(\"]\", maxsplit=1)[0]\n+        return \"skipped\", int(line)\n+    if line.startswith(\"ERROR\"):\n+        return \"error\", 1\n+    return None, 0\n+\n+\n+def validate_path(p: str) -> Path:\n+    # Validate path and apply glob pattern if provided\n+    path = Path(p)\n+    assert path.is_dir(), f\"Path {path} is not a directory\"\n+    return path\n+\n+\n+def get_gpu_name(gpu_name: str | None) -> str:\n+    # Get GPU name if available\n+    if gpu_name is None:\n+        try:\n+            import torch\n+\n+            gpu_name = torch.cuda.get_device_name()\n+        except Exception as e:\n+            print(f\"Failed to get GPU name with {e}\")\n+            gpu_name = \"unknown\"\n+    else:\n+        gpu_name = gpu_name.replace(\" \", \"_\").lower()\n+        gpu_name = simplify_gpu_name(gpu_name, DEFAULT_GPU_NAMES)\n+\n+    return gpu_name\n+\n+\n+def get_commit_hash(commit_hash: str | None) -> str:\n+    # Get commit hash if available\n+    if commit_hash is None:\n+        try:\n+            commit_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"utf-8\").strip()\n+        except Exception as e:\n+            print(f\"Failed to get commit hash with {e}\")\n+            commit_hash = \"unknown\"\n+\n+    return commit_hash[:7]\n+\n+\n+@dataclass\n+class Args:\n+    path: Path\n+    machine_type: str\n+    gpu_name: str\n+    commit_hash: str\n+    job: str | None\n+    report_repo_id: str | None\n+\n+\n+def get_arguments(args: argparse.Namespace) -> Args:\n+    path = validate_path(args.path)\n+    machine_type = args.machine_type\n+    gpu_name = get_gpu_name(args.gpu_name)\n+    commit_hash = get_commit_hash(args.commit_hash)\n+    job = args.job\n+    report_repo_id = args.report_repo_id\n+    return Args(path, machine_type, gpu_name, commit_hash, job, report_repo_id)\n+\n+\n+def upload_collated_report(job: str, report_repo_id: str, filename: str):\n+    # Alternatively we can check for the existence of the collated_reports file and upload in notification_service.py\n+    import os\n+\n+    from get_previous_daily_ci import get_last_daily_ci_run\n+    from huggingface_hub import HfApi\n+\n+    api = HfApi()\n+\n+    # if it is not a scheduled run, upload the reports to a subfolder under `report_repo_folder`\n+    report_repo_subfolder = \"\"\n+    if os.getenv(\"GITHUB_EVENT_NAME\") != \"schedule\":\n+        report_repo_subfolder = f\"{os.getenv('GITHUB_RUN_NUMBER')}-{os.getenv('GITHUB_RUN_ID')}\"\n+        report_repo_subfolder = f\"runs/{report_repo_subfolder}\"\n+\n+    workflow_run = get_last_daily_ci_run(\n+        token=os.environ[\"ACCESS_REPO_INFO_TOKEN\"], workflow_run_id=os.getenv(\"GITHUB_RUN_ID\")\n+    )\n+    workflow_run_created_time = workflow_run[\"created_at\"]\n+    report_repo_folder = workflow_run_created_time.split(\"T\")[0]\n+\n+    if report_repo_subfolder:\n+        report_repo_folder = f\"{report_repo_folder}/{report_repo_subfolder}\"\n+\n+    api.upload_file(\n+        path_or_fileobj=f\"ci_results_{job}/{filename}\",\n+        path_in_repo=f\"{report_repo_folder}/ci_results_{job}/{filename}\",\n+        repo_id=report_repo_id,\n+        repo_type=\"dataset\",\n+        token=os.getenv(\"TRANSFORMERS_CI_RESULTS_UPLOAD_TOKEN\"),\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser(description=\"Post process models test reports.\")\n+    parser.add_argument(\"--path\", \"-p\", help=\"Path to the reports folder\")\n+    parser.add_argument(\n+        \"--machine-type\", \"-m\", help=\"Process single or multi GPU results\", choices=[\"single-gpu\", \"multi-gpu\"]\n+    )\n+    parser.add_argument(\"--gpu-name\", \"-g\", help=\"GPU name\", default=None)\n+    parser.add_argument(\"--commit-hash\", \"-c\", help=\"Commit hash\", default=None)\n+    parser.add_argument(\"--job\", \"-j\", help=\"Optional job name required for uploading reports\", default=None)\n+    parser.add_argument(\n+        \"--report-repo-id\", \"-r\", help=\"Optional report repository ID required for uploading reports\", default=None\n+    )\n+    args = get_arguments(parser.parse_args())\n+\n+    # Initialize accumulators for collated report\n+    total_status_count = {\n+        \"passed\": 0,\n+        \"failed\": 0,\n+        \"skipped\": 0,\n+        \"error\": 0,\n+        None: 0,\n+    }\n+    collated_report_buffer = []\n+\n+    path = args.path\n+    machine_type = args.machine_type\n+    gpu_name = args.gpu_name\n+    commit_hash = args.commit_hash\n+    job = args.job\n+    report_repo_id = args.report_repo_id\n+\n+    # Find the origin directory based on machine type\n+    origin = path\n+    for p in path.iterdir():\n+        if machine_type in p.name:\n+            origin = p\n+            break\n+\n+    # Loop through model directories and create collated reports\n+    for model_dir in sorted(origin.iterdir()):\n+        # Create a new entry for the model\n+        model_name = model_dir.name.removesuffix(\"_test_reports\")\n+        report = {\"model\": model_name, \"results\": []}\n+        results = []\n+\n+        # Read short summary\n+        with open(model_dir / \"summary_short.txt\", \"r\") as f:\n+            short_summary_lines = f.readlines()\n+\n+        # Parse short summary\n+        for line in short_summary_lines[1:]:\n+            status, count = parse_short_summary_line(line)\n+            total_status_count[status] += count\n+            if status:\n+                result = {\n+                    \"status\": status,\n+                    \"test\": line.split(status.upper(), maxsplit=1)[1].strip(),\n+                    \"count\": count,\n+                }\n+                results.append(result)\n+\n+        # Add short summaries to report\n+        report[\"results\"] = results\n+\n+        collated_report_buffer.append(report)\n+\n+    # Write collated report\n+    with open(f\"collated_reports_{commit_hash}.json\", \"w\") as f:\n+        json.dump(\n+            {\n+                \"gpu_name\": gpu_name,\n+                \"machine_type\": machine_type,\n+                \"commit_hash\": commit_hash,\n+                \"total_status_count\": total_status_count,\n+                \"results\": collated_report_buffer,\n+            },\n+            f,\n+            indent=2,\n+        )\n+\n+    if job and report_repo_id:\n+        upload_collated_report(job, report_repo_id, f\"collated_reports_{commit_hash}.json\")"
        }
    ],
    "stats": {
        "total": 268,
        "additions": 268,
        "deletions": 0
    }
}