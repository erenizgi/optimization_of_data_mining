{
    "author": "DuyguA",
    "message": "Fix for slow the bug tokenizer adding spaces to single id decodes (#32564)\n\n* _decode signature change and quick return\r\n\r\n* added bunch of decoding tests\r\n\r\n* signature match and return\r\n\r\n* added tests for decoding\r\n\r\n* merged decoding test\r\n\r\n* more tests for special tokens\r\n\r\n* cosmetics\r\n\r\n* fixed param\r\n\r\n* ruffed the file\r\n\r\n* refinement for single special tokens\r\n\r\n* added test for single special tokens\r\n\r\n* slight change to test name\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\r\n\r\n* minor change test name for skip tokens\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\r\n\r\n* killed already defined var\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\r\n\r\n* minor update with vars\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\r\n\r\n* killed already defined var once more\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>",
    "sha": "52e22cbf677f58a1cca347117e8974fa50c9f2d7",
    "files": [
        {
            "sha": "df13a029a6c6ff1a1c791bbe64cdcb19a4f73fb9",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e22cbf677f58a1cca347117e8974fa50c9f2d7/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e22cbf677f58a1cca347117e8974fa50c9f2d7/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=52e22cbf677f58a1cca347117e8974fa50c9f2d7",
            "patch": "@@ -1077,7 +1077,7 @@ def convert_tokens_to_string(self, tokens: List[str]) -> str:\n \n     def _decode(\n         self,\n-        token_ids: List[int],\n+        token_ids: Union[int, List[int]],\n         skip_special_tokens: bool = False,\n         clean_up_tokenization_spaces: bool = None,\n         spaces_between_special_tokens: bool = True,\n@@ -1086,6 +1086,10 @@ def _decode(\n         self._decode_use_source_tokenizer = kwargs.pop(\"use_source_tokenizer\", False)\n \n         filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n+        # If given is a single id, prevents splitting the string in upcoming loop\n+        if isinstance(filtered_tokens, str):\n+            filtered_tokens = [filtered_tokens]\n+\n         legacy_added_tokens = set(self._added_tokens_encoder.keys()) - set(self.all_special_tokens) | {\n             token for token in self.additional_special_tokens if self.convert_tokens_to_ids(token) >= self.vocab_size\n         }\n@@ -1096,7 +1100,7 @@ def _decode(\n         current_sub_text = []\n         # TODO @ArthurZ in version 5, special tokens should be handled in convert_tokens_to_string, while _convert_tokens_to_string\n         for token in filtered_tokens:\n-            if skip_special_tokens and token in self.all_special_ids:\n+            if skip_special_tokens and token in self.all_special_tokens:\n                 continue\n             if token in legacy_added_tokens:\n                 if current_sub_text:"
        },
        {
            "sha": "2c8f71ba97724633fc65a54f4078ba6100b9aca3",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/52e22cbf677f58a1cca347117e8974fa50c9f2d7/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52e22cbf677f58a1cca347117e8974fa50c9f2d7/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=52e22cbf677f58a1cca347117e8974fa50c9f2d7",
            "patch": "@@ -253,6 +253,71 @@ def test_padding_accepts_tensors(self):\n         self.assertTrue(isinstance(batch[\"input_ids\"], np.ndarray))\n         self.assertEqual(batch[\"input_ids\"].tolist(), [[0, 1, 2, tokenizer.pad_token_id], [0, 1, 2, 3]])\n \n+    @require_tokenizers\n+    def test_decoding_single_token(self):\n+        for tokenizer_class in [BertTokenizer, BertTokenizerFast]:\n+            with self.subTest(f\"{tokenizer_class}\"):\n+                tokenizer = tokenizer_class.from_pretrained(\"google-bert/bert-base-cased\")\n+\n+                token_id = 2300\n+                decoded_flat = tokenizer.decode(token_id)\n+                decoded_list = tokenizer.decode([token_id])\n+\n+                self.assertEqual(decoded_flat, \"Force\")\n+                self.assertEqual(decoded_list, \"Force\")\n+\n+                token_id = 0\n+                decoded_flat = tokenizer.decode(token_id)\n+                decoded_list = tokenizer.decode([token_id])\n+\n+                self.assertEqual(decoded_flat, \"[PAD]\")\n+                self.assertEqual(decoded_list, \"[PAD]\")\n+\n+                last_item_id = tokenizer.vocab_size - 1\n+                decoded_flat = tokenizer.decode(last_item_id)\n+                decoded_list = tokenizer.decode([last_item_id])\n+\n+                self.assertEqual(decoded_flat, \"##：\")\n+                self.assertEqual(decoded_list, \"##：\")\n+\n+    @require_tokenizers\n+    def test_decoding_skip_special_tokens(self):\n+        for tokenizer_class in [BertTokenizer, BertTokenizerFast]:\n+            with self.subTest(f\"{tokenizer_class}\"):\n+                tokenizer = tokenizer_class.from_pretrained(\"google-bert/bert-base-cased\")\n+                tokenizer.add_tokens([\"ஐ\"], special_tokens=True)\n+\n+                # test special token with other tokens, skip the special tokens\n+                sentence = \"This is a beautiful flower ஐ\"\n+                ids = tokenizer(sentence)[\"input_ids\"]\n+                decoded_sent = tokenizer.decode(ids, skip_special_tokens=True)\n+                self.assertEqual(decoded_sent, \"This is a beautiful flower\")\n+\n+                # test special token with other tokens, do not skip the special tokens\n+                ids = tokenizer(sentence)[\"input_ids\"]\n+                decoded_sent = tokenizer.decode(ids, skip_special_tokens=False)\n+                self.assertEqual(decoded_sent, \"[CLS] This is a beautiful flower ஐ [SEP]\")\n+\n+                # test special token stand alone, skip the special tokens\n+                sentence = \"ஐ\"\n+                ids = tokenizer(sentence)[\"input_ids\"]\n+                decoded_sent = tokenizer.decode(ids, skip_special_tokens=True)\n+                self.assertEqual(decoded_sent, \"\")\n+\n+                # test special token stand alone, do not skip the special tokens\n+                ids = tokenizer(sentence)[\"input_ids\"]\n+                decoded_sent = tokenizer.decode(ids, skip_special_tokens=False)\n+                self.assertEqual(decoded_sent, \"[CLS] ஐ [SEP]\")\n+\n+                # test single special token alone, skip\n+                pad_id = 0\n+                decoded_sent = tokenizer.decode(pad_id, skip_special_tokens=True)\n+                self.assertEqual(decoded_sent, \"\")\n+\n+                # test single special token alone, do not skip\n+                decoded_sent = tokenizer.decode(pad_id, skip_special_tokens=False)\n+                self.assertEqual(decoded_sent, \"[PAD]\")\n+\n     @require_torch\n     def test_padding_accepts_tensors_pt(self):\n         import torch"
        }
    ],
    "stats": {
        "total": 73,
        "additions": 71,
        "deletions": 2
    }
}