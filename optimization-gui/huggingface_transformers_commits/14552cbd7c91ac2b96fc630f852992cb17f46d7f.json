{
    "author": "zucchini-nlp",
    "message": "VLMs: even more clean-up (#36249)\n\n* squash\n\n* style",
    "sha": "14552cbd7c91ac2b96fc630f852992cb17f46d7f",
    "files": [
        {
            "sha": "538a9aaf7fcbfeb4d9472864b7a1b9a83ffc6b2a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 45,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1651,54 +1651,23 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n-        if past_key_values is not None:\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                if inputs_embeds is not None and input_ids.shape[1] == 0:\n-                    position_ids = position_ids[:, -inputs_embeds.shape[1] :]\n-                else:\n-                    position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n \n-        if cache_position[0] == 0:\n+        if cache_position[0] != 0:\n             # If we're in cached decoding stage, pixel values should be `None` because input ids do not contain special image token anymore\n             # Otherwise we need pixel values to be passed to model\n-            model_inputs[\"pixel_values\"] = pixel_values\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n+            model_inputs[\"pixel_values\"] = None\n+\n         return model_inputs\n \n "
        },
        {
            "sha": "b3331cf1293a17d06683ea4bfcecb20f526f3471",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1967,5 +1967,36 @@ def forward(\n \n         return outputs\n \n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+\n+        return model_inputs\n+\n \n __all__ = [\"Emu3ForConditionalGeneration\", \"Emu3ForCausalLM\", \"Emu3TextModel\", \"Emu3PreTrainedModel\", \"Emu3VQVAE\"]"
        },
        {
            "sha": "cdb4ee5d6fa9c884d6860b18dccfcf76f144eb39",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1275,6 +1275,37 @@ def forward(\n \n         return outputs\n \n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        pixel_values=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+\n+        return model_inputs\n+\n \n __all__ = [\n     \"Emu3ForConditionalGeneration\","
        },
        {
            "sha": "3f6d31aab3886880bad27f4225ab08ba40919314",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 13,
            "deletions": 29,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -345,36 +345,20 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        if past_key_values is not None:\n-            input_ids = input_ids[:, -1:]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -1:]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_key_values is None:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids}\n-\n-        if image_patches_indices is not None:\n-            model_inputs[\"image_patches_indices\"] = image_patches_indices\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"image_patches_indices\": image_patches_indices if past_key_values is None else None,\n-                \"image_patches\": image_patches if past_key_values is None else None,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            image_patches=image_patches,\n+            image_patches_indices=image_patches_indices,\n+            **kwargs,\n         )\n+\n+        if past_key_values is not None:\n+            model_inputs[\"image_patches_indices\"] = None\n+            model_inputs[\"image_patches\"] = None\n+\n         return model_inputs\n \n     @staticmethod"
        },
        {
            "sha": "7a476a13103c16817a79250addfca85b0199cf09",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 20,
            "deletions": 50,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1667,63 +1667,33 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- custom processing based on `config.use_resampler`\n \n-        model_inputs = {}\n+        images_kwargs = {}\n         if image_hidden_states is not None:\n             if self.config.use_resampler:\n-                model_inputs[\"perceiver_embeddings\"] = image_hidden_states\n+                images_kwargs[\"perceiver_embeddings\"] = image_hidden_states\n             else:\n-                model_inputs[\"image_encoder_embeddings\"] = image_hidden_states\n+                images_kwargs[\"image_encoder_embeddings\"] = image_hidden_states\n         else:\n-            model_inputs[\"pixel_values\"] = pixel_values\n-\n-        # If we have cache: let's slice `input_ids` or `input embeds` through `cache_position`, to keep only the unprocessed tokens\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:\n-                if input_ids.shape[1] == 0:\n-                    inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-                else:\n-                    input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:\n-                input_ids = input_ids[:, cache_position]\n-                if image_attention_mask is not None:\n-                    image_attention_mask = image_attention_mask[:, -input_ids.shape[1] :]\n+            images_kwargs[\"pixel_values\"] = pixel_values\n+        images_kwargs[\"interpolate_pos_encoding\"] = kwargs.pop(\"interpolate_pos_encoding\", False)\n \n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-\n-            # If past_key_values are present then slice the postion ids for only only the unprocessed tokens.\n-            if past_key_values:\n-                if inputs_embeds is not None and input_ids.shape[1] == 0:\n-                    position_ids = position_ids[:, -inputs_embeds.shape[1] :]\n-                else:\n-                    position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs.update({\"inputs_embeds\": inputs_embeds, \"input_ids\": None})\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs.update(\n-                {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-            )\n-\n-        model_inputs.update(\n-            {\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"cache_position\": cache_position,\n-                \"position_ids\": position_ids,\n-                \"attention_mask\": attention_mask,\n-                \"image_attention_mask\": image_attention_mask,\n-                \"interpolate_pos_encoding\": kwargs.get(\"interpolate_pos_encoding\", False),\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            image_attention_mask=image_attention_mask,\n+            **images_kwargs,\n+            **kwargs,\n         )\n \n+        if image_attention_mask is not None and inputs_embeds is None:\n+            seq_length = model_inputs[\"input_ids\"].shape[1]\n+            model_inputs[\"image_attention_mask\"] = image_attention_mask[:, -seq_length:]\n+\n         return model_inputs\n \n     def _update_model_kwargs_for_generation("
        },
        {
            "sha": "2c44f998574d93c0ad705da014c39dd494d343a2",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 38,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1226,6 +1226,10 @@ def forward(self, image_hidden_states, attention_mask):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n@@ -1334,6 +1338,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Idefics2BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1443,6 +1448,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             return_dict=return_dict,\n         )\n \n@@ -1527,6 +1533,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics2CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1603,6 +1610,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             return_dict=return_dict,\n         )\n \n@@ -1659,49 +1667,28 @@ def prepare_inputs_for_generation(\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n         # precedence is moved to the model, we can remove this fn)\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:\n-                input_ids = input_ids[:, cache_position]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            pixel_values=pixel_values,\n+            pixel_attention_mask=pixel_attention_mask,\n+            image_hidden_states=image_hidden_states,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires noth ids and embeds to be present\n+        # but IDEFICS requires both ids and embeds to be present\n         if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": input_ids}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n+            model_inputs[\"input_ids\"] = input_ids\n \n         if image_hidden_states is not None:\n-            pixel_values = None\n-            pixel_attention_mask = None\n-        else:\n-            pixel_values = pixel_values\n-            pixel_attention_mask = pixel_attention_mask\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_attention_mask\": pixel_attention_mask,\n-                \"image_hidden_states\": image_hidden_states,\n-            }\n-        )\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_attention_mask\"] = None\n+\n         return model_inputs\n \n     def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):"
        },
        {
            "sha": "251e11067fffaed54863cb855eb792e95ff2a37a",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 36,
            "deletions": 39,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -812,6 +812,10 @@ def forward(\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n@@ -928,6 +932,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, Idefics3BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n@@ -1024,6 +1029,7 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             return_dict=return_dict,\n         )\n \n@@ -1110,7 +1116,9 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, Idefics3CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1119,6 +1127,13 @@ def forward(\n                 config.vocab_size]` or `model.image_token_id` (where `model` is your instance of `Idefics3ForConditionalGeneration`).\n                 Tokens with indices set to `model.image_token_id` are ignored (masked), the loss is only\n                 computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n         Returns:\n \n         Example:\n@@ -1193,11 +1208,14 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             return_dict=return_dict,\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1248,49 +1266,28 @@ def prepare_inputs_for_generation(\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n         # precedence is moved to the model, we can remove this fn)\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:\n-                input_ids = input_ids[:, cache_position]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            pixel_values=pixel_values,\n+            pixel_attention_mask=pixel_attention_mask,\n+            image_hidden_states=image_hidden_states,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires noth ids and embeds to be present\n+        # but IDEFICS requires both ids and embeds to be present\n         if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": input_ids}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n+            model_inputs[\"input_ids\"] = input_ids\n \n         if image_hidden_states is not None:\n-            pixel_values = None\n-            pixel_attention_mask = None\n-        else:\n-            pixel_values = pixel_values\n-            pixel_attention_mask = pixel_attention_mask\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_attention_mask\": pixel_attention_mask,\n-                \"image_hidden_states\": image_hidden_states,\n-            }\n-        )\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_attention_mask\"] = None\n+\n         return model_inputs\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration._update_model_kwargs_for_generation"
        },
        {
            "sha": "13c0273b1724d65d7ba05e3885d342fbc95c30c7",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 29,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -1699,31 +1699,18 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        input_shape = input_ids.shape\n-        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n-        if attention_mask is None:\n-            attention_mask = input_ids.new_ones(input_shape)\n-\n-        position_ids = None\n-        if cache_position is None:\n-            past_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n-            cache_position = torch.arange(past_length, input_ids.shape[1], dtype=torch.long, device=input_ids.device)\n+        # Kosmos2 has offset for position ids, so we need to create them correctly\n+        position_ids = create_position_ids_from_input_ids(\n+            input_ids,\n+            padding_idx=self.config.pad_token_id,\n+            past_key_values_length=0,\n+        )\n \n         if past_key_values is not None:\n-            position_ids = create_position_ids_from_input_ids(\n-                input_ids,\n-                padding_idx=self.config.pad_token_id,\n-                past_key_values_length=0,\n-            )\n-\n-            if input_ids.shape[1] != cache_position.shape[0]:\n-                input_ids = input_ids[:, cache_position]\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n             image_embeds = None\n             image_embeds_position_mask = None\n+        # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n         elif image_embeds_position_mask is not None:\n-            # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n             batch_size, seq_len = input_ids.size()\n             mask_len = image_embeds_position_mask.size()[-1]\n             image_embeds_position_mask = torch.cat(\n@@ -1734,15 +1721,19 @@ def prepare_inputs_for_generation(\n                 dim=1,\n             )\n \n-        return {\n-            \"input_ids\": input_ids,\n-            \"image_embeds\": image_embeds,\n-            \"image_embeds_position_mask\": image_embeds_position_mask,\n-            \"past_key_values\": past_key_values,\n-            \"attention_mask\": attention_mask,\n-            \"position_ids\": position_ids,\n-            \"use_cache\": use_cache,\n-        }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            image_embeds=image_embeds,\n+            image_embeds_position_mask=image_embeds_position_mask,\n+            use_cache=use_cache,\n+            position_ids=position_ids,\n+            cache_position=cache_position,\n+            **model_kwargs,\n+        )\n+\n+        return model_inputs\n \n     @staticmethod\n     # Copied from transformers.models.umt5.modeling_umt5.UMT5ForConditionalGeneration._reorder_cache"
        },
        {
            "sha": "e392634e59727ee299724ce953b41a73cf6afaad",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 20,
            "deletions": 60,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -45,7 +45,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1922,68 +1921,29 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n-        if past_key_values is not None:\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if cache_position[0] != 0:\n-            pixel_values = None\n-            pixel_values_videos = None\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            second_per_grid_ts=second_per_grid_ts,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n \n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = inputs_embeds.shape\n-                device = inputs_embeds.device\n-            else:\n-                batch_size, sequence_length = input_ids.shape\n-                device = input_ids.device\n+        # Qwen2-5-VL position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n \n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n \n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_values_videos\": pixel_values_videos,\n-                \"image_grid_thw\": image_grid_thw,\n-                \"video_grid_thw\": video_grid_thw,\n-                \"cache_position\": cache_position,\n-                \"second_per_grid_ts\": second_per_grid_ts,\n-            }\n-        )\n         return model_inputs\n \n     def _get_image_nums_and_video_nums("
        },
        {
            "sha": "2d8695b5a407de033fe687f0b6a9ce6744c9fbcb",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 21,
            "deletions": 61,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -44,13 +44,12 @@\n from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLProcessor\n \n from ...activations import ACT2FN\n-from ...cache_utils import StaticCache\n from ...configuration_utils import PretrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, VideoInput\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_flash_attn_2_available, is_torchdynamo_compiling, logging\n+from ...utils import is_flash_attn_2_available, logging\n \n \n if is_flash_attn_2_available():\n@@ -788,68 +787,29 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n-        if past_key_values is not None:\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if cache_position[0] != 0:\n-            pixel_values = None\n-            pixel_values_videos = None\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            second_per_grid_ts=second_per_grid_ts,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n \n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n+        # Qwen2-5-VL position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n \n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = inputs_embeds.shape\n-                device = inputs_embeds.device\n-            else:\n-                batch_size, sequence_length = input_ids.shape\n-                device = input_ids.device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n \n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_values_videos\": pixel_values_videos,\n-                \"image_grid_thw\": image_grid_thw,\n-                \"video_grid_thw\": video_grid_thw,\n-                \"cache_position\": cache_position,\n-                \"second_per_grid_ts\": second_per_grid_ts,\n-            }\n-        )\n         return model_inputs\n \n "
        },
        {
            "sha": "fd494f04782fe3d1dea9345e79dc895597553d06",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 19,
            "deletions": 59,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -41,7 +41,6 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n-    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -1791,67 +1790,28 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n-        if past_key_values is not None:\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        if cache_position[0] != 0:\n-            pixel_values = None\n-            pixel_values_videos = None\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n \n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = inputs_embeds.shape\n-                device = inputs_embeds.device\n-            else:\n-                batch_size, sequence_length = input_ids.shape\n-                device = input_ids.device\n+        # Qwen2-VL position_ids are prepareed with rope_deltas in forward\n+        model_inputs[\"position_ids\"] = None\n \n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n+        if model_inputs[\"cache_position\"][0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_values_videos\"] = None\n \n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_values_videos\": pixel_values_videos,\n-                \"image_grid_thw\": image_grid_thw,\n-                \"video_grid_thw\": video_grid_thw,\n-                \"cache_position\": cache_position,\n-            }\n-        )\n         return model_inputs\n \n     def _get_image_nums_and_video_nums("
        },
        {
            "sha": "c34aaaa62c995c08083843efda2386947d0084bb",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 27,
            "deletions": 39,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/14552cbd7c91ac2b96fc630f852992cb17f46d7f/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=14552cbd7c91ac2b96fc630f852992cb17f46d7f",
            "patch": "@@ -733,6 +733,10 @@ def forward(self, image_hidden_states):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n@@ -1083,7 +1087,9 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[Tuple, SmolVLMCausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1151,11 +1157,14 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n             return_dict=return_dict,\n         )\n \n         hidden_states = outputs[0]\n-        logits = self.lm_head(hidden_states)\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1205,49 +1214,28 @@ def prepare_inputs_for_generation(\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n         # precedence is moved to the model, we can remove this fn)\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:\n-                input_ids = input_ids[:, cache_position]\n-\n-        position_ids = kwargs.get(\"position_ids\", None)\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            pixel_values=pixel_values,\n+            pixel_attention_mask=pixel_attention_mask,\n+            image_hidden_states=image_hidden_states,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires noth ids and embeds to be present\n+        # but IDEFICS requires both ids and embeds to be present\n         if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": input_ids}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n+            model_inputs[\"input_ids\"] = input_ids\n \n         if image_hidden_states is not None:\n-            pixel_values = None\n-            pixel_attention_mask = None\n-        else:\n-            pixel_values = pixel_values\n-            pixel_attention_mask = pixel_attention_mask\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": kwargs.get(\"use_cache\"),\n-                \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"pixel_attention_mask\": pixel_attention_mask,\n-                \"image_hidden_states\": image_hidden_states,\n-            }\n-        )\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"pixel_attention_mask\"] = None\n+\n         return model_inputs\n \n     def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):"
        }
    ],
    "stats": {
        "total": 726,
        "additions": 277,
        "deletions": 449
    }
}