{
    "author": "Cyrilvallez",
    "message": "Do not use accelerate hooks if the device_map has only 1 device (#43019)\n\n* unique device\n\n* comment\n\n* fix\n\n* simplify and add mismatched\n\n* style",
    "sha": "315dcbe45cee1489a32fc228a80502b0a150936c",
    "files": [
        {
            "sha": "b50bbabf76e867ef19a2728ba857652365aa496a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 46,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/315dcbe45cee1489a32fc228a80502b0a150936c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/315dcbe45cee1489a32fc228a80502b0a150936c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=315dcbe45cee1489a32fc228a80502b0a150936c",
            "patch": "@@ -3963,7 +3963,7 @@ def from_pretrained(\n             weight_mapping=weight_conversions,\n         )\n \n-        model.eval()  # Set model in evaluation mode to deactivate DropOut modules by default\n+        model.eval()  # Set model in evaluation mode to deactivate Dropout modules by default\n         model.set_use_kernels(use_kernels, kernel_config)\n \n         # If it is a model with generation capabilities, attempt to load generation files (generation config,\n@@ -3979,8 +3979,8 @@ def from_pretrained(\n                 **kwargs,\n             )\n \n-        # for device_map=\"auto\" : dispatch model with hooks on all devices if necessary\n-        if device_map is not None and device_mesh is None:\n+        # If the device_map has more than 1 device: dispatch model with hooks on all devices\n+        if device_map is not None and len(set(device_map.values())) > 1:\n             accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload_index, offload_buffers)\n \n         if hf_quantizer is not None:\n@@ -4056,7 +4056,6 @@ def _load_pretrained_model(\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n             caching_allocator_warmup(model, expanded_device_map, hf_quantizer)\n \n-        tp_plan = getattr(model, \"_tp_plan\", None)\n         error_msgs = []\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n@@ -4091,17 +4090,17 @@ def _load_pretrained_model(\n \n             missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, conversion_errors = (\n                 convert_and_load_state_dict_in_model(\n-                    model,\n-                    merged_state_dict,\n-                    weight_mapping,\n-                    tp_plan,\n-                    hf_quantizer,\n-                    dtype,\n-                    device_map,\n-                    model.dtype_plan,\n-                    device_mesh,\n-                    disk_offload_index,\n-                    disk_offload_folder,\n+                    model=model,\n+                    state_dict=merged_state_dict,\n+                    weight_mapping=weight_mapping,\n+                    tp_plan=model._tp_plan,\n+                    hf_quantizer=hf_quantizer,\n+                    dtype=dtype,\n+                    device_map=device_map,\n+                    dtype_plan=model.dtype_plan,\n+                    device_mesh=device_mesh,\n+                    disk_offload_index=disk_offload_index,\n+                    disk_offload_folder=disk_offload_folder,\n                 )\n             )\n \n@@ -4112,10 +4111,10 @@ def _load_pretrained_model(\n         # Marks tied weights as `_is_hf_initialized` to avoid initializing them (it's very important for efficiency)\n         model.mark_tied_weights_as_initialized()\n \n-        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n-        # loading the weights as they are not in the loaded state dict)\n-        miss_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n-        model._move_missing_keys_from_meta_to_cpu(miss_and_mismatched, hf_quantizer)\n+        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they were not moved when\n+        # loading the weights as they were not in the loaded state dict)\n+        missing_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n+        model._move_missing_keys_from_meta_to_cpu(missing_and_mismatched, hf_quantizer)\n \n         # Correctly initialize the missing (and potentially mismatched) keys (all parameters without the `_is_hf_initialzed` flag)\n         model._initialize_missing_keys(is_quantized)\n@@ -4126,33 +4125,28 @@ def _load_pretrained_model(\n         # Adjust missing and unexpected keys\n         missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys)\n \n-        # Post-processing for tensor parallelism\n-        if device_mesh is not None:\n-            # When using TP, the device map is a single device for all parameters\n-            tp_device = list(device_map.values())[0]\n-            # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n-            # not part of the state_dict (persistent=False)\n-            for buffer in model.buffers():  # TODO to avoid this buffer could be added to the ckpt\n-                if buffer.device != tp_device:\n-                    buffer.data = buffer.to(tp_device)\n-\n-            # In this case, the top-most task module weights were not moved to device and parallelized as they\n-            # were not part of the loaded weights: do it now\n-            if missing_keys:\n-                state_dict = model.state_dict()\n-                for name in missing_keys:\n-                    param = state_dict[name]\n-                    # Shard the param\n+        unique_devices = set(device_map.values()) if device_map is not None else set()\n+        # Post-processing for only 1-value device_map (this includes TP) as we won't use hooks in this case\n+        if len(unique_devices) == 1:\n+            device = unique_devices.pop()\n+            # This is needed for all non-persistent buffers (such as RotaryEmbedding modules), which were not initialized\n+            # on the correct device as it is not part of the state_dict\n+            for _, buffer in model.named_non_persistent_buffers():\n+                buffer.data = buffer.data.to(device)\n+\n+            # The missing/mismatch weights were not moved to device (and parallelized for TP) as they were not part of the\n+            # loaded weights: do it now if we have any\n+            missing_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n+            for name in missing_and_mismatched:\n+                param = model.get_parameter_or_buffer(name)\n+                # For TP, shard the param\n+                if device_mesh is not None:\n                     shard_and_distribute_module(\n-                        model,\n-                        param.to(tp_device),\n-                        param,\n-                        name,\n-                        None,\n-                        False,\n-                        device_mesh.get_local_rank(),\n-                        device_mesh,\n+                        model, param.to(device), param, name, None, False, device_mesh.get_local_rank(), device_mesh\n                     )\n+                # Otherwise, just move it to device\n+                else:\n+                    param.data = param.data.to(device)\n \n         log_state_dict_report(\n             model=model,\n@@ -4421,8 +4415,6 @@ def _adjust_missing_and_unexpected_keys(\n     ) -> tuple[set[str], set[str]]:\n         \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n         raising unneeded warnings/errors.\n-        Also, set the `_is_hf_initialized` on tied weight keys, to avoid initializing them as they are going to\n-        be tied anyway.\n         \"\"\"\n         # Old checkpoints may have keys for rotary_emb.inv_freq forach layer, however we moved this buffer to the main model\n         # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 38,
        "deletions": 46
    }
}