{
    "author": "gante",
    "message": "[RoPE] explicit factor > implicit factor in YaRN (#40320)\n\nexplicit factor > implicit factor",
    "sha": "6451294f6f28f0d4842f78bff4c9a70c208f4ea4",
    "files": [
        {
            "sha": "34c136980234c7a8c62676ae6f8cc1cbe89724bb",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 9,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6451294f6f28f0d4842f78bff4c9a70c208f4ea4/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6451294f6f28f0d4842f78bff4c9a70c208f4ea4/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=6451294f6f28f0d4842f78bff4c9a70c208f4ea4",
            "patch": "@@ -220,15 +220,9 @@ def _compute_yarn_parameters(\n     attention_factor = config.rope_scaling.get(\"attention_factor\")\n     mscale = config.rope_scaling.get(\"mscale\")\n     mscale_all_dim = config.rope_scaling.get(\"mscale_all_dim\")\n-\n-    # NOTE: DeekSeek-V3 (and potentially other models) modify `max_position_embeddings` and have a\n-    # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n-    # values to compute the default attention scaling factor, instead of using `factor`.\n-    if \"original_max_position_embeddings\" in config.rope_scaling:\n-        original_max_position_embeddings = config.rope_scaling[\"original_max_position_embeddings\"]\n-        factor = config.max_position_embeddings / original_max_position_embeddings\n-    else:\n-        original_max_position_embeddings = config.max_position_embeddings\n+    original_max_position_embeddings = (\n+        config.rope_scaling.get(\"original_max_position_embeddings\") or config.max_position_embeddings\n+    )\n \n     def get_mscale(scale, mscale=1):\n         if scale <= 1:\n@@ -496,6 +490,33 @@ def _validate_yarn_parameters(config: PretrainedConfig, ignore_keys: Optional[se\n             f\"(defaults to 32 if None) and beta_slow={beta_slow} (defaults to 1 if None)\"\n         )\n \n+    # Models should set `config.rope_scaling[\"original_max_position_embeddings\"]` to their original (pre-yarn) context\n+    # length, with `config.max_position_embeddings` corresponding to their post-yarn context length.\n+    # However, for BC purposes, we allow the former to be unset.\n+    original_max_position_embeddings = config.rope_scaling.get(\"original_max_position_embeddings\")\n+    if original_max_position_embeddings is not None:\n+        # Double-check: `factor` should be the ratio between the pre-yarn and post-yarn context lengths.\n+        implicit_factor = config.max_position_embeddings / original_max_position_embeddings\n+        if implicit_factor != factor:\n+            logger.warning_once(\n+                f\"The explicitly set RoPE scaling factor (config.rope_scaling['factor'] = {factor}) does not match \"\n+                \"the ratio implicitly set by other parameters (implicit factor = \"\n+                \"post-yarn context length / pre-yarn context length = \"\n+                \"config.max_position_embeddings / config.rope_scaling['original_max_position_embeddings'] = \"\n+                f\"{implicit_factor}). Using the explicit factor ({factor}) in YaRN. This may cause unexpected \"\n+                \"behaviour in model usage, please correct the 'max_position_embeddings' fields in the model config.\"\n+            )\n+    # No `config.rope_scaling[\"original_max_position_embeddings\"]`. Is `config.max_position_embeddings` the\n+    # pre-yarn or the post-yarn context length?\n+    # BC: we assume it is the pre-yarn context length.\n+    else:\n+        logger.warning_once(\n+            \"config.rope_scaling['original_max_position_embeddings'], the pre-yarn context length, is unset. We will \"\n+            \"**assume** config.max_position_embeddings holds the pre-yarn context length. Some use cases may expect \"\n+            \"config.max_position_embeddings to hold the post-yarn context length (pre-yarn context length * \"\n+            \"factor) -- we recommend updating both fields for optimal downstream model usage.\"\n+        )\n+\n \n def _validate_longrope_parameters(config: PretrainedConfig, ignore_keys: Optional[set] = None):\n     rope_scaling = config.rope_scaling"
        },
        {
            "sha": "7e000e0ff1a1dc75d55599e01ceb307445918f87",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/6451294f6f28f0d4842f78bff4c9a70c208f4ea4/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6451294f6f28f0d4842f78bff4c9a70c208f4ea4/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=6451294f6f28f0d4842f78bff4c9a70c208f4ea4",
            "patch": "@@ -77,6 +77,44 @@ def test_rope_validation(self):\n                     self.assertEqual(len(logs.output), 1)\n                     self.assertIn(model_specific_kwarg, logs.output[0])\n \n+    def test_yarn_original_original_max_position_embeddings_validation(self):\n+        \"\"\"Tests that models with no/bad `original_max_position_embeddings` raise a warning\"\"\"\n+        config = LlamaConfig()\n+\n+        # good rope config: has a factor AND original_max_position_embeddings -> no warnings\n+        rope_config = {\n+            \"rope_type\": \"yarn\",\n+            \"factor\": 2.0,\n+            \"original_max_position_embeddings\": int(config.max_position_embeddings / 2.0),\n+        }\n+        config.rope_scaling = rope_config\n+        with self.assertRaises(AssertionError):  # confirm that no warnings are thrown\n+            with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n+                rope_config_validation(config)\n+\n+        # bad rope config, no `original_max_position_embeddings` -> warning\n+        rope_config = {\n+            \"rope_type\": \"yarn\",\n+            \"factor\": 2.0,\n+        }\n+        config.rope_scaling = rope_config\n+        with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n+            rope_config_validation(config)\n+            self.assertEqual(len(logs.output), 1)\n+            self.assertIn(\"is unset\", logs.output[0])\n+\n+        # bad rope config, bad implicit fator -> warning\n+        rope_config = {\n+            \"rope_type\": \"yarn\",\n+            \"factor\": 2.0,\n+            \"original_max_position_embeddings\": 1,\n+        }\n+        config.rope_scaling = rope_config\n+        with self.assertLogs(\"transformers.modeling_rope_utils\", level=\"WARNING\") as logs:\n+            rope_config_validation(config)\n+            self.assertEqual(len(logs.output), 1)\n+            self.assertIn(\"implicit factor\", logs.output[0])\n+\n     def test_default_rope_numerically(self):\n         # Note: some RoPE scaling methods start off by calling the default RoPE frequencies. If this test fails, then\n         # multiple RoPE strategies will fail."
        }
    ],
    "stats": {
        "total": 77,
        "additions": 68,
        "deletions": 9
    }
}