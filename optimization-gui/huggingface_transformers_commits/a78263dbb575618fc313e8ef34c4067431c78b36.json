{
    "author": "Cyrilvallez",
    "message": "fix",
    "sha": "a78263dbb575618fc313e8ef34c4067431c78b36",
    "files": [
        {
            "sha": "bba22e65e61074a3e576ff66a132ccf9d23bb5c6",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a78263dbb575618fc313e8ef34c4067431c78b36/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a78263dbb575618fc313e8ef34c4067431c78b36/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=a78263dbb575618fc313e8ef34c4067431c78b36",
            "patch": "@@ -1067,16 +1067,6 @@ def tie_weights(self):\n         If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n         weights instead.\n         \"\"\"\n-        if getattr(self.config, \"tie_word_embeddings\", True):\n-            self._tied_weights_keys = []\n-            output_embeddings = self.get_output_embeddings()\n-            input_embeddings = self.get_input_embeddings()\n-\n-            for i in range(self.config.n_codes_total - self.config.n_codes_given):\n-                # self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight\n-                self._tie_or_clone_weights(output_embeddings[i], input_embeddings[i + 1])\n-                self._tied_weights_keys.append(f\"lm_heads.{i}.weight\")\n-\n         for module in self.modules():\n             if hasattr(module, \"_tie_weights\"):\n                 module._tie_weights()\n@@ -1621,6 +1611,17 @@ def generate(\n \n         return audio\n \n+    def tie_weights(self):\n+        \"\"\"\n+        Tie the weights between the input embeddings list and the output embeddings list.\n+\n+        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n+        weights instead.\n+        \"\"\"\n+        for module in self.modules():\n+            if hasattr(module, \"_tie_weights\"):\n+                module._tie_weights()\n+\n \n __all__ = [\n     \"BarkFineModel\","
        }
    ],
    "stats": {
        "total": 21,
        "additions": 11,
        "deletions": 10
    }
}