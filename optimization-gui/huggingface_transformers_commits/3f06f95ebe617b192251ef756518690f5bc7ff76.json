{
    "author": "SunMarc",
    "message": "Revert \"Fix FSDP resume Initialization issue\" (#34193)\n\nRevert \"Fix FSDP resume Initialization issue (#34032)\"\r\n\r\nThis reverts commit 4de1bdbf637fe6411c104c62ab385f660bfb1064.",
    "sha": "3f06f95ebe617b192251ef756518690f5bc7ff76",
    "files": [
        {
            "sha": "20b9f6dad231d127ae611fa4e8e75d583132d4f6",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f06f95ebe617b192251ef756518690f5bc7ff76/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f06f95ebe617b192251ef756518690f5bc7ff76/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3f06f95ebe617b192251ef756518690f5bc7ff76",
            "patch": "@@ -273,39 +273,6 @@ def _get_fsdp_ckpt_kwargs():\n         return {}\n \n \n-def _init_fsdp(model, accelerator, device):\n-    \"\"\"\n-    Initialize Fully Sharded Data Parallel (FSDP) for the model.\n-\n-    This function is needed to properly initialize FSDP when resuming from a checkpoint.\n-    It runs a forward pass with dummy inputs to ensure FSDP is fully initialized.\n-    See https://github.com/huggingface/transformers/issues/31892 for more details.\n-\n-    Args:\n-        model: The model to initialize with FSDP.\n-        accelerator: The Accelerator object.\n-        device: The device to run the model on.\n-\n-    Returns:\n-        The initialized FSDP model.\n-    \"\"\"\n-    model = accelerator.prepare(model)\n-    model.train()\n-    with torch.no_grad():\n-        # Run a forward pass with dummy inputs to initialize FSDP\n-        dummy_input = {\n-            name: torch.ones(\n-                (1, 512),\n-                dtype=torch.long,\n-                device=device,\n-            )\n-            for name in model.forward.__code__.co_varnames\n-            if name != \"self\"\n-        }\n-        _ = model(**dummy_input)\n-    return model\n-\n-\n if TYPE_CHECKING:\n     import optuna\n \n@@ -634,10 +601,6 @@ def __init__(\n                     \" `Trainer`. Make sure the lines `import torch_xla.core.xla_model as xm` and\"\n                     \" `model.to(xm.xla_device())` is performed before the optimizer creation in your script.\"\n                 )\n-\n-        if self.is_fsdp_enabled:\n-            self.model = _init_fsdp(self.model, self.accelerator, self.args.device)\n-\n         if (self.is_fsdp_xla_enabled or self.is_fsdp_enabled) and (\n             self.optimizer is not None or self.lr_scheduler is not None\n         ):"
        },
        {
            "sha": "cbc93faf50e7a3910808c93c19d3680871e9e551",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f06f95ebe617b192251ef756518690f5bc7ff76/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f06f95ebe617b192251ef756518690f5bc7ff76/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=3f06f95ebe617b192251ef756518690f5bc7ff76",
            "patch": "@@ -4914,34 +4914,3 @@ def test_get_optimizer_group(self):\n             param = next(model.parameters())\n             group = trainer.get_optimizer_group(param)\n             self.assertIn(param, group[\"params\"])\n-\n-\n-@require_torch_gpu\n-@require_torch\n-@require_accelerate\n-class TestFSDPInitialization(unittest.TestCase):\n-    def test_fsdp_initialization(self):\n-        config = RegressionModelConfig(a=1, b=1, double_output=False)\n-        model = RegressionPreTrainedModel(config)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            training_args = TrainingArguments(\n-                output_dir=tmp_dir,\n-                fsdp=True,\n-                fsdp_config={\"min_num_params\": 1},\n-                no_cuda=True,\n-            )\n-            trainer = Trainer(model=model, args=training_args)\n-\n-            # Check for FSDP enabled\n-            self.assertTrue(trainer.is_fsdp_enabled)\n-\n-            # Check if model is wrapped with FSDP\n-            from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n-\n-            self.assertTrue(trainer.model, FSDP)\n-\n-            # Running a forward pass to ensure FSDP is initialized\n-            dummy_input = torch.ones((1, 1), dtype=torch.float)\n-            output = trainer.model(dummy_input)\n-            self.assertTrue(output)"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 0,
        "deletions": 68
    }
}