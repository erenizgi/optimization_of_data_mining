{
    "author": "zucchini-nlp",
    "message": "Whisper: fix static cache CI (#35852)\n\n* fix\r\n\r\n* remove overriden method\r\n\r\n* small change",
    "sha": "365fecb4d0b6c87f20b93561e11c3d4c77938012",
    "files": [
        {
            "sha": "fed276b3238b97e72f0a7690b6ce0521c5c63c44",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=365fecb4d0b6c87f20b93561e11c3d4c77938012",
            "patch": "@@ -406,23 +406,28 @@ def prepare_inputs_for_generation(\n             model_inputs[input_ids_key] = input_ids.clone(memory_format=torch.contiguous_format)\n \n         # 4. Create missing `position_ids` on the fly\n+        attention_mask = (\n+            kwargs.pop(\"decoder_attention_mask\", None) if self.config.is_encoder_decoder else attention_mask\n+        )\n+        attention_mask_key = \"decoder_attention_mask\" if self.config.is_encoder_decoder else \"attention_mask\"\n+        position_ids_key = \"decoder_position_ids\" if self.config.is_encoder_decoder else \"position_ids\"\n         if (\n             attention_mask is not None\n-            and kwargs.get(\"position_ids\") is None\n-            and \"position_ids\" in set(inspect.signature(self.forward).parameters.keys())\n+            and kwargs.get(position_ids_key) is None\n+            and position_ids_key in set(inspect.signature(self.forward).parameters.keys())\n         ):\n             position_ids = attention_mask.long().cumsum(-1) - 1\n             position_ids.masked_fill_(attention_mask == 0, 1)\n-            kwargs[\"position_ids\"] = position_ids  # placed in kwargs for further processing (see below)\n+            kwargs[position_ids_key] = position_ids  # placed in kwargs for further processing (see below)\n \n         # 5. Slice model inputs if it's an input that should have the same length as `input_ids`\n-        for model_input_name in [\"position_ids\", \"token_type_ids\"]:\n+        for model_input_name in [\"position_ids\", \"token_type_ids\", \"decoder_position_ids\"]:\n             model_input = kwargs.get(model_input_name)\n             if model_input is not None:\n                 if past_key_values is not None:\n                     current_input_length = (\n                         model_inputs[\"inputs_embeds\"].shape[1]\n-                        if model_inputs[\"inputs_embeds\"] is not None\n+                        if model_inputs.get(\"inputs_embeds\") is not None\n                         else model_inputs[input_ids_key].shape[1]\n                     )\n                     model_input = model_input[:, -current_input_length:]\n@@ -469,7 +474,7 @@ def prepare_inputs_for_generation(\n                     past_key_values=past_key_values,\n                 )\n         if attention_mask is not None:\n-            model_inputs[\"attention_mask\"] = attention_mask\n+            model_inputs[attention_mask_key] = attention_mask\n \n         # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():"
        },
        {
            "sha": "035b4fb8907ec607134fe5664ce1175c7144dbf7",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=365fecb4d0b6c87f20b93561e11c3d4c77938012",
            "patch": "@@ -1234,7 +1234,7 @@ def _expand_variables_for_generation(\n     def _setup_no_speech_detection(logits_processor, segment_input, decoder_input_ids, kwargs):\n         set_inputs = _get_attr_from_logit_processors(logits_processor, WhisperNoSpeechDetection, \"set_inputs\")\n         extra_kwargs = {k: v for k, v in kwargs.items() if torch.is_tensor(v)}\n-        set_inputs({\"inputs\": segment_input, \"decoder_input_ids\": decoder_input_ids, **extra_kwargs})\n+        set_inputs({\"inputs\": segment_input, \"input_ids\": decoder_input_ids, **extra_kwargs})\n \n     @staticmethod\n     def _retrieve_total_input_frames(input_features, input_stride, kwargs):"
        },
        {
            "sha": "f7bbffdbc585136c309170e33d6464d32a6c8ac9",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 83,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/365fecb4d0b6c87f20b93561e11c3d4c77938012/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=365fecb4d0b6c87f20b93561e11c3d4c77938012",
            "patch": "@@ -1255,7 +1255,7 @@ def forward(\n             )\n \n         if position_ids is None:\n-            position_ids = cache_position.unsqueeze(0)\n+            position_ids = cache_position.unsqueeze(0).repeat(input_shape[0], 1)\n \n         # embed positions\n         if input_ids is not None:\n@@ -1806,88 +1806,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        attention_mask=None,\n-        decoder_attention_mask=None,\n-        cache_position=None,\n-        **kwargs,\n-    ):\n-        # Overwritten -- encoder-decoder whisper has custom logic, but it's close to the general function. Next time\n-        # this function needs to be touched, let's try to sort out the commonalities between the two and remove the\n-        # overwrite.\n-\n-        decoder_position_ids = None\n-        if decoder_attention_mask is not None:\n-            decoder_position_ids = (decoder_attention_mask.cumsum(-1) - 1).clamp(min=0)\n-\n-        past_length = 0\n-        if past_key_values is not None:\n-            if isinstance(past_key_values, EncoderDecoderCache):\n-                past_length = cache_position[0] if cache_position is not None else past_key_values.get_seq_length()\n-            else:\n-                past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-            if decoder_position_ids is not None:\n-                decoder_position_ids = decoder_position_ids[:, remove_prefix_length:]\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                decoder_position_ids = decoder_position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        if cache_position is None:\n-            cache_position = torch.arange(\n-                past_length, past_length + decoder_input_ids.shape[1], device=decoder_input_ids.device\n-            )\n-        elif use_cache:\n-            cache_position = cache_position[-decoder_input_ids.shape[1] :]\n-\n-        # The `contiguous()` here is necessary to have a static stride during decoding. torchdynamo otherwise\n-        # recompiles graphs as the stride of the inputs is a guard. Ref: https://github.com/huggingface/transformers/pull/29114\n-        decoder_input_ids = decoder_input_ids.contiguous()\n-\n-        if (\n-            isinstance(past_key_values, EncoderDecoderCache)\n-            and (\n-                isinstance(past_key_values.self_attention_cache, StaticCache)\n-                or isinstance(past_key_values.cross_attention_cache, StaticCache)\n-            )\n-            and decoder_attention_mask is not None\n-            and decoder_attention_mask.ndim == 2\n-        ):\n-            batch_size, sequence_length = decoder_input_ids.shape\n-\n-            decoder_attention_mask = self.get_decoder()._prepare_4d_causal_attention_mask_with_cache_position(\n-                decoder_attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.self_attention_cache.get_max_cache_shape(),\n-                dtype=self.proj_out.weight.dtype,\n-                device=decoder_input_ids.device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-\n-        return {\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"use_cache\": use_cache,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"decoder_position_ids\": decoder_position_ids,\n-            \"cache_position\": cache_position,\n-        }\n-\n \n class WhisperDecoderWrapper(WhisperPreTrainedModel):\n     \"\"\""
        },
        {
            "sha": "fe41afabf49b2f9ad8e5fec52c403d236bdcb9fb",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/365fecb4d0b6c87f20b93561e11c3d4c77938012/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/365fecb4d0b6c87f20b93561e11c3d4c77938012/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=365fecb4d0b6c87f20b93561e11c3d4c77938012",
            "patch": "@@ -3323,8 +3323,8 @@ def test_tiny_static_generation(self):\n         input_features = input_features.to(torch_device)\n         eager_generated_ids = model.generate(input_features, max_new_tokens=64)\n \n+        # Using statiic cache compiles forward for each decoding step, so we don't have to manually compile\n         model.generation_config.cache_implementation = \"static\"\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n \n         # compile the forward pass and assert equivalence\n         static_generated_ids = model.generate(input_features, max_new_tokens=64)\n@@ -3379,9 +3379,8 @@ def test_tiny_static_generation_long_form(self):\n         set_seed(42)\n         eager_generated_ids = model.generate(**inputs, **gen_kwargs)\n \n-        # compile the forward pass and assert equivalence\n+        # Using statiic cache compiles forward for each decoding step, so we don't have to manually compile\n         model.generation_config.cache_implementation = \"static\"\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n \n         set_seed(42)\n         static_generated_ids = model.generate(**inputs, **gen_kwargs)"
        }
    ],
    "stats": {
        "total": 108,
        "additions": 15,
        "deletions": 93
    }
}