{
    "author": "RyanMullins",
    "message": "Gemma 3: Adding explicit GenerationConfig and refactoring conversion â€¦ (#36833)\n\nGemma 3: Adding explicit GenerationConfig and refactoring conversion script",
    "sha": "8124a234ca819652f4e316f845460f67069b01a7",
    "files": [
        {
            "sha": "2d8ecf4ac730292055782022a4c3a79b1dd3ca85",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights_orbax_to_hf.py",
            "status": "modified",
            "additions": 100,
            "deletions": 103,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/8124a234ca819652f4e316f845460f67069b01a7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8124a234ca819652f4e316f845460f67069b01a7/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights_orbax_to_hf.py?ref=8124a234ca819652f4e316f845460f67069b01a7",
            "patch": "@@ -20,11 +20,9 @@\n     --variant='gemma3_4b' \\\n     --tokenizer_path=\"$HOME/gemma3/tokenizer/gemma3_cleaned_262144_v2.spiece.model\" \\\n     --checkpoint_path=\"$HOME/gemma3/gemma3_4b_pt_orbax/\" \\\n-    --output_path=\"$HOME/gemma3/gemma3_4b_pt_safetensors/\" \\\n-    --precision='bfloat16'\n+    --output_path=\"$HOME/gemma3/gemma3_4b_pt_safetensors/\"\n \"\"\"\n \n-import dataclasses\n from collections.abc import Iterator, Sequence\n from typing import Any\n \n@@ -35,19 +33,18 @@\n from absl import app, flags, logging\n from orbax import checkpoint as obc\n \n-from ...image_utils import PILImageResampling\n-from ..gemma import GemmaTokenizerFast\n-from . import (\n+from transformers import (\n+    Gemma3Config,\n     Gemma3ForCausalLM,\n     Gemma3ForConditionalGeneration,\n     Gemma3ImageProcessor,\n     Gemma3Processor,\n-)\n-from .configuration_gemma3 import (\n-    Gemma3Config,\n     Gemma3TextConfig,\n+    GemmaTokenizerFast,\n+    GenerationConfig,\n     SiglipVisionConfig,\n )\n+from transformers.image_utils import PILImageResampling\n \n \n # ==== Internal Constants and Classes ====\n@@ -95,11 +92,7 @@\n {%- endif -%}\n \"\"\"\n \n-_DTYPES = {\n-    \"float32\": torch.float32,\n-    \"bfloat16\": torch.bfloat16,\n-    \"float16\": torch.float16,\n-}\n+_DTYPES = {\"float32\", \"bfloat16\", \"float16\"}\n \n _SIGLIP_BASE = \"SigLiPFromPatches_0/siglip_encoder\"\n _SIGLIP_EMBEDDING = \"SigLiPFromPatches_0/siglip_encoder/embedding\"\n@@ -209,42 +202,32 @@\n \n # ==== Flags ====\n \n-CHECKPOINT_PATH = flags.DEFINE_string(\n+_CHECKPOINT_PATH = flags.DEFINE_string(\n     name=\"checkpoint_path\",\n     default=None,\n     help=\"Path to the Orbax checkpoint.\",\n     required=True,\n )\n \n-INCLUDE_CHAT_TEMPLATE = flags.DEFINE_bool(\n+_INCLUDE_CHAT_TEMPLATE = flags.DEFINE_bool(\n     name=\"include_chat_template\", default=False, help=\"If true, will save the default chat template with the tokenizer\"\n )\n \n-OUTPUT_PATH = flags.DEFINE_string(\n+_OUTPUT_PATH = flags.DEFINE_string(\n     name=\"output_path\",\n     default=None,\n     help=\"Path to store the HF checkpoint.\",\n     required=True,\n )\n \n-PRECISION = flags.DEFINE_enum(\n-    name=\"precision\",\n-    default=None,\n+_TRANSFORMER_DTYPE = flags.DEFINE_enum(\n+    name=\"text_dtype\",\n+    default=\"bfloat16\",\n     help=\"The floating point precision (aka dtype) of the model.\",\n-    enum_values=set(_DTYPES.keys()),\n-    required=True,\n-)\n-\n-_TEXT_ONLY = flags.DEFINE_bool(\n-    name=\"text_only\",\n-    default=False,\n-    help=(\n-        \"If True, the model is loaded and saved as a Gemma3ForCausalLM, \"\n-        \"otherwise model saed as Gemma3ForConditionalGeneration.\"\n-    ),\n+    enum_values=_DTYPES,\n )\n \n-TOKENIZER_PATH = flags.DEFINE_string(\n+_TOKENIZER_PATH = flags.DEFINE_string(\n     name=\"tokenizer_path\",\n     default=None,\n     help=\"Path to the SentencePiece model file.\",\n@@ -258,6 +241,19 @@\n     enum_values=set(_VARIANTS.keys()),\n )\n \n+_VERBOSE = flags.DEFINE_bool(\n+    name=\"verbose\",\n+    default=False,\n+    help=\"If true, log the path, shape, and dtype of every converted layer.\",\n+)\n+\n+_VISION_DTYPE = flags.DEFINE_enum(\n+    name=\"vision_dtype\",\n+    default=\"float32\",\n+    help=\"The floating point precision (aka dtype) of the model.\",\n+    enum_values=_DTYPES,\n+)\n+\n \n def convert_siglip_weight(\n     config: SiglipVisionConfig,\n@@ -343,8 +339,6 @@ def convert_siglip_weight(\n     else:\n         raise ValueError(f\"Unexpected path `{path}`.\")\n \n-    if \"vision\" in normalized_path:\n-        print(normalized_path)\n     return normalized_path, updated_weights\n \n \n@@ -369,7 +363,7 @@ def convert_transformer_weights(\n             # Tied to language_model.lm_head.weight, assigned at the end.\n             converted_paths = [\"language_model.model.embed_tokens.weight\"]\n \n-            if not _TEXT_ONLY.value:\n+            if _VARIANT.value != _VARIANT_GEMMA_3_1B:\n                 # Gemma3 model doesn't have image soft token in input and output embeddings, resize to avoid bugs we had with Mllama\n                 pre_expansion_embeddings = weights\n                 mu = np.mean(pre_expansion_embeddings, axis=0)\n@@ -378,12 +372,12 @@ def convert_transformer_weights(\n                 weights = np.vstack([pre_expansion_embeddings, new_embeddings])\n \n             converted_weights = [weights]\n-        elif _TEXT_ONLY.value or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n+        elif _VARIANT.value == _VARIANT_GEMMA_3_1B or prop in (\"mm_output_embedding\", \"mm_input_embedding_extra\"):\n             return zip([], [])\n         else:\n             raise ValueError(f\"Unexpected member, {prop}, in Embedder.\")\n     elif path.startswith(f\"{_TRANSFORMER_EMBEDDER}/mm\"):\n-        if _TEXT_ONLY.value:\n+        if _VARIANT.value == _VARIANT_GEMMA_3_1B:\n             return zip([], [])\n \n         if path.endswith(\"/mm_input_projection\"):\n@@ -463,86 +457,103 @@ def convert_transformer_weights(\n     return zip(converted_paths, converted_weights)\n \n \n-@dataclasses.dataclass(frozen=True)\n-class ConversionResult:\n-    state_tree: dict[str, torch.Tensor]\n-    config: Gemma3Config\n-\n-\n-def convert(\n-    checkpoint_path: str,\n-    config: Gemma3Config,\n-    target_dtype: torch.dtype,\n-) -> ConversionResult:\n+def convert(checkpoint_path: str, config: Gemma3Config) -> dict[str, torch.Tensor]:\n     \"\"\"Loads Orbax checkpoint from `input_path` and converts it to HF tree.\"\"\"\n     checkpointer = obc.PyTreeCheckpointer()\n     ckpt = checkpointer.restore(checkpoint_path)\n     hf_tree: dict[str, torch.Tensor] = {}\n \n-    def update_tree(path: str, weights: np.ndarray) -> None:\n-        torch_tensor = torch.from_numpy(weights.astype(\"float32\")).type(target_dtype)\n-        logging.info(\n-            \"%s converted shape=%s with dtype=%s\",\n-            path,\n-            weights.shape,\n-            torch_tensor.dtype,\n-        )\n-        hf_tree[path] = torch_tensor\n+    def update_tree(path: str, weights: np.ndarray, target_dtype: torch.dtype) -> None:\n+        hf_tree[path] = torch.from_numpy(weights.astype(\"float32\")).type(target_dtype)\n+        if _VERBOSE.value:\n+            logging.info(\n+                \"%s converted shape=%s with dtype=%s\",\n+                path,\n+                weights.shape,\n+                target_dtype,\n+            )\n \n     for paths, value in tree.flatten_with_path(ckpt):\n         if paths[0].startswith(\"SigLiPFromPatches_\"):\n             if config.vision_config is None:\n                 continue\n \n             path, weights = convert_siglip_weight(config=config.vision_config, paths=paths, weights=value)\n-            update_tree(path, weights)\n+            update_tree(path, weights, config.vision_config.torch_dtype)\n         else:\n             for path, weights in convert_transformer_weights(config=config.text_config, paths=paths, weights=value):\n                 if config.vision_config is None:\n                     path = path[len(\"language_model.\") :]\n \n-                update_tree(path, weights)\n+                update_tree(path, weights, config.text_config.torch_dtype)\n \n     if config.vision_config is None:\n         hf_tree[\"lm_head.weight\"] = hf_tree[\"model.embed_tokens.weight\"]\n     else:\n         hf_tree[\"language_model.lm_head.weight\"] = hf_tree[\"language_model.model.embed_tokens.weight\"]\n \n-    return ConversionResult(state_tree=hf_tree, config=config)\n+    return hf_tree\n \n \n def main(*args):\n     del args\n \n+    output_path = _OUTPUT_PATH.value\n     variant = _VARIANT.value\n-    dtype = getattr(torch, PRECISION.value)\n+\n     config = _VARIANTS[variant]\n-    output_path = OUTPUT_PATH.value\n+    config.text_config.torch_dtype = getattr(torch, _TRANSFORMER_DTYPE.value)\n+    config.vision_config.torch_dtype = getattr(torch, _VISION_DTYPE.value)\n+    if _INCLUDE_CHAT_TEMPLATE.value:\n+        # Chat template is included for instruction tuned models, which treat\n+        # both \"<eos>\" and \"<end_of_turn>\" as generation stoppers.\n+        config.eos_token_id = [1, 106]\n \n-    if variant == _VARIANT_GEMMA_3_1B:\n-        flags.FLAGS.set_default(_TEXT_ONLY.name, True)\n+    logging.info(\n+        \"Converting Gemma 3 (%s) @ %s (language) and %s (vision)\",\n+        variant,\n+        _TRANSFORMER_DTYPE.value,\n+        _VISION_DTYPE.value,\n+    )\n+    state_tree = convert(_CHECKPOINT_PATH.value, config)\n+    logging.info(\"Converted Gemma 3 (%s) state tree from Orbax to Hugging Face.\", variant)\n+\n+    with accelerate.init_empty_weights():\n+        if variant == _VARIANT_GEMMA_3_1B:\n+            model = Gemma3ForCausalLM(config=config.text_config)\n+        else:\n+            model = Gemma3ForConditionalGeneration(config)\n+\n+    model.load_state_dict(state_tree, assign=True, strict=True)\n+    logging.info(\n+        \"Loaded Gemma 3 (%s) in Hugging Face Transformers as a %s instance.\",\n+        variant,\n+        type(model).__name__,\n+    )\n+    model.save_pretrained(output_path, safe_serialization=True)\n+    logging.info(\n+        \"Saved Gemma 3 (%s) to SafeTensors in %s using %s\",\n+        variant,\n+        output_path,\n+        type(model).__name__,\n+    )\n+    del model\n+    del state_tree\n \n     tokenizer = GemmaTokenizerFast(\n-        TOKENIZER_PATH.value,\n+        _TOKENIZER_PATH.value,\n         add_bos_token=True,\n         extra_special_tokens={\n             \"image_token\": \"<image_soft_token>\",  # Should be ID=262_144\n             \"boi_token\": \"<start_of_image>\",  # Should be ID=255_999\n             \"eoi_token\": \"<end_of_image>\",  # Should be ID=256_000\n         },\n+        chat_template=_CHAT_TEMPLATE if _INCLUDE_CHAT_TEMPLATE.value else None,\n     )\n+    tokenizer.save_pretrained(output_path)\n+    logging.info(\"Saved GemmaTokenizer for %s to %s\", variant, output_path)\n \n-    if INCLUDE_CHAT_TEMPLATE.value:\n-        # Include chat template for CausalLM models\n-        tokenizer.chat_template = _CHAT_TEMPLATE\n-        config.eos_token_id = [1, 106]\n-\n-    if _TEXT_ONLY.value:\n-        config.vision_config = None\n-        tokenizer.save_pretrained(output_path)\n-        logging.info(\"Saved GemmaTokenizer for %s to %s\", variant, output_path)\n-        del tokenizer\n-    else:\n+    if variant != _VARIANT_GEMMA_3_1B:\n         image_processor = Gemma3ImageProcessor(\n             image_seq_length=256,\n             image_mean=(0.5,) * 3,\n@@ -553,39 +564,25 @@ def main(*args):\n         processor = Gemma3Processor(\n             image_processor=image_processor,\n             tokenizer=tokenizer,\n+            chat_template=tokenizer.chat_template,\n         )\n-        if INCLUDE_CHAT_TEMPLATE.value:\n-            # Duplicate so multimodal instruct models can also be used for CausalLM\n-            processor.chat_template = tokenizer.chat_template\n-\n         processor.save_pretrained(output_path)\n         logging.info(\"Saved Gemma3Processor for %s to %s\", variant, output_path)\n         del processor\n-        del tokenizer\n-\n-    logging.info(\"Gemma 3 (%s) configured as: %s\", variant, config)\n-    logging.info(\"Converting Gemma 3 (%s) @ %s\", variant, dtype)\n-    result = convert(CHECKPOINT_PATH.value, config, dtype)\n-    logging.info(\"Converted Gemma 3 (%s) state tree from Orbax to Hugging Face.\", variant)\n-\n-    with accelerate.init_empty_weights():\n-        if config.vision_config is None:\n-            model = Gemma3ForCausalLM(config=config.text_config)\n-        else:\n-            model = Gemma3ForConditionalGeneration(config)\n \n-    model.load_state_dict(result.state_tree, assign=True, strict=True)\n-    model.config.torch_dtype = dtype\n-    logging.info(\"Loaded Gemma 3 (%s) in Hugging Face Transformers as a %s instance.\", variant, type(model).__name__)\n-    model.save_pretrained(output_path, safe_serialization=True)\n-    logging.info(\n-        \"Saved Gemma 3 (%s) to SafeTensors in %s using %s\",\n-        variant,\n-        output_path,\n-        type(model).__name__,\n+    del tokenizer\n+\n+    generation_config = GenerationConfig(\n+        pad_token_id=config.pad_token_id,\n+        bos_token_id=config.bos_token_id,\n+        eos_token_id=config.eos_token_id,\n+        cache_implementation=\"hybrid\",\n+        temperature=1.0,\n+        do_sample=True,\n+        top_k=64,\n+        top_p=0.95,\n     )\n-    del model\n-    del result\n+    generation_config.save_pretrained(output_path)\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 203,
        "additions": 100,
        "deletions": 103
    }
}