{
    "author": "shenxiangzhuang",
    "message": "fix(type): padding_side type should be Optional[str] (#36326)",
    "sha": "18276b03f759771779c2995937ecaefa782a2c3c",
    "files": [
        {
            "sha": "2a2dda439e2c720cd008cb3fbddf56090b5dc7d7",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -414,7 +414,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -570,7 +570,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -630,7 +630,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -684,7 +684,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -758,7 +758,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -807,7 +807,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -875,7 +875,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -930,7 +930,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1261,7 +1261,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "0b974c283da7a8775df96a6030728841840a58d2",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -165,7 +165,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -321,7 +321,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -386,7 +386,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -458,7 +458,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -612,7 +612,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -674,7 +674,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "12114ac3391f0413a6280056f505a2f502b65079",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -543,7 +543,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -700,7 +700,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -761,7 +761,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -816,7 +816,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -891,7 +891,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -941,7 +941,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1010,7 +1010,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1065,7 +1065,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1400,7 +1400,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "50875e234107ff3e0792225807853d301f85363c",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -209,7 +209,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -366,7 +366,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -433,7 +433,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -505,7 +505,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -665,7 +665,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -728,7 +728,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "1c94bfbd25918ee03c04d5b8204cdb2b828bf2d9",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -447,7 +447,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -602,7 +602,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -656,7 +656,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -729,7 +729,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -784,7 +784,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1102,7 +1102,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "d45d4d988b6e85d1e63d3cad93ed061e91abba5f",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -277,7 +277,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -445,7 +445,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -600,7 +600,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -662,7 +662,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "dce0e7dc7c08773521e7584d3cea02cfb12e3a7b",
            "filename": "src/transformers/models/led/tokenization_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -412,7 +412,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad("
        },
        {
            "sha": "0cc29622987acc8b88c2d26ecb9d33dda5c1f677",
            "filename": "src/transformers/models/led/tokenization_led_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Ftokenization_led_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -280,7 +280,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         encoded_inputs = super()._pad("
        },
        {
            "sha": "beb06aed6e4e523e537ba2abd4811b5153905428",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -570,7 +570,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -718,7 +718,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -801,7 +801,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1077,7 +1077,7 @@ def _batch_prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1165,7 +1165,7 @@ def prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1394,7 +1394,7 @@ def pad(\n         max_length: Optional[int] = None,\n         max_entity_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -1554,7 +1554,7 @@ def _pad(\n         max_entity_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "814b6a2ab6dd978eb5bbe724578909c05328acce",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -503,7 +503,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -655,7 +655,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -715,7 +715,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -769,7 +769,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -843,7 +843,7 @@ def encode(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -892,7 +892,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -960,7 +960,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1015,7 +1015,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1375,7 +1375,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "49c3bfd0345ca6140897981e334de7af12056330",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -278,7 +278,7 @@ def __call__(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -430,7 +430,7 @@ def batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -495,7 +495,7 @@ def encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -567,7 +567,7 @@ def _batch_encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -722,7 +722,7 @@ def _encode_plus(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -784,7 +784,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "6bd9ed1a50ec7193ebf38058fc2da676b82c8ae8",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -399,7 +399,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -548,7 +548,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -632,7 +632,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: Optional[bool] = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -911,7 +911,7 @@ def _batch_prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1000,7 +1000,7 @@ def prepare_for_model(\n         max_entity_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1230,7 +1230,7 @@ def pad(\n         max_length: Optional[int] = None,\n         max_entity_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -1391,7 +1391,7 @@ def _pad(\n         max_entity_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "0d2e9bb6b94ba73f066a98b8437300784bdb5773",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -210,7 +210,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -310,7 +310,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -484,7 +484,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         # Load from model defaults\n@@ -557,7 +557,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -658,7 +658,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        },
        {
            "sha": "0e536aee273e3077002b8b7fca622e7143c5ab0a",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -522,7 +522,7 @@ def __call__(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -639,7 +639,7 @@ def batch_encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -748,7 +748,7 @@ def _batch_encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = None,\n@@ -809,7 +809,7 @@ def _batch_prepare_for_model(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -927,7 +927,7 @@ def encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1010,7 +1010,7 @@ def _encode_plus(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -1070,7 +1070,7 @@ def prepare_for_model(\n         truncation: Union[bool, str, TapasTruncationStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = True,\n         return_attention_mask: Optional[bool] = True,\n@@ -1775,7 +1775,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "c3d1b4a5c4e8909e34b34d6cdd5d46934b710887",
            "filename": "src/transformers/models/udop/tokenization_udop.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -551,7 +551,7 @@ def call_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -707,7 +707,7 @@ def batch_encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -818,7 +818,7 @@ def encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -899,7 +899,7 @@ def _batch_encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -953,7 +953,7 @@ def _batch_prepare_for_model_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1026,7 +1026,7 @@ def _encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1081,7 +1081,7 @@ def prepare_for_model_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -1401,7 +1401,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "2e129bad9afaf5d2b0bcbe49b16aab6e676c2773",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -286,7 +286,7 @@ def call_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -456,7 +456,7 @@ def batch_encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -533,7 +533,7 @@ def _batch_encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -691,7 +691,7 @@ def _encode_plus_boxes(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -803,7 +803,7 @@ def encode_plus_boxes(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -875,7 +875,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "9e9048ee037326adbaab35dc7f6fd159b22df25a",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -781,7 +781,7 @@ def __call__(\n         padding: Union[bool, str, PaddingStrategy] = False,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n         **kwargs,"
        },
        {
            "sha": "d31df91b68bf42cd5c6a998fb9a7a638930a7e78",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -752,7 +752,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -838,7 +838,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -920,7 +920,7 @@ def _batch_prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        },
        {
            "sha": "121d3eaa83a80ecad1eff1867f59bf79204c0448",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -2622,7 +2622,7 @@ def encode(\n         truncation: Union[bool, str, TruncationStrategy] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> List[int]:\n@@ -2813,7 +2813,7 @@ def __call__(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -2900,7 +2900,7 @@ def _call_one(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3019,7 +3019,7 @@ def encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3094,7 +3094,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3126,7 +3126,7 @@ def batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3203,7 +3203,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3229,7 +3229,7 @@ def pad(\n         padding: Union[bool, str, PaddingStrategy] = True,\n         max_length: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         verbose: bool = True,\n@@ -3447,7 +3447,7 @@ def prepare_for_model(\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -3704,7 +3704,7 @@ def _pad(\n         max_length: Optional[int] = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_attention_mask: Optional[bool] = None,\n     ) -> dict:\n         \"\"\""
        },
        {
            "sha": "1eb61804c1e075a1a25f5bef96be77ec1492b27f",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18276b03f759771779c2995937ecaefa782a2c3c/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=18276b03f759771779c2995937ecaefa782a2c3c",
            "patch": "@@ -427,7 +427,7 @@ def set_truncation_and_padding(\n         max_length: int,\n         stride: int,\n         pad_to_multiple_of: Optional[int],\n-        padding_side: Optional[bool],\n+        padding_side: Optional[str],\n     ):\n         \"\"\"\n         Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n@@ -507,7 +507,7 @@ def _batch_encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[str] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,\n@@ -597,7 +597,7 @@ def _encode_plus(\n         stride: int = 0,\n         is_split_into_words: bool = False,\n         pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[bool] = None,\n+        padding_side: Optional[str] = None,\n         return_tensors: Optional[bool] = None,\n         return_token_type_ids: Optional[bool] = None,\n         return_attention_mask: Optional[bool] = None,"
        }
    ],
    "stats": {
        "total": 230,
        "additions": 115,
        "deletions": 115
    }
}