{
    "author": "geetu040",
    "message": "Add Apple's Depth-Pro for depth estimation (#34583)\n\n* implement config and model building blocks\r\n\r\n* refactor model architechture\r\n\r\n* update model outputs\r\n\r\n* update init param to include use_fov_model\r\n\r\n* update param name in config\r\n\r\n* fix hidden_states and attentions outputs for fov\r\n\r\n* sort config\r\n\r\n* complete minor todos\r\n\r\n* update patching\r\n\r\n* update config for encoder\r\n\r\n* fix config\r\n\r\n* use correct defaults in config\r\n\r\n* update merge for compatibility with different image size\r\n\r\n* restructure encoder for custom configuration\r\n\r\n* make fov model compatible with custom config\r\n\r\n* replace word \"decoder\" with \"fusion\"\r\n\r\n* weight conversion script\r\n\r\n* fix fov squeeze\r\n\r\n* update conversion script (without test)\r\n\r\n* upload ruff image processing\r\n\r\n* create fast image processing\r\n\r\n* use torch interpolation for image processing\r\n\r\n* complete post_process_depth_estimation\r\n\r\n* config: fix imports and sort args\r\n\r\n* apply inference in weight conversion\r\n\r\n* use mllama script instead for weight conversion\r\n\r\n* clean weight conversion script\r\n\r\n* add depth-pro status in other files\r\n\r\n* fill docstring in config\r\n\r\n* formatting\r\n\r\n* more formatting\r\n\r\n* formatting with ruff\r\n\r\n* formatting with style\r\n\r\n* fix copied classes\r\n\r\n* add examples; update weight convert script\r\n\r\n* fix using check_table.py and isort\r\n\r\n* fix config docstring\r\n\r\n* add depth pro to sdpa docs\r\n\r\n* undo unintentional changes in configuration_gemma.py\r\n\r\n* minor fixes\r\n\r\n* test image processing\r\n\r\n* fixes and tests\r\n\r\n* more fixes\r\n\r\n* use output states from image_encoder instead\r\n\r\n* Revert \"use output states from image_encoder instead\"\r\n\r\nThis reverts commit 2408ec54e4f27d2abbecdb8374e58f34d91d8e96.\r\n\r\n* make embeddings dynamic\r\n\r\n* reshape output hidden states and attentions as part of computation graph\r\n\r\n* fix ruff formating\r\n\r\n* fix docstring failure\r\n\r\n* use num_fov_head_layers in tests\r\n\r\n* update doc\r\n\r\n* check consistency with config\r\n\r\n* ruff formatting\r\n\r\n* update test case\r\n\r\n* fix ruff formatting\r\n\r\n* add tests for fov\r\n\r\n* use interpolation in postprocess\r\n\r\n* run and fix slow tests locally\r\n\r\n* use scaled_images_features for image and fov encoder\r\n\r\n* return fused_hidden_states in fusion stage\r\n\r\n* fix example\r\n\r\n* fix ruff\r\n\r\n* fix copyright license for all files\r\n\r\n* add __all__ for each file\r\n\r\n* minor fixes\r\n- fix download spell\r\n- add push_to_hub option\r\n- fix Optional type hinting\r\n- apply single loop for DepthProImageProcessor.preprocess\r\n\r\n* return list in post_process_depth_estimation\r\n\r\n* minor fixes\r\n- capitalize start of docstring\r\n- use ignore copy\r\n- fix examples\r\n- move docstring templates and custom output classes to top\r\n- remove \"-> None\" typehinting from __init__\r\n- type hinting for forward passes\r\n- fix docstrings for custom output classes\r\n\r\n* fix \"ruff check\"\r\n\r\n* update upsample and projection\r\n\r\n* major changes: (image size and merge optimization)\r\n- add support for images of any size\r\n- optimize merge operation\r\n- remove image_size from config\r\n- use full names instead of B, C, H, W\r\n- remove interpolation from fusion stage\r\n- add interpolation after merge\r\n- move validations to config\r\n- update integration test\r\n- add type hints for functions\r\n\r\n* fix push_to_hub option in weights conversion\r\n\r\n* remove image_size in weights conversion\r\n\r\n* major changes in the architecture\r\n- remove all DepthProViT modules and support different backbones using the AutoModel API\r\n- set default use_fov_model to False\r\n- validate parameters in configuration\r\n- update interpolate function: use \"nearest\" for faster computation\r\n- update reshape_feature function: remove all special tokens, possible from different backbones\r\n- update merge function: use padding from config instead of merge_out_size\r\n- remove patch_to_batch and batch_to_patch conversions for now\r\n- calculate out_size dynamically in the encoder\r\n- leave head_mask calculation to the backbone\r\n- fix bugs with merge\r\n- add more comments\r\n- update tests\r\n\r\n* placeholder for unused config attributes\r\n\r\n* improve docs amid review\r\n\r\n* minor change in docs\r\n\r\n* further optimize merge\r\n\r\n* fix formatting\r\n\r\n* remove unused patch/batch convertion functions\r\n\r\n* use original F.interpolate\r\n\r\n* improve function naming\r\n\r\n* minor chages\r\n- use torch_int instead of int\r\n- use proper for newly initialized tensors\r\n- use user provided return_dict for patch_encoder\r\n- use if-else block instead in self.use_fov_model\r\n\r\n* rearchitect upsample block for improved modularity\r\n\r\n* update upsample keys in weight conversion\r\n\r\n* improve padding in merge_patches\r\n\r\n* use double-loop for merge\r\n\r\n* update comments\r\n\r\n* create feature_extractor, reduce some forward code\r\n\r\n* introduce config.use_mask_token in dinov2\r\n\r\n* minor fixes\r\n\r\n* minor fixes for onnx\r\n\r\n* update __init__ to latest format\r\n\r\n* remove DepthProConfig.to_dict()\r\n\r\n* major changes in backbone\r\n\r\n* update config in weight conversion\r\n\r\n* formatting\r\n\r\n* converted model is fp32\r\n\r\n* improve naming and docs for feature_extractor->reconstruct_feature_maps\r\n\r\n* minor fixes; amid review\r\n\r\n* create intermediate vars in func call\r\n\r\n* use torch.testing.assert_close\r\n\r\n* use ModuleList instead of Sequential and ModuleDict\r\n\r\n* update docs\r\n\r\n* include fov in integraiton tests\r\n\r\n* update docs\r\n\r\n* improve initialization of convolution layers\r\n\r\n* fix unused fov keys\r\n\r\n* update tests\r\n\r\n* ruff format\r\n\r\n* fix test, amid kaimming initialization\r\n\r\n* add depthpro to toctree\r\n\r\n* add residual layer to _no_split_modules\r\n\r\n* architecture rework\r\n\r\n* Update src/transformers/models/depth_pro/image_processing_depth_pro.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* Update src/transformers/models/depth_pro/image_processing_depth_pro_fast.py\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* update docs\r\n\r\n* improve merge_patches\r\n\r\n* use flatten with fov_output\r\n\r\n* ruff formatting\r\n\r\n* update resources section in docs\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* fix typo \"final_kernal_size\"\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* fix output typehint for DepthProDepthEstimator\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* residual operation in 2 steps\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* use image_size instead of global patch_size in interpolation\r\n\r\n* replace all Sequential with ModuleList\r\n\r\n* update fov\r\n\r\n* update heads\r\n\r\n* fix and update conversion script for heads\r\n\r\n* ruff formatting\r\n\r\n* remove float32 conversion\r\n\r\n* use \"Fov\" instead of \"FOV\" in class names\r\n\r\n* use \"Fov\" instead of \"FOV\" in config docs\r\n\r\n* remove prune_heads\r\n\r\n* update fusion stage\r\n\r\n* use device in examples\r\n\r\n* update processor\r\n\r\n* ruff fixes\r\n\r\n* add do_rescale in image_processor_dict\r\n\r\n* skip test: test_fast_is_faster_than_slow\r\n\r\n* ruff formatting\r\n\r\n* DepthProImageProcessorFast in other files\r\n\r\n* revert antialias removal\r\n\r\n* add antialias in BaseImageProcessorFast\r\n\r\n* Revert \"revert antialias removal\"\r\n\r\nThis reverts commit 5caa0bd8f9f7463b98410c04e6cfe8fef3adee18.\r\n\r\n* Revert \"add antialias in BaseImageProcessorFast\"\r\n\r\nThis reverts commit 3ae1134780ae236872985523d9c0a444eabcc179.\r\n\r\n* update processor for grouping and antialias\r\n\r\n* try test_fast_is_faster_than_slow without \"skip\" or \"flanky\"\r\n\r\n* update checkpoint\r\n\r\n* update checkpoint\r\n\r\n* use @is_flanky for processor test\r\n\r\n* update checkpoint to \"apple/DepthPro-hf\"\r\n\r\n---------\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
    "files": [
        {
            "sha": "cb4960d783e00f0291c9d611b06c315f65ac1d4a",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -653,6 +653,8 @@\n         title: Depth Anything\n       - local: model_doc/depth_anything_v2\n         title: Depth Anything V2\n+      - local: model_doc/depth_pro\n+        title: DepthPro\n       - local: model_doc/deta\n         title: DETA\n       - local: model_doc/detr"
        },
        {
            "sha": "89d7434b5a20359a783d1f744501bc365bad1a4e",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -123,6 +123,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                          [DeiT](model_doc/deit)                          |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                        [DePlot](model_doc/deplot)                        |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                [Depth Anything](model_doc/depth_anything)                |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                     [DepthPro](model_doc/depth_pro)                      |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                          [DETA](model_doc/deta)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                          [DETR](model_doc/detr)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                      [DialoGPT](model_doc/dialogpt)                      |       ‚úÖ        |         ‚úÖ         |      ‚úÖ      |"
        },
        {
            "sha": "2447b7d93dd51daf1ee3f334399735afae7a7571",
            "filename": "docs/source/en/model_doc/depth_pro.md",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdepth_pro.md?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,183 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# DepthPro\n+\n+## Overview\n+\n+The DepthPro model was proposed in [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/abs/2410.02073) by Aleksei Bochkovskii, Ama√´l Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan R. Richter, Vladlen Koltun.\n+\n+DepthPro is a foundation model for zero-shot metric monocular depth estimation, designed to generate high-resolution depth maps with remarkable sharpness and fine-grained details. It employs a multi-scale Vision Transformer (ViT)-based architecture, where images are downsampled, divided into patches, and processed using a shared Dinov2 encoder. The extracted patch-level features are merged, upsampled, and refined using a DPT-like fusion stage, enabling precise depth estimation.\n+\n+The abstract from the paper is the following:\n+\n+*We present a foundation model for zero-shot metric monocular depth estimation. Our model, Depth Pro, synthesizes high-resolution depth maps with unparalleled sharpness and high-frequency details. The predictions are metric, with absolute scale, without relying on the availability of metadata such as camera intrinsics. And the model is fast, producing a 2.25-megapixel depth map in 0.3 seconds on a standard GPU. These characteristics are enabled by a number of technical contributions, including an efficient multi-scale vision transformer for dense prediction, a training protocol that combines real and synthetic datasets to achieve high metric accuracy alongside fine boundary tracing, dedicated evaluation metrics for boundary accuracy in estimated depth maps, and state-of-the-art focal length estimation from a single image. Extensive experiments analyze specific design choices and demonstrate that Depth Pro outperforms prior work along multiple dimensions.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_teaser.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> DepthPro Outputs. Taken from the <a href=\"https://github.com/apple/ml-depth-pro\" target=\"_blank\">official code</a>. </small>\n+\n+This model was contributed by [geetu040](https://github.com/geetu040). The original code can be found [here](https://github.com/apple/ml-depth-pro).\n+\n+## Usage Tips\n+\n+The DepthPro model processes an input image by first downsampling it at multiple scales and splitting each scaled version into patches. These patches are then encoded using a shared Vision Transformer (ViT)-based Dinov2 patch encoder, while the full image is processed by a separate image encoder. The extracted patch features are merged into feature maps, upsampled, and fused using a DPT-like decoder to generate the final depth estimation. If enabled, an additional Field of View (FOV) encoder processes the image for estimating the camera's field of view, aiding in depth accuracy.\n+\n+```py\n+>>> import requests\n+>>> from PIL import Image\n+>>> import torch\n+>>> from transformers import DepthProImageProcessorFast, DepthProForDepthEstimation\n+\n+>>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+\n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> image_processor = DepthProImageProcessorFast.from_pretrained(\"apple/DepthPro-hf\")\n+>>> model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\").to(device)\n+\n+>>> inputs = image_processor(images=image, return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> post_processed_output = image_processor.post_process_depth_estimation(\n+...     outputs, target_sizes=[(image.height, image.width)],\n+... )\n+\n+>>> field_of_view = post_processed_output[0][\"field_of_view\"]\n+>>> focal_length = post_processed_output[0][\"focal_length\"]\n+>>> depth = post_processed_output[0][\"predicted_depth\"]\n+>>> depth = (depth - depth.min()) / depth.max()\n+>>> depth = depth * 255.\n+>>> depth = depth.detach().cpu().numpy()\n+>>> depth = Image.fromarray(depth.astype(\"uint8\"))\n+```\n+\n+### Architecture and Configuration\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/depth_pro_architecture.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> DepthPro architecture. Taken from the <a href=\"https://arxiv.org/abs/2410.02073\" target=\"_blank\">original paper</a>. </small>\n+\n+The `DepthProForDepthEstimation` model uses a `DepthProEncoder`, for encoding the input image and a `FeatureFusionStage` for fusing the output features from encoder.\n+\n+The `DepthProEncoder` further uses two encoders:\n+- `patch_encoder`\n+   - Input image is scaled with multiple ratios, as specified in the `scaled_images_ratios` configuration.\n+   - Each scaled image is split into smaller **patches** of size `patch_size` with overlapping areas determined by `scaled_images_overlap_ratios`.\n+   - These patches are processed by the **`patch_encoder`**\n+- `image_encoder`\n+   - Input image is also rescaled to `patch_size` and processed by the **`image_encoder`**\n+\n+Both these encoders can be configured via `patch_model_config` and `image_model_config` respectively, both of which are seperate `Dinov2Model` by default.\n+\n+Outputs from both encoders (`last_hidden_state`) and selected intermediate states (`hidden_states`) from **`patch_encoder`** are fused by a `DPT`-based `FeatureFusionStage` for depth estimation.\n+\n+### Field-of-View (FOV) Prediction\n+\n+The network is supplemented with a focal length estimation head. A small convolutional head ingests frozen features from the depth estimation network and task-specific features from a separate ViT image encoder to predict the horizontal angular field-of-view.\n+\n+The `use_fov_model` parameter in `DepthProConfig` controls whether **FOV prediction** is enabled. By default, it is set to `False` to conserve memory and computation. When enabled, the **FOV encoder** is instantiated based on the `fov_model_config` parameter, which defaults to a `Dinov2Model`. The `use_fov_model` parameter can also be passed when initializing the `DepthProForDepthEstimation` model.\n+\n+The pretrained model at checkpoint `apple/DepthPro-hf` uses the FOV encoder. To use the pretrained-model without FOV encoder, set `use_fov_model=False` when loading the model, which saves computation.\n+```py\n+>>> from transformers import DepthProForDepthEstimation\n+>>> model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", use_fov_model=False)\n+```\n+\n+To instantiate a new model with FOV encoder, set `use_fov_model=True` in the config.\n+```py\n+>>> from transformers import DepthProConfig, DepthProForDepthEstimation\n+>>> config = DepthProConfig(use_fov_model=True)\n+>>> model = DepthProForDepthEstimation(config)\n+```\n+\n+Or set `use_fov_model=True` when initializing the model, which overrides the value in config.\n+```py\n+>>> from transformers import DepthProConfig, DepthProForDepthEstimation\n+>>> config = DepthProConfig()\n+>>> model = DepthProForDepthEstimation(config, use_fov_model=True)\n+```\n+\n+### Using Scaled Dot Product Attention (SDPA)\n+\n+PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function \n+encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the \n+[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n+page for more information.\n+\n+SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set \n+`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+\n+```py\n+from transformers import DepthProForDepthEstimation\n+model = DepthProForDepthEstimation.from_pretrained(\"apple/DepthPro-hf\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n+```\n+\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+\n+On a local benchmark (A100-40GB, PyTorch 2.3.0, OS Ubuntu 22.04) with `float32` and `google/vit-base-patch16-224` model, we saw the following speedups during inference.\n+\n+|   Batch size |   Average inference time (ms), eager mode |   Average inference time (ms), sdpa model |   Speed up, Sdpa / Eager (x) |\n+|--------------|-------------------------------------------|-------------------------------------------|------------------------------|\n+|            1 |                                         7 |                                         6 |                      1.17 |\n+|            2 |                                         8 |                                         6 |                      1.33 |\n+|            4 |                                         8 |                                         6 |                      1.33 |\n+|            8 |                                         8 |                                         6 |                      1.33 |\n+\n+## Resources\n+\n+A list of official Hugging Face and community (indicated by üåé) resources to help you get started with DepthPro:\n+\n+- Research Paper: [Depth Pro: Sharp Monocular Metric Depth in Less Than a Second](https://arxiv.org/pdf/2410.02073)\n+- Official Implementation: [apple/ml-depth-pro](https://github.com/apple/ml-depth-pro)\n+- DepthPro Inference Notebook: [DepthPro Inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/DepthPro_inference.ipynb)\n+- DepthPro for Super Resolution and Image Segmentation\n+    - Read blog on Medium: [Depth Pro: Beyond Depth](https://medium.com/@raoarmaghanshakir040/depth-pro-beyond-depth-9d822fc557ba)\n+    - Code on Github: [geetu040/depthpro-beyond-depth](https://github.com/geetu040/depthpro-beyond-depth)\n+\n+If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+\n+## DepthProConfig\n+\n+[[autodoc]] DepthProConfig\n+\n+## DepthProImageProcessor\n+\n+[[autodoc]] DepthProImageProcessor\n+    - preprocess\n+    - post_process_depth_estimation\n+\n+## DepthProImageProcessorFast\n+\n+[[autodoc]] DepthProImageProcessorFast\n+    - preprocess\n+    - post_process_depth_estimation\n+\n+## DepthProModel\n+\n+[[autodoc]] DepthProModel\n+    - forward\n+\n+## DepthProForDepthEstimation\n+\n+[[autodoc]] DepthProForDepthEstimation\n+    - forward"
        },
        {
            "sha": "59b686436018b5b7964dd1a783c481c09e78dcec",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -244,6 +244,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [data2vec_vision](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecVisionModel)\n * [Dbrx](https://huggingface.co/docs/transformers/model_doc/dbrx#transformers.DbrxModel)\n * [DeiT](https://huggingface.co/docs/transformers/model_doc/deit#transformers.DeiTModel)\n+* [DepthPro](https://huggingface.co/docs/transformers/model_doc/depth_pro#transformers.DepthProModel)\n * [DiffLlama](https://huggingface.co/docs/transformers/model_doc/diffllama#transformers.DiffLlamaModel)\n * [Dinov2](https://huggingface.co/docs/transformers/en/model_doc/dinov2)\n * [Dinov2_with_registers](https://huggingface.co/docs/transformers/en/model_doc/dinov2)"
        },
        {
            "sha": "82a12f63b958b8ae5c43e0ddff30d7ddd99ec74b",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -400,6 +400,7 @@\n     \"models.deprecated.vit_hybrid\": [\"ViTHybridConfig\"],\n     \"models.deprecated.xlm_prophetnet\": [\"XLMProphetNetConfig\"],\n     \"models.depth_anything\": [\"DepthAnythingConfig\"],\n+    \"models.depth_pro\": [\"DepthProConfig\"],\n     \"models.detr\": [\"DetrConfig\"],\n     \"models.dialogpt\": [],\n     \"models.diffllama\": [\"DiffLlamaConfig\"],\n@@ -1237,6 +1238,7 @@\n     _import_structure[\"models.deprecated.efficientformer\"].append(\"EfficientFormerImageProcessor\")\n     _import_structure[\"models.deprecated.tvlt\"].append(\"TvltImageProcessor\")\n     _import_structure[\"models.deprecated.vit_hybrid\"].extend([\"ViTHybridImageProcessor\"])\n+    _import_structure[\"models.depth_pro\"].extend([\"DepthProImageProcessor\", \"DepthProImageProcessorFast\"])\n     _import_structure[\"models.detr\"].extend([\"DetrFeatureExtractor\", \"DetrImageProcessor\"])\n     _import_structure[\"models.donut\"].extend([\"DonutFeatureExtractor\", \"DonutImageProcessor\"])\n     _import_structure[\"models.dpt\"].extend([\"DPTFeatureExtractor\", \"DPTImageProcessor\"])\n@@ -1314,6 +1316,7 @@\n     _import_structure[\"models.convnext\"].append(\"ConvNextImageProcessorFast\")\n     _import_structure[\"models.deformable_detr\"].append(\"DeformableDetrImageProcessorFast\")\n     _import_structure[\"models.deit\"].append(\"DeiTImageProcessorFast\")\n+    _import_structure[\"models.depth_pro\"].append(\"DepthProImageProcessorFast\")\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n     _import_structure[\"models.llava\"].append(\"LlavaImageProcessorFast\")\n     _import_structure[\"models.llava_next\"].append(\"LlavaNextImageProcessorFast\")\n@@ -2181,6 +2184,13 @@\n             \"DepthAnythingPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.depth_pro\"].extend(\n+        [\n+            \"DepthProForDepthEstimation\",\n+            \"DepthProModel\",\n+            \"DepthProPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.detr\"].extend(\n         [\n             \"DetrForObjectDetection\",\n@@ -5498,6 +5508,7 @@\n         XLMProphetNetConfig,\n     )\n     from .models.depth_anything import DepthAnythingConfig\n+    from .models.depth_pro import DepthProConfig\n     from .models.detr import DetrConfig\n     from .models.diffllama import DiffLlamaConfig\n     from .models.dinat import DinatConfig\n@@ -6367,6 +6378,7 @@\n         from .models.deprecated.efficientformer import EfficientFormerImageProcessor\n         from .models.deprecated.tvlt import TvltImageProcessor\n         from .models.deprecated.vit_hybrid import ViTHybridImageProcessor\n+        from .models.depth_pro import DepthProImageProcessor, DepthProImageProcessorFast\n         from .models.detr import DetrFeatureExtractor, DetrImageProcessor\n         from .models.donut import DonutFeatureExtractor, DonutImageProcessor\n         from .models.dpt import DPTFeatureExtractor, DPTImageProcessor\n@@ -6460,6 +6472,7 @@\n         from .models.convnext import ConvNextImageProcessorFast\n         from .models.deformable_detr import DeformableDetrImageProcessorFast\n         from .models.deit import DeiTImageProcessorFast\n+        from .models.depth_pro import DepthProImageProcessorFast\n         from .models.detr import DetrImageProcessorFast\n         from .models.llava import LlavaImageProcessorFast\n         from .models.llava_next import LlavaNextImageProcessorFast\n@@ -7178,6 +7191,11 @@\n             DepthAnythingForDepthEstimation,\n             DepthAnythingPreTrainedModel,\n         )\n+        from .models.depth_pro import (\n+            DepthProForDepthEstimation,\n+            DepthProModel,\n+            DepthProPreTrainedModel,\n+        )\n         from .models.detr import (\n             DetrForObjectDetection,\n             DetrForSegmentation,"
        },
        {
            "sha": "d21d35212144b85412d31badda674753713db3bc",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -283,6 +283,7 @@ def resize(\n         image: \"torch.Tensor\",\n         size: SizeDict,\n         interpolation: \"F.InterpolationMode\" = None,\n+        antialias: bool = True,\n         **kwargs,\n     ) -> \"torch.Tensor\":\n         \"\"\"\n@@ -324,7 +325,7 @@ def resize(\n                 \"Size must contain 'height' and 'width' keys, or 'max_height' and 'max_width', or 'shortest_edge' key. Got\"\n                 f\" {size}.\"\n             )\n-        return F.resize(image, new_size, interpolation=interpolation)\n+        return F.resize(image, new_size, interpolation=interpolation, antialias=antialias)\n \n     def rescale(\n         self,"
        },
        {
            "sha": "1a8bef3e9e13cdee7d3805c37791775175cf2cff",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -74,6 +74,7 @@\n     deit,\n     deprecated,\n     depth_anything,\n+    depth_pro,\n     detr,\n     dialogpt,\n     diffllama,"
        },
        {
            "sha": "15d8e67001864899a66bb153905cf4c03367efe7",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -91,6 +91,7 @@\n         (\"deformable_detr\", \"DeformableDetrConfig\"),\n         (\"deit\", \"DeiTConfig\"),\n         (\"depth_anything\", \"DepthAnythingConfig\"),\n+        (\"depth_pro\", \"DepthProConfig\"),\n         (\"deta\", \"DetaConfig\"),\n         (\"detr\", \"DetrConfig\"),\n         (\"diffllama\", \"DiffLlamaConfig\"),\n@@ -415,6 +416,7 @@\n         (\"deplot\", \"DePlot\"),\n         (\"depth_anything\", \"Depth Anything\"),\n         (\"depth_anything_v2\", \"Depth Anything V2\"),\n+        (\"depth_pro\", \"DepthPro\"),\n         (\"deta\", \"DETA\"),\n         (\"detr\", \"DETR\"),\n         (\"dialogpt\", \"DialoGPT\"),"
        },
        {
            "sha": "724137bd62cd9403e2168cc3f49fe63c5fe75fd4",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -74,6 +74,7 @@\n             (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n             (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n             (\"depth_anything\", (\"DPTImageProcessor\",)),\n+            (\"depth_pro\", (\"DepthProImageProcessor\", \"DepthProImageProcessorFast\")),\n             (\"deta\", (\"DetaImageProcessor\",)),\n             (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n             (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "49d48d0912c319874eec8a5785facce09d19d9c9",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -89,6 +89,7 @@\n         (\"decision_transformer\", \"DecisionTransformerModel\"),\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n+        (\"depth_pro\", \"DepthProModel\"),\n         (\"deta\", \"DetaModel\"),\n         (\"detr\", \"DetrModel\"),\n         (\"diffllama\", \"DiffLlamaModel\"),\n@@ -598,6 +599,7 @@\n         (\"data2vec-vision\", \"Data2VecVisionModel\"),\n         (\"deformable_detr\", \"DeformableDetrModel\"),\n         (\"deit\", \"DeiTModel\"),\n+        (\"depth_pro\", \"DepthProModel\"),\n         (\"deta\", \"DetaModel\"),\n         (\"detr\", \"DetrModel\"),\n         (\"dinat\", \"DinatModel\"),\n@@ -918,6 +920,7 @@\n     [\n         # Model for depth estimation mapping\n         (\"depth_anything\", \"DepthAnythingForDepthEstimation\"),\n+        (\"depth_pro\", \"DepthProForDepthEstimation\"),\n         (\"dpt\", \"DPTForDepthEstimation\"),\n         (\"glpn\", \"GLPNForDepthEstimation\"),\n         (\"zoedepth\", \"ZoeDepthForDepthEstimation\"),"
        },
        {
            "sha": "5968aae67b523ef207b1ad9ed00199b412ee6cf5",
            "filename": "src/transformers/models/depth_pro/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2F__init__.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_depth_pro import *\n+    from .image_processing_depth_pro import *\n+    from .image_processing_depth_pro_fast import *\n+    from .modeling_depth_pro import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "36de741b704a38c8ce0ec565d15bbb9c0aae97ee",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "added",
            "additions": 205,
            "deletions": 0,
            "changes": 205,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,205 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"DepthPro model configuration\"\"\"\n+\n+from copy import deepcopy\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DepthProConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`DepthProModel`]. It is used to instantiate a\n+    DepthPro model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the DepthPro\n+    [apple/DepthPro](https://huggingface.co/apple/DepthPro) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        fusion_hidden_size (`int`, *optional*, defaults to 256):\n+            The number of channels before fusion.\n+        patch_size (`int`, *optional*, defaults to 384):\n+            The size (resolution) of each patch. This is also the image_size for backbone model.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        intermediate_hook_ids (`List[int]`, *optional*, defaults to `[11, 5]`):\n+            Indices of the intermediate hidden states from the patch encoder to use for fusion.\n+        intermediate_feature_dims (`List[int]`, *optional*, defaults to `[256, 256]`):\n+            Hidden state dimensions during upsampling for each intermediate hidden state in `intermediate_hook_ids`.\n+        scaled_images_ratios (`List[float]`, *optional*, defaults to `[0.25, 0.5, 1]`):\n+            Ratios of scaled images to be used by the patch encoder.\n+        scaled_images_overlap_ratios (`List[float]`, *optional*, defaults to `[0.0, 0.5, 0.25]`):\n+            Overlap ratios between patches for each scaled image in `scaled_images_ratios`.\n+        scaled_images_feature_dims (`List[int]`, *optional*, defaults to `[1024, 1024, 512]`):\n+            Hidden state dimensions during upsampling for each scaled image in `scaled_images_ratios`.\n+        merge_padding_value (`int`, *optional*, defaults to 3):\n+            When merging smaller patches back to the image size, overlapping sections of this size are removed.\n+        use_batch_norm_in_fusion_residual (`bool`, *optional*, defaults to `False`):\n+            Whether to use batch normalization in the pre-activate residual units of the fusion blocks.\n+        use_bias_in_fusion_residual (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the pre-activate residual units of the fusion blocks.\n+        use_fov_model (`bool`, *optional*, defaults to `False`):\n+            Whether to use `DepthProFovModel` to generate the field of view.\n+        num_fov_head_layers (`int`, *optional*, defaults to 2):\n+            Number of convolution layers in the head of `DepthProFovModel`.\n+        image_model_config (`Union[Dict[str, Any], PretrainedConfig]`, *optional*):\n+            The configuration of the image encoder model, which is loaded using the [`AutoModel`] API.\n+            By default, Dinov2 model is used as backbone.\n+        patch_model_config (`Union[Dict[str, Any], PretrainedConfig]`, *optional*):\n+            The configuration of the patch encoder model, which is loaded using the [`AutoModel`] API.\n+            By default, Dinov2 model is used as backbone.\n+        fov_model_config (`Union[Dict[str, Any], PretrainedConfig]`, *optional*):\n+            The configuration of the fov encoder model, which is loaded using the [`AutoModel`] API.\n+            By default, Dinov2 model is used as backbone.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import DepthProConfig, DepthProModel\n+\n+    >>> # Initializing a DepthPro apple/DepthPro style configuration\n+    >>> configuration = DepthProConfig()\n+\n+    >>> # Initializing a model (with random weights) from the apple/DepthPro style configuration\n+    >>> model = DepthProModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"depth_pro\"\n+    sub_configs = {\"image_model_config\": AutoConfig, \"patch_model_config\": AutoConfig, \"fov_model_config\": AutoConfig}\n+\n+    def __init__(\n+        self,\n+        fusion_hidden_size=256,\n+        patch_size=384,\n+        initializer_range=0.02,\n+        intermediate_hook_ids=[11, 5],\n+        intermediate_feature_dims=[256, 256],\n+        scaled_images_ratios=[0.25, 0.5, 1],\n+        scaled_images_overlap_ratios=[0.0, 0.5, 0.25],\n+        scaled_images_feature_dims=[1024, 1024, 512],\n+        merge_padding_value=3,\n+        use_batch_norm_in_fusion_residual=False,\n+        use_bias_in_fusion_residual=True,\n+        use_fov_model=False,\n+        num_fov_head_layers=2,\n+        image_model_config=None,\n+        patch_model_config=None,\n+        fov_model_config=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        # scaled_images_ratios is sorted\n+        if scaled_images_ratios != sorted(scaled_images_ratios):\n+            raise ValueError(\n+                f\"Values in scaled_images_ratios={scaled_images_ratios} \" \"should be sorted from low to high\"\n+            )\n+\n+        # scaled_images_ratios, scaled_images_overlap_ratios, scaled_images_feature_dims should be consistent\n+        if not (len(scaled_images_ratios) == len(scaled_images_overlap_ratios) == len(scaled_images_feature_dims)):\n+            raise ValueError(\n+                f\"len(scaled_images_ratios)={len(scaled_images_ratios)} and \"\n+                f\"len(scaled_images_overlap_ratios)={len(scaled_images_overlap_ratios)} and \"\n+                f\"len(scaled_images_feature_dims)={len(scaled_images_feature_dims)}, \"\n+                f\"should match in config.\"\n+            )\n+\n+        # intermediate_hook_ids, intermediate_feature_dims should be consistent\n+        if not (len(intermediate_hook_ids) == len(intermediate_feature_dims)):\n+            raise ValueError(\n+                f\"len(intermediate_hook_ids)={len(intermediate_hook_ids)} and \"\n+                f\"len(intermediate_feature_dims)={len(intermediate_feature_dims)}, \"\n+                f\"should match in config.\"\n+            )\n+\n+        # fusion_hidden_size should be consistent with num_fov_head_layers\n+        if fusion_hidden_size // 2**num_fov_head_layers == 0:\n+            raise ValueError(\n+                f\"fusion_hidden_size={fusion_hidden_size} should be consistent with num_fov_head_layers={num_fov_head_layers} \"\n+                \"i.e fusion_hidden_size // 2**num_fov_head_layers > 0\"\n+            )\n+\n+        self.fusion_hidden_size = fusion_hidden_size\n+        self.patch_size = patch_size\n+        self.initializer_range = initializer_range\n+        self.use_batch_norm_in_fusion_residual = use_batch_norm_in_fusion_residual\n+        self.use_bias_in_fusion_residual = use_bias_in_fusion_residual\n+        self.use_fov_model = use_fov_model\n+        self.num_fov_head_layers = num_fov_head_layers\n+        self.intermediate_hook_ids = intermediate_hook_ids\n+        self.intermediate_feature_dims = intermediate_feature_dims\n+        self.scaled_images_ratios = scaled_images_ratios\n+        self.scaled_images_overlap_ratios = scaled_images_overlap_ratios\n+        self.scaled_images_feature_dims = scaled_images_feature_dims\n+        self.merge_padding_value = merge_padding_value\n+        self.image_model_config = image_model_config\n+        self.patch_model_config = patch_model_config\n+        self.fov_model_config = fov_model_config\n+\n+        for sub_config_key in self.sub_configs.keys():\n+            sub_config = getattr(self, sub_config_key)\n+\n+            if sub_config is None:\n+                sub_config = CONFIG_MAPPING[\"dinov2\"](image_size=patch_size)\n+                logger.info(\n+                    f\"`{sub_config_key}` is `None`. Initializing `{sub_config_key}` with the `Dinov2Config` \"\n+                    f\"with default values except `{sub_config_key}.image_size` is set to `config.patch_size`.\"\n+                )\n+            elif isinstance(sub_config, dict):\n+                sub_config = deepcopy(sub_config)\n+                if \"model_type\" not in sub_config:\n+                    raise KeyError(\n+                        f\"The `model_type` key is missing in the `{sub_config_key}` dictionary. Please provide the model type.\"\n+                    )\n+                elif sub_config[\"model_type\"] not in CONFIG_MAPPING:\n+                    raise ValueError(\n+                        f\"The model type `{sub_config['model_type']}` in `{sub_config_key}` is not supported. Please provide a valid model type.\"\n+                    )\n+                image_size = sub_config.get(\"image_size\")\n+                if image_size != patch_size:\n+                    logger.info(\n+                        f\"The `image_size` in `{sub_config_key}` is set to `{image_size}`, \"\n+                        f\"but it does not match the required `patch_size` of `{patch_size}`. \"\n+                        f\"Updating `image_size` to `{patch_size}` for consistency. \"\n+                        f\"Ensure that `image_size` aligns with `patch_size` in the configuration.\"\n+                    )\n+                    sub_config.update({\"image_size\": patch_size})\n+                sub_config = CONFIG_MAPPING[sub_config[\"model_type\"]](**sub_config)\n+            elif isinstance(sub_config, PretrainedConfig):\n+                sub_config = sub_config\n+                image_size = getattr(sub_config, \"image_size\", None)\n+                if image_size != patch_size:\n+                    raise ValueError(\n+                        f\"`config.{sub_config_key}.image_size={image_size}` should match `config.patch_size={patch_size}`.\"\n+                    )\n+            else:\n+                raise TypeError(\n+                    f\"Invalid type for `sub_config`. Expected `PretrainedConfig`, `dict`, or `None`, but got {type(sub_config)}.\"\n+                )\n+\n+            setattr(self, sub_config_key, sub_config)\n+\n+\n+__all__ = [\"DepthProConfig\"]"
        },
        {
            "sha": "b24c6a5174f06159f21e9fb5a8c6d12cf2bafbd2",
            "filename": "src/transformers/models/depth_pro/convert_depth_pro_weights_to_hf.py",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,254 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import os\n+\n+import regex as re\n+import torch\n+from huggingface_hub import hf_hub_download\n+\n+from transformers import (\n+    DepthProConfig,\n+    DepthProForDepthEstimation,\n+    DepthProImageProcessorFast,\n+)\n+\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+\n+    # encoder\n+    r\"encoder.(patch|image)_encoder.cls_token\":                                 r\"depth_pro.encoder.\\1_encoder.model.embeddings.cls_token\",\n+    r\"encoder.(patch|image)_encoder.pos_embed\":                                 r\"depth_pro.encoder.\\1_encoder.model.embeddings.position_embeddings\",\n+    r\"encoder.(patch|image)_encoder.patch_embed.proj.(weight|bias)\":            r\"depth_pro.encoder.\\1_encoder.model.embeddings.patch_embeddings.projection.\\2\",\n+    r\"encoder.(patch|image)_encoder.blocks.(\\d+).norm(\\d+).(weight|bias)\":      r\"depth_pro.encoder.\\1_encoder.model.encoder.layer.\\2.norm\\3.\\4\",\n+    r\"encoder.(patch|image)_encoder.blocks.(\\d+).attn.qkv.(weight|bias)\":       r\"depth_pro.encoder.\\1_encoder.model.encoder.layer.\\2.attention.attention.(query|key|value).\\3\",\n+    r\"encoder.(patch|image)_encoder.blocks.(\\d+).attn.proj.(weight|bias)\":      r\"depth_pro.encoder.\\1_encoder.model.encoder.layer.\\2.attention.output.dense.\\3\",\n+    r\"encoder.(patch|image)_encoder.blocks.(\\d+).ls(\\d+).gamma\":                r\"depth_pro.encoder.\\1_encoder.model.encoder.layer.\\2.layer_scale\\3.lambda1\",\n+    r\"encoder.(patch|image)_encoder.blocks.(\\d+).mlp.fc(\\d+).(weight|bias)\":    r\"depth_pro.encoder.\\1_encoder.model.encoder.layer.\\2.mlp.fc\\3.\\4\",\n+    r\"encoder.(patch|image)_encoder.norm.(weight|bias)\":                        r\"depth_pro.encoder.\\1_encoder.model.layernorm.\\2\",\n+    r\"encoder.fuse_lowres.(weight|bias)\":                                       r\"depth_pro.neck.fuse_image_with_low_res.\\1\",\n+\n+    # fov\n+    r\"fov.encoder.0.cls_token\":                                                 r\"fov_model.fov_encoder.model.embeddings.cls_token\",\n+    r\"fov.encoder.0.pos_embed\":                                                 r\"fov_model.fov_encoder.model.embeddings.position_embeddings\",\n+    r\"fov.encoder.0.patch_embed.proj.(weight|bias)\":                            r\"fov_model.fov_encoder.model.embeddings.patch_embeddings.projection.\\1\",\n+    r\"fov.encoder.0.blocks.(\\d+).norm(\\d+).(weight|bias)\":                      r\"fov_model.fov_encoder.model.encoder.layer.\\1.norm\\2.\\3\",\n+    r\"fov.encoder.0.blocks.(\\d+).attn.qkv.(weight|bias)\":                       r\"fov_model.fov_encoder.model.encoder.layer.\\1.attention.attention.(query|key|value).\\2\",\n+    r\"fov.encoder.0.blocks.(\\d+).attn.proj.(weight|bias)\":                      r\"fov_model.fov_encoder.model.encoder.layer.\\1.attention.output.dense.\\2\",\n+    r\"fov.encoder.0.blocks.(\\d+).ls(\\d+).gamma\":                                r\"fov_model.fov_encoder.model.encoder.layer.\\1.layer_scale\\2.lambda1\",\n+    r\"fov.encoder.0.blocks.(\\d+).mlp.fc(\\d+).(weight|bias)\":                    r\"fov_model.fov_encoder.model.encoder.layer.\\1.mlp.fc\\2.\\3\",\n+    r\"fov.encoder.0.norm.(weight|bias)\":                                        r\"fov_model.fov_encoder.model.layernorm.\\1\",\n+    r\"fov.downsample.0.(weight|bias)\":                                          r\"fov_model.conv.\\1\",\n+    r\"fov.encoder.1.(weight|bias)\":                                             r\"fov_model.fov_encoder.neck.\\1\",\n+    r\"fov.head.(\\d+).(weight|bias)\":                                            r\"fov_model.head.layers.\\1.\\2\",\n+\n+    # head\n+    r\"head.(\\d+).(weight|bias)\":                                                r\"head.layers.\\1.\\2\",\n+\n+    # upsamples\n+    r\"encoder.upsample_lowres.(weight|bias)\":                                   r\"depth_pro.neck.feature_upsample.image_block.layers.0.\\1\",\n+    r\"encoder.upsample_latent(\\d+).(\\d+).(weight|bias)\": lambda match: (\n+        f\"depth_pro.neck.feature_upsample.intermediate.{1-int(match.group(1))}.layers.{match.group(2)}.{match.group(3)}\"\n+    ),\n+    r\"encoder.upsample(\\d+).(\\d+).(weight|bias)\": lambda match: (\n+        f\"depth_pro.neck.feature_upsample.scaled_images.{2-int(match.group(1))}.layers.{match.group(2)}.{match.group(3)}\"\n+    ),\n+\n+    # projections between encoder and fusion\n+    r\"decoder.convs.(\\d+).weight\": lambda match: (\n+        f\"depth_pro.neck.feature_projection.projections.{4-int(match.group(1))}.weight\"\n+    ),\n+\n+    # fusion stage\n+    r\"decoder.fusions.([1234]).resnet(\\d+).residual.(\\d+).(weight|bias)\": lambda match: (\n+        f\"fusion_stage.intermediate.{4-int(match.group(1))}.residual_layer{match.group(2)}.convolution{(int(match.group(3))+1)//2}.{match.group(4)}\"\n+    ),\n+    r\"decoder.fusions.0.resnet(\\d+).residual.(\\d+).(weight|bias)\": lambda match: (\n+        f\"fusion_stage.final.residual_layer{match.group(1)}.convolution{(int(match.group(2))+1)//2}.{match.group(3)}\"\n+    ),\n+    r\"decoder.fusions.([1234]).out_conv.(weight|bias)\": lambda match: (\n+        f\"fusion_stage.intermediate.{4-int(match.group(1))}.projection.{match.group(2)}\"\n+    ),\n+    r\"decoder.fusions.0.out_conv.(weight|bias)\": lambda match: (\n+        f\"fusion_stage.final.projection.{match.group(1)}\"\n+    ),\n+    r\"decoder.fusions.(\\d+).deconv.(weight|bias)\": lambda match: (\n+        f\"fusion_stage.intermediate.{4-int(match.group(1))}.deconv.{match.group(2)}\"\n+    ),\n+}\n+# fmt: on\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def get_qkv_state_dict(key, parameter):\n+    \"\"\"\n+    new key which looks like this\n+    xxxx.(q|k|v).xxx    (m, n)\n+\n+    is converted to\n+    xxxx.q.xxxx         (m//3, n)\n+    xxxx.k.xxxx         (m//3, n)\n+    xxxx.v.xxxx         (m//3, n)\n+    \"\"\"\n+    qkv_state_dict = {}\n+    placeholder = re.search(r\"(\\(.*?\\))\", key).group(1)  # finds   \"(query|key|value)\"\n+    replacements_keys = placeholder[1:-1].split(\"|\")  # creates ['query', 'key', 'value']\n+    replacements_vals = torch.split(\n+        parameter, split_size_or_sections=parameter.size(0) // len(replacements_keys), dim=0\n+    )\n+    for replacement_key, replacement_val in zip(replacements_keys, replacements_vals):\n+        qkv_state_dict[key.replace(placeholder, replacement_key)] = replacement_val\n+    return qkv_state_dict\n+\n+\n+def write_model(\n+    hf_repo_id: str,\n+    output_dir: str,\n+    safe_serialization: bool = True,\n+):\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # ------------------------------------------------------------\n+    # Create and save config\n+    # ------------------------------------------------------------\n+\n+    # create config\n+    backbone_config = {\n+        \"model_type\": \"dinov2\",\n+        \"num_hidden_layers\": 24,\n+        \"patch_size\": 16,\n+        \"hidden_size\": 1024,\n+        \"num_attention_heads\": 16,\n+        \"image_size\": 384,\n+        \"use_mask_token\": False,\n+    }\n+    config = DepthProConfig(\n+        # original implementation uses same config for all 3 models\n+        image_model_config=backbone_config,\n+        patch_model_config=backbone_config,\n+        fov_model_config=backbone_config,\n+        use_fov_model=True,\n+    )\n+\n+    # save config\n+    config.save_pretrained(output_dir)\n+    print(\"Model config saved successfully...\")\n+\n+    # ------------------------------------------------------------\n+    # Convert weights\n+    # ------------------------------------------------------------\n+\n+    # download and load state_dict from hf repo\n+    file_path = hf_hub_download(hf_repo_id, \"depth_pro.pt\")\n+    loaded = torch.load(file_path, weights_only=True)\n+\n+    print(\"Converting model...\")\n+    all_keys = list(loaded.keys())\n+    new_keys = convert_old_keys_to_new_keys(all_keys)\n+\n+    state_dict = {}\n+    for key in all_keys:\n+        new_key = new_keys[key]\n+        current_parameter = loaded.pop(key)\n+\n+        if \"qkv\" in key:\n+            qkv_state_dict = get_qkv_state_dict(new_key, current_parameter)\n+            state_dict.update(qkv_state_dict)\n+        else:\n+            state_dict[new_key] = current_parameter\n+\n+    print(\"Loading the checkpoint in a DepthPro model.\")\n+    model = DepthProForDepthEstimation(config)\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    model = DepthProForDepthEstimation.from_pretrained(output_dir, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+    return model\n+\n+\n+def write_image_processor(output_dir: str):\n+    image_processor = DepthProImageProcessorFast()\n+    image_processor.save_pretrained(output_dir)\n+    return image_processor\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_repo_id\",\n+        default=\"apple/DepthPro\",\n+        help=\"Location of official weights from apple on HF\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\"apple_DepthPro\",\n+        help=\"Location to write the converted model and processor\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        action=argparse.BooleanOptionalAction,\n+        help=\"Whether or not to push the converted model to the huggingface hub.\",\n+    )\n+    parser.add_argument(\n+        \"--hub_repo_id\",\n+        default=\"apple/DepthPro-hf\",\n+        help=\"Huggingface hub repo to write the converted model and processor\",\n+    )\n+    args = parser.parse_args()\n+\n+    model = write_model(\n+        hf_repo_id=args.hf_repo_id,\n+        output_dir=args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    image_processor = write_image_processor(\n+        output_dir=args.output_dir,\n+    )\n+\n+    if args.push_to_hub:\n+        print(\"Pushing to hub...\")\n+        model.push_to_hub(args.hub_repo_id)\n+        image_processor.push_to_hub(args.hub_repo_id)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "5871e0f764cdb53321f72793bce17505d2d31364",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro.py",
            "status": "added",
            "additions": 389,
            "deletions": 0,
            "changes": 389,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,389 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for DepthPro.\"\"\"\n+\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_depth_pro import DepthProDepthEstimatorOutput\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import to_channel_dimension_format\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    is_torch_available,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    to_numpy_array,\n+    valid_images,\n+)\n+from ...utils import (\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    logging,\n+    requires_backends,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class DepthProImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a DepthPro image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `(size[\"height\"],\n+            size[\"width\"])`. Can be overridden by the `do_resize` parameter in the `preprocess` method.\n+        size (`dict`, *optional*, defaults to `{\"height\": 1536, \"width\": 1536}`):\n+            Size of the output image after resizing. Can be overridden by the `size` parameter in the `preprocess`\n+            method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overridden by the `resample` parameter in the\n+            `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by the `do_rescale`\n+            parameter in the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by the `rescale_factor` parameter in the\n+            `preprocess` method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by the `do_normalize` parameter in the `preprocess`\n+            method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_MEAN`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `IMAGENET_STANDARD_STD`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"height\": 1536, \"width\": 1536}\n+        size = get_size_dict(size)\n+        self.do_resize = do_resize\n+        self.do_rescale = do_rescale\n+        self.do_normalize = do_normalize\n+        self.size = size\n+        self.resample = resample\n+        self.rescale_factor = rescale_factor\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image to `(size[\"height\"], size[\"width\"])`.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+                `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BILINEAR`.\n+            data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the output image. If unset, the channel dimension format of the input\n+                image is used. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        Returns:\n+            `np.ndarray`: The resized images.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        size = get_size_dict(size)\n+        if \"height\" not in size or \"width\" not in size:\n+            raise ValueError(f\"The `size` dictionary must contain the keys `height` and `width`. Got {size.keys()}\")\n+        output_size = (size[\"height\"], size[\"width\"])\n+\n+        # we use torch interpolation instead of image.resize because DepthProImageProcessor\n+        # rescales, then normalizes, which may cause some values to become negative, before resizing the image.\n+        # image.resize expects all values to be in range [0, 1] or [0, 255] and throws an exception otherwise,\n+        # however pytorch interpolation works with negative values.\n+        # relevant issue here: https://github.com/huggingface/transformers/issues/34920\n+        # input should be (B, C, H, W)\n+        image_tensor = torch.from_numpy(image).unsqueeze(0)\n+        resized_image = torch.nn.functional.interpolate(\n+            input=image_tensor,\n+            size=output_size,\n+            mode=pil_torch_interpolation_mapping[resample].value,\n+        )\n+        resized_image = resized_image.squeeze(0).numpy()\n+        return resized_image\n+\n+    def _validate_input_arguments(\n+        self,\n+        do_resize: bool,\n+        size: Dict[str, int],\n+        resample: PILImageResampling,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Union[float, List[float]],\n+        image_std: Union[float, List[float]],\n+        data_format: Union[str, ChannelDimension],\n+    ):\n+        if do_resize and None in (size, resample):\n+            raise ValueError(\"Size and resample must be specified if do_resize is True.\")\n+\n+        if do_rescale and rescale_factor is None:\n+            raise ValueError(\"Rescale factor must be specified if do_rescale is True.\")\n+\n+        if do_normalize and None in (image_mean, image_std):\n+            raise ValueError(\"Image mean and standard deviation must be specified if do_normalize is True.\")\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[Dict[str, int]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Dictionary in the format `{\"height\": h, \"width\": w}` specifying the size of the output image after\n+                resizing.\n+            resample (`PILImageResampling` filter, *optional*, defaults to `self.resample`):\n+                `PILImageResampling` filter to use if resizing the image e.g. `PILImageResampling.BILINEAR`. Only has\n+                an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use if `do_normalize` is set to `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        resample = resample if resample is not None else self.resample\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+\n+        size = size if size is not None else self.size\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        self._validate_input_arguments(\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            data_format=data_format,\n+        )\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        for image in images:\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            # depth-pro rescales and normalizes the image before resizing it\n+            # uses torch interpolation which requires ChannelDimension.FIRST\n+            if do_resize:\n+                image = to_channel_dimension_format(image, ChannelDimension.FIRST, input_channel_dim=input_data_format)\n+                image = self.resize(image=image, size=size, resample=resample)\n+                image = to_channel_dimension_format(image, data_format, input_channel_dim=ChannelDimension.FIRST)\n+            else:\n+                image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+\n+            all_images.append(image)\n+\n+        data = {\"pixel_values\": all_images}\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthProDepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+    ) -> Dict[str, List[TensorType]]:\n+        \"\"\"\n+        Post-processes the raw depth predictions from the model to generate\n+        final depth predictions which is caliberated using the field of view if provided\n+        and resized to specified target sizes if provided.\n+\n+        Args:\n+            outputs ([`DepthProDepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`Optional[Union[TensorType, List[Tuple[int, int]], None]]`, *optional*, defaults to `None`):\n+                Target sizes to resize the depth predictions. Can be a tensor of shape `(batch_size, 2)`\n+                or a list of tuples `(height, width)` for each image in the batch. If `None`, no resizing\n+                is performed.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions, and field of view (degrees) and focal length (pixels) if `field_of_view` is given in `outputs`.\n+\n+        Raises:\n+            `ValueError`:\n+                If the lengths of `predicted_depths`, `fovs`, or `target_sizes` are mismatched.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+        fov = outputs.field_of_view\n+\n+        batch_size = len(predicted_depth)\n+\n+        if target_sizes is not None and batch_size != len(target_sizes):\n+            raise ValueError(\n+                \"Make sure that you pass in as many fov values as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        fov = [None] * batch_size if fov is None else fov\n+        target_sizes = [None] * batch_size if target_sizes is None else target_sizes\n+        for depth, fov_value, target_size in zip(predicted_depth, fov, target_sizes):\n+            focal_length = None\n+            if target_size is not None:\n+                # scale image w.r.t fov\n+                if fov_value is not None:\n+                    width = target_size[1]\n+                    focal_length = 0.5 * width / torch.tan(0.5 * torch.deg2rad(fov_value))\n+                    depth = depth * width / focal_length\n+\n+                # interpolate\n+                depth = torch.nn.functional.interpolate(\n+                    # input should be (B, C, H, W)\n+                    input=depth.unsqueeze(0).unsqueeze(1),\n+                    size=target_size,\n+                    mode=pil_torch_interpolation_mapping[self.resample].value,\n+                ).squeeze()\n+\n+            # inverse the depth\n+            depth = 1.0 / torch.clamp(depth, min=1e-4, max=1e4)\n+\n+            results.append(\n+                {\n+                    \"predicted_depth\": depth,\n+                    \"field_of_view\": fov_value,\n+                    \"focal_length\": focal_length,\n+                }\n+            )\n+\n+        return results\n+\n+\n+__all__ = [\"DepthProImageProcessor\"]"
        },
        {
            "sha": "43a23bf10b5e21b9406a2c969f8375950f9db7fc",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "added",
            "additions": 187,
            "deletions": 0,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,187 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for DepthPro.\"\"\"\n+\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n+\n+from ...image_processing_base import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BaseImageProcessorFast,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    PILImageResampling,\n+    SizeDict,\n+)\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+    requires_backends,\n+)\n+\n+\n+if TYPE_CHECKING:\n+    from .modeling_depth_pro import DepthProDepthEstimatorOutput\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torchvision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast DepthPro image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+)\n+class DepthProImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 1536, \"width\": 1536}\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+\n+    # DepthPro resizes image after rescaling and normalizing,\n+    # which makes it different from BaseImageProcessorFast._preprocess\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        # Group images by size for batched scaling\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            if do_resize:\n+                stacked_images = self.resize(\n+                    image=stacked_images,\n+                    size=size,\n+                    interpolation=interpolation,\n+                    antialias=False,\n+                )\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+    # Copied from transformers.models.depth_pro.image_processing_depth_pro.DepthProImageProcessor.post_process_depth_estimation\n+    def post_process_depth_estimation(\n+        self,\n+        outputs: \"DepthProDepthEstimatorOutput\",\n+        target_sizes: Optional[Union[TensorType, List[Tuple[int, int]], None]] = None,\n+    ) -> Dict[str, List[TensorType]]:\n+        \"\"\"\n+        Post-processes the raw depth predictions from the model to generate\n+        final depth predictions which is caliberated using the field of view if provided\n+        and resized to specified target sizes if provided.\n+\n+        Args:\n+            outputs ([`DepthProDepthEstimatorOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`Optional[Union[TensorType, List[Tuple[int, int]], None]]`, *optional*, defaults to `None`):\n+                Target sizes to resize the depth predictions. Can be a tensor of shape `(batch_size, 2)`\n+                or a list of tuples `(height, width)` for each image in the batch. If `None`, no resizing\n+                is performed.\n+\n+        Returns:\n+            `List[Dict[str, TensorType]]`: A list of dictionaries of tensors representing the processed depth\n+            predictions, and field of view (degrees) and focal length (pixels) if `field_of_view` is given in `outputs`.\n+\n+        Raises:\n+            `ValueError`:\n+                If the lengths of `predicted_depths`, `fovs`, or `target_sizes` are mismatched.\n+        \"\"\"\n+        requires_backends(self, \"torch\")\n+\n+        predicted_depth = outputs.predicted_depth\n+        fov = outputs.field_of_view\n+\n+        batch_size = len(predicted_depth)\n+\n+        if target_sizes is not None and batch_size != len(target_sizes):\n+            raise ValueError(\n+                \"Make sure that you pass in as many fov values as the batch dimension of the predicted depth\"\n+            )\n+\n+        results = []\n+        fov = [None] * batch_size if fov is None else fov\n+        target_sizes = [None] * batch_size if target_sizes is None else target_sizes\n+        for depth, fov_value, target_size in zip(predicted_depth, fov, target_sizes):\n+            focal_length = None\n+            if target_size is not None:\n+                # scale image w.r.t fov\n+                if fov_value is not None:\n+                    width = target_size[1]\n+                    focal_length = 0.5 * width / torch.tan(0.5 * torch.deg2rad(fov_value))\n+                    depth = depth * width / focal_length\n+\n+                # interpolate\n+                depth = torch.nn.functional.interpolate(\n+                    # input should be (B, C, H, W)\n+                    input=depth.unsqueeze(0).unsqueeze(1),\n+                    size=target_size,\n+                    mode=pil_torch_interpolation_mapping[self.resample].value,\n+                ).squeeze()\n+\n+            # inverse the depth\n+            depth = 1.0 / torch.clamp(depth, min=1e-4, max=1e4)\n+\n+            results.append(\n+                {\n+                    \"predicted_depth\": depth,\n+                    \"field_of_view\": fov_value,\n+                    \"focal_length\": focal_length,\n+                }\n+            )\n+\n+        return results\n+\n+\n+__all__ = [\"DepthProImageProcessorFast\"]"
        },
        {
            "sha": "67715723d1338d0e1f9ea6ba472e62ff7c18baf2",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "added",
            "additions": 1218,
            "deletions": 0,
            "changes": 1218,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,1218 @@\n+# coding=utf-8\n+# Copyright 2024 The Apple Research Team Authors and The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch DepthPro model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...modeling_outputs import BaseModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n+from ..auto import AutoModel\n+from .configuration_depth_pro import DepthProConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+@dataclass\n+class DepthProOutput(ModelOutput):\n+    \"\"\"\n+    Base class for DepthPro's outputs.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        features (`Union[torch.FloatTensor, List[torch.FloatTensor]]`, *optional*):\n+            Features from encoders. Can be a single feature or a list of features.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer and the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, n_patches_per_batch, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    features: Union[torch.FloatTensor, List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+class DepthProDepthEstimatorOutput(ModelOutput):\n+    \"\"\"\n+    Base class for DepthProForDepthEstimation's output.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Classification (or regression if config.num_labels==1) loss.\n+        predicted_depth (`torch.FloatTensor` of shape `(batch_size, height, width)`):\n+            Predicted depth for each pixel.\n+        field_of_view (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned when `use_fov_model` is provided):\n+            Field of View Scaler.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, n_patches_per_batch, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer and the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, n_patches_per_batch, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    predicted_depth: torch.FloatTensor = None\n+    field_of_view: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+def split_to_patches(pixel_values: torch.Tensor, patch_size: int, overlap_ratio: float) -> torch.Tensor:\n+    \"\"\"Creates Patches from Batch.\"\"\"\n+    batch_size, num_channels, height, width = pixel_values.shape\n+\n+    if height == width == patch_size:\n+        # create patches only if scaled image is not already equal to patch size\n+        return pixel_values\n+\n+    stride = torch_int(patch_size * (1 - overlap_ratio))\n+\n+    patches = F.unfold(pixel_values, kernel_size=(patch_size, patch_size), stride=(stride, stride))\n+    patches = patches.permute(2, 0, 1)\n+    patches = patches.reshape(-1, num_channels, patch_size, patch_size)\n+\n+    return patches\n+\n+\n+def reshape_features(hidden_states: torch.Tensor) -> torch.Tensor:\n+    \"\"\"Discard class token and reshape 1D feature map to a 2D grid.\"\"\"\n+    n_samples, seq_len, hidden_size = hidden_states.shape\n+    size = torch_int(seq_len**0.5)\n+\n+    hidden_states = hidden_states[:, -(size**2) :, :]  # remove special tokens if there are any\n+    hidden_states = hidden_states.reshape(n_samples, size, size, hidden_size)\n+    hidden_states = hidden_states.permute(0, 3, 1, 2)\n+\n+    return hidden_states\n+\n+\n+def merge_patches(patches: torch.Tensor, batch_size: int, padding: int) -> torch.Tensor:\n+    \"\"\"Merges smaller patches into image-like feature map.\"\"\"\n+    n_patches, hidden_size, out_size, out_size = patches.shape\n+    n_patches_per_batch = n_patches // batch_size\n+    sqrt_n_patches_per_batch = torch_int(n_patches_per_batch**0.5)\n+    new_out_size = sqrt_n_patches_per_batch * out_size\n+\n+    if n_patches == batch_size:\n+        # merge only if the patches were created from scaled image\n+        # patches are not created when scaled image size is equal to patch size\n+        return patches\n+\n+    if n_patches_per_batch < 4:\n+        # for each batch, atleast 4 small patches are required to\n+        # recreate a large square patch from merging them and later padding is applied\n+        # 3 x (8x8) patches becomes 1 x ( 8x8 ) patch (extra patch ignored, no padding)\n+        # 4 x (8x8) patches becomes 1 x (16x16) patch (padding later)\n+        # 5 x (8x8) patches becomes 1 x (16x16) patch (extra patch ignored, padding later)\n+        # 9 x (8x8) patches becomes 1 x (24x24) patch (padding later)\n+        # thus the following code only rearranges the patches and removes extra ones\n+        padding = 0\n+\n+    # make sure padding is not large enough to remove more than half of the patch\n+    padding = min(out_size // 4, padding)\n+\n+    if padding == 0:\n+        # faster when no padding is required\n+        merged = patches.reshape(n_patches_per_batch, batch_size, hidden_size, out_size, out_size)\n+        merged = merged.permute(1, 2, 0, 3, 4)\n+        merged = merged[:, :, : sqrt_n_patches_per_batch**2, :, :]\n+        merged = merged.reshape(\n+            batch_size, hidden_size, sqrt_n_patches_per_batch, sqrt_n_patches_per_batch, out_size, out_size\n+        )\n+        merged = merged.permute(0, 1, 2, 4, 3, 5)\n+        merged = merged.reshape(batch_size, hidden_size, new_out_size, new_out_size)\n+    else:\n+        # padding example:\n+        # let out_size = 8, new_out_size = 32, padding = 2\n+        # each patch is separated by \"|\"\n+        # and padding is applied to the merging edges of each patch\n+        # 00 01 02 03 04 05 06 07 | 08 09 10 11 12 13 14 15 | 16 17 18 19 20 21 22 23 | 24 25 26 27 28 29 30 31\n+        # 00 01 02 03 04 05 -- -- | -- -- 10 11 12 13 -- -- | -- -- 18 19 20 21 -- -- | -- -- 26 27 28 29 30 31\n+        i = 0\n+        boxes = []\n+        for h in range(sqrt_n_patches_per_batch):\n+            boxes_in_row = []\n+            for w in range(sqrt_n_patches_per_batch):\n+                box = patches[batch_size * i : batch_size * (i + 1)]\n+\n+                # collect paddings\n+                paddings = [0, 0, 0, 0]\n+                if h != 0:\n+                    # remove pad from height if box is not at top border\n+                    paddings[0] = padding\n+                if w != 0:\n+                    # remove pad from width if box is not at left border\n+                    paddings[2] = padding\n+                if h != sqrt_n_patches_per_batch - 1:\n+                    # remove pad from height if box is not at bottom border\n+                    paddings[1] = padding\n+                if w != sqrt_n_patches_per_batch - 1:\n+                    # remove pad from width if box is not at right border\n+                    paddings[3] = padding\n+\n+                # remove paddings\n+                _, _, box_h, box_w = box.shape\n+                pad_top, pad_bottom, pad_left, pad_right = paddings\n+                box = box[:, :, pad_top : box_h - pad_bottom, pad_left : box_w - pad_right]\n+\n+                boxes_in_row.append(box)\n+                i += 1\n+            boxes_in_row = torch.cat(boxes_in_row, dim=-1)\n+            boxes.append(boxes_in_row)\n+        merged = torch.cat(boxes, dim=-2)\n+\n+    return merged\n+\n+\n+def reconstruct_feature_maps(\n+    hidden_state: torch.Tensor, batch_size: int, padding: int, output_size: Tuple[float, float]\n+) -> torch.Tensor:\n+    \"\"\"\n+    Reconstructs feature maps from the hidden state produced by any of the encoder. Converts the hidden state of shape\n+    `(n_patches_per_batch * batch_size, seq_len, hidden_size)` to feature maps of shape\n+    `(batch_size, hidden_size, output_size[0], output_size[1])`.\n+\n+    Args:\n+        hidden_state (torch.Tensor): Input tensor of shape `(n_patches_per_batch * batch_size, seq_len, hidden_size)`\n+            representing the encoded patches.\n+        batch_size (int): The number of samples in a batch.\n+        padding (int): The amount of padding to be removed when merging patches.\n+        output_size (Tuple[float, float]): The desired output size for the feature maps, specified as `(height, width)`.\n+\n+    Returns:\n+        torch.Tensor: Reconstructed feature maps of shape `(batch_size, hidden_size, output_size[0], output_size[1])`.\n+    \"\"\"\n+    # reshape back to image like\n+    features = reshape_features(hidden_state)\n+\n+    # merge all patches in a batch to create one large patch per batch\n+    features = merge_patches(\n+        features,\n+        batch_size=batch_size,\n+        padding=padding,\n+    )\n+\n+    # interpolate patches to base size\n+    features = F.interpolate(\n+        features,\n+        size=output_size,\n+        mode=\"bilinear\",\n+        align_corners=False,\n+    )\n+\n+    return features\n+\n+\n+class DepthProPatchEncoder(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.intermediate_hook_ids = config.intermediate_hook_ids\n+        self.intermediate_feature_dims = config.intermediate_feature_dims\n+        self.scaled_images_ratios = config.scaled_images_ratios\n+        self.scaled_images_overlap_ratios = config.scaled_images_overlap_ratios\n+        self.scaled_images_feature_dims = config.scaled_images_feature_dims\n+        self.merge_padding_value = config.merge_padding_value\n+\n+        self.n_scaled_images = len(config.scaled_images_ratios)\n+        self.n_intermediate_hooks = len(config.intermediate_hook_ids)\n+        self.out_size = config.image_model_config.image_size // config.image_model_config.patch_size\n+\n+        self.model = AutoModel.from_config(config.patch_model_config)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+    ) -> List[torch.Tensor]:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+\n+        if min(self.scaled_images_ratios) * min(height, width) < self.config.patch_size:\n+            raise ValueError(\n+                f\"Image size {height}x{width} is too small to be scaled \"\n+                f\"with scaled_images_ratios={self.scaled_images_ratios} \"\n+                f\"when patch_size={self.config.patch_size}.\"\n+            )\n+\n+        # STEP 1: create 3-level image\n+\n+        scaled_images = []\n+        for ratio in self.scaled_images_ratios:\n+            scaled_images.append(\n+                F.interpolate(\n+                    pixel_values,\n+                    scale_factor=ratio,\n+                    mode=\"bilinear\",\n+                    align_corners=False,\n+                )\n+            )\n+\n+        # STEP 2: create patches\n+\n+        for i in range(self.n_scaled_images):\n+            scaled_images[i] = split_to_patches(\n+                scaled_images[i],\n+                patch_size=self.config.patch_size,\n+                overlap_ratio=self.scaled_images_overlap_ratios[i],\n+            )\n+        n_patches_per_scaled_image = [len(i) for i in scaled_images]\n+        patches = torch.cat(scaled_images[::-1], dim=0)  # -1 as patch encoder expects high res patches first\n+\n+        # STEP 3: apply patch encoder\n+\n+        encodings = self.model(\n+            # each patch is processed as a separate batch\n+            patches,\n+            head_mask=head_mask,\n+            # required for intermediate features\n+            output_hidden_states=self.n_intermediate_hooks > 0,\n+        )\n+\n+        scaled_images_last_hidden_state = torch.split_with_sizes(encodings[0], n_patches_per_scaled_image[::-1])\n+        # -1 (reverse list) as patch encoder returns high res patches first, we need low res first\n+        scaled_images_last_hidden_state = scaled_images_last_hidden_state[::-1]\n+\n+        # calculate base height and width\n+        # base height and width are the dimensions of the lowest resolution features\n+        exponent_value = torch_int(math.log2(width / self.out_size))\n+        base_height = height // 2**exponent_value\n+        base_width = width // 2**exponent_value\n+\n+        # STEP 4: get patch features (high_res, med_res, low_res) - (3-5) in diagram\n+\n+        scaled_images_features = []\n+        for i in range(self.n_scaled_images):\n+            hidden_state = scaled_images_last_hidden_state[i]\n+            batch_size = batch_size\n+            padding = torch_int(self.merge_padding_value * (1 / self.scaled_images_ratios[i]))\n+            output_height = base_height * 2**i\n+            output_width = base_width * 2**i\n+            features = reconstruct_feature_maps(\n+                hidden_state,\n+                batch_size=batch_size,\n+                padding=padding,\n+                output_size=(output_height, output_width),\n+            )\n+            scaled_images_features.append(features)\n+\n+        # STEP 5: get intermediate features - (1-2) in diagram\n+\n+        intermediate_features = []\n+        for i in range(self.n_intermediate_hooks):\n+            # +1 to correct index position as hidden_states contain embedding output as well\n+            hidden_state = encodings[2][self.intermediate_hook_ids[i] + 1]\n+            padding = torch_int(self.merge_padding_value * (1 / self.scaled_images_ratios[-1]))\n+            output_height = base_height * 2 ** (self.n_scaled_images - 1)\n+            output_width = base_width * 2 ** (self.n_scaled_images - 1)\n+            features = reconstruct_feature_maps(\n+                hidden_state,\n+                batch_size=batch_size,\n+                padding=padding,\n+                output_size=(output_height, output_width),\n+            )\n+            intermediate_features.append(features)\n+\n+        # STEP 7: combine all features\n+        features = [*scaled_images_features, *intermediate_features]\n+\n+        return features\n+\n+\n+class DepthProImageEncoder(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.out_size = config.image_model_config.image_size // config.image_model_config.patch_size\n+\n+        self.model = AutoModel.from_config(config.image_model_config)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+        return_dict: bool = True,\n+    ) -> Union[tuple, DepthProOutput]:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+\n+        # scale the image for image_encoder\n+        size = self.config.image_model_config.image_size\n+        pixel_values = F.interpolate(\n+            pixel_values,\n+            size=(size, size),\n+            mode=\"bilinear\",\n+            align_corners=False,\n+        )\n+        encodings = self.model(\n+            pixel_values=pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        # calculate base height and width\n+        # base height and width are the dimensions of the lowest resolution features\n+        exponent_value = torch_int(math.log2(width / self.out_size))\n+        base_height = height // 2**exponent_value\n+        base_width = width // 2**exponent_value\n+\n+        features = reconstruct_feature_maps(\n+            encodings[0],\n+            batch_size=batch_size,\n+            padding=0,\n+            output_size=(base_height, base_width),\n+        )\n+\n+        if not return_dict:\n+            return (encodings[0], features) + encodings[2:]  # ignore last_hidden_state and poooler output\n+\n+        return DepthProOutput(\n+            last_hidden_state=encodings.last_hidden_state,\n+            features=features,\n+            hidden_states=encodings.hidden_states,\n+            attentions=encodings.attentions,\n+        )\n+\n+\n+class DepthProEncoder(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.intermediate_hook_ids = config.intermediate_hook_ids\n+        self.intermediate_feature_dims = config.intermediate_feature_dims\n+        self.scaled_images_ratios = config.scaled_images_ratios\n+        self.scaled_images_overlap_ratios = config.scaled_images_overlap_ratios\n+        self.scaled_images_feature_dims = config.scaled_images_feature_dims\n+        self.merge_padding_value = config.merge_padding_value\n+\n+        self.n_scaled_images = len(self.scaled_images_ratios)\n+        self.n_intermediate_hooks = len(self.intermediate_hook_ids)\n+\n+        self.patch_encoder = DepthProPatchEncoder(config)\n+        self.image_encoder = DepthProImageEncoder(config)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        output_hidden_states: bool = False,\n+        return_dict: bool = True,\n+    ) -> Union[tuple, DepthProOutput]:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+\n+        patch_features = self.patch_encoder(\n+            pixel_values,\n+            head_mask=head_mask,\n+        )\n+        image_encodings = self.image_encoder(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        image_features = image_encodings[1]  # index 1 contains features\n+\n+        features = [image_features, *patch_features]\n+\n+        if not return_dict:\n+            return (image_encodings[0], features) + image_encodings[2:]\n+\n+        return DepthProOutput(\n+            last_hidden_state=image_encodings.last_hidden_state,\n+            features=features,\n+            hidden_states=image_encodings.hidden_states,\n+            attentions=image_encodings.attentions,\n+        )\n+\n+\n+class DepthProFeatureUpsampleBlock(nn.Module):\n+    def __init__(\n+        self,\n+        config: DepthProConfig,\n+        input_dims: int,\n+        intermediate_dims: int,\n+        output_dims: int,\n+        n_upsample_layers: int,\n+        use_proj: bool = True,\n+        bias: bool = False,\n+    ):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList()\n+\n+        # create first projection layer\n+        if use_proj:\n+            proj = nn.Conv2d(\n+                in_channels=input_dims,\n+                out_channels=intermediate_dims,\n+                kernel_size=1,\n+                stride=1,\n+                padding=0,\n+                bias=bias,\n+            )\n+            self.layers.append(proj)\n+\n+        # create following upsample layers\n+        for i in range(n_upsample_layers):\n+            in_channels = intermediate_dims if i == 0 else output_dims\n+            layer = nn.ConvTranspose2d(\n+                in_channels=in_channels,\n+                out_channels=output_dims,\n+                kernel_size=2,\n+                stride=2,\n+                padding=0,\n+                bias=bias,\n+            )\n+            self.layers.append(layer)\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        for layer in self.layers:\n+            features = layer(features)\n+        return features\n+\n+\n+class DepthProFeatureUpsample(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.n_scaled_images = len(self.config.scaled_images_ratios)\n+        self.n_intermediate_hooks = len(self.config.intermediate_hook_ids)\n+\n+        # for image_features\n+        self.image_block = DepthProFeatureUpsampleBlock(\n+            config=config,\n+            input_dims=config.image_model_config.hidden_size,\n+            intermediate_dims=config.image_model_config.hidden_size,\n+            output_dims=config.scaled_images_feature_dims[0],\n+            n_upsample_layers=1,\n+            use_proj=False,\n+            bias=True,\n+        )\n+\n+        # for scaled_images_features\n+        self.scaled_images = nn.ModuleList()\n+        for i, feature_dims in enumerate(config.scaled_images_feature_dims):\n+            block = DepthProFeatureUpsampleBlock(\n+                config=config,\n+                input_dims=config.patch_model_config.hidden_size,\n+                intermediate_dims=feature_dims,\n+                output_dims=feature_dims,\n+                n_upsample_layers=1,\n+            )\n+            self.scaled_images.append(block)\n+\n+        # for intermediate_features\n+        self.intermediate = nn.ModuleList()\n+        for i, feature_dims in enumerate(config.intermediate_feature_dims):\n+            intermediate_dims = config.fusion_hidden_size if i == 0 else feature_dims\n+            block = DepthProFeatureUpsampleBlock(\n+                config=config,\n+                input_dims=config.patch_model_config.hidden_size,\n+                intermediate_dims=intermediate_dims,\n+                output_dims=feature_dims,\n+                n_upsample_layers=2 + i,\n+            )\n+            self.intermediate.append(block)\n+\n+    def forward(self, features: List[torch.Tensor]) -> List[torch.Tensor]:\n+        features[0] = self.image_block(features[0])\n+\n+        for i in range(self.n_scaled_images):\n+            features[i + 1] = self.scaled_images[i](features[i + 1])\n+\n+        for i in range(self.n_intermediate_hooks):\n+            features[self.n_scaled_images + i + 1] = self.intermediate[i](features[self.n_scaled_images + i + 1])\n+\n+        return features\n+\n+\n+class DepthProFeatureProjection(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        combined_feature_dims = config.scaled_images_feature_dims + config.intermediate_feature_dims\n+        self.projections = nn.ModuleList()\n+        for i, in_channels in enumerate(combined_feature_dims):\n+            if i == len(combined_feature_dims) - 1 and in_channels == config.fusion_hidden_size:\n+                # projection for last layer can be ignored if input and output channels already match\n+                self.projections.append(nn.Identity())\n+            else:\n+                self.projections.append(\n+                    nn.Conv2d(\n+                        in_channels=in_channels,\n+                        out_channels=config.fusion_hidden_size,\n+                        kernel_size=3,\n+                        stride=1,\n+                        padding=1,\n+                        bias=False,\n+                    )\n+                )\n+\n+    def forward(self, features: List[torch.Tensor]) -> List[torch.Tensor]:\n+        projected_features = []\n+        for i, projection in enumerate(self.projections):\n+            upsampled_feature = projection(features[i])\n+            projected_features.append(upsampled_feature)\n+        return projected_features\n+\n+\n+class DepthProNeck(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+\n+        self.feature_upsample = DepthProFeatureUpsample(config)\n+        self.fuse_image_with_low_res = nn.Conv2d(\n+            in_channels=config.scaled_images_feature_dims[0] * 2,\n+            out_channels=config.scaled_images_feature_dims[0],\n+            kernel_size=1,\n+            stride=1,\n+            padding=0,\n+            bias=True,\n+        )\n+        self.feature_projection = DepthProFeatureProjection(config)\n+\n+    def forward(self, features: List[torch.Tensor]) -> List[torch.Tensor]:\n+        features = self.feature_upsample(features)\n+        # global features = low res features + image features\n+        global_features = torch.cat((features[1], features[0]), dim=1)\n+        global_features = self.fuse_image_with_low_res(global_features)\n+        features = [global_features, *features[2:]]\n+        features = self.feature_projection(features)\n+        return features\n+\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"DepthProConfig\"\n+\n+\n+DEPTH_PRO_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+DEPTH_PRO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`DPTImageProcessor.__call__`]\n+            for details.\n+\n+        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n+            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+DEPTH_PRO_FOR_DEPTH_ESTIMATION_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`DepthProConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+        use_fov_model (`bool`, *optional*, defaults to `True`):\n+            Whether to use `DepthProFovModel` to generate the field of view.\n+\"\"\"\n+\n+\n+class DepthProPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = DepthProConfig\n+    base_model_prefix = \"depth_pro\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+    _no_split_modules = [\"DepthProPreActResidualLayer\"]\n+    _keys_to_ignore_on_load_unexpected = [\"fov_model.*\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, nn.Linear):\n+            # Slightly different from the TF version which uses truncated_normal for initialization\n+            # cf https://github.com/pytorch/pytorch/pull/5617\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, (nn.Conv2d, nn.ConvTranspose2d)):\n+            nn.init.kaiming_normal_(module.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+@add_start_docstrings(\n+    \"The bare DepthPro Model transformer outputting raw hidden-states without any specific head on top.\",\n+    DEPTH_PRO_START_DOCSTRING,\n+)\n+class DepthProModel(DepthProPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.config = config\n+        self.encoder = DepthProEncoder(config)\n+        self.neck = DepthProNeck(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.encoder.image_encoder.model.get_input_embeddings()\n+\n+    @add_start_docstrings_to_model_forward(DEPTH_PRO_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, DepthProOutput]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, DepthProModel\n+\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> checkpoint = \"apple/DepthPro-hf\"\n+        >>> processor = AutoProcessor.from_pretrained(checkpoint)\n+        >>> model = DepthProModel.from_pretrained(checkpoint)\n+\n+        >>> # prepare image for the model\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     output = model(**inputs)\n+\n+        >>> output.last_hidden_state.shape\n+        torch.Size([1, 35, 577, 1024])\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        encodings = self.encoder(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        features = encodings[1]  # index 1 contains features\n+        features = self.neck(features)\n+\n+        if not return_dict:\n+            return (encodings[0], features) + encodings[2:]\n+\n+        return DepthProOutput(\n+            last_hidden_state=encodings.last_hidden_state,\n+            features=features,\n+            hidden_states=encodings.hidden_states,\n+            attentions=encodings.attentions,\n+        )\n+\n+\n+# Copied from transformers.models.dpt.modeling_dpt.DPTPreActResidualLayer DPT->DepthPro\n+class DepthProPreActResidualLayer(nn.Module):\n+    \"\"\"\n+    ResidualConvUnit, pre-activate residual unit.\n+\n+    Args:\n+        config (`[DepthProConfig]`):\n+            Model configuration class defining the model architecture.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.use_batch_norm = config.use_batch_norm_in_fusion_residual\n+        use_bias_in_fusion_residual = (\n+            config.use_bias_in_fusion_residual\n+            if config.use_bias_in_fusion_residual is not None\n+            else not self.use_batch_norm\n+        )\n+\n+        self.activation1 = nn.ReLU()\n+        self.convolution1 = nn.Conv2d(\n+            config.fusion_hidden_size,\n+            config.fusion_hidden_size,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+            bias=use_bias_in_fusion_residual,\n+        )\n+\n+        self.activation2 = nn.ReLU()\n+        self.convolution2 = nn.Conv2d(\n+            config.fusion_hidden_size,\n+            config.fusion_hidden_size,\n+            kernel_size=3,\n+            stride=1,\n+            padding=1,\n+            bias=use_bias_in_fusion_residual,\n+        )\n+\n+        if self.use_batch_norm:\n+            self.batch_norm1 = nn.BatchNorm2d(config.fusion_hidden_size)\n+            self.batch_norm2 = nn.BatchNorm2d(config.fusion_hidden_size)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        residual = hidden_state\n+        hidden_state = self.activation1(hidden_state)\n+\n+        hidden_state = self.convolution1(hidden_state)\n+\n+        if self.use_batch_norm:\n+            hidden_state = self.batch_norm1(hidden_state)\n+\n+        hidden_state = self.activation2(hidden_state)\n+        hidden_state = self.convolution2(hidden_state)\n+\n+        if self.use_batch_norm:\n+            hidden_state = self.batch_norm2(hidden_state)\n+\n+        return hidden_state + residual\n+\n+\n+# Modified from transformers.models.dpt.modeling_dpt.DPTFeatureFusionLayer\n+# except it uses deconv and skip_add and needs no interpolation\n+class DepthProFeatureFusionLayer(nn.Module):\n+    def __init__(self, config: DepthProConfig, use_deconv: bool = True):\n+        super().__init__()\n+        self.config = config\n+        self.use_deconv = use_deconv\n+\n+        self.residual_layer1 = DepthProPreActResidualLayer(config)\n+        self.residual_layer2 = DepthProPreActResidualLayer(config)\n+\n+        if self.use_deconv:\n+            self.deconv = nn.ConvTranspose2d(\n+                in_channels=config.fusion_hidden_size,\n+                out_channels=config.fusion_hidden_size,\n+                kernel_size=2,\n+                stride=2,\n+                padding=0,\n+                bias=False,\n+            )\n+\n+        self.projection = nn.Conv2d(config.fusion_hidden_size, config.fusion_hidden_size, kernel_size=1, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor, residual: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        if residual is not None:\n+            residual = self.residual_layer1(residual)\n+            hidden_state = hidden_state + residual\n+\n+        hidden_state = self.residual_layer2(hidden_state)\n+        if self.use_deconv:\n+            hidden_state = self.deconv(hidden_state)\n+        hidden_state = self.projection(hidden_state)\n+\n+        return hidden_state\n+\n+\n+# Modified from transformers.models.dpt.modeling_dpt.DPTFeatureFusionStage with DPT->DepthPro\n+# with deconv and reversed layers\n+class DepthProFeatureFusionStage(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        self.num_layers = len(config.intermediate_hook_ids) + len(config.scaled_images_ratios)\n+        self.intermediate = nn.ModuleList()\n+        for _ in range(self.num_layers - 1):\n+            self.intermediate.append(DepthProFeatureFusionLayer(config))\n+\n+        # final layer doesnot require deconvolution\n+        self.final = DepthProFeatureFusionLayer(config, use_deconv=False)\n+\n+    def forward(self, hidden_states: List[torch.Tensor]) -> List[torch.Tensor]:\n+        if self.num_layers != len(hidden_states):\n+            raise ValueError(\n+                f\"num_layers={self.num_layers} in DepthProFeatureFusionStage\"\n+                f\"doesnot match len(hidden_states)={len(hidden_states)}\"\n+            )\n+\n+        fused_hidden_states = []\n+        fused_hidden_state = None\n+        for hidden_state, layer in zip(hidden_states[:-1], self.intermediate):\n+            if fused_hidden_state is None:\n+                # first layer only uses the last hidden_state\n+                fused_hidden_state = layer(hidden_state)\n+            else:\n+                fused_hidden_state = layer(fused_hidden_state, hidden_state)\n+            fused_hidden_states.append(fused_hidden_state)\n+\n+        hidden_state = hidden_states[-1]\n+        fused_hidden_state = self.final(fused_hidden_state, hidden_state)\n+        fused_hidden_states.append(fused_hidden_state)\n+\n+        return fused_hidden_states\n+\n+\n+class DepthProFovEncoder(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.out_size = config.image_model_config.image_size // config.image_model_config.patch_size\n+\n+        self.model = AutoModel.from_config(config.fov_model_config)\n+        self.neck = nn.Linear(config.fov_model_config.hidden_size, config.fusion_hidden_size // 2)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        batch_size, num_channels, height, width = pixel_values.shape\n+\n+        # scale the image for fov_encoder\n+        size = self.config.fov_model_config.image_size\n+        pixel_values = F.interpolate(\n+            pixel_values,\n+            size=(size, size),\n+            mode=\"bilinear\",\n+            align_corners=False,\n+        )\n+        encodings = self.model(\n+            pixel_values=pixel_values,\n+            head_mask=head_mask,\n+        )\n+        hidden_state = encodings[0]\n+        hidden_state = self.neck(hidden_state)\n+\n+        # calculate base height and width\n+        # base height and width are the dimensions of the lowest resolution features\n+        exponent_value = torch_int(math.log2(width / self.out_size))\n+        base_height = height // 2**exponent_value\n+        base_width = width // 2**exponent_value\n+\n+        features = reconstruct_feature_maps(\n+            hidden_state,\n+            batch_size=batch_size,\n+            padding=0,\n+            output_size=(base_height, base_width),\n+        )\n+\n+        return features\n+\n+\n+class DepthProFovHead(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.fusion_hidden_size = config.fusion_hidden_size\n+        self.out_size = config.image_model_config.image_size // config.image_model_config.patch_size\n+\n+        # create initial head layers\n+        self.layers = nn.ModuleList()\n+        for i in range(config.num_fov_head_layers):\n+            self.layers.append(\n+                nn.Conv2d(\n+                    math.ceil(self.fusion_hidden_size / 2 ** (i + 1)),\n+                    math.ceil(self.fusion_hidden_size / 2 ** (i + 2)),\n+                    kernel_size=3,\n+                    stride=2,\n+                    padding=1,\n+                )\n+            )\n+            self.layers.append(nn.ReLU(True))\n+        # calculate expected shapes to finally generate a scalar output from final head layer\n+        final_in_channels = math.ceil(self.fusion_hidden_size / 2 ** (config.num_fov_head_layers + 1))\n+        final_kernel_size = torch_int((self.out_size - 1) / 2**config.num_fov_head_layers + 1)\n+        self.layers.append(\n+            nn.Conv2d(\n+                in_channels=final_in_channels, out_channels=1, kernel_size=final_kernel_size, stride=1, padding=0\n+            )\n+        )\n+\n+    def forward(self, features: torch.Tensor) -> torch.Tensor:\n+        features = F.interpolate(\n+            features,\n+            size=(self.out_size, self.out_size),\n+            mode=\"bilinear\",\n+            align_corners=False,\n+        )\n+        for layer in self.layers:\n+            features = layer(features)\n+        return features\n+\n+\n+class DepthProFovModel(nn.Module):\n+    def __init__(self, config: DepthProConfig):\n+        super().__init__()\n+        self.config = config\n+        self.fusion_hidden_size = config.fusion_hidden_size\n+\n+        self.fov_encoder = DepthProFovEncoder(config)\n+        self.conv = nn.Conv2d(\n+            self.fusion_hidden_size, self.fusion_hidden_size // 2, kernel_size=3, stride=2, padding=1\n+        )\n+        self.activation = nn.ReLU(inplace=True)\n+        self.head = DepthProFovHead(config)\n+\n+    def forward(\n+        self,\n+        pixel_values: torch.Tensor,\n+        global_features: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+    ) -> torch.Tensor:\n+        fov_features = self.fov_encoder(pixel_values, head_mask)\n+\n+        global_features = self.conv(global_features)\n+        global_features = self.activation(global_features)\n+\n+        fov_features = fov_features + global_features\n+        fov_output = self.head(fov_features)\n+        fov_output = fov_output.flatten()\n+\n+        return fov_output\n+\n+\n+class DepthProDepthEstimationHead(nn.Module):\n+    \"\"\"\n+    The DepthProDepthEstimationHead module serves as the output head for depth estimation tasks.\n+    This module comprises a sequence of convolutional and transposed convolutional layers\n+    that process the feature map from the fusion to produce a single-channel depth map.\n+    Key operations include dimensionality reduction and upsampling to match the input resolution.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+\n+        features = config.fusion_hidden_size\n+        self.layers = nn.ModuleList(\n+            [\n+                nn.Conv2d(features, features // 2, kernel_size=3, stride=1, padding=1),\n+                nn.ConvTranspose2d(\n+                    in_channels=features // 2,\n+                    out_channels=features // 2,\n+                    kernel_size=2,\n+                    stride=2,\n+                    padding=0,\n+                    bias=True,\n+                ),\n+                nn.Conv2d(features // 2, 32, kernel_size=3, stride=1, padding=1),\n+                nn.ReLU(True),\n+                nn.Conv2d(32, 1, kernel_size=1, stride=1, padding=0),\n+                nn.ReLU(),\n+            ]\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        for layer in self.layers:\n+            hidden_states = layer(hidden_states)\n+\n+        predicted_depth = hidden_states.squeeze(dim=1)\n+        return predicted_depth\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    DepthPro Model with a depth estimation head on top (consisting of 3 convolutional layers).\n+    \"\"\",\n+    DEPTH_PRO_FOR_DEPTH_ESTIMATION_START_DOCSTRING,\n+)\n+class DepthProForDepthEstimation(DepthProPreTrainedModel):\n+    def __init__(self, config, use_fov_model=None):\n+        super().__init__(config)\n+        self.config = config\n+        self.use_fov_model = use_fov_model if use_fov_model is not None else self.config.use_fov_model\n+\n+        # dinov2 (vit) like encoders\n+        self.depth_pro = DepthProModel(config)\n+\n+        # dpt (vit) like fusion stage\n+        self.fusion_stage = DepthProFeatureFusionStage(config)\n+\n+        # depth estimation head\n+        self.head = DepthProDepthEstimationHead(config)\n+\n+        # dinov2 (vit) like encoder\n+        self.fov_model = DepthProFovModel(config) if self.use_fov_model else None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(DEPTH_PRO_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=DepthProDepthEstimatorOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple[torch.Tensor], DepthProDepthEstimatorOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n+            Ground truth depth estimation maps for computing the loss.\n+\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, DepthProForDepthEstimation\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> checkpoint = \"apple/DepthPro-hf\"\n+        >>> processor = AutoImageProcessor.from_pretrained(checkpoint)\n+        >>> model = DepthProForDepthEstimation.from_pretrained(checkpoint)\n+\n+        >>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+        >>> model.to(device)\n+\n+        >>> # prepare image for the model\n+        >>> inputs = processor(images=image, return_tensors=\"pt\").to(device)\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n+        >>> # interpolate to original size\n+        >>> post_processed_output = processor.post_process_depth_estimation(\n+        ...     outputs, target_sizes=[(image.height, image.width)],\n+        ... )\n+\n+        >>> # get the field of view (fov) predictions\n+        >>> field_of_view = post_processed_output[0][\"field_of_view\"]\n+        >>> focal_length = post_processed_output[0][\"focal_length\"]\n+\n+        >>> # visualize the prediction\n+        >>> predicted_depth = post_processed_output[0][\"predicted_depth\"]\n+        >>> depth = predicted_depth * 255 / predicted_depth.max()\n+        >>> depth = depth.detach().cpu().numpy()\n+        >>> depth = Image.fromarray(depth.astype(\"uint8\"))\n+        ```\"\"\"\n+        loss = None\n+        if labels is not None:\n+            raise NotImplementedError(\"Training is not implemented yet\")\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        depth_pro_outputs = self.depth_pro(\n+            pixel_values=pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+        )\n+        features = depth_pro_outputs.features\n+        fused_hidden_states = self.fusion_stage(features)\n+        predicted_depth = self.head(fused_hidden_states[-1])\n+\n+        if self.use_fov_model:\n+            # frozen features from encoder are used\n+            features_for_fov = features[0].detach()\n+            fov = self.fov_model(\n+                pixel_values=pixel_values,\n+                global_features=features_for_fov,\n+                head_mask=head_mask,\n+            )\n+        else:\n+            fov = None\n+\n+        if not return_dict:\n+            outputs = [loss, predicted_depth, fov, depth_pro_outputs.hidden_states, depth_pro_outputs.attentions]\n+            return tuple(v for v in outputs if v is not None)\n+\n+        return DepthProDepthEstimatorOutput(\n+            loss=loss,\n+            predicted_depth=predicted_depth,\n+            field_of_view=fov,\n+            hidden_states=depth_pro_outputs.hidden_states,\n+            attentions=depth_pro_outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"DepthProPreTrainedModel\", \"DepthProModel\", \"DepthProForDepthEstimation\"]"
        },
        {
            "sha": "f4b29273a509d3e25d7a11eb77de8808e0893f96",
            "filename": "src/transformers/models/dinov2/configuration_dinov2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fconfiguration_dinov2.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -88,6 +88,8 @@ class Dinov2Config(BackboneConfigMixin, PretrainedConfig):\n             Whether to reshape the feature maps to 4D tensors of shape `(batch_size, hidden_size, height, width)` in\n             case the model is used as backbone. If `False`, the feature maps will be 3D tensors of shape `(batch_size,\n             seq_len, hidden_size)`.\n+        use_mask_token (`bool`, *optional*, defaults to `True`):\n+            Whether to use mask_token in embeddings.\n \n     Example:\n \n@@ -128,6 +130,7 @@ def __init__(\n         out_indices=None,\n         apply_layernorm=True,\n         reshape_hidden_states=True,\n+        use_mask_token=True,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -154,6 +157,7 @@ def __init__(\n         )\n         self.apply_layernorm = apply_layernorm\n         self.reshape_hidden_states = reshape_hidden_states\n+        self.use_mask_token = use_mask_token\n \n \n class Dinov2OnnxConfig(OnnxConfig):"
        },
        {
            "sha": "33ec1c054990df1f0f85b66c33917d890b460ced",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -67,12 +67,14 @@ def __init__(self, config: Dinov2Config) -> None:\n         super().__init__()\n \n         self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n-        self.mask_token = nn.Parameter(torch.zeros(1, config.hidden_size))\n+        if config.use_mask_token:\n+            self.mask_token = nn.Parameter(torch.zeros(1, config.hidden_size))\n         self.patch_embeddings = Dinov2PatchEmbeddings(config)\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         self.patch_size = config.patch_size\n+        self.use_mask_token = config.use_mask_token\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n@@ -120,7 +122,7 @@ def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Te\n         target_dtype = self.patch_embeddings.projection.weight.dtype\n         embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n \n-        if bool_masked_pos is not None:\n+        if bool_masked_pos is not None and self.use_mask_token:\n             embeddings = torch.where(\n                 bool_masked_pos.unsqueeze(-1), self.mask_token.to(embeddings.dtype).unsqueeze(0), embeddings\n             )"
        },
        {
            "sha": "cf2a6e04c4ea4e4e2397e7112e29e91f3756e613",
            "filename": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -136,11 +136,12 @@ def setup(self):\n             jax.nn.initializers.variance_scaling(self.config.initializer_range**2, \"fan_in\", \"truncated_normal\"),\n             (1, 1, self.config.hidden_size),\n         )\n-        self.mask_token = self.param(\n-            \"mask_token\",\n-            jax.nn.initializers.variance_scaling(self.config.initializer_range**2, \"fan_in\", \"truncated_normal\"),\n-            (1, self.config.hidden_size),\n-        )\n+        if self.config.use_mask_token:\n+            self.mask_token = self.param(\n+                \"mask_token\",\n+                jax.nn.initializers.variance_scaling(self.config.initializer_range**2, \"fan_in\", \"truncated_normal\"),\n+                (1, self.config.hidden_size),\n+            )\n         self.patch_embeddings = FlaxDinov2PatchEmbeddings(self.config, dtype=self.dtype)\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = self.param("
        },
        {
            "sha": "5a78ab786a9563c7ed05fa2e77b1811c9951e106",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -3551,6 +3551,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class DepthProForDepthEstimation(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DepthProModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class DepthProPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class DetrForObjectDetection(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "87b60fbc0463a893acdc8de38f31a625143ed2a1",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -44,6 +44,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class DepthProImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class DetrImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "aeccf53742ae7040ead90a18fa9ff928ff0ac828",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -184,6 +184,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class DepthProImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n+class DepthProImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class DetrFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/depth_pro/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2F__init__.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c"
        },
        {
            "sha": "b30931a86cdb96f6b00d453f4af5075b6d5baf41",
            "filename": "tests/models/depth_pro/test_image_processing_depth_pro.py",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2Ftest_image_processing_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2Ftest_image_processing_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_image_processing_depth_pro.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,124 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from transformers.testing_utils import is_flaky, require_torch, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from transformers import DepthProImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import DepthProImageProcessorFast\n+\n+\n+class DepthProImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_rescale=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+    ):\n+        super().__init__()\n+        size = size if size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_rescale = do_rescale\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_rescale\": self.do_rescale,\n+            \"do_normalize\": self.do_normalize,\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class DepthProImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = DepthProImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = DepthProImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = DepthProImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+        self.assertTrue(hasattr(image_processing, \"resample\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n+        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    @is_flaky(\n+        description=\"fast and slow, both processors use torch implementation, see: https://github.com/huggingface/transformers/issues/34920\",\n+    )\n+    def test_fast_is_faster_than_slow(self):\n+        super().test_fast_is_faster_than_slow()"
        },
        {
            "sha": "44529270fd94dc3d65d768b99fd7cd60a374ba1f",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "added",
            "additions": 398,
            "deletions": 0,
            "changes": 398,
            "blob_url": "https://github.com/huggingface/transformers/blob/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=9a6be63fdb77af107b340cfbdcc3f0d9d47d7c9c",
            "patch": "@@ -0,0 +1,398 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch DepthPro model.\"\"\"\n+\n+import unittest\n+\n+from transformers import DepthProConfig\n+from transformers.file_utils import is_torch_available, is_vision_available\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import DepthProForDepthEstimation, DepthProModel\n+    from transformers.models.auto.modeling_auto import MODEL_MAPPING_NAMES\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import DepthProImageProcessor\n+\n+\n+class DepthProModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=8,\n+        image_size=64,\n+        patch_size=16,\n+        num_channels=3,\n+        is_training=True,\n+        use_labels=True,\n+        fusion_hidden_size=16,\n+        intermediate_hook_ids=[1, 0],\n+        intermediate_feature_dims=[10, 8],\n+        scaled_images_ratios=[0.5, 1.0],\n+        scaled_images_overlap_ratios=[0.0, 0.2],\n+        scaled_images_feature_dims=[12, 12],\n+        initializer_range=0.02,\n+        use_fov_model=False,\n+        image_model_config={\n+            \"model_type\": \"dinov2\",\n+            \"num_hidden_layers\": 2,\n+            \"hidden_size\": 16,\n+            \"num_attention_heads\": 1,\n+            \"patch_size\": 4,\n+        },\n+        patch_model_config={\n+            \"model_type\": \"vit\",\n+            \"num_hidden_layers\": 2,\n+            \"hidden_size\": 24,\n+            \"num_attention_heads\": 2,\n+            \"patch_size\": 6,\n+        },\n+        fov_model_config={\n+            \"model_type\": \"vit\",\n+            \"num_hidden_layers\": 2,\n+            \"hidden_size\": 32,\n+            \"num_attention_heads\": 4,\n+            \"patch_size\": 8,\n+        },\n+        num_labels=3,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.fusion_hidden_size = fusion_hidden_size\n+        self.intermediate_hook_ids = intermediate_hook_ids\n+        self.intermediate_feature_dims = intermediate_feature_dims\n+        self.scaled_images_ratios = scaled_images_ratios\n+        self.scaled_images_overlap_ratios = scaled_images_overlap_ratios\n+        self.scaled_images_feature_dims = scaled_images_feature_dims\n+        self.initializer_range = initializer_range\n+        self.use_fov_model = use_fov_model\n+        self.image_model_config = image_model_config\n+        self.patch_model_config = patch_model_config\n+        self.fov_model_config = fov_model_config\n+        self.num_labels = num_labels\n+\n+        self.hidden_size = image_model_config[\"hidden_size\"]\n+        self.num_hidden_layers = image_model_config[\"num_hidden_layers\"]\n+        self.num_attention_heads = image_model_config[\"num_attention_heads\"]\n+\n+        # may be different for a backbone other than dinov2\n+        self.out_size = patch_size // image_model_config[\"patch_size\"]\n+        self.seq_length = self.out_size**2 + 1  # we add 1 for the [CLS] token\n+\n+        n_fusion_blocks = len(intermediate_hook_ids) + len(scaled_images_ratios)\n+        self.expected_depth_size = 2 ** (n_fusion_blocks + 1) * self.out_size\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size, self.image_size, self.image_size], self.num_labels)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def get_config(self):\n+        return DepthProConfig(\n+            patch_size=self.patch_size,\n+            fusion_hidden_size=self.fusion_hidden_size,\n+            intermediate_hook_ids=self.intermediate_hook_ids,\n+            intermediate_feature_dims=self.intermediate_feature_dims,\n+            scaled_images_ratios=self.scaled_images_ratios,\n+            scaled_images_overlap_ratios=self.scaled_images_overlap_ratios,\n+            scaled_images_feature_dims=self.scaled_images_feature_dims,\n+            initializer_range=self.initializer_range,\n+            image_model_config=self.image_model_config,\n+            patch_model_config=self.patch_model_config,\n+            fov_model_config=self.fov_model_config,\n+            use_fov_model=self.use_fov_model,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = DepthProModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+\n+    def create_and_check_for_depth_estimation(self, config, pixel_values, labels):\n+        config.num_labels = self.num_labels\n+        model = DepthProForDepthEstimation(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        self.parent.assertEqual(\n+            result.predicted_depth.shape, (self.batch_size, self.expected_depth_size, self.expected_depth_size)\n+        )\n+\n+    def create_and_check_for_fov(self, config, pixel_values, labels):\n+        model = DepthProForDepthEstimation(config, use_fov_model=True)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        # check if the fov_model (DinoV2-based encoder) is created\n+        self.parent.assertIsNotNone(model.fov_model)\n+\n+        batched_pixel_values = pixel_values\n+        row_pixel_values = pixel_values[:1]\n+\n+        with torch.no_grad():\n+            model_batched_output_fov = model(batched_pixel_values).field_of_view\n+            model_row_output_fov = model(row_pixel_values).field_of_view\n+\n+        # check if fov is returned\n+        self.parent.assertIsNotNone(model_batched_output_fov)\n+        self.parent.assertIsNotNone(model_row_output_fov)\n+\n+        # check output shape consistency for fov\n+        self.parent.assertEqual(model_batched_output_fov.shape, (self.batch_size,))\n+\n+        # check equivalence between batched and single row outputs for fov\n+        diff = torch.max(torch.abs(model_row_output_fov - model_batched_output_fov[:1]))\n+        model_name = model.__class__.__name__\n+        self.parent.assertTrue(\n+            diff <= 1e-03,\n+            msg=(f\"Batched and Single row outputs are not equal in {model_name} for fov. \" f\"Difference={diff}.\"),\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, labels = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class DepthProModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as DepthPro does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (DepthProModel, DepthProForDepthEstimation) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"depth-estimation\": DepthProForDepthEstimation,\n+            \"image-feature-extraction\": DepthProModel,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = DepthProModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=DepthProConfig, has_text_modality=False, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"DepthPro does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_for_depth_estimation(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_depth_estimation(*config_and_inputs)\n+\n+    def test_for_fov(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_fov(*config_and_inputs)\n+\n+    def test_training(self):\n+        for model_class in self.all_model_classes:\n+            if model_class.__name__ == \"DepthProForDepthEstimation\":\n+                continue\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config.return_dict = True\n+\n+            if model_class.__name__ in MODEL_MAPPING_NAMES.values():\n+                continue\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.train()\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            loss = model(**inputs).loss\n+            loss.backward()\n+\n+    def test_training_gradient_checkpointing(self):\n+        for model_class in self.all_model_classes:\n+            if model_class.__name__ == \"DepthProForDepthEstimation\":\n+                continue\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            config.use_cache = False\n+            config.return_dict = True\n+\n+            if model_class.__name__ in MODEL_MAPPING_NAMES.values() or not model_class.supports_gradient_checkpointing:\n+                continue\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.gradient_checkpointing_enable()\n+            model.train()\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            loss = model(**inputs).loss\n+            loss.backward()\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                non_uniform_init_parms = [\n+                    # these encoders are vision transformers\n+                    # any layer outside these encoders is either Conv2d or ConvTranspose2d\n+                    # which use kaiming initialization\n+                    \"patch_encoder\",\n+                    \"image_encoder\",\n+                    \"fov_model.encoder\",\n+                ]\n+                if param.requires_grad:\n+                    if any(x in name for x in non_uniform_init_parms):\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertTrue(\n+                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+    # this started when switched from normal initialization to kaiming_normal intialization\n+    # maybe because the magnitude of offset values from ViT-encoders increases when followed by many convolution layers\n+    def test_batching_equivalence(self, atol=1e-4, rtol=1e-4):\n+        super().test_batching_equivalence(atol=atol, rtol=rtol)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_path = \"apple/DepthPro-hf\"\n+        model = DepthProModel.from_pretrained(model_path)\n+        self.assertIsNotNone(model)\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+@slow\n+class DepthProModelIntegrationTest(unittest.TestCase):\n+    def test_inference_depth_estimation(self):\n+        model_path = \"apple/DepthPro-hf\"\n+        image_processor = DepthProImageProcessor.from_pretrained(model_path)\n+        model = DepthProForDepthEstimation.from_pretrained(model_path).to(torch_device)\n+        config = model.config\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the predicted depth\n+        n_fusion_blocks = len(config.intermediate_hook_ids) + len(config.scaled_images_ratios)\n+        out_size = config.image_model_config.image_size // config.image_model_config.patch_size\n+        expected_depth_size = 2 ** (n_fusion_blocks + 1) * out_size\n+\n+        expected_shape = torch.Size((1, expected_depth_size, expected_depth_size))\n+        self.assertEqual(outputs.predicted_depth.shape, expected_shape)\n+\n+        expected_slice = torch.tensor(\n+            [[1.0582, 1.1225, 1.1335], [1.1154, 1.1398, 1.1486], [1.1434, 1.1500, 1.1643]]\n+        ).to(torch_device)\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4, rtol=1e-4)\n+\n+        # verify the predicted fov\n+        expected_shape = torch.Size((1,))\n+        self.assertEqual(outputs.field_of_view.shape, expected_shape)\n+\n+        expected_slice = torch.tensor([47.2459]).to(torch_device)\n+        torch.testing.assert_close(outputs.field_of_view, expected_slice, atol=1e-4, rtol=1e-4)\n+\n+    def test_post_processing_depth_estimation(self):\n+        model_path = \"apple/DepthPro-hf\"\n+        image_processor = DepthProImageProcessor.from_pretrained(model_path)\n+        model = DepthProForDepthEstimation.from_pretrained(model_path)\n+\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        outputs = image_processor.post_process_depth_estimation(\n+            outputs,\n+            target_sizes=[[image.height, image.width]],\n+        )\n+        predicted_depth = outputs[0][\"predicted_depth\"]\n+        expected_shape = torch.Size((image.height, image.width))\n+        self.assertTrue(predicted_depth.shape == expected_shape)"
        }
    ],
    "stats": {
        "total": 3082,
        "additions": 3074,
        "deletions": 8
    }
}