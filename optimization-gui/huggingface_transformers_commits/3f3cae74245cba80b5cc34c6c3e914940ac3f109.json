{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨ [saving] Default to 50GB shards, and remove non-safe serialization (#42734)\n\n* switch\n\n* remove now useless save_function\n\n* a bit more involved than i thought\n\n* all converters\n\n* fix\n\n* pretty print\n\n* fix\n\n* trainer\n\n* update musicgen.md docs\n\n* marc comments\n\n* doc and last missed instances\n\n* CI\n\n---------\n\nCo-authored-by: Wauplin <lucainp@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "3f3cae74245cba80b5cc34c6c3e914940ac3f109",
    "files": [
        {
            "sha": "5076e60b6e97d6999cd50d826b47b939562685c9",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -60,7 +60,7 @@ This model was contributed by [sanchit-gandhi](https://huggingface.co/sanchit-ga\n \n ```bash\n python src/transformers/models/musicgen/convert_musicgen_transformers.py \\\n-    --checkpoint small --pytorch_dump_folder /output/path --safe_serialization \n+    --checkpoint small --pytorch_dump_folder /output/path\n ```\n \n ## Generation"
        },
        {
            "sha": "05f4238760b982cdcd8bbfd8b760dc25d6202a5c",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -639,30 +639,35 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n \n ## Serialization\n \n-torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/notes/extending.html#subclassing-torch-tensor) for maximum flexibility in supporting new quantized torch.Tensor formats. [Safetensors](https://huggingface.co/docs/safetensors/en/index) serialization and deserialization does not work with torchao.\n-\n-To avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n+Saving the quantized model with `save_pretrained` (in [safetensors](https://huggingface.co/docs/safetensors/en/index) format) is only supported for torchao >= v0.15. For any version below, it is only possible to manually save as unsafe `.bin` checkpoints with [torch.save](https://docs.pytorch.org/docs/stable/generated/torch.save.html).\n \n <hfoptions id=\"serialization-examples\">\n <hfoption id=\"save-locally\">\n \n ```py\n-# don't serialize model with Safetensors\n+# torchao >= 0.15\n output_dir = \"llama3-8b-int4wo-128\"\n-quantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n+quantized_model.save_pretrained(\"llama3-8b-int4wo-128\")\n ```\n \n </hfoption>\n <hfoption id=\"push-to-huggingface-hub\">\n \n ```py\n-# don't serialize model with Safetensors\n+# torchao >= 0.15\n USER_ID = \"your_huggingface_user_id\"\n REPO_ID = \"llama3-8b-int4wo-128\"\n-quantized_model.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\", safe_serialization=False)\n+quantized_model.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n tokenizer.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n ```\n \n+\n+```py\n+# torchao < 0.15 -> unsafe serialization\n+filename = \"llama3-8b-int4wo-128/pytorch_model.bin\"\n+torch.save(quantized_model.state_dict(), filename)\n+```\n+\n </hfoption>\n </hfoptions>\n \n@@ -687,7 +692,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n )\n # save the quantized model\n output_dir = \"llama-3.1-8b-torchao-int8\"\n-quantized_model.save_pretrained(output_dir, safe_serialization=False)\n+quantized_model.save_pretrained(output_dir)\n \n # reload the quantized model\n reloaded_model = AutoModelForCausalLM.from_pretrained(\n@@ -724,7 +729,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n )\n # save the quantized model\n output_dir = \"llama-3.1-8b-torchao-int4-cpu\"\n-quantized_model.save_pretrained(output_dir, safe_serialization=False)\n+quantized_model.save_pretrained(output_dir)\n \n # reload the quantized model\n reloaded_model = AutoModelForCausalLM.from_pretrained("
        },
        {
            "sha": "d500e9aec41c120b20ba91b826edde9512534c7f",
            "filename": "examples/3D_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2F3D_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2F3D_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2F3D_parallel.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -340,7 +340,7 @@ def collate_fn(batch):\n     else:\n         # Fallback to regular save for non-distributed case\n         save_dir = \"test_model_nondist\"\n-        model.save_pretrained(save_dir, safe_serialization=False)\n+        model.save_pretrained(save_dir)\n         tokenizer.save_pretrained(save_dir)  # Save tokenizer too\n         logger.info(f\"Saved model to {save_dir}\")\n "
        },
        {
            "sha": "0c1f200ba0c9dcbab66701485c2e4e0d8c263395",
            "filename": "examples/pytorch/3d_parallel_checks.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2Fpytorch%2F3d_parallel_checks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2Fpytorch%2F3d_parallel_checks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2F3d_parallel_checks.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -458,7 +458,7 @@ def collate_fn(batch):\n     else:\n         # Fallback to regular save for non-distributed case\n         save_dir = \"test_model_nondist\"\n-        model.save_pretrained(save_dir, safe_serialization=False)\n+        model.save_pretrained(save_dir)\n         tokenizer.save_pretrained(save_dir)  # Save tokenizer too\n         logger.info(f\"Saved model to {save_dir}\")\n "
        },
        {
            "sha": "e19279a9936b5d1fca7bae3496ae3ee3491c2822",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -216,7 +216,7 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"\n         return True\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "7909da6ffedd7a17bd6beb65962112c905453dcd",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 77,
            "deletions": 97,
            "changes": 174,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -93,7 +93,6 @@\n from .safetensors_conversion import auto_conversion\n from .utils import (\n     ADAPTER_SAFE_WEIGHTS_NAME,\n-    ADAPTER_WEIGHTS_NAME,\n     DUMMY_INPUTS,\n     SAFE_WEIGHTS_INDEX_NAME,\n     SAFE_WEIGHTS_NAME,\n@@ -551,8 +550,7 @@ def _get_resolved_checkpoint_files(\n                             raise OSError(\n                                 f\"{pretrained_model_name_or_path} does not appear to have a file named\"\n                                 f\" {_add_variant(SAFE_WEIGHTS_NAME, variant)} or {_add_variant(SAFE_WEIGHTS_INDEX_NAME, variant)} \"\n-                                \"and thus cannot be loaded with `safetensors`. Please make sure that the model has \"\n-                                \"been saved with `safe_serialization=True` or do not set `use_safetensors=True`.\"\n+                                \"and thus cannot be loaded with `safetensors`. Please do not set `use_safetensors=True`.\"\n                             )\n                     else:\n                         # This repo has no safetensors file of any kind, we switch to PyTorch.\n@@ -3009,10 +3007,8 @@ def save_pretrained(\n         save_directory: Union[str, os.PathLike],\n         is_main_process: bool = True,\n         state_dict: Optional[dict] = None,\n-        save_function: Callable = torch.save,\n         push_to_hub: bool = False,\n-        max_shard_size: Union[int, str] = \"5GB\",\n-        safe_serialization: bool = True,\n+        max_shard_size: Union[int, str] = \"50GB\",\n         variant: Optional[str] = None,\n         token: Optional[Union[str, bool]] = None,\n         save_peft_format: bool = True,\n@@ -3034,18 +3030,13 @@ def save_pretrained(\n                 The state dictionary of the model to save. Will default to `self.state_dict()`, but can be used to only\n                 save parts of the model or if special precautions need to be taken when recovering the state dictionary\n                 of a model (like when using model parallelism).\n-            save_function (`Callable`):\n-                The function to use to save the state dictionary. Useful on distributed training like TPUs when one\n-                need to replace `torch.save` by another method.\n             push_to_hub (`bool`, *optional*, defaults to `False`):\n                 Whether or not to push your model to the Hugging Face model hub after saving it. You can specify the\n                 repository you want to push to with `repo_id` (will default to the name of `save_directory` in your\n                 namespace).\n-            max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n+            max_shard_size (`int` or `str`, *optional*, defaults to `\"50GB\"`):\n                 The maximum size for a checkpoint before being sharded. Checkpoints shard will then be each of size\n                 lower than this size. If expressed as a string, needs to be digits followed by a unit (like `\"5MB\"`).\n-                We default it to 5GB in order for models to be able to run easily on free-tier google colab instances\n-                without CPU OOM issues.\n \n                 <Tip warning={true}>\n \n@@ -3054,10 +3045,8 @@ def save_pretrained(\n \n                 </Tip>\n \n-            safe_serialization (`bool`, *optional*, defaults to `True`):\n-                Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\n             variant (`str`, *optional*):\n-                If specified, weights are saved in the format pytorch_model.<variant>.bin.\n+                If specified, weights are saved in the format model.<variant>.safetensors.\n             token (`str` or `bool`, *optional*):\n                 The token to use as HTTP bearer authorization for remote files. If `True`, or not specified, will use\n                 the token generated when running `hf auth login` (stored in `~/.huggingface`).\n@@ -3079,9 +3068,7 @@ def save_pretrained(\n \n         hf_quantizer = getattr(self, \"hf_quantizer\", None)\n         quantization_serializable = (\n-            hf_quantizer is not None\n-            and isinstance(hf_quantizer, HfQuantizer)\n-            and hf_quantizer.is_serializable(safe_serialization=safe_serialization)\n+            hf_quantizer is not None and isinstance(hf_quantizer, HfQuantizer) and hf_quantizer.is_serializable()\n         )\n \n         if hf_quantizer is not None and not _hf_peft_config_loaded and not quantization_serializable:\n@@ -3117,7 +3104,7 @@ def save_pretrained(\n \n         metadata = {}\n         if hf_quantizer is not None:\n-            state_dict, metadata = hf_quantizer.get_state_dict_and_metadata(self, safe_serialization)\n+            state_dict, metadata = hf_quantizer.get_state_dict_and_metadata(self)\n         metadata[\"format\"] = \"pt\"\n \n         # Only save the model itself if we are using distributed training\n@@ -3209,86 +3196,83 @@ def save_pretrained(\n         if self._tp_size is not None:\n             state_dict = replace_state_dict_local_with_dtensor(state_dict, self._tp_plan, self._device_mesh)\n \n-        if safe_serialization:\n-            # TODO: fix safe_serialization for tied weights\n-            # Safetensors does not allow tensor aliasing.\n-            # We're going to remove aliases before saving\n-            ptrs = collections.defaultdict(list)\n-            for name, tensor in state_dict.items():\n-                if not isinstance(tensor, torch.Tensor):\n-                    # Sometimes in the state_dict we have non-tensor objects.\n-                    # e.g. in bitsandbytes we have some `str` objects in the state_dict\n-                    # In the non-tensor case, fall back to the pointer of the object itself\n-                    ptrs[id(tensor)].append(name)\n-\n-                elif tensor.device.type == \"meta\":\n-                    # In offloaded cases, there may be meta tensors in the state_dict.\n-                    # For these cases, key by the pointer of the original tensor object\n-                    # (state_dict tensors are detached and therefore no longer shared)\n-                    tensor = self.get_parameter(name)\n-                    ptrs[id(tensor)].append(name)\n+        # Safetensors does not allow tensor aliasing - we're going to remove aliases before saving\n+        ptrs = collections.defaultdict(list)\n+        for name, tensor in state_dict.items():\n+            if not isinstance(tensor, torch.Tensor):\n+                # Sometimes in the state_dict we have non-tensor objects.\n+                # e.g. in bitsandbytes we have some `str` objects in the state_dict\n+                # In the non-tensor case, fall back to the pointer of the object itself\n+                ptrs[id(tensor)].append(name)\n+\n+            elif tensor.device.type == \"meta\":\n+                # In offloaded cases, there may be meta tensors in the state_dict.\n+                # For these cases, key by the pointer of the original tensor object\n+                # (state_dict tensors are detached and therefore no longer shared)\n+                tensor = self.get_parameter(name)\n+                ptrs[id(tensor)].append(name)\n \n-                else:\n-                    ptrs[id_tensor_storage(tensor)].append(name)\n-\n-            shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n-\n-            # Recursively descend to find tied weight keys\n-            _tied_weights_keys = set(_get_tied_weight_keys(self))\n-            error_names = []\n-            to_delete_names = set()\n-            for names in shared_ptrs.values():\n-                # Removing the keys which are declared as known duplicates on\n-                # load. This allows to make sure the name which is kept is consistent.\n-                if _tied_weights_keys is not None:\n-                    found = 0\n-                    for name in sorted(names):\n-                        matches_pattern = any(re.search(pat, name) for pat in _tied_weights_keys)\n-                        if matches_pattern and name in state_dict:\n-                            found += 1\n-                            if found < len(names):\n-                                to_delete_names.add(name)\n-            # We are entering a place where the weights and the transformers configuration do NOT match.\n-            shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n-            # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n-            # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n-            for name in disjoint_names:\n-                state_dict[name] = state_dict[name].clone()\n-\n-            # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n-            # If the link between tensors was done at runtime then `from_pretrained` will not get\n-            # the key back leading to random tensor. A proper warning will be shown\n-            # during reload (if applicable), but since the file is not necessarily compatible with\n-            # the config, better show a proper warning.\n-            shared_names, identical_names = _find_identical(shared_names, state_dict)\n-            # delete tensors that have identical storage\n-            for inames in identical_names:\n-                known = inames.intersection(to_delete_names)\n-                for name in known:\n-                    del state_dict[name]\n-                unknown = inames.difference(to_delete_names)\n-                if len(unknown) > 1:\n-                    error_names.append(unknown)\n-\n-            if shared_names:\n-                error_names.extend(shared_names)\n-\n-            if len(error_names) > 0:\n-                raise RuntimeError(\n-                    f\"The weights trying to be saved contained shared tensors {error_names} which are not properly defined. We found `_tied_weights_keys` to be: {_tied_weights_keys}.\\n\"\n-                    \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n-                )\n+            else:\n+                ptrs[id_tensor_storage(tensor)].append(name)\n+\n+        shared_ptrs = {ptr: names for ptr, names in ptrs.items() if len(names) > 1}\n+\n+        # Recursively descend to find tied weight keys\n+        _tied_weights_keys = set(_get_tied_weight_keys(self))\n+        error_names = []\n+        to_delete_names = set()\n+        for names in shared_ptrs.values():\n+            # Removing the keys which are declared as known duplicates on\n+            # load. This allows to make sure the name which is kept is consistent.\n+            if _tied_weights_keys is not None:\n+                found = 0\n+                for name in sorted(names):\n+                    matches_pattern = any(re.search(pat, name) for pat in _tied_weights_keys)\n+                    if matches_pattern and name in state_dict:\n+                        found += 1\n+                        if found < len(names):\n+                            to_delete_names.add(name)\n+        # We are entering a place where the weights and the transformers configuration do NOT match.\n+        shared_names, disjoint_names = _find_disjoint(shared_ptrs.values(), state_dict)\n+        # Those are actually tensor sharing but disjoint from each other, we can safely clone them\n+        # Reloaded won't have the same property, but it shouldn't matter in any meaningful way.\n+        for name in disjoint_names:\n+            state_dict[name] = state_dict[name].clone()\n+\n+        # When not all duplicates have been cleaned, still remove those keys, but put a clear warning.\n+        # If the link between tensors was done at runtime then `from_pretrained` will not get\n+        # the key back leading to random tensor. A proper warning will be shown\n+        # during reload (if applicable), but since the file is not necessarily compatible with\n+        # the config, better show a proper warning.\n+        shared_names, identical_names = _find_identical(shared_names, state_dict)\n+        # delete tensors that have identical storage\n+        for inames in identical_names:\n+            known = inames.intersection(to_delete_names)\n+            for name in known:\n+                del state_dict[name]\n+            unknown = inames.difference(to_delete_names)\n+            if len(unknown) > 1:\n+                error_names.append(unknown)\n+\n+        if shared_names:\n+            error_names.extend(shared_names)\n+\n+        if len(error_names) > 0:\n+            raise RuntimeError(\n+                f\"The weights trying to be saved contained shared tensors {error_names} which are not properly defined. We found `_tied_weights_keys` to be: {_tied_weights_keys}.\\n\"\n+                \"This can also just mean that the module's tied weight keys are wrong vs the actual tied weights in the model.\",\n+            )\n \n         # Revert all renaming and/or weight operations\n         if save_original_format:\n             state_dict = revert_weight_conversion(self, state_dict)\n \n         # Shard the model if it is too big.\n         if not _hf_peft_config_loaded:\n-            weights_name = SAFE_WEIGHTS_NAME if safe_serialization else WEIGHTS_NAME\n+            weights_name = SAFE_WEIGHTS_NAME\n             weights_name = _add_variant(weights_name, variant)\n         else:\n-            weights_name = ADAPTER_SAFE_WEIGHTS_NAME if safe_serialization else ADAPTER_WEIGHTS_NAME\n+            weights_name = ADAPTER_SAFE_WEIGHTS_NAME\n \n         filename_pattern = weights_name.replace(\".bin\", \"{suffix}.bin\").replace(\".safetensors\", \"{suffix}.safetensors\")\n         state_dict_split = split_torch_state_dict_into_shards(\n@@ -3357,21 +3341,17 @@ def save_pretrained(\n                 del shard_state_dict\n                 gc.collect()\n \n-            if safe_serialization:\n-                # At some point we will need to deal better with save_function (used for TPU and other distributed\n-                # joyfulness), but for now this enough. # TODO: we should def parallelize this we are otherwise just waiting\n-                # too much before scheduling the next write when its in a different file\n-                safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n-            else:\n-                save_function(shard, os.path.join(save_directory, shard_file))\n+            # TODO: we should def parallelize this we are otherwise just waiting\n+            # too much before scheduling the next write when its in a different file\n+            safe_save_file(shard, os.path.join(save_directory, shard_file), metadata=metadata)\n \n         del state_dict\n \n         if index is None:\n             path_to_weights = os.path.join(save_directory, weights_name)\n             logger.info(f\"Model weights saved in {path_to_weights}\")\n         else:\n-            save_index_file = SAFE_WEIGHTS_INDEX_NAME if safe_serialization else WEIGHTS_INDEX_NAME\n+            save_index_file = SAFE_WEIGHTS_INDEX_NAME\n             save_index_file = os.path.join(save_directory, _add_variant(save_index_file, variant))\n             # Save the index as well\n             with open(save_index_file, \"w\", encoding=\"utf-8\") as f:"
        },
        {
            "sha": "a4cac6c9d9c0eb959c4a4c6fc09dc3f1efa7446b",
            "filename": "src/transformers/models/aimv2/convert_aimv2_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconvert_aimv2_original_pytorch_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -146,15 +146,13 @@ def get_model_config_mapping(model_id: str):\n def write_model(\n     hf_repo_id: str,\n     output_dir: str,\n-    safe_serialization: bool = True,\n ):\n     \"\"\"\n     Converts a model checkpoint to Hugging Face format and saves it.\n \n     Args:\n         hf_repo_id (str): The Hugging Face repo ID to load from.\n         output_dir (str): The directory to save the converted model.\n-        safe_serialization (bool): Whether to use safe serialization.\n \n     Returns:\n         model: The reloaded Hugging Face model.\n@@ -202,7 +200,7 @@ def write_model(\n     print(\"Checkpoint loaded successfully.\")\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n     gc.collect()\n \n@@ -233,9 +231,6 @@ def main():\n         default=\"aimv2_model\",\n         help=\"Location to write the converted model and processor\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=argparse.BooleanOptionalAction,\n@@ -251,7 +246,6 @@ def main():\n     model = write_model(\n         hf_repo_id=args.hf_repo_id,\n         output_dir=args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     image_processor = write_image_processor("
        },
        {
            "sha": "36a54a781073659a46f4cf7cfb9d801775166374",
            "filename": "src/transformers/models/aria/convert_aria_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconvert_aria_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -121,9 +121,6 @@ def convert_aria_llama_to_hf(text_model_id, vision_model_id, output_hub_path, ol\n     state_dict = convert_state_dict_to_hf(state_dict)\n     model.load_state_dict(state_dict, strict=False, assign=True)\n \n-    # print(\"Saving models\")\n-    # model.save_pretrained(\"local_aria\", safe_serialization=False)\n-    # processor.save_pretrained(\"local_aria\")\n     print(\"Pushing to hub\")\n     model.push_to_hub(output_hub_path, create_pr=True)\n     processor.push_to_hub(output_hub_path, create_pr=True)"
        },
        {
            "sha": "27317f3ddf415f74899f47b57ed5ef7c09c91f61",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -377,7 +377,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         model = ChameleonForConditionalGeneration(config)\n \n     model.load_state_dict(state_dict, assign=True, strict=False)\n-    model.save_pretrained(model_path, safe_serialization=True)\n+    model.save_pretrained(model_path)\n \n     # Load and save the processor\n     tokenizer = LlamaTokenizerFast("
        },
        {
            "sha": "22453daca66717620a0b94924089d5b7a473ce17",
            "filename": "src/transformers/models/csm/convert_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fconvert_csm.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -81,7 +81,6 @@ def write_model(\n     model_name,\n     codec_model_path_or_repo,\n     output_dir,\n-    safe_serialization=True,\n ):\n     print(\"Converting the model.\")\n     os.makedirs(output_dir, exist_ok=True)\n@@ -222,7 +221,7 @@ def write_model(\n     model.generation_config.depth_decoder_temperature = 0.9\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -317,17 +316,13 @@ def main():\n         \"--output_dir\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     args = parser.parse_args()\n \n     write_model(\n         args.input_path_or_repo,\n         args.model_name,\n         args.codec_model_path_or_repo,\n         output_dir=args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     write_tokenizer(args.output_dir)"
        },
        {
            "sha": "ed21077ac6fc163b7721b2aab2d46a811fdda271",
            "filename": "src/transformers/models/deepseek_vl/convert_deepseek_vl_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -210,7 +210,6 @@ def convert_model(\n     hf_repo_id: str,\n     output_dir: Optional[str] = None,\n     output_hub_path: Optional[str] = None,\n-    safe_serialization: bool = True,\n ):\n     if output_dir:\n         os.makedirs(output_dir, exist_ok=True)\n@@ -307,10 +306,10 @@ def convert_model(\n     # Save the model\n     if output_dir:\n         print(f\"Saving model to {output_dir}...\")\n-        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        model.save_pretrained(output_dir)\n     if output_hub_path:\n         print(f\"Pushing model to hub at {output_hub_path}...\")\n-        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+        model.push_to_hub(output_hub_path)\n \n     del state_dict, model\n     gc.collect()\n@@ -339,16 +338,12 @@ def main():\n         default=None,\n         help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     args = parser.parse_args()\n \n     convert_model(\n         hf_repo_id=args.hf_repo_id,\n         output_dir=args.output_dir,\n         output_hub_path=args.output_hub_path,\n-        safe_serialization=args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "83745adf2e278b6f15326acbd988b7b007c7f5c5",
            "filename": "src/transformers/models/deepseek_vl_hybrid/convert_deepseek_vl_hybrid_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -237,7 +237,6 @@ def convert_model(\n     hf_repo_id: str,\n     output_dir: Optional[str] = None,\n     output_hub_path: Optional[str] = None,\n-    safe_serialization: bool = True,\n ):\n     if output_dir:\n         os.makedirs(output_dir, exist_ok=True)\n@@ -345,10 +344,10 @@ def convert_model(\n     # Save the model\n     if output_dir:\n         print(f\"Saving model to {output_dir}...\")\n-        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        model.save_pretrained(output_dir)\n     if output_hub_path:\n         print(f\"Pushing model to hub at {output_hub_path}...\")\n-        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+        model.push_to_hub(output_hub_path)\n \n     del state_dict, model\n     gc.collect()\n@@ -377,16 +376,12 @@ def main():\n         default=None,\n         help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     args = parser.parse_args()\n \n     convert_model(\n         hf_repo_id=args.hf_repo_id,\n         output_dir=args.output_dir,\n         output_hub_path=args.output_hub_path,\n-        safe_serialization=args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "7fb00c11864ee439bde88fcfa8549d9cfa256d6b",
            "filename": "src/transformers/models/depth_pro/convert_depth_pro_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconvert_depth_pro_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -132,7 +132,6 @@ def get_qkv_state_dict(key, parameter):\n def write_model(\n     hf_repo_id: str,\n     output_dir: str,\n-    safe_serialization: bool = True,\n ):\n     os.makedirs(output_dir, exist_ok=True)\n \n@@ -191,7 +190,7 @@ def write_model(\n     print(\"Checkpoint loaded successfully.\")\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -220,9 +219,6 @@ def main():\n         default=\"apple_DepthPro\",\n         help=\"Location to write the converted model and processor\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=argparse.BooleanOptionalAction,\n@@ -238,7 +234,6 @@ def main():\n     model = write_model(\n         hf_repo_id=args.hf_repo_id,\n         output_dir=args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     image_processor = write_image_processor("
        },
        {
            "sha": "4a64719d8faa331da4957bf89d832a905d5bd1ae",
            "filename": "src/transformers/models/efficientloftr/convert_efficientloftr_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fconvert_efficientloftr_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -130,7 +130,6 @@ def write_model(\n     model_repo,\n     file_name,\n     organization,\n-    safe_serialization=True,\n     push_to_hub=False,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -173,7 +172,7 @@ def write_model(\n     del model.config._name_or_path\n \n     print(\"Saving the model...\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -252,6 +251,5 @@ def write_image_processor(save_dir, model_name, organization, push_to_hub=False)\n         args.repo_id,\n         args.file_name,\n         args.organization,\n-        safe_serialization=True,\n         push_to_hub=args.push_to_hub,\n     )"
        },
        {
            "sha": "325b8731a56a54647b44551ca17157667df4e94f",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -303,7 +303,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n     state_dict = convert_state_dict_to_hf(model_vqgan.state_dict(), state_dict)\n \n     model.load_state_dict(state_dict, assign=True, strict=True)\n-    model.save_pretrained(output_dir, safe_serialization=True)\n+    model.save_pretrained(output_dir)\n \n     if hub_model_id is not None:\n         model.push_to_hub(hub_model_id)"
        },
        {
            "sha": "7fb0dac4bc531eae9bcb7332ad6746457414c87f",
            "filename": "src/transformers/models/eomt/convert_eomt_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -193,7 +193,6 @@ def convert_model(\n     local_dir=None,\n     output_dir=None,\n     output_hub_path=None,\n-    safe_serialization=True,\n     revision=None,\n ):\n     \"\"\"Convert and save the model weights, processor, and configuration.\"\"\"\n@@ -271,10 +270,10 @@ def convert_model(\n     # Save the model\n     if output_dir:\n         print(f\"Saving model to {output_dir}...\")\n-        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        model.save_pretrained(output_dir)\n     if output_hub_path:\n         print(f\"Pushing model to hub at {output_hub_path}...\")\n-        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+        model.push_to_hub(output_hub_path)\n \n     del state_dict, model\n     gc.collect()\n@@ -313,11 +312,6 @@ def main():\n         help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n         default=None,\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        help=\"Whether to save using safetensors\",\n-    )\n     args = parser.parse_args()\n \n     if args.output_dir is None and args.output_hub_path is None:\n@@ -331,7 +325,6 @@ def main():\n         local_dir=args.local_dir,\n         output_dir=args.output_dir,\n         output_hub_path=args.output_hub_path,\n-        safe_serialization=args.safe_serialization,\n         revision=args.revision,\n     )\n "
        },
        {
            "sha": "919c016a0bb83fc339c31b2444e246744bb944bd",
            "filename": "src/transformers/models/fuyu/convert_fuyu_model_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconvert_fuyu_model_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconvert_fuyu_model_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconvert_fuyu_model_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -77,7 +77,7 @@ def rename_state_dict(state_dict):\n     return model_state_dict\n \n \n-def convert_fuyu_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_model_path, safe_serialization=False):\n+def convert_fuyu_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_model_path):\n     sys.path.insert(0, ada_lib_path)\n     model_state_dict_base = torch.load(pt_model_path, map_location=\"cpu\", weights_only=True)\n     state_dict = flatdict.FlatDict(model_state_dict_base[\"model\"], \".\")\n@@ -86,7 +86,7 @@ def convert_fuyu_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_model_pat\n     transformers_config = FuyuConfig()\n     model = FuyuForCausalLM(transformers_config).to(torch.bfloat16)\n     model.load_state_dict(state_dict)\n-    model.save_pretrained(pytorch_dump_folder_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(pytorch_dump_folder_path)\n     transformers_config.save_pretrained(pytorch_dump_folder_path)\n \n \n@@ -108,14 +108,12 @@ def main():\n         \"--ada_lib_path\",\n         help=\"Location of original source code from adept to deserialize .pt checkpoint\",\n     )\n-    parser.add_argument(\"--safe_serialization\", type=bool, help=\"Whether or not to save using `safetensors`.\")\n     args = parser.parse_args()\n     spm_path = os.path.join(args.input_dir, \"adept_vocab.model\")\n \n     convert_fuyu_checkpoint(\n         pytorch_dump_folder_path=args.output_dir,\n         pt_model_path=args.pt_model_path,\n-        safe_serialization=args.safe_serialization,\n         ada_lib_path=args.ada_lib_path,\n     )\n     tokenizer = tokenizer_class(spm_path, bos_token=\"|ENDOFTEXT|\", eos_token=\"|ENDOFTEXT|\")"
        },
        {
            "sha": "a40d37303ef76e70e05ac21b34dc12b439143ead",
            "filename": "src/transformers/models/gemma/convert_gemma_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -65,7 +65,7 @@\n LAYER_NAME_MAPPING = {\"embedder.weight\": \"model.embed_tokens.weight\"}\n \n \n-def write_model(save_path, input_base_path, config, safe_serialization=True, push_to_hub=False, dtype=torch.float32):\n+def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=torch.float32):\n     num_attn_heads = config.num_attention_heads\n     hidden_size = config.hidden_size\n     num_kv_heads = config.num_key_value_heads\n@@ -120,9 +120,9 @@ def write_model(save_path, input_base_path, config, safe_serialization=True, pus\n \n     if push_to_hub:\n         print(f\"pushing the model to {save_path}\")\n-        model.push_to_hub(save_path, safe_serialization=safe_serialization, private=True)\n+        model.push_to_hub(save_path, private=True)\n     else:\n-        model.save_pretrained(save_path, safe_serialization=safe_serialization)\n+        model.save_pretrained(save_path)\n \n \n def write_tokenizer(input_tokenizer_path, save_path, push_to_hub=False):\n@@ -158,12 +158,6 @@ def main():\n         default=\"google/gemma-7b\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--pickle_serialization\",\n-        help=\"Whether or not to save using `safetensors`.\",\n-        action=\"store_true\",\n-        default=False,\n-    )\n     parser.add_argument(\n         \"--convert_tokenizer\",\n         help=\"Whether or not to convert the tokenizer as well.\",\n@@ -196,7 +190,6 @@ def main():\n         config=config,\n         input_base_path=args.input_checkpoint,\n         save_path=args.output_dir,\n-        safe_serialization=not args.pickle_serialization,\n         push_to_hub=args.push_to_hub,\n         dtype=dtype,\n     )"
        },
        {
            "sha": "0bb4e39a41e69f89b951c93f92971ef05a3b9bac",
            "filename": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -81,7 +81,7 @@\n LAYER_NAME_MAPPING = {\"embedder.weight\": \"model.embed_tokens.weight\"}\n \n \n-def write_model(save_path, input_base_path, config, safe_serialization=True, push_to_hub=False, dtype=torch.float32):\n+def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=torch.float32):\n     num_attn_heads = config.num_attention_heads\n     hidden_size = config.hidden_size\n     num_kv_heads = config.num_key_value_heads\n@@ -153,9 +153,9 @@ def write_model(save_path, input_base_path, config, safe_serialization=True, pus\n \n     if push_to_hub:\n         print(f\"pushing the model to {save_path}\")\n-        model.push_to_hub(save_path, safe_serialization=safe_serialization, private=True)\n+        model.push_to_hub(save_path, private=True)\n     else:\n-        model.save_pretrained(save_path, safe_serialization=safe_serialization)\n+        model.save_pretrained(save_path)\n \n \n def write_tokenizer(input_tokenizer_path, save_path, push_to_hub=False):\n@@ -191,12 +191,6 @@ def main():\n         default=\"google/gemma-9b\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--pickle_serialization\",\n-        help=\"Whether or not to save using `safetensors`.\",\n-        action=\"store_true\",\n-        default=False,\n-    )\n     parser.add_argument(\n         \"--convert_tokenizer\",\n         help=\"Whether or not to convert the tokenizer as well.\",\n@@ -229,7 +223,6 @@ def main():\n             config=config,\n             input_base_path=args.input_checkpoint,\n             save_path=args.output_dir,\n-            safe_serialization=not args.pickle_serialization,\n             push_to_hub=args.push_to_hub,\n             dtype=dtype,\n         )"
        },
        {
            "sha": "aab45dacc4c8aefe34a858e66dd851fc093119ac",
            "filename": "src/transformers/models/gemma3/convert_gemma3_weights.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconvert_gemma3_weights.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -633,7 +633,7 @@ def main(*args):\n         variant,\n         type(model).__name__,\n     )\n-    model.save_pretrained(output_path, safe_serialization=True)\n+    model.save_pretrained(output_path)\n     logging.info(\n         \"Saved Gemma 3 (%s) to SafeTensors in %s using %s\",\n         variant,"
        },
        {
            "sha": "393c936e7bf236c6e10eb9b2577dee0466a37335",
            "filename": "src/transformers/models/gemma3n/convert_gemma3n_weights.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconvert_gemma3n_weights.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -730,7 +730,7 @@ def main(*args):\n         variant,\n         type(model).__name__,\n     )\n-    model.save_pretrained(output_path, state_dict=state_tree, safe_serialization=True)\n+    model.save_pretrained(output_path, state_dict=state_tree)\n     logging.info(\n         \"Saved Gemma 3 (%s) to SafeTensors in %s using %s\",\n         variant,"
        },
        {
            "sha": "21251763720309b55c3949ee41a0a19efa524950",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -146,7 +146,6 @@ def convert_moe_packed_tensors(\n def write_model(\n     model_path,\n     input_base_path,\n-    safe_serialization=True,\n     instruct=False,\n     mxfp4=False,\n ):\n@@ -255,7 +254,7 @@ def write_model(\n         del config._name_or_path\n \n         print(\"Saving the model\")\n-        model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+        model.save_pretrained(model_path)\n         del state_dict, model\n \n     else:\n@@ -793,9 +792,6 @@ def main():\n         default=\"/fsx/mohamed/oai-hf/tests/120b_converted_packed\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--special_tokens\",\n         default=None,\n@@ -824,7 +820,6 @@ def main():\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         instruct=args.instruct,\n         mxfp4=args.mxfp4,\n     )"
        },
        {
            "sha": "ac840881f608d52ddf5e91f863832da35e47333c",
            "filename": "src/transformers/models/ijepa/convert_ijepa_to_hf.py",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fconvert_ijepa_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -147,7 +147,7 @@ def get_ijepa_config(model_name):\n \n \n @torch.no_grad()\n-def write_model(model_name, output_dir, safe_serialization, push_to_hub, verify_logits):\n+def write_model(model_name, output_dir, push_to_hub, verify_logits):\n     \"\"\"\n     Copy/paste/tweak model's weights to our IJEPA structure.\n     \"\"\"\n@@ -211,12 +211,12 @@ def write_model(model_name, output_dir, safe_serialization, push_to_hub, verify_\n     if output_dir:\n         Path(output_dir).mkdir(exist_ok=True)\n         print(f\"Saving model {model_name} to {output_dir}\")\n-        image_processor.save_pretrained(output_dir, safe_serialization=safe_serialization)\n-        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        image_processor.save_pretrained(output_dir)\n+        model.save_pretrained(output_dir)\n \n     if push_to_hub:\n-        image_processor.push_to_hub(repo_id=f\"jmtzt/{model_name}\", safe_serialization=safe_serialization)\n-        model.push_to_hub(repo_id=f\"jmtzt/{model_name}\", safe_serialization=safe_serialization)\n+        image_processor.push_to_hub(repo_id=f\"jmtzt/{model_name}\")\n+        model.push_to_hub(repo_id=f\"jmtzt/{model_name}\")\n \n     if output_dir:\n         del model, state_dict\n@@ -247,9 +247,6 @@ def main():\n         type=str,\n         help=\"Path to the output PyTorch model directory.\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--push_to_hub\",\n         action=\"store_true\",\n@@ -261,7 +258,7 @@ def main():\n \n     parser.set_defaults()\n     args = parser.parse_args()\n-    write_model(args.model_name, args.output_dir, args.safe_serialization, args.push_to_hub, args.verify_logits)\n+    write_model(args.model_name, args.output_dir, args.push_to_hub, args.verify_logits)\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "17fe87f8b5e295ad354a8da3cc7f00fb07c82e75",
            "filename": "src/transformers/models/janus/convert_janus_weights_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -16,7 +16,7 @@\n \"\"\"\n Example of run command (run from root):\n \n-python src/transformers/models/janus/convert_janus_weights_to_hf.py --repo_id deepseek-ai/Janus-Pro-1B --local_dir tmp/hub_code_in --output_dir tmp/hub_code_out --safe_serialization\n+python src/transformers/models/janus/convert_janus_weights_to_hf.py --repo_id deepseek-ai/Janus-Pro-1B --local_dir tmp/hub_code_in --output_dir tmp/hub_code_out\n Using provided local directory: tmp/hub_code_in\n \"\"\"\n \n@@ -245,7 +245,6 @@ def convert_model(\n     text_model_id=None,\n     output_dir=None,\n     output_hub_path=None,\n-    safe_serialization=True,\n     revision=None,\n ):\n     \"\"\"Convert and save the model weights, processor, and configuration.\"\"\"\n@@ -431,10 +430,10 @@ def convert_model(\n     # Save the model\n     if output_dir:\n         print(f\"Saving model to {output_dir}...\")\n-        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+        model.save_pretrained(output_dir)\n     if output_hub_path:\n         print(f\"Pushing model to hub at {output_hub_path}...\")\n-        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+        model.push_to_hub(output_hub_path)\n \n     del state_dict, model\n     gc.collect()\n@@ -479,11 +478,6 @@ def main():\n         help=\"Hub ID of the text model to get tokenizer from. Optional if tokenizer.json exists in the model directory.\",\n         required=False,\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        help=\"Whether to save using safetensors\",\n-    )\n     args = parser.parse_args()\n \n     if args.output_dir is None and args.output_hub_path is None:\n@@ -498,7 +492,6 @@ def main():\n         text_model_id=args.text_model_id,\n         output_dir=args.output_dir,\n         output_hub_path=args.output_hub_path,\n-        safe_serialization=args.safe_serialization,\n         revision=args.revision,\n     )\n "
        },
        {
            "sha": "e3947eb10327e36e41f7587234ac4aa14f40e99e",
            "filename": "src/transformers/models/kyutai_speech_to_text/convert_kyutai_speech_to_text_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconvert_kyutai_speech_to_text_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconvert_kyutai_speech_to_text_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fconvert_kyutai_speech_to_text_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -184,7 +184,6 @@ def write_model(\n     codec_model_path_or_repo,\n     codec_model_name,\n     output_dir,\n-    safe_serialization=True,\n     unwanted_prefix=\"transformer.\",\n ):\n     print(\"Converting the model.\")\n@@ -253,7 +252,7 @@ def write_model(\n     model.codec_model.generation_config.use_cache = True\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -342,9 +341,6 @@ def main():\n         \"--output_dir\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--audio_delay_seconds\",\n         type=float,\n@@ -365,7 +361,6 @@ def main():\n         args.codec_model_path_or_repo,\n         args.mimi_name,\n         args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     write_processor("
        },
        {
            "sha": "4a9971e91c918fcfa95f6abbb63f20eab1a87f76",
            "filename": "src/transformers/models/lightglue/convert_lightglue_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fconvert_lightglue_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -150,7 +150,6 @@ def write_model(\n     model_path,\n     checkpoint_url,\n     organization,\n-    safe_serialization=True,\n     push_to_hub=False,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -198,7 +197,7 @@ def write_model(\n     del model.config._name_or_path\n \n     print(\"Saving the model...\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -275,6 +274,5 @@ def write_image_processor(save_dir, model_name, organization, push_to_hub=False)\n         args.pytorch_dump_folder_path,\n         args.checkpoint_url,\n         args.organization,\n-        safe_serialization=True,\n         push_to_hub=args.push_to_hub,\n     )"
        },
        {
            "sha": "e0826b6d954b1e2e3ed63c0dc73938a8411cca42",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -185,7 +185,6 @@ def write_model(\n     model_path,\n     input_base_path,\n     model_size=None,\n-    safe_serialization=True,\n     llama_version=\"1\",\n     vocab_size=None,\n     num_shards=None,\n@@ -428,10 +427,10 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         if push_to_hub:\n             print(\"Pushing to the hub.\")\n             model_name = model_path.split(os.path.sep)[-1]\n-            model.push_to_hub(model_name, safe_serialization=safe_serialization, private=True)\n+            model.push_to_hub(model_name, private=True)\n         else:\n             print(\"Saving to disk.\")\n-            model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+            model.save_pretrained(model_path)\n \n \n class Llama3Converter(TikTokenConverter):\n@@ -541,9 +540,6 @@ def main():\n         action=\"store_true\",\n         default=False,\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     # Different Llama versions used different default values for max_position_embeddings, hence the need to be able to specify which version is being used.\n     parser.add_argument(\n         \"--llama_version\",\n@@ -594,7 +590,6 @@ def main():\n             model_path=args.output_dir,\n             input_base_path=args.input_dir,\n             model_size=args.model_size,\n-            safe_serialization=args.safe_serialization,\n             llama_version=args.llama_version,\n             vocab_size=vocab_size,\n             num_shards=args.num_shards,"
        },
        {
            "sha": "d19eaa0adc355c9026e032dfa484001c34c11490",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -215,7 +215,6 @@ def write_model(\n     input_base_path,\n     num_shards,\n     convert_checkpoints,\n-    safe_serialization=True,\n     instruct=False,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -518,7 +517,7 @@ def write_model(\n         model.load_state_dict(state_dict, strict=True, assign=True)\n         print(\"Model reloaded successfully.\")\n         print(\"Saving the model.\")\n-        model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+        model.save_pretrained(model_path)\n         del state_dict, model\n \n         # Safety check: reload the converted model\n@@ -704,9 +703,6 @@ def write_tokenizer(args):\n         type=str,\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--special_tokens\",\n         default=None,\n@@ -736,7 +732,6 @@ def write_tokenizer(args):\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         num_shards=args.num_shards,\n         instruct=args.instruct,\n         convert_checkpoints=args.convert_checkpoints,"
        },
        {
            "sha": "0ae7cdd5475ca5bed9e762afda47cc333c3e2c9a",
            "filename": "src/transformers/models/mixtral/convert_mixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconvert_mixtral_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -58,7 +58,7 @@ def write_json(text, path):\n         json.dump(text, f)\n \n \n-def write_model(model_path, input_base_path, model_size, safe_serialization=True):\n+def write_model(model_path, input_base_path, model_size):\n     os.makedirs(model_path, exist_ok=True)\n \n     params = read_json(os.path.join(input_base_path, \"params.json\"))\n@@ -214,7 +214,7 @@ def permute(w, n_heads=n_heads, dim1=dim, dim2=dim):\n     for n, p in model.named_parameters():\n         assert p.device.type != \"meta\", f\"{n} has not been loaded!\"\n \n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n \n \n def main():\n@@ -231,13 +231,11 @@ def main():\n         default=\"7B\",\n     )\n     parser.add_argument(\"--output_dir\", help=\"Location to write HF model\", required=True)\n-    parser.add_argument(\"--safe_serialization\", type=bool, help=\"Whether or not to save using `safetensors`.\")\n     args = parser.parse_args()\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n         model_size=args.model_size,\n-        safe_serialization=args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "313756245c4c919f5eebe73bc8d51f48ed593fe4",
            "filename": "src/transformers/models/mllama/convert_mllama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -210,7 +210,6 @@ def write_model(\n     model_path,\n     input_base_path,\n     num_shards,\n-    safe_serialization=True,\n     instruct=False,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -448,7 +447,7 @@ def write_model(\n     del model.config._name_or_path\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -599,9 +598,6 @@ def main():\n         default=\"Llama-3.2-11B-Vision\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--special_tokens\",\n         default=None,\n@@ -623,7 +619,6 @@ def main():\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         num_shards=args.num_shards,\n         instruct=args.instruct,\n     )"
        },
        {
            "sha": "60aae32cc621f3d4044b382ec3c828845b23b4c2",
            "filename": "src/transformers/models/musicgen/convert_musicgen_transformers.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fconvert_musicgen_transformers.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -128,9 +128,7 @@ def decoder_config_from_checkpoint(checkpoint: str) -> MusicgenDecoderConfig:\n \n \n @torch.no_grad()\n-def convert_musicgen_checkpoint(\n-    checkpoint, pytorch_dump_folder=None, repo_id=None, device=\"cpu\", safe_serialization=False\n-):\n+def convert_musicgen_checkpoint(checkpoint, pytorch_dump_folder=None, repo_id=None, device=\"cpu\"):\n     fairseq_model = MusicGen.get_pretrained(checkpoint, device=device)\n     decoder_config = decoder_config_from_checkpoint(checkpoint)\n \n@@ -192,12 +190,12 @@ def convert_musicgen_checkpoint(\n     if pytorch_dump_folder is not None:\n         Path(pytorch_dump_folder).mkdir(exist_ok=True)\n         logger.info(f\"Saving model {checkpoint} to {pytorch_dump_folder}\")\n-        model.save_pretrained(pytorch_dump_folder, safe_serialization=safe_serialization)\n+        model.save_pretrained(pytorch_dump_folder)\n         processor.save_pretrained(pytorch_dump_folder)\n \n     if repo_id:\n         logger.info(f\"Pushing model {checkpoint} to {repo_id}\")\n-        model.push_to_hub(repo_id, safe_serialization=safe_serialization)\n+        model.push_to_hub(repo_id)\n         processor.push_to_hub(repo_id)\n \n \n@@ -226,11 +224,6 @@ def convert_musicgen_checkpoint(\n     parser.add_argument(\n         \"--device\", default=\"cpu\", type=str, help=\"Torch device to run the conversion, either cpu or cuda.\"\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        help=\"Whether to save the model using `safetensors` or the traditional PyTorch way (that uses `pickle`).\",\n-    )\n \n     args = parser.parse_args()\n     convert_musicgen_checkpoint(args.checkpoint, args.pytorch_dump_folder, args.push_to_hub)"
        },
        {
            "sha": "80f61694e033c60ab9516fa37f34f7291dad649a",
            "filename": "src/transformers/models/nanochat/convert_nanochat_checkpoints.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnanochat%2Fconvert_nanochat_checkpoints.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -128,7 +128,7 @@ def load_config_from_checkpoint(input_path: Path) -> NanoChatConfig:\n     return config\n \n \n-def write_model(input_dir, output_dir, safe_serialization=True):\n+def write_model(input_dir, output_dir):\n     \"\"\"Convert NanoChat model from original checkpoint format to HuggingFace format.\"\"\"\n     print(\"Converting the model.\")\n     os.makedirs(output_dir, exist_ok=True)\n@@ -212,7 +212,7 @@ def assign(\n         del model.config._name_or_path\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -284,12 +284,6 @@ def main():\n         required=True,\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        default=True,\n-        help=\"Whether or not to save using `safetensors`.\",\n-    )\n     parser.add_argument(\n         \"--test_prompt\",\n         type=str,\n@@ -301,7 +295,6 @@ def main():\n     write_model(\n         args.input_dir,\n         args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     write_tokenizer(args.input_dir, args.output_dir)"
        },
        {
            "sha": "6cb626e143f8d4acf9b7c873a60fb78e29c29fa3",
            "filename": "src/transformers/models/olmo/convert_olmo_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconvert_olmo_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -62,7 +62,7 @@ def write_json(text, path):\n         json.dump(text, f)\n \n \n-def write_model(model_path, input_base_path, tokenizer_path=None, safe_serialization=True, fix_eos_token_id=True):\n+def write_model(model_path, input_base_path, tokenizer_path=None, fix_eos_token_id=True):\n     os.makedirs(model_path, exist_ok=True)\n     tmp_model_path = os.path.join(model_path, \"tmp\")\n     os.makedirs(tmp_model_path, exist_ok=True)\n@@ -179,7 +179,7 @@ def write_model(model_path, input_base_path, tokenizer_path=None, safe_serializa\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     shutil.rmtree(tmp_model_path)\n \n \n@@ -232,13 +232,11 @@ def main():\n         dest=\"fix_eos_token_id\",\n         help=\"If set, does not change eos token id from 0 to 50279 if it is 0. Changing 0 to 50279 is a bug fix, so use this option with care.\",\n     )\n-    parser.add_argument(\"--safe_serialization\", type=bool, help=\"Whether or not to save using `safetensors`.\")\n     # Different OLMo versions used different default values for max_position_embeddings, hence the need to be able to specify which version is being used.\n     args = parser.parse_args()\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         tokenizer_path=args.tokenizer_json_path,\n         fix_eos_token_id=args.fix_eos_token_id,\n     )"
        },
        {
            "sha": "0c7c1f9f9848d1a099a88802efbe9287e7b2d07b",
            "filename": "src/transformers/models/olmo2/convert_olmo2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconvert_olmo2_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -70,7 +70,6 @@ def write_model(\n     input_base_path,\n     include_tokenizer=True,\n     tokenizer_path=None,\n-    safe_serialization=True,\n     fix_eos_token_id=True,\n     tmp_cleanup=True,\n ):\n@@ -209,7 +208,7 @@ def write_model(\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     if tmp_cleanup:\n         # Make cleanup optional; attempting to `rmtree` the `tmp_model_path` causes\n         # errors if using NFS.\n@@ -284,17 +283,10 @@ def main():\n         dest=\"tmp_cleanup\",\n         help=\"If passed, don't remove temp dir at end of HF conversion.\",\n     )\n-    parser.add_argument(\n-        \"--no_safe_serialization\",\n-        action=\"store_false\",\n-        dest=\"safe_serialization\",\n-        help=\"Whether or not to save using `safetensors`.\",\n-    )\n     args = parser.parse_args()\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         include_tokenizer=args.include_tokenizer,\n         tokenizer_path=args.tokenizer_json_path,\n         fix_eos_token_id=args.fix_eos_token_id,"
        },
        {
            "sha": "eedbfa70614836eea70730ff663981049e797fae",
            "filename": "src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -290,7 +290,6 @@ def write_model(\n     input_base_path,\n     include_tokenizer=True,\n     tokenizer_id=None,\n-    safe_serialization=True,\n     tmp_cleanup=True,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -407,7 +406,7 @@ def write_model(\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     if tmp_cleanup:\n         # Make cleanup optional; attempting to `rmtree` the `tmp_model_path` causes\n         # errors if using NFS.\n@@ -454,17 +453,10 @@ def main():\n         dest=\"tmp_cleanup\",\n         help=\"If passed, don't remove temp dir at end of HF conversion.\",\n     )\n-    parser.add_argument(\n-        \"--no_safe_serialization\",\n-        action=\"store_false\",\n-        dest=\"safe_serialization\",\n-        help=\"Whether or not to save using `safetensors`.\",\n-    )\n     args = parser.parse_args()\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         include_tokenizer=args.include_tokenizer,\n         tokenizer_id=args.tokenizer,\n         tmp_cleanup=args.tmp_cleanup,"
        },
        {
            "sha": "35f223acf461b9ee4b27ba5691a49fbad7dfeee4",
            "filename": "src/transformers/models/olmoe/convert_olmoe_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmoe%2Fconvert_olmoe_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Folmoe%2Fconvert_olmoe_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconvert_olmoe_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -88,7 +88,7 @@ def write_json(text, path):\n         json.dump(text, f)\n \n \n-def write_model(model_path, input_base_path, tokenizer_path=None, safe_serialization=True, fix_eos_token_id=True):\n+def write_model(model_path, input_base_path, tokenizer_path=None, fix_eos_token_id=True):\n     os.makedirs(model_path, exist_ok=True)\n     tmp_model_path = os.path.join(model_path, \"tmp\")\n     os.makedirs(tmp_model_path, exist_ok=True)\n@@ -211,7 +211,7 @@ def write_model(model_path, input_base_path, tokenizer_path=None, safe_serializa\n     # Avoid saving this as part of the config.\n     del model.config._name_or_path\n     print(\"Saving in the Transformers format.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     shutil.rmtree(tmp_model_path)\n \n \n@@ -264,14 +264,10 @@ def main():\n         dest=\"fix_eos_token_id\",\n         help=\"If set, does not change eos token id from 0 to 50279 if it is 0. Changing 0 to 50279 is a bug fix, so use this option with care.\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", type=bool, default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     args = parser.parse_args()\n     write_model(\n         model_path=args.output_dir,\n         input_base_path=args.input_dir,\n-        safe_serialization=args.safe_serialization,\n         tokenizer_path=args.tokenizer_json_path,\n         fix_eos_token_id=args.fix_eos_token_id,\n     )"
        },
        {
            "sha": "29bf75e991e4f4a81bcce03abb49753f20aa7c97",
            "filename": "src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -356,7 +356,7 @@ def convert_paligemma2_checkpoint(\n         # convert to needed precision\n \n         model.to(DTYPES[precision])\n-        model.save_pretrained(pytorch_dump_folder_path, safe_serialization=True)\n+        model.save_pretrained(pytorch_dump_folder_path)\n         processor.save_pretrained(pytorch_dump_folder_path)\n \n     else:"
        },
        {
            "sha": "dafc4dfa343146d1f69528df664c4018948a3e31",
            "filename": "src/transformers/models/paligemma/convert_paligemma_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -287,7 +287,7 @@ def convert_paligemma_checkpoint(\n         dim=0,\n     )\n \n-    model.save_pretrained(pytorch_dump_folder_path, max_shard_size=\"2GB\", safe_serialization=True)\n+    model.save_pretrained(pytorch_dump_folder_path, max_shard_size=\"2GB\")\n     processor.save_pretrained(pytorch_dump_folder_path)\n \n "
        },
        {
            "sha": "b053cdcee037adf15dbbdf33246df736430142a0",
            "filename": "src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -197,7 +197,6 @@ def write_model(\n     input_base_path,\n     params,\n     image_token_id,\n-    safe_serialization=True,\n     tokenizer=None,\n     num_shards=None,\n     push_to_hub=False,\n@@ -426,10 +425,10 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         model_name = model_path.split(os.path.sep)[-1]\n         if push_to_hub:\n             print(\"Pushing to the hub.\")\n-            model.push_to_hub(model_name, safe_serialization=safe_serialization, private=True)\n+            model.push_to_hub(model_name, private=True)\n         else:\n             print(\"Saving to disk.\")\n-            model.save_pretrained(model_name, safe_serialization=safe_serialization)\n+            model.save_pretrained(model_name)\n \n \n class Llama3Converter(TikTokenConverter):\n@@ -563,12 +562,6 @@ def main():\n         action=\"store_true\",\n         default=False,\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        default=True,\n-        help=\"Whether or not to save using `safetensors`.\",\n-    )\n     parser.add_argument(\n         \"--num_shards\",\n         default=None,\n@@ -601,7 +594,6 @@ def main():\n         input_base_path=args.input_dir,\n         params=params,\n         image_token_id=tokenizer.image_token_id,\n-        safe_serialization=args.safe_serialization,\n         tokenizer=tokenizer,\n         num_shards=args.num_shards,\n         push_to_hub=args.push_to_hub,"
        },
        {
            "sha": "86d8762ebb0148f0dbe8c37bb4094f9cfd0acac2",
            "filename": "src/transformers/models/persimmon/convert_persimmon_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconvert_persimmon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconvert_persimmon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fconvert_persimmon_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -78,7 +78,7 @@ def rename_state_dict(state_dict):\n     return model_state_dict\n \n \n-def convert_persimmon_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_model_path, safe_serialization=False):\n+def convert_persimmon_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_model_path):\n     import sys\n \n     sys.path.insert(0, ada_lib_path)\n@@ -89,7 +89,7 @@ def convert_persimmon_checkpoint(pytorch_dump_folder_path, ada_lib_path, pt_mode\n     transformers_config = PersimmonConfig()\n     model = PersimmonForCausalLM(transformers_config, eos_token_id=71013, bos_token_id=71013).to(torch.bfloat16)\n     model.load_state_dict(state_dict)\n-    model.save_pretrained(pytorch_dump_folder_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(pytorch_dump_folder_path)\n     transformers_config.save_pretrained(pytorch_dump_folder_path)\n \n \n@@ -111,14 +111,12 @@ def main():\n         \"--ada_lib_path\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\"--safe_serialization\", type=bool, help=\"Whether or not to save using `safetensors`.\")\n     args = parser.parse_args()\n     spm_path = os.path.join(args.input_dir, \"adept_vocab.model\")\n \n     convert_persimmon_checkpoint(\n         pytorch_dump_folder_path=args.output_dir,\n         pt_model_path=args.pt_model_path,\n-        safe_serialization=args.safe_serialization,\n         ada_lib_path=args.ada_lib_path,\n     )\n     tokenizer = tokenizer_class(spm_path, bos_token=\"|ENDOFTEXT|\", eos_token=\"|ENDOFTEXT|\")"
        },
        {
            "sha": "4934dec5acf10b5b7c5d796cdfd5ab8450081269",
            "filename": "src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -69,7 +69,7 @@\n LAYER_NAME_MAPPING = {\"embedder.weight\": \"model.embed_tokens.weight\"}\n \n \n-def write_model(save_path, input_base_path, config, safe_serialization=True, push_to_hub=False, dtype=torch.float32):\n+def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=torch.float32):\n     print(f\"Fetching all parameters from the checkpoint at '{input_base_path}'\")\n     model_state_dict = torch.load(input_base_path, map_location=\"cpu\", weights_only=True)\n \n@@ -138,7 +138,7 @@ def write_model(save_path, input_base_path, config, safe_serialization=True, pus\n     if push_to_hub:\n         print(f\"pushing the model to {save_path}\")\n     else:\n-        model.save_pretrained(save_path, safe_serialization=safe_serialization)\n+        model.save_pretrained(save_path)\n \n \n def write_tokenizer(input_tokenizer_path, save_path, push_to_hub=False):\n@@ -174,12 +174,6 @@ def main():\n         default=\"google/recurrent-gemma-2b-it-hf\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--pickle_serialization\",\n-        help=\"Whether or not to save using `safetensors`.\",\n-        action=\"store_true\",\n-        default=False,\n-    )\n     parser.add_argument(\n         \"--convert_tokenizer\",\n         help=\"Whether or not to convert the tokenizer as well.\",\n@@ -212,7 +206,6 @@ def main():\n         config=config,\n         input_base_path=args.input_checkpoint,\n         save_path=args.output_dir,\n-        safe_serialization=not args.pickle_serialization,\n         push_to_hub=args.push_to_hub,\n         dtype=dtype,\n     )"
        },
        {
            "sha": "2bae1caf44a2a208fd2638c9890a77273edff4c5",
            "filename": "src/transformers/models/sam3/convert_sam3_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fconvert_sam3_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -300,7 +300,6 @@ def convert_sam3_checkpoint(\n     config: Optional[Sam3Config] = None,\n     push_to_hub: bool = False,\n     repo_id: Optional[str] = None,\n-    safe_serialization: bool = True,\n ):\n     \"\"\"\n     Convert SAM3 checkpoint from original format to HuggingFace format.\n@@ -311,7 +310,6 @@ def convert_sam3_checkpoint(\n         config: Optional Sam3Config to use (otherwise creates default)\n         push_to_hub: Whether to push the model to the Hub\n         repo_id: Repository ID for pushing to Hub\n-        safe_serialization: Whether to save using safetensors\n     \"\"\"\n     # Create output directory\n     os.makedirs(output_path, exist_ok=True)\n@@ -382,7 +380,6 @@ def convert_sam3_checkpoint(\n     print(f\"Saving converted model to {output_path}\")\n     model.save_pretrained(\n         output_path,\n-        safe_serialization=safe_serialization,\n     )\n \n     # Save processor\n@@ -453,12 +450,6 @@ def main():\n         default=None,\n         help=\"Repository ID for pushing to Hub (e.g., 'facebook/sam3-large')\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        default=True,\n-        help=\"Whether to save using safetensors format\",\n-    )\n \n     args = parser.parse_args()\n \n@@ -467,7 +458,6 @@ def main():\n         output_path=args.output_path,\n         push_to_hub=args.push_to_hub,\n         repo_id=args.repo_id,\n-        safe_serialization=args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "d7499b58928e35e60d6e222886bedf11e9df9f97",
            "filename": "src/transformers/models/sam3_video/convert_sam3_video_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fconvert_sam3_video_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -572,7 +572,6 @@ def convert_sam3_checkpoint(\n     config: Optional[Sam3VideoConfig] = None,\n     push_to_hub: bool = False,\n     repo_id: Optional[str] = None,\n-    safe_serialization: bool = True,\n ):\n     \"\"\"\n     Convert SAM3 checkpoint from original format to HuggingFace format.\n@@ -583,7 +582,6 @@ def convert_sam3_checkpoint(\n         config: Optional Sam3VideoConfig to use (otherwise creates default)\n         push_to_hub: Whether to push the model to the Hub\n         repo_id: Repository ID for pushing to Hub\n-        safe_serialization: Whether to save using safetensors\n     \"\"\"\n     # Create output directory\n     os.makedirs(output_path, exist_ok=True)\n@@ -663,7 +661,6 @@ def convert_sam3_checkpoint(\n     print(f\"Saving converted model to {output_path}\")\n     model.save_pretrained(\n         output_path,\n-        safe_serialization=safe_serialization,\n     )\n \n     # Save processor\n@@ -770,12 +767,6 @@ def main():\n         default=None,\n         help=\"Repository ID for pushing to Hub (e.g., 'facebook/sam3-large')\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\",\n-        action=\"store_true\",\n-        default=True,\n-        help=\"Whether to save using safetensors format\",\n-    )\n \n     args = parser.parse_args()\n \n@@ -784,7 +775,6 @@ def main():\n         output_path=args.output_path,\n         push_to_hub=args.push_to_hub,\n         repo_id=args.repo_id,\n-        safe_serialization=args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "ec02efd26a447d89c48b1c1c4ea143005ef2c82f",
            "filename": "src/transformers/models/shieldgemma2/convert_shieldgemma2_weights_orbax_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fconvert_shieldgemma2_weights_orbax_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -460,7 +460,7 @@ def main(*args):\n     model.load_state_dict(result.state_tree, assign=True, strict=True)\n     model.config.dtype = dtype\n     logging.info(\"Loaded Shieldgemma2 in Hugging Face Transformers.\")\n-    model.save_pretrained(output_path, safe_serialization=True)\n+    model.save_pretrained(output_path)\n     logging.info(\"Saved Shieldgemma2 to SafeTensors in %s\", output_path)\n     del model\n     del result"
        },
        {
            "sha": "0f78b645c6526844020e1be2a69df2d64384c0cf",
            "filename": "src/transformers/models/superglue/convert_superglue_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconvert_superglue_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -223,7 +223,6 @@ def add_keypoint_detector_state_dict(superglue_state_dict):\n def write_model(\n     model_path,\n     checkpoint_url,\n-    safe_serialization=True,\n     push_to_hub=False,\n ):\n     os.makedirs(model_path, exist_ok=True)\n@@ -270,7 +269,7 @@ def write_model(\n     del model.config._name_or_path\n \n     print(\"Saving the model...\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(model_path)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -336,6 +335,4 @@ def write_image_processor(save_dir, model_name, organization, push_to_hub=False)\n     )\n \n     args = parser.parse_args()\n-    write_model(\n-        args.pytorch_dump_folder_path, args.checkpoint_url, safe_serialization=True, push_to_hub=args.push_to_hub\n-    )\n+    write_model(args.pytorch_dump_folder_path, args.checkpoint_url, push_to_hub=args.push_to_hub)"
        },
        {
            "sha": "7a7d4cdc17b315b056b0ffbed21b878135be765f",
            "filename": "src/transformers/models/timesfm/convert_timesfm_orignal_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesfm%2Fconvert_timesfm_orignal_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -33,7 +33,7 @@ def get_nested_attr(obj, key):\n     return obj\n \n \n-def write_model(model_path, safe_serialization=True, huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"):\n+def write_model(model_path, huggingface_repo_id=\"google/timesfm-2.0-500m-pytorch\"):\n     os.makedirs(model_path, exist_ok=True)\n     tmp_model_path = os.path.join(model_path, \"tmp\")\n     os.makedirs(tmp_model_path, exist_ok=True)\n@@ -143,7 +143,7 @@ def write_model(model_path, safe_serialization=True, huggingface_repo_id=\"google\n             except AttributeError:\n                 print(f\"Skipping {old_key} (not found in original model).\")\n \n-    timesfm_model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    timesfm_model.save_pretrained(model_path)\n     shutil.rmtree(tmp_model_path)\n \n \n@@ -248,9 +248,6 @@ def main():\n         required=True,\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", type=bool, default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     parser.add_argument(\n         \"--huggingface_repo_id\",\n         type=str,\n@@ -260,14 +257,11 @@ def main():\n     args = parser.parse_args()\n \n     # if the saved model file exists, skip the conversion\n-    if os.path.exists(\n-        os.path.join(args.output_dir, \"model.safetensors\" if args.safe_serialization else \"pytorch_model.bin\")\n-    ):\n+    if os.path.exists(os.path.join(args.output_dir, \"model.safetensors\")):\n         print(f\"Model already exists in {args.output_dir}, skipping conversion.\")\n     else:\n         write_model(\n             model_path=args.output_dir,\n-            safe_serialization=args.safe_serialization,\n             huggingface_repo_id=args.huggingface_repo_id,\n         )\n     check_outputs(args.output_dir, args.huggingface_repo_id)"
        },
        {
            "sha": "eb93b129d45b1d444b2f8430be01741c059198c4",
            "filename": "src/transformers/models/univnet/convert_univnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fconvert_univnet.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -104,7 +104,6 @@ def convert_univnet_checkpoint(\n     pytorch_dump_folder_path,\n     config_path=None,\n     repo_id=None,\n-    safe_serialization=False,\n ):\n     model_state_dict_base = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n     # Get the generator's state dict\n@@ -126,7 +125,7 @@ def convert_univnet_checkpoint(\n     # Remove weight norm in preparation for inference\n     model.remove_weight_norm()\n \n-    model.save_pretrained(pytorch_dump_folder_path, safe_serialization=safe_serialization)\n+    model.save_pretrained(pytorch_dump_folder_path)\n \n     if repo_id:\n         print(\"Pushing to the hub...\")\n@@ -143,9 +142,6 @@ def main():\n     parser.add_argument(\n         \"--push_to_hub\", default=None, type=str, help=\"Where to upload the converted model on the Hugging Face hub.\"\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", action=\"store_true\", help=\"Whether to save the model using `safetensors`.\"\n-    )\n \n     args = parser.parse_args()\n \n@@ -154,7 +150,6 @@ def main():\n         args.pytorch_dump_folder_path,\n         args.config_path,\n         args.push_to_hub,\n-        args.safe_serialization,\n     )\n \n "
        },
        {
            "sha": "524c2dba53773bc9af922650d0377a7e27ac68bb",
            "filename": "src/transformers/models/voxtral/convert_voxtral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -176,7 +176,6 @@ def write_model(\n     model_name,\n     config_name,\n     output_dir,\n-    safe_serialization=True,\n ):\n     print(\"Converting the model.\")\n     os.makedirs(output_dir, exist_ok=True)\n@@ -224,7 +223,7 @@ def write_model(\n     model.generation_config.pad_token_id = 11\n \n     print(\"Saving the model.\")\n-    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    model.save_pretrained(output_dir)\n     del state_dict, model\n \n     # Safety check: reload the converted model\n@@ -278,17 +277,13 @@ def main():\n         \"--output_dir\",\n         help=\"Location to write HF model and tokenizer\",\n     )\n-    parser.add_argument(\n-        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n-    )\n     args = parser.parse_args()\n \n     write_model(\n         args.input_path_or_repo,\n         args.model_name,\n         args.config_name,\n         args.output_dir,\n-        safe_serialization=args.safe_serialization,\n     )\n \n     write_processor("
        },
        {
            "sha": "af00d81ab75cee0417733cca882147698160a26d",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -950,20 +950,13 @@ def __repr__(self):\n             pipe_information[\"output_modalities\"] = self.model.output_modalities\n         return f\"{self.__class__.__name__}: {pipe_information}\"\n \n-    def save_pretrained(\n-        self,\n-        save_directory: str | os.PathLike,\n-        safe_serialization: bool = True,\n-        **kwargs: Any,\n-    ):\n+    def save_pretrained(self, save_directory: str | os.PathLike, **kwargs: Any):\n         \"\"\"\n         Save the pipeline's model and tokenizer.\n \n         Args:\n             save_directory (`str` or `os.PathLike`):\n                 A path to the directory where to saved. It will be created if it doesn't exist.\n-            safe_serialization (`str`):\n-                Whether to save the model using `safetensors` or PyTorch serialization.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n@@ -992,7 +985,6 @@ def save_pretrained(\n             # Save the pipeline custom code\n             custom_object_save(self, save_directory)\n \n-        kwargs[\"safe_serialization\"] = safe_serialization\n         self.model.save_pretrained(save_directory, **kwargs)\n \n         if self.tokenizer is not None:"
        },
        {
            "sha": "a6e8a3f2388f8553e725493d830e30677055ddc7",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -301,12 +301,12 @@ def is_compileable(self) -> bool:\n         \"\"\"Flag indicating whether the quantized model can be compiled\"\"\"\n         return False\n \n-    def get_state_dict_and_metadata(self, model, safe_serialization=False):\n+    def get_state_dict_and_metadata(self, model):\n         \"\"\"Get state dict and metadata. Useful when we need to modify a bit the state dict due to quantization\"\"\"\n         return None, {}\n \n     @abstractmethod\n-    def is_serializable(self, safe_serialization=None): ...\n+    def is_serializable(self): ...\n \n     @property\n     @abstractmethod"
        },
        {
            "sha": "83556de23cf7d8d0759d6fa64995d4e66d3f1a4e",
            "filename": "src/transformers/quantizers/quantizer_aqlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_aqlm.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -86,5 +86,5 @@ def is_trainable(self) -> bool:\n             )\n             return False\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True"
        },
        {
            "sha": "c621a1ef2a85f806a7a4745ea9c04257c2e8280e",
            "filename": "src/transformers/quantizers/quantizer_auto_round.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -75,6 +75,6 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def is_trainable(self) -> bool:\n         return False\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         ## for gptq/awq models, the quantization config will be changed\n         return True"
        },
        {
            "sha": "e0b40f921e3612483719e2b21381806685df55c2",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -143,7 +143,7 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n \n             model = post_init_awq_ipex_modules(model)\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         # AWQ through auto-awq has been always serializable, except if the model is fused.\n         if self.quantization_config.do_fuse:\n             logger.warning(\"You cannot save an AWQ model that uses fused modules!\")"
        },
        {
            "sha": "ca31c94b9d10b3e83971503978c0e344aa157e5c",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -92,7 +92,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         target_dtype = torch.int8\n         return target_dtype\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "d39f1dc4ba90096065d6ea21df0cdc900ae1186c",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -185,7 +185,7 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n         model.is_4bit_serializable = self.is_serializable()\n         return model\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "d3cd135446c91093acf04306d22a67c76cf79502",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -154,7 +154,7 @@ def _process_model_before_weight_loading(\n             pre_quantized=self.pre_quantized,\n         )\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "d426c9a166d9c46d45866da00e8ca8fa31a51426",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -109,6 +109,6 @@ def is_qat_trainable(self) -> bool:\n         # models need to be decompressed carry out qat\n         return not self.run_compressed or not self.quantization_config.is_quantization_compressed\n \n-    def is_serializable(self, **kwargs) -> bool:\n+    def is_serializable(self) -> bool:\n         \"\"\"Models quantized using compressed tensors can be saved to disk\"\"\"\n         return True"
        },
        {
            "sha": "51c78d417523bd5388b2961cdae8ea14156ff3d1",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -105,7 +105,7 @@ def _process_model_before_weight_loading(\n             model, modules_to_not_convert=self.modules_to_not_convert, pre_quantized=self.pre_quantized\n         )\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "498269135990c0f6ebc3135c5f3cd6ab80dc9707",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -172,7 +172,7 @@ def update_tp_plan(self, config):\n \n         return config\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "268bb91ca421114b4dcba738bc9cdae236ff8651",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -130,7 +130,7 @@ def update_tp_plan(self, config):\n \n         return config\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "c46bb35fc963bc4005006ef0e18e63078cd2fec2",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -118,5 +118,5 @@ def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n             )\n         return trainable\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True"
        },
        {
            "sha": "5cd97b9b99a15fb3e459c4a4190845fc4cb0d01d",
            "filename": "src/transformers/quantizers/quantizer_gptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_gptq.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -118,5 +118,5 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def is_trainable(self) -> bool:\n         return True\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True"
        },
        {
            "sha": "7b57b7ff093264f82308f496656a8a4a46f031cd",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -160,7 +160,7 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n     def is_trainable(self) -> bool:\n         return False\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         return True\n \n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:"
        },
        {
            "sha": "d213fb8931b62ddcc1e1cf35c23a4cf8d2e76055",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -254,7 +254,7 @@ def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs\n         model.is_hqq_serializable = self.is_serializable()\n         return model\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "d50a28368bfc809961d7bfcfed0496ed3f5438f2",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -228,7 +228,7 @@ def get_param_name(self, param_name: str) -> str:\n                 return param_name.replace(\"down_proj\", \"down_proj_blocks\")\n         return param_name\n \n-    def get_state_dict_and_metadata(self, model, **kwargs):\n+    def get_state_dict_and_metadata(self, model):\n         from ..integrations import Mxfp4GptOssExperts\n \n         state_dict = model.state_dict()\n@@ -267,7 +267,7 @@ def get_state_dict_and_metadata(self, model, **kwargs):\n         metadata = {}\n         return state_dict, metadata\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True\n \n     @property"
        },
        {
            "sha": "188ae20efd4cf357a698106b8d482d2a03ff87b0",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -112,7 +112,7 @@ def _process_model_before_weight_loading(\n     def is_trainable(self) -> bool:\n         return True\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return False\n \n     def get_quantize_ops(self):"
        },
        {
            "sha": "4190d0d4c6426a7cf1e240b0dfae6f6eb4d28854",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -72,7 +72,7 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwarg\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         return True\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return False\n \n     @property"
        },
        {
            "sha": "b534788551dfc2be67d51f9c6c1016cf0559e886",
            "filename": "src/transformers/quantizers/quantizer_spqr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_spqr.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -80,5 +80,5 @@ def _process_model_before_weight_loading(\n     def is_trainable(self):\n         return False\n \n-    def is_serializable(self, **kwargs):\n+    def is_serializable(self):\n         return True"
        },
        {
            "sha": "8614096f8ac69ae597093f825475b7c8d3e65b2b",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 13,
            "deletions": 26,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -152,20 +152,16 @@ def update_dtype(self, dtype):\n                 dtype = torch.float32\n         return dtype\n \n-    def get_state_dict_and_metadata(self, model, safe_serialization: bool | None = False):\n+    def get_state_dict_and_metadata(self, model):\n         \"\"\"\n-        If the model is safe serializable, we flatten the state dict of tensor subclasses so that it is compatible with\n-        the safetensors format.\n+        We flatten the state dict of tensor subclasses so that it is compatible with the safetensors format.\n         \"\"\"\n-        if safe_serialization:\n-            if TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n-                return flatten_tensor_state_dict(model.state_dict())\n-            else:\n-                raise RuntimeError(\n-                    f\"In order to use safetensors with torchao, please use torchao version >= 0.15.0. Current version: {TORCHAO_VERSION}\"\n-                )\n+        if TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n+            return flatten_tensor_state_dict(model.state_dict()), {}\n         else:\n-            return None, {}\n+            raise RuntimeError(\n+                f\"In order to use safetensors with torchao, please use torchao version >= 0.15.0. Current version: {TORCHAO_VERSION}\"\n+            )\n \n     def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         from accelerate.utils import CustomDtype\n@@ -289,23 +285,14 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n             return model\n         return\n \n-    def is_serializable(self, safe_serialization=None) -> bool:\n-        if safe_serialization:\n-            _is_torchao_serializable = TORCHAO_VERSION >= version.parse(\"0.15.0\")\n-            if not TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n-                logger.warning(\n-                    f\"torchao quantized model only supports safe serialization for torchao version >= 0.15.0, please set `safe_serialization` to False for \\\n-                    {type(self.quantization_config.quant_type)} and {TORCHAO_VERSION}.\"\n-                )\n-            return _is_torchao_serializable\n-\n-        if self.offload and self.quantization_config.modules_to_not_convert is None:\n+    def is_serializable(self) -> bool:\n+        _is_torchao_serializable = TORCHAO_VERSION >= version.parse(\"0.15.0\")\n+        if not TORCHAO_VERSION >= version.parse(\"0.15.0\"):\n             logger.warning(\n-                \"The model contains offloaded modules and these modules are not quantized. We don't recommend saving the model as we won't be able to reload them.\"\n-                \"If you want to specify modules to not quantize, please specify modules_to_not_convert in the quantization_config.\"\n+                \"torchao quantized model only supports serialization for torchao version >= 0.15.0, please upgrade \"\n+                \"your version to save the quantized model\"\n             )\n-            return False\n-        return True\n+        return _is_torchao_serializable\n \n     def get_accelerator_warm_up_factor(self):\n         \"\"\""
        },
        {
            "sha": "0a3600f4d44a2d6b6a9cb02cb88d45004e54a824",
            "filename": "src/transformers/quantizers/quantizer_vptq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_vptq.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -78,5 +78,5 @@ def _process_model_before_weight_loading(\n     def is_trainable(self) -> bool:\n         return False\n \n-    def is_serializable(self, safe_serialization=None):\n+    def is_serializable(self):\n         return True"
        },
        {
            "sha": "4ecc5604e47e540ff59d23c9b4b1f3cdff11e30a",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 51,
            "deletions": 0,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -38,6 +38,7 @@\n import unittest\n from collections import UserDict, defaultdict\n from collections.abc import Callable, Generator, Iterable, Iterator, Mapping\n+from contextlib import contextmanager\n from dataclasses import MISSING, fields\n from functools import cache, wraps\n from io import StringIO\n@@ -72,7 +73,9 @@\n from .utils import (\n     ACCELERATE_MIN_VERSION,\n     GGUF_MIN_VERSION,\n+    SAFE_WEIGHTS_INDEX_NAME,\n     TRITON_MIN_VERSION,\n+    WEIGHTS_INDEX_NAME,\n     is_accelerate_available,\n     is_apex_available,\n     is_apollo_torch_available,\n@@ -218,6 +221,9 @@\n \n if is_torch_available():\n     import torch\n+    from safetensors.torch import load_file\n+\n+    from .modeling_utils import PreTrainedModel\n \n     IS_ROCM_SYSTEM = torch.version.hip is not None\n     IS_CUDA_SYSTEM = torch.version.cuda is not None\n@@ -4313,3 +4319,48 @@ def build_cpu_memory_monitor(logger_instance: logging.Logger | None = None) -> C\n         else:\n             logger_instance.warning(\"psutil not available, memory monitoring disabled\")\n     return monitor\n+\n+\n+def convert_all_safetensors_to_bins(folder: str):\n+    \"\"\"Convert all safetensors files into torch bin files, to mimic saving with torch (since we still support loading\n+    bin files, but not saving them anymore)\"\"\"\n+    for file in os.listdir(folder):\n+        path = os.path.join(folder, file)\n+        if file.endswith(\".safetensors\"):\n+            new_path = path.replace(\".safetensors\", \".bin\").replace(\"model\", \"pytorch_model\")\n+            state_dict = load_file(path)\n+            os.remove(path)\n+            torch.save(state_dict, new_path)\n+        # Adapt the index as well\n+        elif file == SAFE_WEIGHTS_INDEX_NAME:\n+            new_path = os.path.join(folder, WEIGHTS_INDEX_NAME)\n+            with open(path) as f:\n+                index = json.loads(f.read())\n+            os.remove(path)\n+            if \"weight_map\" in index.keys():\n+                weight_map = index[\"weight_map\"]\n+                new_weight_map = {}\n+                for k, v in weight_map.items():\n+                    new_weight_map[k] = v.replace(\".safetensors\", \".bin\").replace(\"model\", \"pytorch_model\")\n+            index[\"weight_map\"] = new_weight_map\n+            with open(new_path, \"w\") as f:\n+                f.write(json.dumps(index, indent=4))\n+\n+\n+@contextmanager\n+def force_serialization_as_bin_files():\n+    \"\"\"Since we don't support saving with torch `.bin` files anymore, but still support loading them, we use this context\n+    to easily create the bin files and try to load them back\"\"\"\n+    try:\n+        # Monkey patch the method to save as bin files\n+        original_save = PreTrainedModel.save_pretrained\n+\n+        def new_save(self, save_directory, *args, **kwargs):\n+            original_save(self, save_directory, *args, **kwargs)\n+            convert_all_safetensors_to_bins(save_directory)\n+\n+        PreTrainedModel.save_pretrained = new_save\n+\n+        yield\n+    finally:\n+        PreTrainedModel.save_pretrained = original_save"
        },
        {
            "sha": "bb2009e364a7f02618ce9376784217758c022df5",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 25,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -2801,7 +2801,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 )\n             else:\n                 # We load the model state dict on the CPU to avoid an OOM error.\n-                if self.args.save_safetensors and os.path.isfile(safe_weights_file):\n+                if os.path.isfile(safe_weights_file):\n                     state_dict = safetensors.torch.load_file(safe_weights_file, device=\"cpu\")\n                 else:\n                     check_torch_load_is_safe()\n@@ -2841,9 +2841,7 @@ def _load_from_checkpoint(self, resume_from_checkpoint, model=None):\n                 logger.warning(f\"Could not load adapter model, make sure to have PEFT >= {MIN_PEFT_VERSION} installed\")\n         else:\n             # We load the sharded checkpoint\n-            load_result = load_sharded_checkpoint(\n-                model, resume_from_checkpoint, strict=is_sagemaker_mp_enabled(), prefer_safe=self.args.save_safetensors\n-            )\n+            load_result = load_sharded_checkpoint(model, resume_from_checkpoint, strict=is_sagemaker_mp_enabled())\n             if not is_sagemaker_mp_enabled():\n                 self._issue_warnings_after_load(load_result)\n \n@@ -2926,7 +2924,7 @@ def _load_best_model(self):\n                         has_been_loaded = False\n                 else:\n                     # We load the model state dict on the CPU to avoid an OOM error.\n-                    if self.args.save_safetensors and os.path.isfile(best_safe_model_path):\n+                    if os.path.isfile(best_safe_model_path):\n                         state_dict = safetensors.torch.load_file(best_safe_model_path, device=\"cpu\")\n                     else:\n                         check_torch_load_is_safe()\n@@ -4080,12 +4078,7 @@ def _save_tpu(self, output_dir: str | None = None):\n                 model = model.module.module\n                 unwrapped_model = self.accelerator.unwrap_model(model)\n                 if isinstance(unwrapped_model, supported_classes):\n-                    unwrapped_model.save_pretrained(\n-                        output_dir,\n-                        state_dict=full_state_dict,\n-                        save_function=xm.save,\n-                        safe_serialization=self.args.save_safetensors,\n-                    )\n+                    unwrapped_model.save_pretrained(output_dir, state_dict=full_state_dict)\n                 else:\n                     logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n                     xm.save(full_state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n@@ -4095,8 +4088,6 @@ def _save_tpu(self, output_dir: str | None = None):\n                     output_dir,\n                     is_main_process=self.args.should_save,\n                     state_dict=xm._maybe_convert_to_cpu(model.state_dict()),\n-                    save_function=xm.save,\n-                    safe_serialization=self.args.save_safetensors,\n                 )\n             else:\n                 logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n@@ -4106,8 +4097,6 @@ def _save_tpu(self, output_dir: str | None = None):\n             model.save_pretrained(\n                 output_dir,\n                 is_main_process=self.args.should_save,\n-                save_function=xm.save,\n-                safe_serialization=self.args.save_safetensors,\n                 state_dict=xm._maybe_convert_to_cpu(model.state_dict()),\n             )\n         if self.processing_class is not None and self.args.should_save:\n@@ -4128,20 +4117,15 @@ def _save(self, output_dir: str | None = None, state_dict=None):\n \n             if isinstance(self.accelerator.unwrap_model(self.model, keep_torch_compile=False), supported_classes):\n                 self.accelerator.unwrap_model(self.model, keep_torch_compile=False).save_pretrained(\n-                    output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n+                    output_dir, state_dict=state_dict\n                 )\n             else:\n                 logger.info(\"Trainer.model is not a `PreTrainedModel`, only saving its state dict.\")\n-                if self.args.save_safetensors:\n-                    safetensors.torch.save_file(\n-                        state_dict, os.path.join(output_dir, SAFE_WEIGHTS_NAME), metadata={\"format\": \"pt\"}\n-                    )\n-                else:\n-                    torch.save(state_dict, os.path.join(output_dir, WEIGHTS_NAME))\n+                safetensors.torch.save_file(\n+                    state_dict, os.path.join(output_dir, SAFE_WEIGHTS_NAME), metadata={\"format\": \"pt\"}\n+                )\n         else:\n-            self.model.save_pretrained(\n-                output_dir, state_dict=state_dict, safe_serialization=self.args.save_safetensors\n-            )\n+            self.model.save_pretrained(output_dir, state_dict=state_dict)\n \n         if self.processing_class is not None:\n             self.processing_class.save_pretrained(output_dir)"
        },
        {
            "sha": "82bbf6cb33debc26dbd04fcef494ebf113433000",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -351,9 +351,6 @@ class TrainingArguments:\n             SIGTERM with adequate time before the job time limit. Calculate the required grace period as: longest\n             possible iteration time + checkpoint saving time. For example, if an iteration takes 2 minutes and\n             checkpoint saving takes 2 minutes, set at least 4 minutes (240 seconds) of grace time.\n-        save_safetensors (`bool`, *optional*, defaults to `True`):\n-            Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n-            default `torch.load` and `torch.save`.\n         save_on_each_node (`bool`, *optional*, defaults to `False`):\n             When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n             the main one.\n@@ -960,12 +957,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    save_safetensors: bool = field(\n-        default=True,\n-        metadata={\n-            \"help\": \"Use safetensors saving and loading for state dicts instead of default torch.load and torch.save.\"\n-        },\n-    )\n     save_on_each_node: bool = field(\n         default=False,\n         metadata={\n@@ -1531,14 +1522,6 @@ def __post_init__(self):\n                         f\"steps, but found {self.save_steps}, which is not a round multiple of {self.eval_steps}.\"\n                     )\n \n-        if not self.save_safetensors:\n-            logger.info(\n-                f\"Found safetensors installation, but --save_safetensors={self.save_safetensors}. \"\n-                f\"Safetensors should be a preferred weights saving format due to security and performance reasons. \"\n-                f\"If your model cannot be saved by safetensors please feel free to open an issue at \"\n-                f\"https://github.com/huggingface/safetensors!\"\n-            )\n-\n         if (\n             self.load_best_model_at_end or self.lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU\n         ) and self.metric_for_best_model is None:"
        },
        {
            "sha": "127ae9bdc5958671cacc3e50bb0d335f22de09d9",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -721,8 +721,7 @@ def push_to_hub(\n         revision: str | None = None,\n         create_pr: bool = False,\n         # Serialization details\n-        max_shard_size: int | str | None = \"5GB\",\n-        safe_serialization: bool = True,\n+        max_shard_size: int | str | None = \"50GB\",\n         tags: list[str] | None = None,\n     ) -> str:\n         \"\"\"\n@@ -745,13 +744,10 @@ def push_to_hub(\n                 Branch to push the uploaded files to.\n             create_pr (`bool`, *optional*, defaults to `False`):\n                 Whether or not to create a PR with the uploaded files or directly commit.\n-            max_shard_size (`int` or `str`, *optional*, defaults to `\"5GB\"`):\n+            max_shard_size (`int` or `str`, *optional*, defaults to `\"50GB\"`):\n                 Only applicable for models. The maximum size for a checkpoint before being sharded. Checkpoints shard\n                 will then be each of size lower than this size. If expressed as a string, needs to be digits followed\n-                by a unit (like `\"5MB\"`). We default it to `\"5GB\"` so that users can easily load models on free-tier\n-                Google Colab instances without any CPU OOM issues.\n-            safe_serialization (`bool`, *optional*, defaults to `True`):\n-                Whether or not to convert the model weights in safetensors format for safer serialization.\n+                by a unit (like `\"5MB\"`).\n             tags (`list[str]`, *optional*):\n                 List of tags to push on the Hub.\n \n@@ -777,7 +773,7 @@ def push_to_hub(\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             # Save all files.\n-            self.save_pretrained(tmp_dir, max_shard_size=max_shard_size, safe_serialization=safe_serialization)\n+            self.save_pretrained(tmp_dir, max_shard_size=max_shard_size)\n \n             # Update model card\n             model_card.save(os.path.join(tmp_dir, \"README.md\"))"
        },
        {
            "sha": "9c147bab8d0873ae822990ce9d8a59d8202603f1",
            "filename": "templates/adding_a_new_example_script/{{cookiecutter.directory_name}}/run_{{cookiecutter.example_shortcut}}.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/templates%2Fadding_a_new_example_script%2F%7B%7Bcookiecutter.directory_name%7D%7D%2Frun_%7B%7Bcookiecutter.example_shortcut%7D%7D.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -907,7 +907,7 @@ def tokenize_function(examples):\n     if args.output_dir is not None:\n         accelerator.wait_for_everyone()\n         unwrapped_model = accelerator.unwrap_model(model)\n-        unwrapped_model.save_pretrained(args.output_dir, save_function=accelerator.save)\n+        unwrapped_model.save_pretrained(args.output_dir)\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "74d07c684621c8ebeed9160224c73a4b9c3c546e",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 4,
            "deletions": 14,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -140,13 +140,6 @@ def test_peft_save_pretrained(self):\n                     peft_model = transformers_class.from_pretrained(tmpdirname).to(torch_device)\n                     self.assertTrue(self._check_lora_correctly_converted(peft_model))\n \n-                    peft_model.save_pretrained(tmpdirname, safe_serialization=False)\n-                    self.assertTrue(\"adapter_model.bin\" in os.listdir(tmpdirname))\n-                    self.assertTrue(\"adapter_config.json\" in os.listdir(tmpdirname))\n-\n-                    peft_model = transformers_class.from_pretrained(tmpdirname).to(torch_device)\n-                    self.assertTrue(self._check_lora_correctly_converted(peft_model))\n-\n     def test_peft_enable_disable_adapters(self):\n         \"\"\"\n         A test that checks if `enable_adapters` and `disable_adapters` methods work as expected.\n@@ -521,7 +514,6 @@ def test_peft_save_quantized(self):\n     def test_peft_save_quantized_regression(self):\n         \"\"\"\n         Simple test that tests the basic usage of PEFT model save_pretrained with quantized base models\n-        Regression test to make sure everything works as expected before the safetensors integration.\n         \"\"\"\n         # 4bit\n         for model_id in self.peft_test_model_ids:\n@@ -536,10 +528,9 @@ def test_peft_save_quantized_regression(self):\n                 self.assertTrue(peft_model.hf_device_map is not None)\n \n                 with tempfile.TemporaryDirectory() as tmpdirname:\n-                    peft_model.save_pretrained(tmpdirname, safe_serialization=False)\n-                    self.assertTrue(\"adapter_model.bin\" in os.listdir(tmpdirname))\n+                    peft_model.save_pretrained(tmpdirname)\n+                    self.assertTrue(\"adapter_model.safetensors\" in os.listdir(tmpdirname))\n                     self.assertTrue(\"adapter_config.json\" in os.listdir(tmpdirname))\n-                    self.assertTrue(\"pytorch_model.bin\" not in os.listdir(tmpdirname))\n                     self.assertTrue(\"model.safetensors\" not in os.listdir(tmpdirname))\n \n         # 8-bit\n@@ -555,11 +546,10 @@ def test_peft_save_quantized_regression(self):\n                 self.assertTrue(peft_model.hf_device_map is not None)\n \n                 with tempfile.TemporaryDirectory() as tmpdirname:\n-                    peft_model.save_pretrained(tmpdirname, safe_serialization=False)\n+                    peft_model.save_pretrained(tmpdirname)\n \n-                    self.assertTrue(\"adapter_model.bin\" in os.listdir(tmpdirname))\n+                    self.assertTrue(\"adapter_model.safetensors\" in os.listdir(tmpdirname))\n                     self.assertTrue(\"adapter_config.json\" in os.listdir(tmpdirname))\n-                    self.assertTrue(\"pytorch_model.bin\" not in os.listdir(tmpdirname))\n                     self.assertTrue(\"model.safetensors\" not in os.listdir(tmpdirname))\n \n     def test_peft_pipeline(self):"
        },
        {
            "sha": "33bc8190dda48c9d28c6509f5b74eb4e49da06a2",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -203,15 +203,6 @@ def test_quantized_model_layers(self):\n             self.quantized_model.to(0)\n         self.assertEqual(self.quantized_model.model.layers[0].self_attn.k_proj.weight._data.device.type, torch_device)\n \n-    def test_serialization_bin(self):\n-        \"\"\"\n-        Test the serialization, the loading and the inference of the quantized weights\n-        \"\"\"\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            with self.assertRaises(ValueError) as e:\n-                self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n-            self.assertIn(\"The model is quantized with quanto and is not serializable\", str(e.exception))\n-\n     def test_serialization_safetensors(self):\n         \"\"\"\n         Test the serialization, the loading and the inference of the quantized weights"
        },
        {
            "sha": "e49278274298389d38968898b01e0a5b7b7bf420",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -720,16 +720,16 @@ def test_original_model_expected_output(self):\n \n         self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n-    def check_serialization_expected_output(self, device, expected_output, safe_serialization=False):\n+    def check_serialization_expected_output(self, device, expected_output):\n         \"\"\"\n         Test if we can serialize and load/infer the model again on the same device\n         \"\"\"\n         dtype = torch.bfloat16 if isinstance(self.quant_scheme, Int4WeightOnlyConfig) else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n-            self.quantized_model.save_pretrained(tmpdirname, safe_serialization=safe_serialization)\n+            self.quantized_model.save_pretrained(tmpdirname)\n \n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n-                tmpdirname, dtype=dtype, device_map=device, torch_dtype=dtype, use_safetensors=safe_serialization\n+                tmpdirname, dtype=dtype, device_map=device, torch_dtype=dtype\n             )\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(device)\n \n@@ -791,7 +791,7 @@ def test_serialization_expected_output(self, config, expected_output):\n             device_map=device,\n             quantization_config=self.quant_config,\n         )\n-        self.check_serialization_expected_output(device, expected_output, safe_serialization=True)\n+        self.check_serialization_expected_output(device, expected_output)\n \n \n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):"
        },
        {
            "sha": "ab638c7edf6ac7cfcca38715995c3c4717724f84",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -80,6 +80,7 @@\n )\n from transformers.testing_utils import (\n     CaptureLogger,\n+    force_serialization_as_bin_files,\n     get_device_properties,\n     hub_retry,\n     is_flaky,\n@@ -1995,7 +1996,7 @@ def test_can_use_safetensors(self):\n             model_tied = model_class(config)\n             with tempfile.TemporaryDirectory() as d:\n                 try:\n-                    model_tied.save_pretrained(d, safe_serialization=True)\n+                    model_tied.save_pretrained(d)\n                 except Exception as e:\n                     raise Exception(f\"Class {model_class.__name__} cannot be saved using safetensors: {e}\")\n                 with self.subTest(model_class):\n@@ -2379,7 +2380,10 @@ def test_disk_offload_bin(self):\n \n             model_size = compute_module_sizes(model)[0][\"\"]\n             with tempfile.TemporaryDirectory() as tmp_dir:\n-                model.cpu().save_pretrained(tmp_dir, safe_serialization=False)\n+                # Since we don't support saving with bins files anymore, but still support loading we use this context\n+                # to easily create the bins files and try to load them\n+                with force_serialization_as_bin_files():\n+                    model.cpu().save_pretrained(tmp_dir)\n \n                 with self.assertRaises(ValueError):\n                     max_size = int(self.model_split_percents[0] * model_size)"
        },
        {
            "sha": "ffcced52a85db77ea714a91b52e028a5717d3d82",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 45,
            "deletions": 91,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -24,7 +24,6 @@\n import tempfile\n import unittest\n from functools import partial\n-from itertools import product\n from pathlib import Path\n from typing import Any\n from unittest.mock import Mock, patch\n@@ -114,9 +113,6 @@\n from transformers.utils import (\n     SAFE_WEIGHTS_INDEX_NAME,\n     SAFE_WEIGHTS_NAME,\n-    WEIGHTS_INDEX_NAME,\n-    WEIGHTS_NAME,\n-    check_torch_load_is_safe,\n     is_accelerate_available,\n     is_apex_available,\n     is_bitsandbytes_available,\n@@ -640,10 +636,8 @@ def _tokenize_function(examples):\n \n \n class TrainerIntegrationCommon:\n-    def check_saved_checkpoints(\n-        self, output_dir, freq, total, is_pretrained=True, safe_weights=True, use_scaler=False\n-    ):\n-        weights_file = WEIGHTS_NAME if not safe_weights else SAFE_WEIGHTS_NAME\n+    def check_saved_checkpoints(self, output_dir, freq, total, is_pretrained=True, use_scaler=False):\n+        weights_file = SAFE_WEIGHTS_NAME\n         file_list = [weights_file, \"training_args.bin\", \"optimizer.pt\", \"scheduler.pt\", \"trainer_state.json\"]\n         if is_pretrained:\n             file_list.append(\"config.json\")\n@@ -656,7 +650,14 @@ def check_saved_checkpoints(\n                 self.assertTrue(os.path.isfile(os.path.join(checkpoint, filename)))\n \n     def check_best_model_has_been_loaded(\n-        self, output_dir, freq, total, trainer, metric, greater_is_better=False, is_pretrained=True, safe_weights=True\n+        self,\n+        output_dir,\n+        freq,\n+        total,\n+        trainer,\n+        metric,\n+        greater_is_better=False,\n+        is_pretrained=True,\n     ):\n         checkpoint = os.path.join(output_dir, f\"checkpoint-{(total // freq) * freq}\")\n         log_history = TrainerState.load_from_json(os.path.join(checkpoint, \"trainer_state.json\")).log_history\n@@ -670,11 +671,7 @@ def check_best_model_has_been_loaded(\n             best_model.to(trainer.args.device)\n         else:\n             best_model = RegressionModel()\n-            if not safe_weights:\n-                check_torch_load_is_safe()\n-                state_dict = torch.load(os.path.join(checkpoint, WEIGHTS_NAME), weights_only=True)\n-            else:\n-                state_dict = safetensors.torch.load_file(os.path.join(checkpoint, SAFE_WEIGHTS_NAME))\n+            state_dict = safetensors.torch.load_file(os.path.join(checkpoint, SAFE_WEIGHTS_NAME))\n             best_model.load_state_dict(state_dict)\n             best_model.to(trainer.args.device)\n         torch.testing.assert_close(best_model.a, trainer.model.a)\n@@ -707,26 +704,15 @@ def check_trainer_state_are_the_same(self, trainer_state, trainer_state1):\n \n             self.assertEqual(log, log1)\n \n-    def convert_to_sharded_checkpoint(self, folder, save_safe=True, load_safe=True):\n+    def convert_to_sharded_checkpoint(self, folder):\n         # Converts a checkpoint of a regression model to a sharded checkpoint.\n-        if load_safe:\n-            loader = safetensors.torch.load_file\n-            weights_file = os.path.join(folder, SAFE_WEIGHTS_NAME)\n-        else:\n-            check_torch_load_is_safe()\n-            loader = torch.load\n-            weights_file = os.path.join(folder, WEIGHTS_NAME)\n-\n-        if save_safe:\n-            extension = \"safetensors\"\n-            saver = safetensors.torch.save_file\n-            index_file = os.path.join(folder, SAFE_WEIGHTS_INDEX_NAME)\n-            shard_name = SAFE_WEIGHTS_NAME\n-        else:\n-            extension = \"bin\"\n-            saver = torch.save\n-            index_file = os.path.join(folder, WEIGHTS_INDEX_NAME)\n-            shard_name = WEIGHTS_NAME\n+        loader = safetensors.torch.load_file\n+        weights_file = os.path.join(folder, SAFE_WEIGHTS_NAME)\n+\n+        extension = \"safetensors\"\n+        saver = safetensors.torch.save_file\n+        index_file = os.path.join(folder, SAFE_WEIGHTS_INDEX_NAME)\n+        shard_name = SAFE_WEIGHTS_NAME\n \n         state_dict = loader(weights_file)\n \n@@ -3101,25 +3087,6 @@ def test_save_checkpoints(self):\n         trainer.train()\n         self.check_saved_checkpoints(tmp_dir, 5, int(self.n_epochs * 64 / self.batch_size), False)\n \n-    def test_safe_checkpoints(self):\n-        for save_safetensors in [True, False]:\n-            tmp_dir = self.get_auto_remove_tmp_dir()\n-            trainer = get_regression_trainer(output_dir=tmp_dir, save_steps=5, save_safetensors=save_safetensors)\n-            trainer.train()\n-            self.check_saved_checkpoints(\n-                tmp_dir, 5, int(self.n_epochs * 64 / self.batch_size), safe_weights=save_safetensors\n-            )\n-\n-            # With a regular model that is not a PreTrainedModel\n-            tmp_dir = self.get_auto_remove_tmp_dir()\n-            trainer = get_regression_trainer(\n-                output_dir=tmp_dir, save_steps=5, pretrained=False, save_safetensors=save_safetensors\n-            )\n-            trainer.train()\n-            self.check_saved_checkpoints(\n-                tmp_dir, 5, int(self.n_epochs * 64 / self.batch_size), False, safe_weights=save_safetensors\n-            )\n-\n     def test_save_collator_tokenizer_by_default(self):\n         class FakeCollator:\n             def __init__(self):\n@@ -3131,9 +3098,7 @@ def __call__(self, features: list[Any], return_tensors=\"pt\") -> dict[str, Any]:\n \n         data_collator = FakeCollator()\n         tmp_dir = self.get_auto_remove_tmp_dir()\n-        trainer = get_regression_trainer(\n-            output_dir=tmp_dir, save_steps=5, save_safetensors=True, data_collator=data_collator\n-        )\n+        trainer = get_regression_trainer(output_dir=tmp_dir, save_steps=5, data_collator=data_collator)\n         trainer.train()\n         loaded_tokenizer = AutoTokenizer.from_pretrained(os.path.join(tmp_dir, os.listdir(tmp_dir)[0]))\n         assert len(loaded_tokenizer) == len(trainer.data_collator.tokenizer), \"Failed to load updated tokenizer\"\n@@ -3338,9 +3303,7 @@ def test_can_resume_training_lm(self):\n             # Delete the reference\n             del model_params, trainer\n             # Checks if all checkpoints are there, +1 is necessary because range is 1-indexed\n-            self.check_saved_checkpoints(\n-                tmpdir, freq=1, total=training_steps + 1, is_pretrained=True, safe_weights=True, use_scaler=True\n-            )\n+            self.check_saved_checkpoints(tmpdir, freq=1, total=training_steps + 1, is_pretrained=True, use_scaler=True)\n \n             # Checkpoint at intermediate step\n             checkpoint = os.path.join(tmpdir, f\"checkpoint-{resume_from_step + 1}\")\n@@ -3550,39 +3513,34 @@ def test_resume_training_with_shard_checkpoint(self):\n             self.check_trainer_state_are_the_same(state, state1)\n \n     @require_torch_up_to_2_accelerators\n-    def test_resume_training_with_safe_checkpoint(self):\n+    def test_resume_training_with_checkpoint(self):\n         # This test will fail for more than 2 GPUs since the batch size will get bigger and with the number of\n         # save_steps, the checkpoint will resume training at epoch 2 or more (so the data seen by the model\n         # won't be the same since the training dataloader is shuffled).\n \n-        for initial_safe in [False, True]:\n-            for loaded_safe in [False, True]:\n-                with tempfile.TemporaryDirectory() as tmpdir:\n-                    trainer = get_regression_trainer(\n-                        output_dir=tmpdir,\n-                        train_len=128,\n-                        save_steps=5,\n-                        learning_rate=0.1,\n-                        save_safetensors=initial_safe,\n-                    )\n-                    trainer.train()\n-                    (a, b) = trainer.model.a.item(), trainer.model.b.item()\n-                    state = dataclasses.asdict(trainer.state)\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            trainer = get_regression_trainer(\n+                output_dir=tmpdir,\n+                train_len=128,\n+                save_steps=5,\n+                learning_rate=0.1,\n+            )\n+            trainer.train()\n+            (a, b) = trainer.model.a.item(), trainer.model.b.item()\n+            state = dataclasses.asdict(trainer.state)\n \n-                    checkpoint = os.path.join(tmpdir, \"checkpoint-5\")\n-                    self.convert_to_sharded_checkpoint(checkpoint, load_safe=initial_safe, save_safe=loaded_safe)\n+            checkpoint = os.path.join(tmpdir, \"checkpoint-5\")\n+            self.convert_to_sharded_checkpoint(checkpoint)\n \n-                    # Reinitialize trainer\n-                    trainer = get_regression_trainer(\n-                        output_dir=tmpdir, train_len=128, save_steps=5, learning_rate=0.1, save_safetensors=loaded_safe\n-                    )\n+            # Reinitialize trainer\n+            trainer = get_regression_trainer(output_dir=tmpdir, train_len=128, save_steps=5, learning_rate=0.1)\n \n-                    trainer.train(resume_from_checkpoint=checkpoint)\n-                    (a1, b1) = trainer.model.a.item(), trainer.model.b.item()\n-                    state1 = dataclasses.asdict(trainer.state)\n-                    self.assertEqual(a, a1)\n-                    self.assertEqual(b, b1)\n-                    self.check_trainer_state_are_the_same(state, state1)\n+            trainer.train(resume_from_checkpoint=checkpoint)\n+            (a1, b1) = trainer.model.a.item(), trainer.model.b.item()\n+            state1 = dataclasses.asdict(trainer.state)\n+            self.assertEqual(a, a1)\n+            self.assertEqual(b, b1)\n+            self.check_trainer_state_are_the_same(state, state1)\n \n     @require_torch_up_to_2_accelerators\n     def test_resume_training_with_gradient_accumulation(self):\n@@ -3735,8 +3693,7 @@ def test_load_best_model_at_end(self):\n \n     def test_load_best_model_from_safetensors(self):\n         total = int(self.n_epochs * 64 / self.batch_size)\n-        for save_safetensors, pretrained in product([False, True], [False, True]):\n-            save_safetensors = True\n+        for pretrained in [False, True]:\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 trainer = get_regression_trainer(\n                     a=1.5,\n@@ -3747,15 +3704,12 @@ def test_load_best_model_from_safetensors(self):\n                     eval_strategy=\"steps\",\n                     save_steps=5,\n                     load_best_model_at_end=True,\n-                    save_safetensors=save_safetensors,\n                     pretrained=pretrained,\n                 )\n                 self.assertFalse(trainer.args.greater_is_better)\n                 trainer.train()\n-                self.check_saved_checkpoints(tmpdir, 5, total, is_pretrained=pretrained, safe_weights=save_safetensors)\n-                self.check_best_model_has_been_loaded(\n-                    tmpdir, 5, total, trainer, \"eval_loss\", is_pretrained=pretrained, safe_weights=save_safetensors\n-                )\n+                self.check_saved_checkpoints(tmpdir, 5, total, is_pretrained=pretrained)\n+                self.check_best_model_has_been_loaded(tmpdir, 5, total, trainer, \"eval_loss\", is_pretrained=pretrained)\n \n     @slow\n     def test_trainer_eval_mrpc(self):"
        },
        {
            "sha": "050db4823d0797571ca40aa7cc1a51151d5090f2",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 82,
            "deletions": 93,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f3cae74245cba80b5cc34c6c3e914940ac3f109/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=3f3cae74245cba80b5cc34c6c3e914940ac3f109",
            "patch": "@@ -67,6 +67,7 @@\n     LoggingLevel,\n     TemporaryHubRepo,\n     TestCasePlus,\n+    force_serialization_as_bin_files,\n     hub_retry,\n     is_staging_test,\n     require_accelerate,\n@@ -83,7 +84,6 @@\n     SAFE_WEIGHTS_NAME,\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n-    check_torch_load_is_safe,\n )\n from transformers.utils.import_utils import (\n     is_flash_attn_2_available,\n@@ -102,6 +102,7 @@\n \n if is_torch_available():\n     import torch\n+    from safetensors.torch import load_file\n     from safetensors.torch import save_file as safe_save_file\n     from test_module.custom_modeling import CustomModel\n     from torch import nn\n@@ -753,46 +754,44 @@ def test_model_from_config_attn_implementation(self):\n             model = AutoModelForCausalLM.from_config(config=config, attn_implementation=requested_attn_implementation)\n             self.assertEqual(model.config._attn_implementation, requested_attn_implementation)\n \n-    def test_checkpoint_sharding_local_bin(self):\n+    def test_checkpoint_sharding_local(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             # We use the same folder for various sizes to make sure a new save erases the old checkpoint.\n             for max_size in [\"50kB\", \"100kB\", \"200kB\"]:\n-                model.save_pretrained(tmp_dir, max_shard_size=max_size, safe_serialization=False)\n+                model.save_pretrained(tmp_dir, max_shard_size=max_size)\n \n                 # Get each shard file and its size\n                 shard_to_size = {}\n                 for shard in os.listdir(tmp_dir):\n-                    if shard.endswith(\".bin\"):\n+                    if shard.endswith(\".safetensors\"):\n                         shard_file = os.path.join(tmp_dir, shard)\n                         shard_to_size[shard_file] = os.path.getsize(shard_file)\n \n-                index_file = os.path.join(tmp_dir, WEIGHTS_INDEX_NAME)\n+                index_file = os.path.join(tmp_dir, SAFE_WEIGHTS_INDEX_NAME)\n                 # Check there is an index but no regular weight file\n                 self.assertTrue(os.path.isfile(index_file))\n-                self.assertFalse(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_NAME)))\n+                self.assertFalse(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_NAME)))\n \n                 # Check a file is bigger than max_size only when it has a single weight\n                 for shard_file, size in shard_to_size.items():\n                     max_size_int = int(max_size[:-2]) * 10**3\n-                    # Note: pickle adds some junk so the weight of the file can end up being slightly bigger than\n-                    # the size asked for (since we count parameters)\n+                    # Note: the file can end up being slightly bigger than the size asked for (since we count parameters)\n                     if size >= max_size_int + 50000:\n-                        check_torch_load_is_safe()\n-                        state_dict = torch.load(shard_file, weights_only=True)\n+                        state_dict = load_file(shard_file)\n                         self.assertEqual(len(state_dict), 1)\n \n                 # Check the index and the shard files found match\n                 with open(index_file, encoding=\"utf-8\") as f:\n                     index = json.loads(f.read())\n \n                 all_shards = set(index[\"weight_map\"].values())\n-                shards_found = {f for f in os.listdir(tmp_dir) if f.endswith(\".bin\")}\n+                shards_found = {f for f in os.listdir(tmp_dir) if f.endswith(\".safetensors\")}\n                 self.assertSetEqual(all_shards, shards_found)\n \n                 # Finally, check the model can be reloaded\n-                new_model = BertModel.from_pretrained(tmp_dir, use_safetensors=False)\n+                new_model = BertModel.from_pretrained(tmp_dir)\n                 for p1, p2 in zip(model.parameters(), new_model.parameters()):\n                     torch.testing.assert_close(p1, p2)\n \n@@ -803,56 +802,11 @@ def test_checkpoint_sharding_from_hub(self):\n         for p1, p2 in zip(model.parameters(), ref_model.parameters()):\n             torch.testing.assert_close(p1, p2)\n \n-    def test_checkpoint_variant_local_bin(self):\n+    def test_checkpoint_variant_local(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, variant=\"v2\", safe_serialization=False)\n-\n-            weights_name = \".\".join(WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"bin\"])\n-\n-            weights_file = os.path.join(tmp_dir, weights_name)\n-            self.assertTrue(os.path.isfile(weights_file))\n-            self.assertFalse(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_NAME)))\n-\n-            with self.assertRaises(EnvironmentError):\n-                _ = BertModel.from_pretrained(tmp_dir)\n-\n-            new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\", use_safetensors=False)\n-\n-        for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            torch.testing.assert_close(p1, p2)\n-\n-    @unittest.skip(\"Skipping it for now, not sure how critial but does not look hard to fix.\")\n-    def test_checkpoint_variant_local_sharded_bin(self):\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, variant=\"v2\", max_shard_size=\"50kB\", safe_serialization=False)\n-\n-            weights_index_name = \".\".join(WEIGHTS_INDEX_NAME.split(\".\")[:-1] + [\"v2\"] + [\"json\"])\n-            weights_index_file = os.path.join(tmp_dir, weights_index_name)\n-            self.assertTrue(os.path.isfile(weights_index_file))\n-            self.assertFalse(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_INDEX_NAME)))\n-\n-            for i in range(1, 5):\n-                weights_name = \".\".join(WEIGHTS_NAME.split(\".\")[:-1] + [f\"v2-0000{i}-of-00005\"] + [\"bin\"])\n-                weights_name_file = os.path.join(tmp_dir, weights_name)\n-                self.assertTrue(os.path.isfile(weights_name_file))\n-\n-            with self.assertRaises(EnvironmentError):\n-                _ = BertModel.from_pretrained(tmp_dir)\n-\n-            new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\", use_safe_tensors=False)\n-\n-        for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            torch.testing.assert_close(p1, p2)\n-\n-    def test_checkpoint_variant_local_safe(self):\n-        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, variant=\"v2\", safe_serialization=True)\n+            model.save_pretrained(tmp_dir, variant=\"v2\")\n \n             weights_name = \".\".join(SAFE_WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"safetensors\"])\n \n@@ -868,11 +822,11 @@ def test_checkpoint_variant_local_safe(self):\n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n             torch.testing.assert_close(p1, p2)\n \n-    def test_checkpoint_variant_local_sharded_safe(self):\n+    def test_checkpoint_variant_local_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, variant=\"v2\", max_shard_size=\"50kB\", safe_serialization=True)\n+            model.save_pretrained(tmp_dir, variant=\"v2\", max_shard_size=\"50kB\")\n \n             weights_index_name = \".\".join(SAFE_WEIGHTS_INDEX_NAME.split(\".\")[:-1] + [\"v2\"] + [\"json\"])\n             weights_index_file = os.path.join(tmp_dir, weights_index_name)\n@@ -901,7 +855,7 @@ def test_checkpoint_loading_only_safetensors_available(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, max_shard_size=\"50kB\", safe_serialization=True)\n+            model.save_pretrained(tmp_dir, max_shard_size=\"50kB\")\n \n             weights_index_name = \".\".join(SAFE_WEIGHTS_INDEX_NAME.split(\".\")[:-1] + [\"json\"])\n             weights_index_file = os.path.join(tmp_dir, weights_index_name)\n@@ -912,7 +866,7 @@ def test_checkpoint_loading_only_safetensors_available(self):\n                 weights_name_file = os.path.join(tmp_dir, weights_name)\n                 self.assertTrue(os.path.isfile(weights_name_file))\n \n-            # Setting use_safetensors=False should raise an error as the checkpoint was saved with safetensors=True\n+            # Setting use_safetensors=False should raise an error as the checkpoint was saved in safetensors\n             with self.assertRaises(OSError):\n                 _ = BertModel.from_pretrained(tmp_dir, use_safetensors=False)\n \n@@ -934,10 +888,12 @@ def test_checkpoint_loading_only_pytorch_bin_available(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, max_shard_size=\"50kB\", safe_serialization=False)\n+            # Since we don't support saving with bins files anymore, but still support loading we use this context\n+            # to easily create the bins files and try to load them\n+            with force_serialization_as_bin_files():\n+                model.save_pretrained(tmp_dir, max_shard_size=\"50kB\")\n \n-            weights_index_name = \".\".join(WEIGHTS_INDEX_NAME.split(\".\")[:-1] + [\"json\"])\n-            weights_index_file = os.path.join(tmp_dir, weights_index_name)\n+            weights_index_file = os.path.join(tmp_dir, WEIGHTS_INDEX_NAME)\n             self.assertTrue(os.path.isfile(weights_index_file))\n \n             for i in range(1, 5):\n@@ -999,24 +955,27 @@ def test_checkpoint_variant_hub_sharded_safe(self):\n             )\n         self.assertIsNotNone(model)\n \n-    def test_checkpoint_variant_save_load_bin(self):\n+    def test_checkpoint_variant_save_load(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             model = BertModel.from_pretrained(\n-                \"hf-internal-testing/tiny-random-bert-variant\", cache_dir=tmp_dir, variant=\"v2\", use_safetensors=False\n+                \"hf-internal-testing/tiny-random-bert-variant\",\n+                cache_dir=tmp_dir,\n+                variant=\"v2\",\n+                use_safetensors=False,\n             )\n-            weights_name = \".\".join(WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"bin\"])\n+            weights_name = \".\".join(SAFE_WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"safetensors\"])\n \n-            model.save_pretrained(tmp_dir, variant=\"v2\", safe_serialization=False)\n+            model.save_pretrained(tmp_dir, variant=\"v2\")\n             # saving will create a variant checkpoint\n             self.assertTrue(os.path.isfile(os.path.join(tmp_dir, weights_name)))\n \n-            model.save_pretrained(tmp_dir, safe_serialization=False)\n+            model.save_pretrained(tmp_dir)\n             # saving shouldn't delete variant checkpoints\n-            weights_name = \".\".join(WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"bin\"])\n+            weights_name = \".\".join(SAFE_WEIGHTS_NAME.split(\".\")[:-1] + [\"v2\"] + [\"safetensors\"])\n             self.assertTrue(os.path.isfile(os.path.join(tmp_dir, weights_name)))\n \n             # there should be a normal checkpoint\n-            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_NAME)))\n+            self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_NAME)))\n \n         self.assertIsNotNone(model)\n \n@@ -1134,8 +1093,7 @@ def test_from_pretrained_non_contiguous_checkpoint(self):\n         self.assertTrue(model.owlvit.visual_projection.weight.is_contiguous())\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=False)\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n+            model.save_pretrained(tmp_dir)\n \n     def test_cached_files_are_used_when_internet_is_down(self):\n         # A mock response for an HTTP head request to emulate server down\n@@ -1302,7 +1260,7 @@ def test_use_safetensors(self):\n     def test_safetensors_save_and_load(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n+            model.save_pretrained(tmp_dir)\n             # No pytorch_model.bin file, only a model.safetensors\n             self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_NAME)))\n             self.assertFalse(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_NAME)))\n@@ -1324,7 +1282,7 @@ def test_safetensors_load_from_hub(self):\n     def test_safetensors_save_and_load_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True, max_shard_size=\"100kB\")\n+            model.save_pretrained(tmp_dir, max_shard_size=\"100kB\")\n             # No pytorch_model.bin index file, only a model.safetensors index\n             self.assertFalse(os.path.isfile(os.path.join(tmp_dir, WEIGHTS_INDEX_NAME)))\n             self.assertTrue(os.path.isfile(os.path.join(tmp_dir, SAFE_WEIGHTS_INDEX_NAME)))\n@@ -1699,7 +1657,7 @@ def test_safetensors_torch_from_torch(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n+            model.save_pretrained(tmp_dir)\n             new_model = BertModel.from_pretrained(tmp_dir)\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n@@ -1709,7 +1667,7 @@ def test_safetensors_torch_from_torch_sharded(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True, max_shard_size=\"100kB\")\n+            model.save_pretrained(tmp_dir, max_shard_size=\"100kB\")\n             new_model = BertModel.from_pretrained(tmp_dir)\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n@@ -1736,7 +1694,7 @@ def test_model_from_pretrained_from_mlx(self):\n         self.assertIsNotNone(model)\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir, safe_serialization=True)\n+            model.save_pretrained(tmp_dir)\n             with safe_open(os.path.join(tmp_dir, \"model.safetensors\"), framework=\"pt\") as f:\n                 metadata = f.metadata()\n                 self.assertEqual(metadata.get(\"format\"), \"pt\")\n@@ -2289,7 +2247,10 @@ def test_safetensors_on_the_fly_conversion(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token)\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True)\n \n         with self.subTest(\"Initial and converted models are equal\"):\n@@ -2308,7 +2269,10 @@ def test_safetensors_on_the_fly_conversion_private(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False, private=True)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, private=True)\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n \n         with self.subTest(\"Initial and converted models are equal\"):\n@@ -2327,7 +2291,10 @@ def test_safetensors_on_the_fly_conversion_gated(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token)\n         self.api.update_repo_settings(self.repo_name, gated=\"auto\")\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n \n@@ -2347,7 +2314,10 @@ def test_safetensors_on_the_fly_sharded_conversion(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False, max_shard_size=\"200kb\")\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, max_shard_size=\"200kb\")\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True)\n \n         with self.subTest(\"Initial and converted models are equal\"):\n@@ -2366,9 +2336,10 @@ def test_safetensors_on_the_fly_sharded_conversion_private(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(\n-            self.repo_name, token=self.token, safe_serialization=False, max_shard_size=\"200kb\", private=True\n-        )\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, max_shard_size=\"200kb\", private=True)\n         converted_model = BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n \n         with self.subTest(\"Initial and converted models are equal\"):\n@@ -2387,7 +2358,10 @@ def test_safetensors_on_the_fly_sharded_conversion_gated(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, max_shard_size=\"200kb\", safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, max_shard_size=\"200kb\")\n         headers = {\"Authorization\": f\"Bearer {self.token}\"}\n         httpx.put(\n             f\"https://huggingface.co/api/models/{self.repo_name}/settings\", json={\"gated\": \"auto\"}, headers=headers\n@@ -2411,7 +2385,10 @@ def test_safetensors_on_the_fly_wrong_user_opened_pr(self):\n         )\n         initial_model = BertModel(config)\n \n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False, private=True)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, private=True)\n         BertModel.from_pretrained(self.repo_name, use_safetensors=True, token=self.token)\n \n         # This should have opened a PR with the user's account\n@@ -2448,10 +2425,16 @@ def test_safetensors_on_the_fly_specific_revision(self):\n         initial_model = BertModel(config)\n \n         # Push a model on `main`\n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token)\n \n         # Push a model on a given revision\n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False, revision=\"new-branch\")\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token, revision=\"new-branch\")\n \n         # Try to convert the model on that revision should raise\n         with self.assertRaises(EnvironmentError):\n@@ -2464,7 +2447,10 @@ def test_absence_of_safetensors_triggers_conversion(self):\n         initial_model = BertModel(config)\n \n         # Push a model on `main`\n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token)\n \n         # Download the model that doesn't have safetensors\n         BertModel.from_pretrained(self.repo_name, token=self.token)\n@@ -2496,7 +2482,10 @@ def test_absence_of_safetensors_triggers_conversion_failed(self, spawn_conversio\n         initial_model = BertModel(config)\n \n         # Push a model on `main`\n-        initial_model.push_to_hub(self.repo_name, token=self.token, safe_serialization=False)\n+        # Since we don't support saving with bins files anymore, but still support loading we use this context\n+        # to easily create the bins files and try to load them\n+        with force_serialization_as_bin_files():\n+            initial_model.push_to_hub(self.repo_name, token=self.token)\n \n         # The auto conversion is mocked to always raise; ensure that it doesn't raise in the main thread\n         BertModel.from_pretrained(self.repo_name, token=self.token)"
        }
    ],
    "stats": {
        "total": 1087,
        "additions": 404,
        "deletions": 683
    }
}