{
    "author": "MekkCyber",
    "message": "[kernels] refactor function kernel calling (#41577)\n\n* refactor function kernel callling\n\n* nit\n\n* don't pass the mapping\n\n* use _kernels_available\n\n* rm import",
    "sha": "1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
    "files": [
        {
            "sha": "c1441ba200471178d29542005d8e93445d96b429",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
            "patch": "@@ -14,12 +14,16 @@\n import re\n from collections.abc import Callable\n from functools import partial\n+from types import ModuleType\n from typing import Optional, Union\n \n from ..modeling_flash_attention_utils import lazy_import_flash_attention\n+from ..utils import logging\n from .flash_attention import flash_attention_forward\n \n \n+logger = logging.get_logger(__name__)\n+\n try:\n     from kernels import (\n         Device,\n@@ -158,6 +162,13 @@ def register_kernel_mapping(*args, **kwargs):\n         raise RuntimeError(\"register_kernel_mapping requires `kernels` to be installed. Run `pip install kernels`.\")\n \n \n+_HUB_KERNEL_MAPPING: dict[str, str] = {\n+    \"causal-conv1d\": \"kernels-community/causal-conv1d\",\n+}\n+\n+_KERNEL_MODULE_MAPPING: dict[str, Optional[ModuleType]] = {}\n+\n+\n def is_kernel(attn_implementation: Optional[str]) -> bool:\n     \"\"\"Check whether `attn_implementation` matches a kernel pattern from the hub.\"\"\"\n     return (\n@@ -220,9 +231,53 @@ def load_and_register_attn_kernel(attn_implementation: str, attention_wrapper: O\n     ALL_MASK_ATTENTION_FUNCTIONS.register(attn_implementation, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n \n \n+def lazy_load_kernel(kernel_name: str, mapping: dict[str, Optional[ModuleType]] = _KERNEL_MODULE_MAPPING):\n+    if kernel_name in mapping and isinstance(mapping[kernel_name], ModuleType):\n+        return mapping[kernel_name]\n+    if kernel_name not in _HUB_KERNEL_MAPPING:\n+        logger.warning(f\"Kernel {kernel_name} not found in _HUB_KERNEL_MAPPING\")\n+        mapping[kernel_name] = None\n+        return None\n+    if _kernels_available:\n+        from kernels import get_kernel\n+\n+        try:\n+            kernel = get_kernel(_HUB_KERNEL_MAPPING[kernel_name])\n+            mapping[kernel_name] = kernel\n+        except FileNotFoundError:\n+            mapping[kernel_name] = None\n+\n+    else:\n+        # Try to import is_{kernel_name}_available from ..utils\n+        import importlib\n+\n+        new_kernel_name = kernel_name.replace(\"-\", \"_\")\n+        func_name = f\"is_{new_kernel_name}_available\"\n+\n+        try:\n+            utils_mod = importlib.import_module(\"..utils.import_utils\", __package__)\n+            is_kernel_available = getattr(utils_mod, func_name, None)\n+        except Exception:\n+            is_kernel_available = None\n+\n+        if callable(is_kernel_available) and is_kernel_available():\n+            # Try to import the module \"{kernel_name}\" from parent package level\n+            try:\n+                module = importlib.import_module(f\"{kernel_name}\")\n+                mapping[kernel_name] = module\n+                return module\n+            except Exception:\n+                mapping[kernel_name] = None\n+        else:\n+            mapping[kernel_name] = None\n+\n+    return mapping[kernel_name]\n+\n+\n __all__ = [\n     \"LayerRepository\",\n     \"use_kernel_forward_from_hub\",\n     \"register_kernel_mapping\",\n     \"replace_kernel_forward_from_hub\",\n+    \"lazy_load_kernel\",\n ]"
        },
        {
            "sha": "b5f03cfe7076b4a411b94b1377e8f1d821405776",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 19,
            "deletions": 32,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
            "patch": "@@ -30,12 +30,11 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -162,33 +161,6 @@ def reset(self):\n             self.ssm_states[layer_idx].zero_()\n \n \n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n-\n-_causal_conv1d_cache = None\n-\n-\n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\"\n     Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n@@ -268,7 +240,12 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -323,7 +300,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +500,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        },
        {
            "sha": "f9af68d785bd4e4a9acf729c1a26ba2d7c69e985",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 19,
            "deletions": 6,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
            "patch": "@@ -19,6 +19,7 @@\n import torch\n from torch import nn\n \n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...utils import auto_docstring, logging\n from ...utils.import_utils import (\n     is_mamba_ssm_available,\n@@ -35,7 +36,6 @@\n     MambaOutput,\n     MambaPreTrainedModel,\n     MambaRMSNorm,\n-    _lazy_load_causal_conv1d,\n )\n \n \n@@ -54,8 +54,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n \n class FalconMambaConfig(MambaConfig):\n     \"\"\"\n@@ -258,7 +256,12 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -324,7 +327,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -518,7 +526,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        },
        {
            "sha": "56744f354b271f32dfc4feab4bd8a78bdb1eff91",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 19,
            "deletions": 31,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=1fb3fc4db0e87fd7c2f57a36b6b32ee6fa69c50c",
            "patch": "@@ -25,6 +25,7 @@\n from ...activations import ACT2FN\n from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n+from ...integrations.hub_kernels import lazy_load_kernel\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -33,8 +34,6 @@\n     logging,\n )\n from ...utils.import_utils import (\n-    is_causal_conv1d_available,\n-    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -54,32 +53,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-_causal_conv1d_cache = None\n-\n-\n-def _lazy_load_causal_conv1d():\n-    global _causal_conv1d_cache\n-    if _causal_conv1d_cache is not None:\n-        return _causal_conv1d_cache\n-\n-    if is_kernels_available():\n-        from kernels import get_kernel\n-\n-        try:\n-            _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n-        except FileNotFoundError:\n-            # no kernel binary match, fallback to slow path\n-            _causal_conv1d_cache = (None, None)\n-        else:\n-            _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n-    elif is_causal_conv1d_available():\n-        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-\n-        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n-    else:\n-        _causal_conv1d_cache = (None, None)\n-    return _causal_conv1d_cache\n-\n \n class MambaCache:\n     \"\"\"\n@@ -236,7 +209,12 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.warn_slow_implementation()\n \n     def warn_slow_implementation(self):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -287,7 +265,12 @@ def cuda_kernels_forward(\n             )\n \n         else:\n-            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+            causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+            causal_conv1d_update, causal_conv1d_fn = (\n+                (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+                if causal_conv1d is not None\n+                else (None, None)\n+            )\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -451,7 +434,12 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n-        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n+        causal_conv1d = lazy_load_kernel(\"causal-conv1d\")\n+        causal_conv1d_update, causal_conv1d_fn = (\n+            (causal_conv1d.causal_conv1d_update, causal_conv1d.causal_conv1d_fn)\n+            if causal_conv1d is not None\n+            else (None, None)\n+        )\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        }
    ],
    "stats": {
        "total": 181,
        "additions": 112,
        "deletions": 69
    }
}