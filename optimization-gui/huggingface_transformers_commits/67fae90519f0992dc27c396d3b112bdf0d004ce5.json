{
    "author": "BlackSamorez",
    "message": "Fix FP-Quant quantization fallback CPU dispatch. (#41619)\n\n* fp_quant fix\n\n* Update quantizer_fp_quant.py",
    "sha": "67fae90519f0992dc27c396d3b112bdf0d004ce5",
    "files": [
        {
            "sha": "88954500a5f7875cea1cc01d1f1ac81c0fcdaa0b",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/67fae90519f0992dc27c396d3b112bdf0d004ce5/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67fae90519f0992dc27c396d3b112bdf0d004ce5/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=67fae90519f0992dc27c396d3b112bdf0d004ce5",
            "patch": "@@ -97,6 +97,10 @@ def create_quantized_param(\n     ):\n         module, _ = get_module_from_name(model, param_name)\n \n+        if target_device == \"cpu\" and param_name.endswith(\"weight\"):\n+            # Works agains hard-coded missing key dispatch to CPU\n+            return\n+\n         # The module holds either:\n         #  * `weight` when `store_master_weights=True`\n         #  * `qweight` and `scales` when `store_master_weights=False` and `pseudoquantization=False`"
        },
        {
            "sha": "ce73f0d80a631bfe1ddddf23a1f447840d58f1ad",
            "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/67fae90519f0992dc27c396d3b112bdf0d004ce5/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/67fae90519f0992dc27c396d3b112bdf0d004ce5/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py?ref=67fae90519f0992dc27c396d3b112bdf0d004ce5",
            "patch": "@@ -160,14 +160,14 @@ def getQuantizationConfig(cls):\n class FPQuantMXFP4Test(FPQuantBaseTest):\n     @classmethod\n     def getQuantizationConfig(cls):\n-        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n+        return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False)\n \n \n @require_qutlass\n class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n     @classmethod\n     def getQuantizationConfig(cls):\n-        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False, hadamard_group_size=128)\n+        return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False, hadamard_group_size=128)\n \n \n @require_qutlass"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}