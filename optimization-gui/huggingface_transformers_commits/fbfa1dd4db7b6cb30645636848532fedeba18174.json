{
    "author": "devxaitist",
    "message": "ğŸŒ [i18n-KO] Translated `siglip.md` to Korean (#37145)\n\n* docs: ko: siglip.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* chore: Correct document title to kebab-case format\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply suggestions from code review\r\n\r\nConvert unnatural language to natural Korean\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>",
    "sha": "fbfa1dd4db7b6cb30645636848532fedeba18174",
    "files": [
        {
            "sha": "1a9c61ce09cf612f273bb911637e8059dba4e8ef",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbfa1dd4db7b6cb30645636848532fedeba18174/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbfa1dd4db7b6cb30645636848532fedeba18174/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=fbfa1dd4db7b6cb30645636848532fedeba18174",
            "patch": "@@ -720,6 +720,8 @@\n         title: Qwen2VL\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Segment Anything\n+      - local: model_doc/siglip\n+        title: SigLIP\n       - local: in_translation\n         title: (ë²ˆì—­ì¤‘) Speech Encoder Decoder Models\n       - local: in_translation"
        },
        {
            "sha": "d0eaf93cf040648a4928230e1aaefbdb23d29efc",
            "filename": "docs/source/ko/model_doc/siglip.md",
            "status": "added",
            "additions": 253,
            "deletions": 0,
            "changes": 253,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbfa1dd4db7b6cb30645636848532fedeba18174/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbfa1dd4db7b6cb30645636848532fedeba18174/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fsiglip.md?ref=fbfa1dd4db7b6cb30645636848532fedeba18174",
            "patch": "@@ -0,0 +1,253 @@\n+<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# SigLIP[[siglip]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## ê°œìš”[[overview]]\n+\n+SigLIP ëª¨ë¸ì€ Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyerì˜ [Sigmoid Loss for Language Image Pre-Training](https://arxiv.org/abs/2303.15343) ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. SigLIPì€ [CLIP](clip)ì—ì„œ ì‚¬ìš©ëœ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê°„ë‹¨í•œ ìŒë³„ ì‹œê·¸ëª¨ì´ë“œ ì†ì‹¤(pairwise sigmoid loss)ë¡œ ëŒ€ì²´í•  ê²ƒì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ëŠ” ImageNetì—ì„œ ì œë¡œìƒ· ë¶„ë¥˜ ì •í™•ë„ ì¸¡ë©´ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ìš°ë¦¬ëŠ” ì–¸ì–´-ì´ë¯¸ì§€ ì‚¬ì „ í•™ìŠµ(Language-Image Pre-training, SigLIP)ì„ ìœ„í•œ ê°„ë‹¨í•œ ìŒë³„ ì‹œê·¸ëª¨ì´ë“œ ì†ì‹¤ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ ì •ê·œí™”ë¥¼ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ ëŒ€ì¡° í•™ìŠµê³¼ ë‹¬ë¦¬, ì‹œê·¸ëª¨ì´ë“œ ì†ì‹¤ì€ ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ìŒì—ë§Œ ì‘ìš©í•˜ë©° ì •ê·œí™”ë¥¼ ìœ„í•´ ìŒë³„ ìœ ì‚¬ì„±ì˜ ì „ì—­ì  ê´€ì ì„ í•„ìš”ë¡œ í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹œê·¸ëª¨ì´ë“œ ì†ì‹¤ì€ ë°°ì¹˜ í¬ê¸°ë¥¼ ë”ìš± í™•ì¥í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ë™ì‹œì— ì‘ì€ ë°°ì¹˜ í¬ê¸°ì—ì„œë„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. Locked-image Tuningê³¼ ê²°í•©í•˜ì—¬, ë‹¨ 4ê°œì˜ TPUv4 ì¹©ë§Œìœ¼ë¡œ ì´í‹€ ë§Œì— 84.5%ì˜ ImageNet ì œë¡œìƒ· ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ëŠ” SigLiT ëª¨ë¸ì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤. ì†ì‹¤ í•¨ìˆ˜ì—ì„œ ë°°ì¹˜ í¬ê¸°ë¥¼ ë¶„ë¦¬í•¨ìœ¼ë¡œì¨ ì˜ˆì œ ëŒ€ ìŒì˜ ì˜í–¥ê³¼ Negative ëŒ€ Positive ë¹„ìœ¨ì„ ì—°êµ¬í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ìœ¼ë¡œ, ìš°ë¦¬ëŠ” ë°°ì¹˜ í¬ê¸°ë¥¼ 100ë§Œ ê°œê¹Œì§€ ê·¹ë‹¨ì ìœ¼ë¡œ ëŠ˜ë ¤ë³´ì•˜ê³ , ë°°ì¹˜ í¬ê¸° ì¦ê°€ì˜ ì´ì ì´ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ë©° 32kì˜ ë” í•©ë¦¬ì ì¸ ë°°ì¹˜ í¬ê¸°ë¡œë„ ì¶©ë¶„í•˜ë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.*\n+\n+## ì‚¬ìš© íŒ[[usage-tips]]\n+\n+- SigLIPì˜ ì‚¬ìš©ë²•ì€ [CLIP](clip)ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. ì£¼ìš” ì°¨ì´ì ì€ í•™ìŠµ ì†ì‹¤ í•¨ìˆ˜ë¡œ, ë°°ì¹˜ ë‚´ ëª¨ë“  ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ ê°„ì˜ ìŒë³„ ìœ ì‚¬ì„±ì— ëŒ€í•œ ì „ì—­ì  ê´€ì ì´ í•„ìš”í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì†Œí”„íŠ¸ë§¥ìŠ¤ ëŒ€ì‹  ë¡œì§“ì— ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n+- í•™ìŠµì€ ì§€ì›ë˜ì§€ë§Œ `torch.distributed` ìœ í‹¸ë¦¬í‹°ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•„ ë°°ì¹˜ í¬ê¸°ì˜ í™•ì¥ì„±ì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ë‹¨ì¼ ë…¸ë“œ ë‹¤ì¤‘ GPU ì„¤ì •ì—ì„œëŠ” DDPì™€ FDSPê°€ ì‘ë™í•©ë‹ˆë‹¤.\n+- ë…ë¦½í˜• [`SiglipTokenizer`] ë˜ëŠ” [`SiglipProcessor`]ë¥¼ ì‚¬ìš©í•  ë•ŒëŠ” ëª¨ë¸ì´ ê·¸ë ‡ê²Œ í•™ìŠµë˜ì—ˆìœ¼ë¯€ë¡œ `padding=\"max_length\"`ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n+- íŒŒì´í”„ë¼ì¸ê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìœ¼ë ¤ë©´ \"This is a photo of {label}.\"ì˜ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip_table.jpeg\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> CLIPê³¼ ë¹„êµí•œ SigLIP í‰ê°€ ê²°ê³¼. <a href=\"https://arxiv.org/abs/2303.15343\">ì›ë³¸ ë…¼ë¬¸</a>ì—ì„œ ë°œì·Œ.</small>\n+\n+ì´ ëª¨ë¸ì€ [nielsr](https://huggingface.co/nielsr)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤.\n+ì›ë³¸ ì½”ë“œëŠ” [ì—¬ê¸°](https://github.com/google-research/big_vision/tree/main)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© ì˜ˆì‹œ[[usage-example]]\n+\n+SigLIPì„ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì—ëŠ” ë‘ ê°€ì§€ ì£¼ìš” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤: ëª¨ë“  ë³µì¡ì„±ì„ ì¶”ìƒí™”í•˜ëŠ” íŒŒì´í”„ë¼ì¸ APIë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, ì§ì ‘ `SiglipModel` í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n+\n+### íŒŒì´í”„ë¼ì¸ API[[pipeline-API]]\n+\n+íŒŒì´í”„ë¼ì¸ì„ ì‚¬ìš©í•˜ë©´ ëª‡ ì¤„ì˜ ì½”ë“œë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n+\n+```python\n+>>> from transformers import pipeline\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> # íŒŒì´í”„ë¼ì¸ ë¡œë“œ\n+>>> image_classifier = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip-base-patch16-224\")\n+\n+>>> # ì´ë¯¸ì§€ ë¡œë“œ\n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> # ì¶”ë¡ \n+>>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n+>>> outputs = image_classifier(image, candidate_labels=candidate_labels)\n+>>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n+>>> print(outputs)\n+[{'score': 0.1979, 'label': '2 cats'}, {'score': 0.0, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n+```\n+\n+### ì§ì ‘ ëª¨ë¸ ì‚¬ìš©í•˜ê¸°[[using-the-model-yourself]]\n+\n+ì „ì²˜ë¦¬ì™€ í›„ì²˜ë¦¬ë¥¼ ì§ì ‘ ìˆ˜í–‰í•˜ë ¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ í•˜ë©´ ë©ë‹ˆë‹¤:\n+\n+```python\n+>>> from PIL import Image\n+>>> import requests\n+>>> from transformers import AutoProcessor, AutoModel\n+>>> import torch\n+\n+>>> model = AutoModel.from_pretrained(\"google/siglip-base-patch16-224\")\n+>>> processor = AutoProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n+# íŒŒì´í”„ë¼ì¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë”°ë¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤\n+>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n+# ì¤‘ìš”: ëª¨ë¸ì´ ì´ë ‡ê²Œ í•™ìŠµë˜ì—ˆìœ¼ë¯€ë¡œ `padding=max_length`ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤\n+>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> logits_per_image = outputs.logits_per_image\n+>>> probs = torch.sigmoid(logits_per_image) # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œ í™•ë¥ ì…ë‹ˆë‹¤\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+19.8% that image 0 is '2 cats'\n+```\n+\n+## ë¦¬ì†ŒìŠ¤[[resources]]\n+\n+SigLIPì„ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê³µì‹ Hugging Face ë° ì»¤ë®¤ë‹ˆí‹°(ğŸŒë¡œ í‘œì‹œ) ë¦¬ì†ŒìŠ¤ ëª©ë¡ì…ë‹ˆë‹¤.\n+\n+- [ì œë¡œìƒ· ì´ë¯¸ì§€ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/zero_shot_image_classification)\n+- SigLIPì— ëŒ€í•œ ë°ëª¨ ë…¸íŠ¸ë¶ì€ [ì—¬ê¸°](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/SigLIP)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ğŸŒ\n+\n+ì—¬ê¸°ì— í¬í•¨ë  ë¦¬ì†ŒìŠ¤ë¥¼ ì œì¶œí•˜ëŠ” ë° ê´€ì‹¬ì´ ìˆìœ¼ì‹œë©´ Pull Requestë¥¼ ì—´ì–´ì£¼ì‹œë©´ ê²€í† í•˜ê² ìŠµë‹ˆë‹¤! ë¦¬ì†ŒìŠ¤ëŠ” ì´ìƒì ìœ¼ë¡œ ê¸°ì¡´ ë¦¬ì†ŒìŠ¤ë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ê²ƒì„ ë³´ì—¬ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+\n+## SigLIPê³¼ Flash Attention 2 ê²°í•©í•˜ê¸°[[combining-siglip-with-flash-attention-2]]\n+\n+ë¨¼ì € Flash Attention 2ì˜ ìµœì‹  ë²„ì „ì„ ì„¤ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+```bash\n+pip install -U flash-attn --no-build-isolation\n+```\n+\n+ë˜í•œ Flash-Attention 2ì™€ í˜¸í™˜ë˜ëŠ” í•˜ë“œì›¨ì–´ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. flash-attn ì €ì¥ì†Œì˜ ê³µì‹ ë¬¸ì„œì—ì„œ ìì„¸íˆ ì•Œì•„ë³´ì„¸ìš”. ë˜í•œ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16`)ë¡œ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+Flash Attention 2ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ì‹¤í–‰í•˜ë ¤ë©´ ì•„ë˜ ì½”ë“œë¥¼ ì°¸ì¡°í•˜ì„¸ìš”:\n+\n+```python\n+>>> import torch\n+>>> import requests\n+>>> from PIL import Image\n+>>> from transformers import SiglipProcessor, SiglipModel\n+>>> device = \"cuda\" # ëª¨ë¸ì„ ë¡œë“œí•  ì¥ì¹˜\n+\n+>>> model = SiglipModel.from_pretrained(\n+...     \"google/siglip-so400m-patch14-384\",\n+...     attn_implementation=\"flash_attention_2\",\n+...     torch_dtype=torch.float16,\n+...     device_map=device,\n+... )\n+>>> processor = SiglipProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n+# íŒŒì´í”„ë¼ì¸ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ì„ ë”°ë¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì–»ìŠµë‹ˆë‹¤\n+>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n+# ì¤‘ìš”: ëª¨ë¸ì´ ì´ë ‡ê²Œ í•™ìŠµë˜ì—ˆìœ¼ë¯€ë¡œ `padding=max_length`ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤\n+>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     with torch.autocast(device):\n+...         outputs = model(**inputs)\n+\n+>>> logits_per_image = outputs.logits_per_image\n+>>> probs = torch.sigmoid(logits_per_image) # ì‹œê·¸ëª¨ì´ë“œ í™œì„±í™” í•¨ìˆ˜ë¥¼ ì ìš©í•œ í™•ë¥ ì…ë‹ˆë‹¤\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+19.8% that image 0 is '2 cats'\n+```\n+\n+\n+## Scaled Dot Product Attention(SDPA) ì‚¬ìš©í•˜ê¸°[using-scaled-dot-product-attention(SDPA)]]\n+\n+PyTorchëŠ” `torch.nn.functional`ì˜ ì¼ë¶€ë¡œ ìŠ¤ì¼€ì¼ëœ ì ê³± ì–´í…ì…˜(SDPA) ì—°ì‚°ìë¥¼ í¬í•¨í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ëŠ” \n+ì…ë ¥ê³¼ ì‚¬ìš© ì¤‘ì¸ í•˜ë“œì›¨ì–´ì— ë”°ë¼ ì ìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ êµ¬í˜„ì„ í¬í•¨í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ \n+[ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html) \n+ë˜ëŠ” [GPU ì¶”ë¡ ](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention) \n+í˜ì´ì§€ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+`from_pretrained()`ì—ì„œ `attn_implementation=\"sdpa\"`ë¥¼ ì„¤ì •í•˜ì—¬ SDPAë¥¼ ëª…ì‹œì ìœ¼ë¡œ ìš”ì²­í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. `torch>=2.1.1`ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n+\n+```python\n+>>> from transformers import SiglipModel\n+\n+>>> model = SiglipModel.from_pretrained(\n+...     \"google/siglip-so400m-patch14-384\",\n+...     attn_implementation=\"sdpa\",\n+...     torch_dtype=torch.float16,\n+...     device_map=device,\n+... )\n+```\n+\n+ìµœìƒì˜ ì†ë„ í–¥ìƒì„ ìœ„í•´ ëª¨ë¸ì„ ë°˜ì •ë°€ë„(ì˜ˆ: `torch.float16` ë˜ëŠ” `torch.bfloat16`)ë¡œ ë¡œë“œí•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤.\n+\n+\n+## ì˜ˆìƒ ì†ë„ í–¥ìƒ[[expected-speedups]]\n+\n+ì•„ë˜ëŠ” `google/siglip-so400m-patch14-384` ì²´í¬í¬ì¸íŠ¸ë¥¼ `float16` ì •ë°€ë„ë¡œ ì‚¬ìš©í•˜ëŠ” transformersì˜ ë„¤ì´í‹°ë¸Œ êµ¬í˜„ê³¼ Flash Attention 2 / SDPA ë²„ì „ì˜ ëª¨ë¸ì„ ë‹¤ì–‘í•œ ë°°ì¹˜ í¬ê¸°ë¡œ ë¹„êµí•œ ì¶”ë¡  ì‹œê°„ì˜ ì˜ˆìƒ ì†ë„ í–¥ìƒ ë‹¤ì´ì–´ê·¸ë¨ì…ë‹ˆë‹¤.\n+\n+<div style=\"text-align: center\">\n+<img src=\"https://i.imgur.com/cWm4rsn.png\">\n+</div>\n+\n+\n+## SiglipConfig\n+\n+[[autodoc]] SiglipConfig\n+    - from_text_vision_configs\n+\n+## SiglipTextConfig\n+\n+[[autodoc]] SiglipTextConfig\n+\n+## SiglipVisionConfig\n+\n+[[autodoc]] SiglipVisionConfig\n+\n+## SiglipTokenizer\n+\n+[[autodoc]] SiglipTokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+## SiglipImageProcessor\n+\n+[[autodoc]] SiglipImageProcessor\n+    - preprocess\n+\n+## SiglipImageProcessorFast\n+\n+[[autodoc]] SiglipImageProcessorFast\n+    - preprocess\n+\n+## SiglipProcessor\n+\n+[[autodoc]] SiglipProcessor\n+\n+## SiglipModel\n+\n+[[autodoc]] SiglipModel\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## SiglipTextModel\n+\n+[[autodoc]] SiglipTextModel\n+    - forward\n+\n+## SiglipVisionModel\n+\n+[[autodoc]] SiglipVisionModel\n+    - forward\n+\n+\n+## SiglipForImageClassification\n+\n+[[autodoc]] SiglipForImageClassification\n+    - forward \n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 255,
        "additions": 255,
        "deletions": 0
    }
}