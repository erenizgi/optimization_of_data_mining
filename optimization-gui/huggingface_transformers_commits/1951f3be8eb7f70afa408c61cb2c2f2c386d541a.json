{
    "author": "zRzRzRzRzRzRzR",
    "message": "Update GLM-4.1V MMRope implementation (#41182)\n\n* update for 4D mask\n\n* update\n\n* Update modular_glm4v.py\n\n* 1\n\n* Revert \"1\"\n\nThis reverts commit d13a763e876fa049c5fb70a8b3447b335dbb6098.\n\n* update as glm4v logtic\n\n* update\n\n* 1\n\n* update\n\n* Create convert_glm4v_moe_mgt_weights_to_hf.py\n\n* update\n\n* update",
    "sha": "1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
    "files": [
        {
            "sha": "bdd5f29b2e20c4997a95306c498e5dd65ccfabb6",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 29,
            "deletions": 11,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -591,14 +591,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -614,7 +614,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -837,17 +836,36 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n+            text_position_ids = None\n+\n+        mask_kwargs = {\n+            \"config\": self.config,\n+            \"input_embeds\": inputs_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"position_ids\": text_position_ids,\n+        }\n+        # Create the masks\n+        causal_mask = create_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n "
        },
        {
            "sha": "7c13788c0baa156378b645486c32baa963055f01",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 29,
            "deletions": 11,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -695,14 +695,14 @@ def __init__(self, config: Glm4vTextConfig, layer_idx: int):\n         self.post_self_attn_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_mlp_layernorm = Glm4vRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         position_embeddings: tuple[torch.Tensor, torch.Tensor],\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n@@ -718,7 +718,6 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n@@ -902,17 +901,36 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n+            text_position_ids = None\n+\n+        mask_kwargs = {\n+            \"config\": self.config,\n+            \"input_embeds\": inputs_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"position_ids\": text_position_ids,\n+        }\n+        # Create the masks\n+        causal_mask = create_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n "
        },
        {
            "sha": "10569e379ad81a1458c6a649e2cd93f1fb02d180",
            "filename": "src/transformers/models/glm4v_moe/convert_glm4v_moe_mgt_weights_to_hf.py",
            "status": "added",
            "additions": 780,
            "deletions": 0,
            "changes": 780,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconvert_glm4v_moe_mgt_weights_to_hf.py?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -0,0 +1,780 @@\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import json\n+import os\n+import pickle\n+import re\n+from pathlib import Path\n+\n+import torch\n+from safetensors.torch import save_file\n+\n+\n+# Avoid Using Megatron Lib\n+class UnpicklerWrapper(pickle.Unpickler):\n+    def find_class(self, mod_name, name):\n+        class DummyClass:\n+            def __init__(self, *args, **kwargs):\n+                pass\n+\n+        if mod_name.startswith(\"megatron\") or mod_name.startswith(\"glm\") or mod_name.startswith(\"__main__\"):\n+            return DummyClass\n+        return super().find_class(mod_name, name)\n+\n+\n+pickle.Unpickler = UnpicklerWrapper\n+\n+\n+def dict_access_multi(a_dict, keys):\n+    if len(keys) == 0:\n+        return a_dict\n+    return dict_access_multi(a_dict[keys[0]], keys[1:])\n+\n+\n+def merge_qkv(\n+    sd_list,\n+    original_tp,\n+    num_attention_heads,\n+    multi_query_group_num,\n+    attention_dim,\n+    interleaved_qkv,\n+):\n+    group_size = (num_attention_heads // multi_query_group_num + 2) * attention_dim\n+    q, k, v = [], [], []\n+    for sd in sd_list:\n+        if interleaved_qkv:\n+            shape = sd.shape\n+            print(f\"zjldbg input {shape} {(multi_query_group_num // original_tp, group_size) + (shape[1:])}\")\n+            print(\n+                f\"zjldbg xxx {shape} {(num_attention_heads // multi_query_group_num * attention_dim, attention_dim) + (shape[1:])}\"\n+            )\n+            q_, k_, v_ = sd.view((multi_query_group_num // original_tp, group_size) + (shape[1:])).split(\n+                [\n+                    (num_attention_heads // multi_query_group_num * attention_dim),\n+                    attention_dim,\n+                    attention_dim,\n+                ],\n+                dim=1,\n+            )\n+            q_ = q_.reshape((-1,) + (shape[1:]))\n+            k_ = k_.reshape((-1,) + (shape[1:]))\n+            v_ = v_.reshape((-1,) + (shape[1:]))\n+        else:\n+            q_, k_, v_ = sd.split(\n+                [\n+                    num_attention_heads * attention_dim // original_tp,\n+                    multi_query_group_num * attention_dim // original_tp,\n+                    multi_query_group_num * attention_dim // original_tp,\n+                ],\n+                dim=0,\n+            )\n+\n+        q.append(q_.clone())\n+        k.append(k_.clone())\n+        v.append(v_.clone())\n+    q = torch.cat(q, dim=0)\n+    k = torch.cat(k, dim=0)\n+    v = torch.cat(v, dim=0)\n+\n+    return q, k, v\n+\n+\n+def merge_glu(sd_list):\n+    return torch.cat(\n+        [sd.chunk(dim=0, chunks=2)[0].clone() for sd in sd_list]\n+        + [sd.chunk(dim=0, chunks=2)[1].clone() for sd in sd_list],\n+        dim=0,\n+    )\n+\n+\n+def merge_glu_vit(sd_list, original_tp=None):\n+    if not isinstance(sd_list, list):\n+        sd_list = [sd_list]\n+    gate_proj = torch.cat([sd.chunk(dim=0, chunks=2)[0].clone() for sd in sd_list], dim=0)\n+    up_proj = torch.cat([sd.chunk(dim=0, chunks=2)[1].clone() for sd in sd_list], dim=0)\n+    return gate_proj, up_proj\n+\n+\n+def split_glu(sd, cnt, idx):\n+    return torch.cat(\n+        (\n+            sd.chunk(dim=0, chunks=2)[0].chunk(cnt, dim=0)[idx].clone(),\n+            sd.chunk(dim=0, chunks=2)[1].chunk(cnt, dim=0)[idx].clone(),\n+        ),\n+        dim=0,\n+    )\n+\n+\n+def merge_qkv_vit(sd_list, source=None):\n+    q, k, v = [], [], []\n+    for sd in sd_list:\n+        q_, k_, v_ = sd.chunk(dim=0, chunks=3)\n+        q.append(q_.clone().contiguous())\n+        k.append(k_.clone().contiguous())\n+        v.append(v_.clone().contiguous())\n+    q = torch.cat(q, dim=0)\n+    k = torch.cat(k, dim=0)\n+    v = torch.cat(v, dim=0)\n+    combined = torch.cat([q, k, v], dim=0)\n+    return combined\n+\n+\n+def find_expert_weight(input_dict, layer_num, fc1=True):\n+    if fc1:\n+        pattern = re.compile(rf\"^decoder\\.layers\\.{layer_num}\\.mlp\\.experts\\.linear_fc1\\.weight(\\d+)$\")\n+    else:\n+        pattern = re.compile(rf\"^decoder\\.layers\\.{layer_num}\\.mlp\\.experts\\.linear_fc2\\.weight(\\d+)$\")\n+    matched = []\n+    for key in input_dict:\n+        match = pattern.match(key)\n+        if match:\n+            weight_num = int(match.group(1))\n+            matched.append((weight_num, key))\n+    matched.sort(key=lambda x: x[0])\n+\n+    weights = [None for _ in range(len(matched) * len(input_dict[matched[0][1]]))]\n+    for idx, key in matched:\n+        for i, weight in enumerate(input_dict[key]):\n+            weights[i * len(matched) + idx] = weight\n+\n+    return weights\n+\n+\n+def merge_tensors(\n+    tp_sd,\n+    keys,\n+    original_tp,\n+    target_tp,\n+    current_tp,\n+    slice_dim=None,\n+    merge_fn=None,\n+):\n+    cnt = original_tp // target_tp\n+    offset = cnt * current_tp\n+    sd_list = [dict_access_multi(tp_sd[i + offset], keys) for i in range(cnt)]\n+    if slice_dim is not None:\n+        return torch.cat(sd_list, dim=slice_dim)\n+    assert merge_fn is not None\n+    return merge_fn(sd_list)\n+\n+\n+def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=46, vision_num_layers=24):\n+    os.makedirs(output_path, exist_ok=True)\n+\n+    layered_dict = {}\n+    for layer_idx in range(num_layers):\n+        layer_key = f\"layer_{layer_idx}\"\n+        layered_dict[layer_key] = {}\n+\n+        for key, value in state_dict.items():\n+            if f\"model.language_model.layers.{layer_idx}.\" in key:\n+                if isinstance(value, list):\n+                    assert len(value) == 1, f\"{key} {value}\"\n+                    value = value[0]\n+                layered_dict[layer_key][key] = value\n+\n+    for layer_idx in range(vision_num_layers):\n+        layer_key = f\"visual_layer_{layer_idx}\"\n+        layered_dict[layer_key] = {}\n+\n+        for key, value in state_dict.items():\n+            if f\"model.visual.blocks.{layer_idx}.\" in key:\n+                layered_dict[layer_key][key] = value\n+\n+    layered_dict[\"others\"] = {}\n+    for key, value in state_dict.items():\n+        if not any(f\"model.language_model.layers.{i}.\" in key for i in range(num_layers)) and not any(\n+            f\"model.visual.blocks.{i}.\" in key for i in range(vision_num_layers)\n+        ):\n+            layered_dict[\"others\"][key] = value\n+\n+    # Determine layer ordering\n+    layer_order = []\n+    for i in range(num_layers):\n+        layer_order.append(f\"layer_{i}\")\n+    for i in range(vision_num_layers):\n+        layer_order.append(f\"visual_layer_{i}\")\n+    layer_order.append(\"others\")\n+\n+    # Calculate sizes and create shards by layer\n+    param_sizes = {}\n+    shards = []\n+    current_shard = {}\n+    current_shard_size = 0\n+    max_shard_size_bytes = max_shard_size_gb * 1024 * 1024 * 1024\n+\n+    for layer_key in layer_order:\n+        layer_weights = layered_dict[layer_key]\n+        layer_size = sum(param.numel() * param.element_size() for param in layer_weights.values())\n+        if current_shard_size + layer_size > max_shard_size_bytes and current_shard:\n+            shards.append(current_shard)\n+            current_shard = {}\n+            current_shard_size = 0\n+        for param_name, param in layer_weights.items():\n+            current_shard[param_name] = param\n+            current_shard_size += param.numel() * param.element_size()\n+            param_sizes[param_name] = param.numel() * param.element_size()\n+    if current_shard:\n+        shards.append(current_shard)\n+    index_dict = {\"metadata\": {\"total_size\": sum(param_sizes.values())}, \"weight_map\": {}}\n+\n+    for i, shard in enumerate(shards):\n+        shard_filename = f\"model-{i + 1:05d}-of-{len(shards):05d}.safetensors\"\n+        shard_path = os.path.join(output_path, shard_filename)\n+\n+        for param_name in shard:\n+            index_dict[\"weight_map\"][param_name] = shard_filename\n+\n+        save_file(shard, shard_path, metadata={\"format\": \"pt\"})\n+        print(f\"Saved shard {i + 1}/{len(shards)}: {shard_filename}\")\n+        print(f\"  Shard size: {sum(p.numel() * p.element_size() for p in shard.values()) / (1024**3):.2f} GB\")\n+        print(f\"  Keys in shard: {len(shard)}\")\n+\n+    index_path = os.path.join(output_path, \"model.safetensors.index.json\")\n+    with open(index_path, \"w\") as f:\n+        json.dump(index_dict, f, indent=2)\n+\n+    return len(shards)\n+\n+\n+def merge_tp_weights(model_path, output_path, vllm_config_path=None):\n+    origin_tp, origin_ep, origin_pp = -1, -1, -1\n+\n+    check_ep_or_pp_later = False\n+    for item in Path(model_path).iterdir():\n+        if item.is_dir():\n+            match = re.match(r\"mp_rank_(\\d{2})(?:_(\\d{3}))?(?:_(\\d{3}))?\", item.name)\n+            if match:\n+                groups = match.groups()\n+                tp = int(groups[0])\n+                origin_tp = max(origin_tp, tp + 1)\n+                # maybe TP-EP or TP-PP, need check later\n+                if groups[1] is not None and groups[2] is None:\n+                    pp = int(groups[1])\n+                    origin_pp = max(origin_pp, pp + 1)\n+                    origin_ep = 1\n+                    check_ep_or_pp_later = True\n+                elif groups[1] is not None and groups[2] is not None:\n+                    pp = int(groups[1])\n+                    ep = int(groups[2])\n+                    origin_pp = max(origin_pp, pp + 1)\n+                    origin_ep = max(origin_ep, ep + 1)\n+                else:\n+                    origin_ep = 1\n+                    origin_pp = 1\n+\n+    tensor_names_by_file = {}\n+    mgt_sd = {}\n+    for item in Path(model_path).iterdir():\n+        if item.is_dir():\n+            match = re.match(r\"mp_rank_(\\d{2})(?:_(\\d{3}))?(?:_(\\d{3}))?$\", item.name)\n+            if match:\n+                groups = match.groups()\n+                tp = int(groups[0])\n+                pp = int(groups[1]) if groups[1] is not None else 0\n+                ep = int(groups[2]) if groups[2] is not None else 0\n+\n+                file_path = item / \"model_optim_rng.pt\"\n+                assert file_path.exists(), f\"model_optim_rng.pt not found in {item}\"\n+\n+                file_sd = torch.load(file_path, map_location=\"cpu\", weights_only=False)\n+\n+                for k in list(file_sd.keys()):\n+                    if \"_extra_state\" in k or \"dummy_parameter\" in k:\n+                        file_sd.pop(k)\n+\n+                mgt_sd[(tp, pp, ep)] = file_sd\n+\n+                tensor_names = set()\n+                if \"model\" in file_sd:\n+                    for key in file_sd[\"model\"].keys():\n+                        tensor_names.add(key)\n+                tensor_names_by_file[(tp, pp, ep)] = tensor_names\n+\n+    change_pp_to_ep = False\n+    if check_ep_or_pp_later:\n+        prefix_distribution = {}\n+\n+        for (tp, pp, ep), prefixes in tensor_names_by_file.items():\n+            for prefix in prefixes:\n+                if prefix not in prefix_distribution:\n+                    prefix_distribution[prefix] = set()\n+                prefix_distribution[prefix].add((tp, pp, ep))\n+\n+        for prefix, locations in prefix_distribution.items():\n+            if len(locations) > 1:\n+                pp_values = {loc[1] for loc in locations}\n+                if len(pp_values) > 1:\n+                    print(f\"find '{prefix}' in multi ranks {pp_values} the parallelism should be TP-EP\")\n+                    origin_ep = origin_pp\n+                    origin_pp = 1\n+                    change_pp_to_ep = True\n+                    break\n+                else:\n+                    print(f\"find '{prefix}' only in one ep, parallelism should be TP-PP\")\n+                    break\n+\n+    print(f\"Detected tensor parallel degree TP={origin_tp} EP={origin_ep} PP={origin_pp}\")\n+    if origin_tp <= 1 and origin_ep <= 1 and origin_pp <= 1:\n+        print(\"Model is already at TP=1 EP=1 PP=1, no need to merge\")\n+        return\n+    assert max(origin_tp, origin_ep) * origin_pp == len(tensor_names_by_file), \"maybe some problem in origin weight\"\n+\n+    organized_sd = {}\n+    for (tp, pp, ep), file_sd in mgt_sd.items():\n+        if change_pp_to_ep:\n+            pp, ep = ep, pp\n+        organized_sd.setdefault(pp, {})\n+        organized_sd[pp][(ep, tp)] = file_sd\n+        find_vpp = \"model0\" in file_sd\n+\n+    # support VPP, if each pp rank has n vpp blocks, we will treat the original model\n+    # was parallel as pp n * origin_pp\n+    if find_vpp:\n+        organized_sd_vpp = {}\n+        for i in range(origin_pp):\n+            for (ep, tp), file_sd in organized_sd[i].items():\n+                model_keys = sorted(\n+                    [key for key in file_sd.keys() if key.startswith(\"model\") and key[5:].isdigit()],\n+                    key=lambda x: int(x[5:]),\n+                )\n+                vp_blocks = len(model_keys)\n+                for idx, key in enumerate(model_keys):\n+                    assert key in file_sd, f\"model {key} not found\"\n+                    organized_sd_vpp.setdefault(idx * origin_pp + i, {})\n+                    organized_sd_vpp[idx * origin_pp + i][(ep, tp)] = {\"model\": file_sd[key]}\n+        origin_pp = origin_pp * vp_blocks\n+        organized_sd = organized_sd_vpp\n+\n+    ignore_list = [\"_extra_state\", \"dummy_parameter\"]\n+    layer_share_list = [\n+        \"norm\",\n+        \"conv3d\",\n+        \"downsample\",\n+        \"router\",\n+        \"mlp.linear_fc2.bias\",\n+        \"self_attention.linear_proj.bias\",\n+        \"position_embeddings\",\n+    ]\n+\n+    full_weights = {}\n+\n+    vit_layer_offset = 0\n+    llm_layer_offset = 0\n+    llm_layer_pattern = re.compile(r\"^(decoder\\.layers\\.)(\\d+)(\\..*)$\")\n+    vit_layer_pattern = re.compile(r\"^(vision_model\\.transformer\\.layers\\.)(\\d+)(\\..*)$\")\n+    for pp in sorted(organized_sd.keys()):\n+        pp_dict = organized_sd[pp]\n+        next_llm_layer_offset = llm_layer_offset\n+        next_vit_layer_offset = vit_layer_offset\n+        ep_map = {}\n+        tp_map = {}\n+        tp_seen = set()\n+        for (ep, tp), item in pp_dict.items():\n+            if tp not in tp_seen:\n+                tp_seen.add(tp)\n+                tp_map[tp] = item\n+            ep_map[ep] = item\n+\n+        for tp in sorted(tp_map.keys()):\n+            sd = tp_map[tp]\n+            for full_name, tensor in sd[\"model\"].items():\n+                if any(x in full_name for x in ignore_list):\n+                    continue\n+                llm_name_match = llm_layer_pattern.match(full_name)\n+                if llm_name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=llm_layer_offset):\n+                        nonlocal next_llm_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_llm_layer_offset = max(next_llm_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = llm_layer_pattern.sub(offset_layer, full_name)\n+                vit_name_match = vit_layer_pattern.match(full_name)\n+                if vit_name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=vit_layer_offset):\n+                        nonlocal next_vit_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_vit_layer_offset = max(next_vit_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = vit_layer_pattern.sub(offset_layer, full_name)\n+                if layer_share_list and any(x in full_name for x in layer_share_list):\n+                    if full_name not in full_weights:\n+                        full_weights[full_name] = tensor\n+                    else:\n+                        assert torch.equal(tensor, full_weights[full_name]), (\n+                            f\"detect diff param in tp named: {full_name}\"\n+                        )\n+                elif not re.search(r\"\\.experts\\.\", full_name):\n+                    full_weights.setdefault(full_name, [None for _ in range(origin_tp)])\n+                    full_weights[full_name][tp] = tensor\n+\n+        for ep in sorted(ep_map.keys()):\n+            sd = ep_map[ep]\n+            for full_name, tensor in sd[\"model\"].items():\n+                if any(x in full_name for x in ignore_list):\n+                    continue\n+                name_match = llm_layer_pattern.match(full_name)\n+                if name_match:\n+                    # Use a closure to avoid global variable issues\n+                    def offset_layer(x, offset=llm_layer_offset):\n+                        nonlocal next_llm_layer_offset\n+                        _real_layer = int(x.group(2)) + offset\n+                        next_llm_layer_offset = max(next_llm_layer_offset, _real_layer + 1)\n+                        return f\"{x.group(1)}{_real_layer}{x.group(3)}\"\n+\n+                    full_name = llm_layer_pattern.sub(offset_layer, full_name)\n+                if re.search(r\"\\.experts\\.\", full_name):\n+                    full_weights.setdefault(full_name, [None for _ in range(origin_ep)])\n+                    full_weights[full_name][ep] = tensor\n+        llm_layer_offset = next_llm_layer_offset\n+        vit_layer_offset = next_vit_layer_offset\n+\n+    for k in sorted(full_weights.keys()):\n+        item = full_weights[k]\n+        if isinstance(item, list):\n+            print(f\"{k} {len(item)} {item[0].shape} {item[0].dtype}\", flush=True)\n+        else:\n+            print(f\"{k} {item.shape} {item.dtype}\", flush=True)\n+\n+    print(f\"Loading vLLM configuration file: {vllm_config_path}\")\n+    with open(vllm_config_path, \"r\") as f:\n+        model_config = json.load(f)\n+        print(model_config)\n+        text_config = model_config.get(\"text_config\", {})\n+        vision_config = model_config.get(\"vision_config\", {})\n+\n+        num_layers = text_config.get(\"num_hidden_layers\", 46)\n+        llm_num_heads = text_config.get(\"num_attention_heads\", 96)\n+        num_kv_heads = text_config.get(\"num_key_value_heads\", 8)\n+        llm_attn_query_size = text_config.get(\"llm_attn_query_size\", 12288)\n+        head_dim = text_config.get(\"attention_dim\", llm_attn_query_size // llm_num_heads)\n+        vision_num_layers = vision_config.get(\"depth\", 24)\n+        vit_n_head = vision_config.get(\"num_heads\", 12)\n+\n+    print(\n+        f\"Model parameters: num_layers={num_layers}, vision_num_layers={vision_num_layers}, \"\n+        f\"num_heads={llm_num_heads}, multi_query_group_num={num_kv_heads}, llm_attn_query_size={llm_attn_query_size}\"\n+    )\n+\n+    print(\"Merging tensor parallel weights...\")\n+\n+    interleaved_qkv = True\n+    multi_query_attention = True\n+    num_attention_heads = llm_num_heads\n+    multi_query_group_num = num_kv_heads\n+    attention_dim = head_dim\n+    complete_state_dict = {}\n+\n+    # LLM\n+    layer_i = 0\n+    while f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\" in full_weights:\n+        if f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.input_layernorm.weight\"] = full_weights[\n+                f\"decoder.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"\n+            ]\n+\n+        if f\"decoder.layers.{layer_i}.pre_mlp_layernorm.weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.pre_mlp_layernorm.weight\"]\n+            )\n+        elif f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\" in full_weights:\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"]\n+            )\n+\n+        q, k, v = merge_qkv(\n+            sd_list=full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n+            original_tp=origin_tp,\n+            num_attention_heads=num_attention_heads,\n+            multi_query_group_num=multi_query_group_num,\n+            attention_dim=attention_dim,\n+            interleaved_qkv=interleaved_qkv,\n+        )\n+\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.weight\"] = q.clone()\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.weight\"] = k.clone()\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.weight\"] = v.clone()\n+\n+        if f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\" in full_weights:\n+            q_bias, k_bias, v_bias = merge_qkv(\n+                sd_list=full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_qkv.bias\"],\n+                original_tp=origin_tp,\n+                num_attention_heads=num_attention_heads,\n+                multi_query_group_num=multi_query_group_num,\n+                attention_dim=attention_dim,\n+                interleaved_qkv=interleaved_qkv,\n+            )\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.q_proj.bias\"] = q_bias.clone()\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.k_proj.bias\"] = k_bias.clone()\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.v_proj.bias\"] = v_bias.clone()\n+\n+        o_proj = torch.cat(full_weights[f\"decoder.layers.{layer_i}.self_attention.linear_proj.weight\"], dim=1)\n+        complete_state_dict[f\"model.language_model.layers.{layer_i}.self_attn.o_proj.weight\"] = o_proj.clone()\n+\n+        if f\"decoder.layers.{layer_i}.mlp.shared_experts.linear_fc1.weight\" in full_weights:\n+            routed_expert_fc1_weights = find_expert_weight(full_weights, layer_i, fc1=True)\n+            for idx, weight in enumerate(routed_expert_fc1_weights):\n+                gate_proj_weight, up_proj_weight = merge_glu_vit([weight])\n+                complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.experts.{idx}.gate_proj.weight\"] = (\n+                    gate_proj_weight.clone()\n+                )\n+                complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.experts.{idx}.up_proj.weight\"] = (\n+                    up_proj_weight.clone()\n+                )\n+\n+            routed_expert_fc2_weights = find_expert_weight(full_weights, layer_i, fc1=False)\n+            for idx, weight in enumerate(routed_expert_fc2_weights):\n+                complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.experts.{idx}.down_proj.weight\"] = (\n+                    weight.clone()\n+                )\n+\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.gate.e_score_correction_bias\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.router.expert_bias\"]\n+            )\n+\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.gate.weight\"] = full_weights[\n+                f\"decoder.layers.{layer_i}.mlp.router.weight\"\n+            ]\n+\n+            gate_proj_weight, up_proj_weight = merge_glu_vit(\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.shared_experts.linear_fc1.weight\"]\n+            )\n+\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.shared_experts.gate_proj.weight\"] = (\n+                gate_proj_weight.clone()\n+            )\n+\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.shared_experts.up_proj.weight\"] = (\n+                up_proj_weight.clone()\n+            )\n+\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.shared_experts.down_proj.weight\"] = (\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.shared_experts.linear_fc2.weight\"]\n+            )\n+\n+        else:\n+            # MLP - Use gate_up_proj\n+            gate_proj_weight, up_proj_weight = merge_glu_vit(\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc1.weight\"]\n+            )\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.gate_proj.weight\"] = (\n+                gate_proj_weight.clone()\n+            )\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.up_proj.weight\"] = up_proj_weight.clone()\n+            complete_state_dict[f\"model.language_model.layers.{layer_i}.mlp.down_proj.weight\"] = torch.cat(\n+                full_weights[f\"decoder.layers.{layer_i}.mlp.linear_fc2.weight\"], dim=1\n+            )\n+        layer_i += 1\n+\n+    # Embedd Model, LM Head, and Norm\n+    embed_tokens = torch.cat(full_weights[\"embedding.word_embeddings.weight\"], dim=0)\n+    complete_state_dict[\"model.language_model.embed_tokens.weight\"] = embed_tokens.clone()\n+\n+    lm_head = torch.cat(full_weights[\"output_layer.weight\"], dim=0)\n+    complete_state_dict[\"lm_head.weight\"] = lm_head.clone()\n+    complete_state_dict[\"model.language_model.norm.weight\"] = full_weights[\"decoder.final_layernorm.weight\"].clone()\n+\n+    # VLM\n+    for layer_i in range(vision_num_layers):\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm1.weight\"] = full_weights[\n+            f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.layer_norm_weight\"\n+        ]\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.norm2.weight\"] = full_weights[\n+            f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.layer_norm_weight\"\n+        ]\n+\n+        # qkv_weight = merge_qkv_vit(\n+        #     full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"])\n+        # complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = qkv_weight.clone()\n+        q, k, v = merge_qkv(\n+            sd_list=full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_qkv.weight\"],\n+            original_tp=origin_tp,\n+            num_attention_heads=vit_n_head,\n+            multi_query_group_num=vit_n_head,\n+            attention_dim=attention_dim,\n+            multi_query_attention=multi_query_attention,\n+            interleaved_qkv=interleaved_qkv,\n+        )\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.qkv.weight\"] = torch.cat((q, k, v), dim=0)\n+\n+        proj_weight = torch.cat(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.self_attention.linear_proj.weight\"], dim=1\n+        )\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.attn.proj.weight\"] = proj_weight.clone()\n+\n+        gate_proj_weight, up_proj_weight = merge_glu_vit(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc1.weight\"]\n+        )\n+\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.gate_proj.weight\"] = gate_proj_weight.clone()\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.up_proj.weight\"] = up_proj_weight.clone()\n+\n+        down_proj_weight = torch.cat(\n+            full_weights[f\"vision_model.transformer.layers.{layer_i}.mlp.linear_fc2.weight\"], dim=1\n+        )\n+        complete_state_dict[f\"model.visual.blocks.{layer_i}.mlp.down_proj.weight\"] = down_proj_weight.clone()\n+\n+    complete_state_dict[\"model.visual.downsample.weight\"] = (\n+        full_weights[\"vision_model.downsample.weight\"].clone().contiguous()\n+    )\n+    complete_state_dict[\"model.visual.downsample.bias\"] = (\n+        full_weights[\"vision_model.downsample.bias\"].clone().contiguous()\n+    )\n+\n+    # Merger\n+    gate_proj, up_proj = merge_glu_vit(full_weights[\"vision_projection.encoder.linear_fc1.weight\"])\n+\n+    down_proj = torch.cat(full_weights[\"vision_projection.encoder.linear_fc2.weight\"], dim=1)\n+    proj = torch.cat(full_weights[\"vision_projection.linear_fc_extra.weight\"], dim=0)\n+\n+    complete_state_dict[\"model.visual.merger.gate_proj.weight\"] = gate_proj.clone().contiguous()\n+    complete_state_dict[\"model.visual.merger.up_proj.weight\"] = up_proj.clone().contiguous()\n+    complete_state_dict[\"model.visual.merger.down_proj.weight\"] = down_proj.clone().contiguous()\n+    complete_state_dict[\"model.visual.merger.proj.weight\"] = proj.clone().contiguous()\n+\n+    if \"vision_projection.layer_norm.weight\" in full_weights:\n+        complete_state_dict[\"model.visual.merger.post_projection_norm.weight\"] = full_weights[\n+            \"vision_projection.layer_norm.weight\"\n+        ]\n+    if \"vision_projection.layer_norm.bias\" in full_weights:\n+        complete_state_dict[\"model.visual.merger.post_projection_norm.bias\"] = full_weights[\n+            \"vision_projection.layer_norm.bias\"\n+        ]\n+\n+    complete_state_dict[\"model.visual.embeddings.position_embedding.weight\"] = (\n+        full_weights[\"vision_model.position_embeddings.weight\"].clone().contiguous()\n+    )\n+    complete_state_dict[\"model.visual.patch_embed.proj.weight\"] = (\n+        full_weights[\"vision_model.conv3d.weight\"].clone().contiguous()\n+    )\n+    complete_state_dict[\"model.visual.patch_embed.proj.bias\"] = (\n+        full_weights[\"vision_model.conv3d.bias\"].clone().contiguous()\n+    )\n+\n+    # Check for additional vision model norm layers mentioned in the expected output\n+    if \"vision_model.post_conv_layernorm.weight\" in full_weights:\n+        complete_state_dict[\"model.visual.post_conv_layernorm.weight\"] = (\n+            full_weights[\"vision_model.post_conv_layernorm.weight\"].clone().contiguous()\n+        )\n+\n+    if \"vision_model.post_layernorm.weight\" in full_weights:\n+        complete_state_dict[\"model.visual.post_layernorm.weight\"] = (\n+            full_weights[\"vision_model.post_layernorm.weight\"].clone().contiguous()\n+        )\n+\n+    print(f\"Total keys in state dict: {len(complete_state_dict)}\")\n+\n+    print(\"bias use Float32\")\n+\n+    save_sharded_model(\n+        complete_state_dict,\n+        output_path=output_path,\n+        max_shard_size_gb=5,\n+        num_layers=num_layers,\n+        vision_num_layers=vision_num_layers,\n+    )\n+\n+    hf_config = {\n+        \"architectures\": [\"Glm4vMoeForConditionalGeneration\"],\n+        \"model_type\": \"glm4v_moe\",\n+        \"attention_bias\": model_config.get(\"add_qkv_bias\", True),\n+        \"attention_dropout\": 0.0,\n+        \"pad_token_id\": model_config.get(\"pad_token_id\", 151329),\n+        \"eos_token_id\": model_config.get(\"eos_token_id\", [151329, 151336, 151338]),\n+        \"image_start_token_id\": model_config.get(\"image_start_token_id\", 151339),\n+        \"image_end_token_id\": model_config.get(\"image_end_token_id\", 151340),\n+        \"video_start_token_id\": model_config.get(\"video_start_token_id\", 151341),\n+        \"video_end_token_id\": model_config.get(\"video_end_token_id\", 151342),\n+        \"image_token_id\": model_config.get(\"image_token_id\", 151343),\n+        \"video_token_id\": model_config.get(\"video_token_id\", 151344),\n+    }\n+    txt_config = {\n+        \"hidden_act\": text_config.get(\"hidden_act\", \"silu\"),\n+        \"hidden_size\": text_config.get(\"hidden_size\", 4096),\n+        \"initializer_range\": 0.02,\n+        \"intermediate_size\": text_config.get(\"intermediate_size\", 10944),\n+        \"max_position_embeddings\": text_config.get(\"seq_length\", 131072),\n+        \"num_attention_heads\": text_config.get(\"num_attention_heads\", 96),\n+        \"num_hidden_layers\": text_config.get(\"num_layers\", 46),\n+        \"num_key_value_heads\": text_config.get(\"multi_query_group_num\", 2),\n+        \"rms_norm_eps\": text_config.get(\"layernorm_epsilon\", 1e-05),\n+        \"rope_theta\": text_config.get(\"rotary_base\", 10000.0),\n+        \"tie_word_embeddings\": False,\n+        \"torch_dtype\": text_config.get(\"torch_dtype\", \"bfloat16\"),\n+        \"transformers_version\": \"4.53.0dev\",\n+        \"use_cache\": text_config.get(\"use_cache\", True),\n+        \"vocab_size\": text_config.get(\"vocab_size\", 151424),\n+        \"partial_rotary_factor\": 0.5,\n+        \"moe_intermediate_size\": text_config.get(\"moe_intermediate_size\", 1408),\n+        \"n_group\": text_config.get(\"n_group\", 1),\n+        \"n_routed_experts\": text_config.get(\"n_routed_experts\", 128),\n+        \"n_shared_experts\": text_config.get(\"n_shared_experts\", 1),\n+        \"norm_topk_prob\": text_config.get(\"norm_topk_prob\", True),\n+        \"num_experts_per_tok\": text_config.get(\"num_experts_per_tok\", 8),\n+    }\n+    hf_config[\"text_config\"] = txt_config\n+\n+    if \"vision_config\" in model_config:\n+        vision_config = {\n+            \"hidden_size\": model_config[\"vision_config\"].get(\"hidden_size\", 1536),\n+            \"depth\": model_config[\"vision_config\"].get(\"num_layers\", 24),\n+            \"num_heads\": model_config[\"vision_config\"].get(\"num_attention_heads\", 12),\n+            \"attention_bias\": model_config[\"vision_config\"].get(\"attention_bias\", False),\n+            \"intermediate_size\": model_config.get(\"ffn_hidden_size\", 13696),\n+            \"hidden_act\": model_config[\"vision_config\"].get(\"hidden_act\", \"silu\"),\n+            \"hidden_dropout_prob\": model_config[\"vision_config\"].get(\"hidden_dropout_prob\", 0.0),\n+            \"initializer_range\": 0.02,\n+            \"image_size\": model_config[\"vision_config\"].get(\"image_size\", 336),\n+            \"patch_size\": model_config[\"vision_config\"].get(\"patch_size\", 14),\n+            \"out_hidden_size\": model_config.get(\"hidden_size\", 4096),\n+            \"rms_norm_eps\": model_config[\"vision_config\"].get(\"layernorm_epsilon\", 1e-05),\n+            \"spatial_merge_size\": model_config[\"vision_config\"].get(\"downsample_ratio\", 2),\n+            \"temporal_patch_size\": model_config[\"vision_config\"].get(\"t_patch\", 2),\n+        }\n+        hf_config[\"vision_config\"] = vision_config\n+\n+    if \"rope_scaling\" in model_config:\n+        hf_config[\"rope_scaling\"] = model_config[\"rope_scaling\"]\n+\n+    config_path = os.path.join(output_path, \"config.json\")\n+    with open(config_path, \"w\") as f:\n+        json.dump(hf_config, f, indent=2)\n+\n+    print(f\"Conversion complete! Model saved to {output_path}\")\n+\n+\n+def parse_args():\n+    parser = argparse.ArgumentParser(description=\"Convert Megatron model to HuggingFace format\")\n+    parser.add_argument(\n+        \"--model_path\",\n+        type=str,\n+        required=True,\n+        help=\"Path to Megatron model directory\",\n+    )\n+    parser.add_argument(\"--output_path\", type=str, required=True, help=\"Output path for HuggingFace model directory\")\n+    parser.add_argument(\n+        \"--config_path\", type=str, help=\"Path to vLLM configuration file for creating HuggingFace config\"\n+    )\n+    return parser.parse_args()\n+\n+\n+if __name__ == \"__main__\":\n+    args = parse_args()\n+    merge_tp_weights(args.model_path, args.output_path, args.config_path)"
        },
        {
            "sha": "221d5d274f1db6f10e042d82f98d2b9902158dc3",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 28,
            "deletions": 9,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1951f3be8eb7f70afa408c61cb2c2f2c386d541a/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=1951f3be8eb7f70afa408c61cb2c2f2c386d541a",
            "patch": "@@ -955,17 +955,36 @@ def forward(\n         # the hard coded `3` is for temporal, height and width.\n         if position_ids is None:\n             position_ids = cache_position.view(1, 1, -1).expand(3, inputs_embeds.shape[0], -1)\n-        elif position_ids.dim() == 2:\n+        elif position_ids.ndim == 2:\n             position_ids = position_ids[None, ...].expand(3, position_ids.shape[0], -1)\n \n-        causal_mask = create_causal_mask(\n-            config=self.config,\n-            input_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            cache_position=cache_position,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n+        # NOTE: we need to pass text position ids for packing. Qwen2-VL uses 3D positions\n+        # where each dim indicates visual spatial positions for temporal/height/width grids.\n+        # There are two scenarios when FA2-like packed masking might be activated.\n+        # 1. User specifically passed packed `position_ids` and no attention mask.\n+        #    In this case we expect the useer to create correct position ids for all 3 grids\n+        #    and prepend text-only position ids to it. The final tensor will be [4, bs, seq-len]\n+        # 2. User runs forward with no attention mask and no position ids. In this case, position ids\n+        #    are prepared by the model (`get_rope_index`) as `[4, bs, seq-len]` tensor. Text-only positions are\n+        #    prepended by us when creating positions so that the mask is constructed correctly. NOTE: failing to pass\n+        #    text-only positions will cause incorrect mask construction, do not change `prepare_input_for_generation`\n+        if position_ids.ndim == 3 and position_ids.shape[0] == 4:\n+            text_position_ids = position_ids[0]\n+            position_ids = position_ids[1:]\n+        else:\n+            # If inputs are not packed (usual 3D positions), do not prepare mask from position_ids\n+            text_position_ids = None\n+\n+        mask_kwargs = {\n+            \"config\": self.config,\n+            \"input_embeds\": inputs_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"position_ids\": text_position_ids,\n+        }\n+        # Create the masks\n+        causal_mask = create_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n "
        }
    ],
    "stats": {
        "total": 897,
        "additions": 866,
        "deletions": 31
    }
}