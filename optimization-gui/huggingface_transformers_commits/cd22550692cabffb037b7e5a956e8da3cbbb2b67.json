{
    "author": "Jin-HoMLee",
    "message": "docs: Update LayoutLM model card according to new standardized format (#40129)\n\n* docs: Update LayoutLM model card with standardized format\n\n* Apply suggestions from code review\r\n\r\nThis commit incorporates all suggestions provided in the recent review. Further changes will be committed separately to address remaining comments.\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Address remaining review comments\n\n* Address few more review comments:\n1. remove transformer-cli section\n2. put resources after notes\n3. change API refs to 2nd level header\n\n* Update layoutlm.md\n\n* Update layoutlm.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "cd22550692cabffb037b7e5a956e8da3cbbb2b67",
    "files": [
        {
            "sha": "c04380f6fec970542926f7613cd3d3d01003d611",
            "filename": "docs/source/en/model_doc/layoutlm.md",
            "status": "modified",
            "additions": 76,
            "deletions": 75,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/cd22550692cabffb037b7e5a956e8da3cbbb2b67/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cd22550692cabffb037b7e5a956e8da3cbbb2b67/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlm.md?ref=cd22550692cabffb037b7e5a956e8da3cbbb2b67",
            "patch": "@@ -1,4 +1,4 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n \n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n@@ -15,51 +15,75 @@ rendered properly in your Markdown viewer.\n -->\n *This model was released on 2019-12-31 and added to Hugging Face Transformers on 2020-11-16.*\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    </div>\n+</div>\n+\n # LayoutLM\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n-</div>\n+[LayoutLM](https://huggingface.co/papers/1912.13318) jointly learns text and the document layout rather than focusing only on text. It incorporates positional layout information and visual features of words from the document images.\n+\n+You can find all the original LayoutLM checkpoints under the [LayoutLM](https://huggingface.co/collections/microsoft/layoutlm-6564539601de72cb631d0902) collection.\n+\n+> [!TIP]\n+> Click on the LayoutLM models in the right sidebar for more examples of how to apply LayoutLM to different vision and language tasks.\n+\n+The example below demonstrates question answering with the [`AutoModel`] class. \n+\n+<hfoptions>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch  \n+from datasets import load_dataset  \n+from transformers import AutoTokenizer, LayoutLMForQuestionAnswering  \n+\n+tokenizer = AutoTokenizer.from_pretrained(\"impira/layoutlm-document-qa\", add_prefix_space=True)  \n+model = LayoutLMForQuestionAnswering.from_pretrained(\"impira/layoutlm-document-qa\", torch_dtype=torch.float16)  \n+\n+dataset = load_dataset(\"nielsr/funsd\", split=\"train\")  \n+example = dataset[0]  \n+question = \"what's his name?\"  \n+words = example[\"words\"]  \n+boxes = example[\"bboxes\"]  \n+\n+encoding = tokenizer(  \n+    question.split(), \n+    words, \n+    is_split_into_words=True, \n+    return_token_type_ids=True, \n+    return_tensors=\"pt\"  \n+)  \n+bbox = []  \n+for i, s, w in zip(encoding.input_ids[0], encoding.sequence_ids(0), encoding.word_ids(0)):  \n+    if s == 1:  \n+        bbox.append(boxes[w])  \n+    elif i == tokenizer.sep_token_id:  \n+        bbox.append([1000] * 4)  \n+    else:  \n+        bbox.append([0] * 4)  \n+encoding[\"bbox\"] = torch.tensor([bbox])  \n+\n+word_ids = encoding.word_ids(0)  \n+outputs = model(**encoding)  \n+loss = outputs.loss  \n+start_scores = outputs.start_logits  \n+end_scores = outputs.end_logits  \n+start, end = word_ids[start_scores.argmax(-1)], word_ids[end_scores.argmax(-1)]  \n+print(\" \".join(words[start : end + 1]))  \n+```\n+\n+</hfoption>\n+</hfoptions>\n \n-<a id='Overview'></a>\n-\n-## Overview\n-\n-The LayoutLM model was proposed in the paper [LayoutLM: Pre-training of Text and Layout for Document Image\n-Understanding](https://huggingface.co/papers/1912.13318) by Yiheng Xu, Minghao Li, Lei Cui, Shaohan Huang, Furu Wei, and\n-Ming Zhou. It's a simple but effective pretraining method of text and layout for document image understanding and\n-information extraction tasks, such as form understanding and receipt understanding. It obtains state-of-the-art results\n-on several downstream tasks:\n-\n-- form understanding: the [FUNSD](https://guillaumejaume.github.io/FUNSD/) dataset (a collection of 199 annotated\n-  forms comprising more than 30,000 words).\n-- receipt understanding: the [SROIE](https://rrc.cvc.uab.es/?ch=13) dataset (a collection of 626 receipts for\n-  training and 347 receipts for testing).\n-- document image classification: the [RVL-CDIP](https://www.cs.cmu.edu/~aharley/rvl-cdip/) dataset (a collection of\n-  400,000 images belonging to one of 16 classes).\n-\n-The abstract from the paper is the following:\n-\n-*Pre-training techniques have been verified successfully in a variety of NLP tasks in recent years. Despite the\n-widespread use of pretraining models for NLP applications, they almost exclusively focus on text-level manipulation,\n-while neglecting layout and style information that is vital for document image understanding. In this paper, we propose\n-the LayoutLM to jointly model interactions between text and layout information across scanned document images, which is\n-beneficial for a great number of real-world document image understanding tasks such as information extraction from\n-scanned documents. Furthermore, we also leverage image features to incorporate words' visual information into LayoutLM.\n-To the best of our knowledge, this is the first time that text and layout are jointly learned in a single framework for\n-document-level pretraining. It achieves new state-of-the-art results in several downstream tasks, including form\n-understanding (from 70.72 to 79.27), receipt understanding (from 94.02 to 95.24) and document image classification\n-(from 93.07 to 94.42).*\n-\n-## Usage tips\n-\n-- In addition to *input_ids*, [`~transformers.LayoutLMModel.forward`] also expects the input `bbox`, which are\n-  the bounding boxes (i.e. 2D-positions) of the input tokens. These can be obtained using an external OCR engine such\n-  as Google's [Tesseract](https://github.com/tesseract-ocr/tesseract) (there's a [Python wrapper](https://pypi.org/project/pytesseract/) available). Each bounding box should be in (x0, y0, x1, y1) format, where\n-  (x0, y0) corresponds to the position of the upper left corner in the bounding box, and (x1, y1) represents the\n-  position of the lower right corner. Note that one first needs to normalize the bounding boxes to be on a 0-1000\n-  scale. To normalize, you can use the following function:\n+## Notes\n+\n+- The original LayoutLM was not designed with a unified processing workflow. Instead, it expects preprocessed text (`words`) and bounding boxes (`boxes`) from an external OCR engine (like [Pytesseract](https://pypi.org/project/pytesseract/)) and provide them as additional inputs to the tokenizer. \n+\n+- The [`~LayoutLMModel.forward`] method expects the input `bbox` (bounding boxes of the input tokens). Each bounding box should be in the format `(x0, y0, x1, y1)`.  `(x0, y0)` corresponds to the upper left corner of the bounding box and `{x1, y1)` corresponds to the lower right corner. The bounding boxes need to be normalized on a 0-1000 scale as shown below.\n \n ```python\n def normalize_bbox(bbox, width, height):\n@@ -71,8 +95,7 @@ def normalize_bbox(bbox, width, height):\n     ]\n ```\n \n-Here, `width` and `height` correspond to the width and height of the original document in which the token\n-occurs. Those can be obtained using the Python Image Library (PIL) library for example, as follows:\n+- `width` and `height` correspond to the width and height of the original document in which the token occurs. These values can be obtained as shown below.\n \n ```python\n from PIL import Image\n@@ -87,35 +110,13 @@ width, height = image.size\n \n A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with LayoutLM. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n+- Read [fine-tuning LayoutLM for document-understanding using Keras & Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras) to learn more.  \n+- Read [fine-tune LayoutLM for document-understanding using only Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm) for more information.  \n+- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb) for a practical example of how to fine-tune LayoutLM.  \n+- Refer to this [notebook](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) for an example of how to fine-tune LayoutLM for sequence classification.  \n+- Refer to this [notebook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) for an example of how to fine-tune LayoutLM for token classification.  \n+- Read [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm) to learn how to deploy LayoutLM.  \n \n-<PipelineTag pipeline=\"document-question-answering\" />\n-\n-- A blog post on [fine-tuning\n-  LayoutLM for document-understanding using Keras & Hugging Face\n-  Transformers](https://www.philschmid.de/fine-tuning-layoutlm-keras).\n-\n-- A blog post on how to [fine-tune LayoutLM for document-understanding using only Hugging Face Transformers](https://www.philschmid.de/fine-tuning-layoutlm).\n-\n-- A notebook on how to [fine-tune LayoutLM on the FUNSD dataset with image embeddings](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb).\n-\n-- See also: [Document question answering task guide](../tasks/document_question_answering)\n-\n-<PipelineTag pipeline=\"text-classification\" />\n-\n-- A notebook on how to [fine-tune LayoutLM for sequence classification on the RVL-CDIP dataset](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb).\n-- [Text classification task guide](../tasks/sequence_classification)\n-\n-<PipelineTag pipeline=\"token-classification\" />\n-\n-- A notebook on how to [ fine-tune LayoutLM for token classification on the FUNSD dataset](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb).\n-- [Token classification task guide](../tasks/token_classification)\n-\n-**Other resources**\n-- [Masked language modeling task guide](../tasks/masked_language_modeling)\n-\n-ðŸš€ Deploy\n-\n-- A blog post on how to [Deploy LayoutLM with Hugging Face Inference Endpoints](https://www.philschmid.de/inference-endpoints-layoutlm).\n \n ## LayoutLMConfig\n \n@@ -124,10 +125,12 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## LayoutLMTokenizer\n \n [[autodoc]] LayoutLMTokenizer\n+    - __call__\n \n ## LayoutLMTokenizerFast\n \n [[autodoc]] LayoutLMTokenizerFast\n+    - __call__\n \n <frameworkcontent>\n <pt>\n@@ -177,5 +180,3 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n </tf>\n </frameworkcontent>\n-\n-"
        }
    ],
    "stats": {
        "total": 151,
        "additions": 76,
        "deletions": 75
    }
}