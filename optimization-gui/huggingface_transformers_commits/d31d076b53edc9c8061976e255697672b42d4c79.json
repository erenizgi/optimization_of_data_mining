{
    "author": "4N3MONE",
    "message": "ğŸŒ [i18n-KO] Translated output.md to Korean (#33607)\n\n* nmt draft\r\n\r\n* fix toctree\r\n\r\n* minor fix\r\n\r\n* Apply suggestions from code review\r\n\r\n* Apply suggestions from code review\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: boyunJang <gobook1234@naver.com>\r\nCo-authored-by: wony617 <49024958+Jwaminju@users.noreply.github.com>\r\n\r\n* Apply suggestions from code review\r\n\r\n* Apply suggestions from code review\r\n\r\n* Update docs/source/ko/main_classes/output.md\r\n\r\n* Update docs/source/ko/_toctree.yml\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n---------\r\n\r\nCo-authored-by: boyunJang <gobook1234@naver.com>\r\nCo-authored-by: wony617 <49024958+Jwaminju@users.noreply.github.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "d31d076b53edc9c8061976e255697672b42d4c79",
    "files": [
        {
            "sha": "4a89c01c4a2bcc60818ead900739f1087b34c6cd",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d31d076b53edc9c8061976e255697672b42d4c79/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d31d076b53edc9c8061976e255697672b42d4c79/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=d31d076b53edc9c8061976e255697672b42d4c79",
            "patch": "@@ -293,8 +293,8 @@\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Optimization\n     - local: in_translation\n-      title: (ë²ˆì—­ì¤‘) Model outputs\n-    - local: in_translation\n+      title: ëª¨ë¸ ì¶œë ¥\n+    - local: main_classes/output\n       title: (ë²ˆì—­ì¤‘) Pipelines\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘) Processors"
        },
        {
            "sha": "e65a2c2c35906eef6cc4011994d4bdd50d9d3d76",
            "filename": "docs/source/ko/main_classes/output.md",
            "status": "added",
            "additions": 314,
            "deletions": 0,
            "changes": 314,
            "blob_url": "https://github.com/huggingface/transformers/blob/d31d076b53edc9c8061976e255697672b42d4c79/docs%2Fsource%2Fko%2Fmain_classes%2Foutput.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d31d076b53edc9c8061976e255697672b42d4c79/docs%2Fsource%2Fko%2Fmain_classes%2Foutput.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Foutput.md?ref=d31d076b53edc9c8061976e255697672b42d4c79",
            "patch": "@@ -0,0 +1,314 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ëª¨ë¸ ì¶œë ¥[[model-outputs]]\n+\n+ëª¨ë“  ëª¨ë¸ì—ëŠ” [`~utils.ModelOutput`]ì˜ ì„œë¸Œí´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ì¸ ëª¨ë¸ ì¶œë ¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë“¤ì€\n+ëª¨ë¸ì—ì„œ ë°˜í™˜ë˜ëŠ” ëª¨ë“  ì •ë³´ë¥¼ í¬í•¨í•˜ëŠ” ë°ì´í„° êµ¬ì¡°ì´ì§€ë§Œ íŠœí”Œì´ë‚˜ ë”•ì…”ë„ˆë¦¬ë¡œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì˜ˆì œë¥¼ í†µí•´ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤:\n+\n+```python\n+from transformers import BertTokenizer, BertForSequenceClassification\n+import torch\n+\n+tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n+model = BertForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n+\n+inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n+labels = torch.tensor([1]).unsqueeze(0)  # ë°°ì¹˜ í¬ê¸° 1\n+outputs = model(**inputs, labels=labels)\n+```\n+\n+`outputs` ê°ì²´ëŠ” [`~modeling_outputs.SequenceClassifierOutput`]ì…ë‹ˆë‹¤.\n+ì•„ë˜ í•´ë‹¹ í´ë˜ìŠ¤ì˜ ë¬¸ì„œì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, `loss`(ì„ íƒì ), `logits`, `hidden_states`(ì„ íƒì ) ë° `attentions`(ì„ íƒì ) í•­ëª©ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” `labels`ë¥¼ ì „ë‹¬í–ˆê¸° ë•Œë¬¸ì— `loss`ê°€ ìˆì§€ë§Œ `hidden_states`ì™€ `attentions`ê°€ ì—†ëŠ”ë°, ì´ëŠ” `output_hidden_states=True` ë˜ëŠ” `output_attentions=True`ë¥¼ ì „ë‹¬í•˜ì§€ ì•Šì•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n+\n+<Tip>\n+\n+`output_hidden_states=True`ë¥¼ ì „ë‹¬í•  ë•Œ `outputs.hidden_states[-1]`ê°€ `outputs.last_hidden_state`ì™€ ì •í™•íˆ ì¼ì¹˜í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+í•˜ì§€ë§Œ í•­ìƒ ê·¸ëŸ° ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì¼ë¶€ ëª¨ë¸ì€ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœê°€ ë°˜í™˜ë  ë•Œ ì •ê·œí™”ë¥¼ ì ìš©í•˜ê±°ë‚˜ ë‹¤ë¥¸ í›„ì† í”„ë¡œì„¸ìŠ¤ë¥¼ ì ìš©í•©ë‹ˆë‹¤.\n+\n+</Tip>\n+\n+\n+ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©í•  ë•Œì™€ ë™ì¼í•˜ê²Œ ê° ì†ì„±ë“¤ì— ì ‘ê·¼í•  ìˆ˜ ìˆìœ¼ë©°, ëª¨ë¸ì´ í•´ë‹¹ ì†ì„±ì„ ë°˜í™˜í•˜ì§€ ì•Šì€ ê²½ìš° `None`ì´ ë°˜í™˜ë©ë‹ˆë‹¤. ì˜ˆì‹œì—ì„œëŠ” `outputs.loss`ëŠ” ëª¨ë¸ì—ì„œ ê³„ì‚°í•œ ì†ì‹¤ì´ê³  `outputs.attentions`ëŠ” `None`ì…ë‹ˆë‹¤.\n+\n+`outputs` ê°ì²´ë¥¼ íŠœí”Œë¡œ ê°„ì£¼í•  ë•ŒëŠ” `None` ê°’ì´ ì—†ëŠ” ì†ì„±ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤. \n+ì˜ˆì‹œì—ì„œëŠ” `loss`ì™€ `logits`ë¼ëŠ” ë‘ ê°œì˜ ìš”ì†Œê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ,\n+\n+```python\n+outputs[:2]\n+```\n+\n+ëŠ” `(outputs.loss, outputs.logits)` íŠœí”Œì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n+\n+`outputs` ê°ì²´ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ê°„ì£¼í•  ë•ŒëŠ” `None` ê°’ì´ ì—†ëŠ” ì†ì„±ë§Œ ê³ ë ¤í•©ë‹ˆë‹¤.\n+ì˜ˆì‹œì—ëŠ” `loss`ì™€ `logits`ë¼ëŠ” ë‘ ê°œì˜ í‚¤ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+ì—¬ê¸°ì„œë¶€í„°ëŠ” ë‘ ê°€ì§€ ì´ìƒì˜ ëª¨ë¸ ìœ í˜•ì—ì„œ ì‚¬ìš©ë˜ëŠ” ì¼ë°˜ ëª¨ë¸ ì¶œë ¥ì„ ë‹¤ë£¹ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì¶œë ¥ ìœ í˜•ì€ í•´ë‹¹ ëª¨ë¸ í˜ì´ì§€ì— ë¬¸ì„œí™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+\n+## ModelOutput[[transformers.utils.ModelOutput]]\n+\n+[[autodoc]] utils.ModelOutput\n+    - to_tuple\n+\n+## BaseModelOutput[[transformers.BaseModelOutput]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutput\n+\n+## BaseModelOutputWithPooling[[transformers.modeling_outputs.BaseModelOutputWithPooling]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutputWithPooling\n+\n+## BaseModelOutputWithCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithCrossAttentions]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutputWithCrossAttentions\n+\n+## BaseModelOutputWithPoolingAndCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutputWithPoolingAndCrossAttentions\n+\n+## BaseModelOutputWithPast[[transformers.modeling_outputs.BaseModelOutputWithPast]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutputWithPast\n+\n+## BaseModelOutputWithPastAndCrossAttentions[[transformers.modeling_outputs.BaseModelOutputWithPastAndCrossAttentions]]\n+\n+[[autodoc]] modeling_outputs.BaseModelOutputWithPastAndCrossAttentions\n+\n+## Seq2SeqModelOutput[[transformers.modeling_outputs.Seq2SeqModelOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqModelOutput\n+\n+## CausalLMOutput[[transformers.modeling_outputs.CausalLMOutput]]\n+\n+[[autodoc]] modeling_outputs.CausalLMOutput\n+\n+## CausalLMOutputWithCrossAttentions[[transformers.modeling_outputs.CausalLMOutputWithCrossAttentions]]\n+\n+[[autodoc]] modeling_outputs.CausalLMOutputWithCrossAttentions\n+\n+## CausalLMOutputWithPast[[transformers.modeling_outputs.CausalLMOutputWithPast]]\n+\n+[[autodoc]] modeling_outputs.CausalLMOutputWithPast\n+\n+## MaskedLMOutput[[transformers.modeling_outputs.MaskedLMOutput]]\n+\n+[[autodoc]] modeling_outputs.MaskedLMOutput\n+\n+## Seq2SeqLMOutput[[transformers.modeling_outputs.Seq2SeqLMOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqLMOutput\n+\n+## NextSentencePredictorOutput[[transformers.modeling_outputs.NextSentencePredictorOutput]]\n+\n+[[autodoc]] modeling_outputs.NextSentencePredictorOutput\n+\n+## SequenceClassifierOutput[[transformers.modeling_outputs.SequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_outputs.SequenceClassifierOutput\n+\n+## Seq2SeqSequenceClassifierOutput[[transformers.modeling_outputs.Seq2SeqSequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqSequenceClassifierOutput\n+\n+## MultipleChoiceModelOutput[[transformers.modeling_outputs.MultipleChoiceModelOutput]]\n+\n+[[autodoc]] modeling_outputs.MultipleChoiceModelOutput\n+\n+## TokenClassifierOutput[[transformers.modeling_outputs.TokenClassifierOutput]]\n+\n+[[autodoc]] modeling_outputs.TokenClassifierOutput\n+\n+## QuestionAnsweringModelOutput[[transformers.modeling_outputs.QuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_outputs.QuestionAnsweringModelOutput\n+\n+## Seq2SeqQuestionAnsweringModelOutput[[transformers.modeling_outputs.Seq2SeqQuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqQuestionAnsweringModelOutput\n+\n+## Seq2SeqSpectrogramOutput[[transformers.modeling_outputs.Seq2SeqSpectrogramOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqSpectrogramOutput\n+\n+## SemanticSegmenterOutput[[transformers.modeling_outputs.SemanticSegmenterOutput]]\n+\n+[[autodoc]] modeling_outputs.SemanticSegmenterOutput\n+\n+## ImageClassifierOutput[[transformers.modeling_outputs.ImageClassifierOutput]]\n+\n+[[autodoc]] modeling_outputs.ImageClassifierOutput\n+\n+## ImageClassifierOutputWithNoAttention[[transformers.modeling_outputs.ImageClassifierOutputWithNoAttention]]\n+\n+[[autodoc]] modeling_outputs.ImageClassifierOutputWithNoAttention\n+\n+## DepthEstimatorOutput[[transformers.modeling_outputs.DepthEstimatorOutput]]\n+\n+[[autodoc]] modeling_outputs.DepthEstimatorOutput\n+\n+## Wav2Vec2BaseModelOutput[[transformers.modeling_outputs.Wav2Vec2BaseModelOutput]]\n+\n+[[autodoc]] modeling_outputs.Wav2Vec2BaseModelOutput\n+\n+## XVectorOutput[[transformers.modeling_outputs.XVectorOutput]]\n+\n+[[autodoc]] modeling_outputs.XVectorOutput\n+\n+## Seq2SeqTSModelOutput[[transformers.modeling_outputs.Seq2SeqTSModelOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqTSModelOutput\n+\n+## Seq2SeqTSPredictionOutput[[transformers.modeling_outputs.Seq2SeqTSPredictionOutput]]\n+\n+[[autodoc]] modeling_outputs.Seq2SeqTSPredictionOutput\n+\n+## SampleTSPredictionOutput[[transformers.modeling_outputs.SampleTSPredictionOutput]]\n+\n+[[autodoc]] modeling_outputs.SampleTSPredictionOutput\n+\n+## TFBaseModelOutput[[transformers.modeling_outputs.TFBaseModelOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFBaseModelOutput\n+\n+## TFBaseModelOutputWithPooling[[transformers.modeling_tf_outputs.TFBaseModelOutputWithPooling]]\n+\n+[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPooling\n+\n+## TFBaseModelOutputWithPoolingAndCrossAttentions[[transformers.modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions]]\n+\n+[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions\n+\n+## TFBaseModelOutputWithPast[[transformers.modeling_tf_outputs.TFBaseModelOutputWithPast]]\n+\n+[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\n+\n+## TFBaseModelOutputWithPastAndCrossAttentions[[transformers.modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions]]\n+\n+[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions\n+\n+## TFSeq2SeqModelOutput[[transformers.modeling_tf_outputs.TFSeq2SeqModelOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFSeq2SeqModelOutput\n+\n+## TFCausalLMOutput[[transformers.modeling_tf_outputs.TFCausalLMOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFCausalLMOutput\n+\n+## TFCausalLMOutputWithCrossAttentions[[transformers.modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions]]\n+\n+[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions\n+\n+## TFCausalLMOutputWithPast[[transformers.modeling_tf_outputs.TFCausalLMOutputWithPast]]\n+\n+[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithPast\n+\n+## TFMaskedLMOutput[[transformers.modeling_tf_outputs.TFMaskedLMOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFMaskedLMOutput\n+\n+## TFSeq2SeqLMOutput[[transformers.modeling_tf_outputs.TFSeq2SeqLMOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFSeq2SeqLMOutput\n+\n+## TFNextSentencePredictorOutput[[transformers.modeling_tf_outputs.TFNextSentencePredictorOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput\n+\n+## TFSequenceClassifierOutput[[transformers.modeling_tf_outputs.TFSequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutput\n+\n+## TFSeq2SeqSequenceClassifierOutput[[transformers.modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput\n+\n+## TFMultipleChoiceModelOutput[[transformers.modeling_tf_outputs.TFMultipleChoiceModelOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFMultipleChoiceModelOutput\n+\n+## TFTokenClassifierOutput[[transformers.modeling_tf_outputs.TFTokenClassifierOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFTokenClassifierOutput\n+\n+## TFQuestionAnsweringModelOutput[[transformers.modeling_tf_outputs.TFQuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFQuestionAnsweringModelOutput\n+\n+## TFSeq2SeqQuestionAnsweringModelOutput[[transformers.modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput\n+\n+## FlaxBaseModelOutput[[transformers.modeling_flax_outputs.FlaxBaseModelOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutput\n+\n+## FlaxBaseModelOutputWithPast[[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPast]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPast\n+\n+## FlaxBaseModelOutputWithPooling[[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPooling]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPooling\n+\n+## FlaxBaseModelOutputWithPastAndCrossAttentions[[transformers.modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions\n+\n+## FlaxSeq2SeqModelOutput[[transformers.modeling_flax_outputs.FlaxSeq2SeqModelOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput\n+\n+## FlaxCausalLMOutputWithCrossAttentions[[transformers.modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions\n+\n+## FlaxMaskedLMOutput[[transformers.modeling_flax_outputs.FlaxMaskedLMOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxMaskedLMOutput\n+\n+## FlaxSeq2SeqLMOutput[[transformers.modeling_flax_outputs.FlaxSeq2SeqLMOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqLMOutput\n+\n+## FlaxNextSentencePredictorOutput[[transformers.modeling_flax_outputs.FlaxNextSentencePredictorOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxNextSentencePredictorOutput\n+\n+## FlaxSequenceClassifierOutput[[transformers.modeling_flax_outputs.FlaxSequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxSequenceClassifierOutput\n+\n+## FlaxSeq2SeqSequenceClassifierOutput[[transformers.modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput\n+\n+## FlaxMultipleChoiceModelOutput[[transformers.modeling_flax_outputs.FlaxMultipleChoiceModelOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxMultipleChoiceModelOutput\n+\n+## FlaxTokenClassifierOutput[[transformers.modeling_flax_outputs.FlaxTokenClassifierOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxTokenClassifierOutput\n+\n+## FlaxQuestionAnsweringModelOutput[[transformers.modeling_flax_outputs.FlaxQuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxQuestionAnsweringModelOutput\n+\n+## FlaxSeq2SeqQuestionAnsweringModelOutput[[transformers.modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput]]\n+\n+[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"
        }
    ],
    "stats": {
        "total": 318,
        "additions": 316,
        "deletions": 2
    }
}