{
    "author": "gante",
    "message": "[tests] remove overload for deleted test (`test_offloaded_cache_implementation`) (#37896)\n\n* remove overload for deleted tests\n\n* make fixup",
    "sha": "0f5a8243c4b3ed8ce5624977f06997ca517c68f8",
    "files": [
        {
            "sha": "589cff4c02b0c1f7adb49a2c5837976378eae33a",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f5a8243c4b3ed8ce5624977f06997ca517c68f8/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f5a8243c4b3ed8ce5624977f06997ca517c68f8/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=0f5a8243c4b3ed8ce5624977f06997ca517c68f8",
            "patch": "@@ -17,7 +17,6 @@\n \n import pytest\n import requests\n-from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -396,12 +395,6 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_model_parallelism(self):\n         pass\n \n-    @parameterized.expand([(\"offloaded\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"Offloaded cache seems to not work with mllama's kv cache type\")\n-    def test_offloaded_cache_implementation(self, cache_implementation):\n-        pass\n-\n     @unittest.skip(\n         reason=\"Mllama cache type doesn't allow correct check on output `past_key_values` due to `Cache.crop()`\"\n     )"
        },
        {
            "sha": "ffc4b59abb743cfe25ba8ea3d77ad5a247d818ed",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f5a8243c4b3ed8ce5624977f06997ca517c68f8/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f5a8243c4b3ed8ce5624977f06997ca517c68f8/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=0f5a8243c4b3ed8ce5624977f06997ca517c68f8",
            "patch": "@@ -546,12 +546,6 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_generate_with_head_masking(self):\n         pass\n \n-    @parameterized.expand([(\"offloaded\",)])\n-    @pytest.mark.generate\n-    @unittest.skip(reason=\"Whisper doesn't work with offloaded cache implementation yet\")\n-    def test_offloaded_cache_implementation(self, cache_implementation):\n-        pass\n-\n     @require_torch_fp16\n     def test_generate_fp16(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs()"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 0,
        "deletions": 13
    }
}