{
    "author": "SunMarc",
    "message": "Revert `local_rank` deletion and some cleaning (#41504)\n\n* forgot those\n\n* clean\n\n* Fix\n\n* merge\n\n* fix\n\n* fix",
    "sha": "f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
    "files": [
        {
            "sha": "054ebd63c30adf44d6eb6c02b4accb918631dd66",
            "filename": "examples/legacy/seq2seq/seq2seq_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fseq2seq_trainer.py?ref=f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
            "patch": "@@ -144,7 +144,7 @@ def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:\n \n             return (\n                 RandomSampler(self.train_dataset)\n-                if self.args.local_rank == -1\n+                if self.args.local_process_index == -1\n                 else DistributedSampler(self.train_dataset)\n             )\n "
        },
        {
            "sha": "412ba01f4e17abc284b738360bcfcd466a23e235",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
            "patch": "@@ -996,6 +996,12 @@ class TrainingArguments:\n             )\n         },\n     )\n+    local_rank: int = field(\n+        default=-1,\n+        metadata={\n+            \"help\": \"When using torch.distributed.launch (Deprecated), it will pass `local_rank` in the script, so we need this for the parser. To get the local rank, prefer using the property `local_process_index`\"\n+        },\n+    )\n     ddp_backend: Optional[str] = field(\n         default=None,\n         metadata={"
        },
        {
            "sha": "265d4a4462aafc88d8131cd7feb391869f894274",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
            "patch": "@@ -518,7 +518,6 @@ def test_hf_ds_config_mismatch(self):\n \n         with mockenv_context(**self.dist_env_1_gpu):\n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=fp16,\n                 deepspeed=ds_config,\n                 per_device_train_batch_size=per_device_train_batch_size,\n@@ -552,7 +551,7 @@ def test_hf_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -566,7 +565,7 @@ def test_ds_scheduler_hf_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -580,7 +579,7 @@ def test_hf_scheduler_ds_optimizer(self):\n             ds_config_zero2_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"none\"\n             ds_config_zero2_dict[\"fp16\"][\"initial_scale_power\"] = 1  # force optimizer on the first step\n             trainer = get_regression_trainer(\n-                a=a, local_rank=0, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                a=a, fp16=True, deepspeed=ds_config_zero2_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             trainer.train()\n         new_a = trainer.model.a.item()\n@@ -598,7 +597,7 @@ def test_stage3_nvme_offload(self):\n             ds_config_zero3_dict[\"zero_optimization\"][\"offload_param\"] = nvme_config\n             ds_config_zero3_dict[\"zero_optimization\"][\"stage3_gather_16bit_weights_on_model_save\"] = True\n             trainer = get_regression_trainer(\n-                local_rank=0, fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n+                fp16=True, deepspeed=ds_config_zero3_dict, output_dir=self.get_auto_remove_tmp_dir()\n             )\n             with CaptureLogger(deepspeed_logger) as cl:\n                 trainer.train()\n@@ -616,7 +615,6 @@ def model_init():\n                 return model\n \n             trainer = get_regression_trainer(\n-                local_rank=0,\n                 fp16=True,\n                 model_init=model_init,\n                 deepspeed=ds_config_zero3_dict,\n@@ -642,7 +640,7 @@ def test_hf_optimizer_with_offload(self, stage, dtype):\n         ds_config_dict[\"zero_optimization\"][\"offload_optimizer\"][\"device\"] = \"cpu\"\n         ds_config_dict[\"zero_force_ds_cpu_optimizer\"] = False  # offload is not efficient w/o CPUAdam\n         with mockenv_context(**self.dist_env_1_gpu):\n-            kwargs = {\"local_rank\": 0, \"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n+            kwargs = {\"deepspeed\": ds_config_dict, \"output_dir\": self.get_auto_remove_tmp_dir()}\n             kwargs[dtype] = True\n             trainer = get_regression_trainer(**kwargs)\n             with CaptureLogger(deepspeed_logger) as cl:\n@@ -659,7 +657,6 @@ def test_fake_notebook_no_launcher(self, stage, dtype):\n         # to reset `deepspeed_logger.handlers[0].setStream(sys.stdout)` or directly capture from the deepspeed_logger.\n         with mockenv_context(**self.dist_env_1_gpu):\n             kwargs = {\n-                \"local_rank\": 0,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"output_dir\": self.get_auto_remove_tmp_dir(),\n             }\n@@ -683,7 +680,6 @@ def test_early_get_last_lr(self, stage, dtype):\n             kwargs = {\n                 \"a\": a,\n                 \"b\": b,\n-                \"local_rank\": 0,\n                 \"train_len\": 8,\n                 \"deepspeed\": self.get_config_dict(stage),\n                 \"per_device_train_batch_size\": 8,\n@@ -729,7 +725,6 @@ def test_gradient_accumulation(self, stage, dtype):\n         kwargs = {\n             \"a\": a,\n             \"b\": b,\n-            \"local_rank\": 0,\n             \"train_len\": train_len,\n             \"deepspeed\": self.get_config_dict(stage),\n             \"output_dir\": self.get_auto_remove_tmp_dir(),"
        },
        {
            "sha": "b87ff42db7bfa42c83e16c5f4840332f326b0ca5",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9f8bf5a1062ce0293cbd42be0126e17a15446e9/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=f9f8bf5a1062ce0293cbd42be0126e17a15446e9",
            "patch": "@@ -1437,9 +1437,7 @@ def test_training_arguments_are_left_untouched(self):\n         args = TrainingArguments(tmp_dir, report_to=[])\n         dict1, dict2 = args.to_dict(), trainer.args.to_dict()\n         for key in dict1:\n-            # Logging dir can be slightly different as they default to something with the time.\n-            if key != \"logging_dir\":\n-                self.assertEqual(dict1[key], dict2[key])\n+            self.assertEqual(dict1[key], dict2[key])\n \n     def test_number_of_steps_in_training(self):\n         # Regular training has n_epochs * len(train_dl) steps\n@@ -5433,7 +5431,6 @@ def hp_name(trial):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -5482,7 +5479,6 @@ def compute_objective(metrics: dict[str, float]) -> list[float]:\n                 num_train_epochs=10,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n                 compute_metrics=AlmostAccuracy(),\n@@ -5572,7 +5568,6 @@ def hp_name(params):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )\n@@ -6170,7 +6165,6 @@ def model_init(config):\n                 num_train_epochs=4,\n                 disable_tqdm=True,\n                 load_best_model_at_end=True,\n-                logging_dir=\"runs\",\n                 run_name=\"test\",\n                 model_init=model_init,\n             )"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 13,
        "deletions": 18
    }
}