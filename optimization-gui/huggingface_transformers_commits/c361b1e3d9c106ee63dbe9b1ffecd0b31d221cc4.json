{
    "author": "faaany",
    "message": "[docs] fix model checkpoint name (#36075)\n\nupdate model name",
    "sha": "c361b1e3d9c106ee63dbe9b1ffecd0b31d221cc4",
    "files": [
        {
            "sha": "158db928812e415cbd95fa6d74240f9b05317b58",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c361b1e3d9c106ee63dbe9b1ffecd0b31d221cc4/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c361b1e3d9c106ee63dbe9b1ffecd0b31d221cc4/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=c361b1e3d9c106ee63dbe9b1ffecd0b31d221cc4",
            "patch": "@@ -130,7 +130,7 @@ Alternative to CLI, you can export a ðŸ¤— Transformers model to ONNX programmati\n >>> from optimum.onnxruntime import ORTModelForSequenceClassification\n >>> from transformers import AutoTokenizer\n \n->>> model_checkpoint = \"distilbert_base_uncased_squad\"\n+>>> model_checkpoint = \"distilbert/distilbert-base-uncased-distilled-squad\"\n >>> save_directory = \"onnx/\"\n \n >>> # Load a model from transformers and export it to ONNX"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}