{
    "author": "yaswanth19",
    "message": "Remove type annotation in Siglip Attention Module (#38503)\n\n* Remove type annotation\n\n* remove print statement",
    "sha": "1094dd34f73dae1d9a91a6632635934516612490",
    "files": [
        {
            "sha": "d6966c31f66967de9429c836e504c4df10ae5f2a",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1094dd34f73dae1d9a91a6632635934516612490/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=1094dd34f73dae1d9a91a6632635934516612490",
            "patch": "@@ -370,7 +370,7 @@ def eager_attention_forward(\n class SiglipAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Union[SiglipVisionConfig, SiglipTextConfig]):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size"
        },
        {
            "sha": "a198cbc347fe5c21ab5183c5e981b4c22311eece",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1094dd34f73dae1d9a91a6632635934516612490/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=1094dd34f73dae1d9a91a6632635934516612490",
            "patch": "@@ -264,7 +264,7 @@ def eager_attention_forward(\n class Siglip2Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n-    def __init__(self, config: Union[Siglip2VisionConfig, Siglip2TextConfig]):\n+    def __init__(self, config):\n         super().__init__()\n         self.config = config\n         self.embed_dim = config.hidden_size"
        },
        {
            "sha": "9f56c79956e69485f90e71a596ee7cbe822d0c8c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1094dd34f73dae1d9a91a6632635934516612490/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1094dd34f73dae1d9a91a6632635934516612490/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1094dd34f73dae1d9a91a6632635934516612490",
            "patch": "@@ -739,7 +739,6 @@ def check_determinism(first, second):\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n-            print(model_class)\n             with torch.no_grad():\n                 first = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n                 second = model(**self._prepare_for_class(inputs_dict, model_class))[0]"
        }
    ],
    "stats": {
        "total": 5,
        "additions": 2,
        "deletions": 3
    }
}