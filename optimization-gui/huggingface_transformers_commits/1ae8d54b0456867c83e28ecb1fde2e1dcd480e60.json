{
    "author": "zucchini-nlp",
    "message": "[chat-template] Unify tests and clean up ðŸ§¼  (#37275)\n\n* fix tests and some clean up\n\n* make one general test for each modality\n\n* remove redundant merging of kwargs\n\n* edge cases\n\n* dont enforce slow when reloading\n\n* fix gemma3 tests\n\n* has to adapt llama 4 after rebase\n\n* remove also from overriden tests\n\n* should be green now",
    "sha": "1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
    "files": [
        {
            "sha": "3a01f652aaa64d8b1123101ecef2593ccd6a7a79",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -181,35 +181,6 @@ processed_chat = processor.apply_chat_template(\n print(processed_chat.keys())\n ```\n \n-</hfoption>\n-<hfoption id=\"custom frame sampling\">\n-\n-Some models don't sample frames *uniformly* and require more complex logic to determine which frames to use. For example, the model may have an *adaptive frame selection* or if the model prioritizes *key moments* in a video rather than evenly spaced frames.\n-\n-If a model has a different sampling strategy, you can write a function that customizes frame selection. The function should include the following requirements.\n-\n-- Use the `sample_indices_fn` parameter to pass a callable function for sampling.\n-- If provided, this function *overrides* the standard `num_frames` and `fps` parameters.\n-- The function receives all the parameters passed to `load_video` and must return valid frame indices to sample from.\n-\n-An example function is shown below. This gives you full control over frame selection, making the model more adaptable to different video scenarios.\n-\n-```py\n-def sample_indices_fn(metadata, **kwargs):\n-    # samples only the first and the second frame\n-    return [0, 1]\n-\n-processed_chat = processor.apply_chat_template(\n-    messages,\n-    add_generation_prompt=True,\n-    tokenize=True,\n-    return_dict=True,\n-    sample_indices_fn=sample_indices_fn,\n-    video_load_backend=\"decord\",\n-)\n-print(processed_chat.keys())\n-```\n-\n </hfoption>\n <hfoption id=\"list of image frames\">\n "
        },
        {
            "sha": "4cf207c45917ea197745dce83a32fa8ed7bd3ec9",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 34,
            "deletions": 19,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -20,10 +20,13 @@\n from datetime import timedelta\n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n \n+import numpy as np\n+\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import (\n     ImageInput,\n     VideoInput,\n+    load_video,\n     make_batched_videos,\n     make_nested_list_of_images,\n )\n@@ -425,32 +428,44 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(image_processor_input_names + tokenizer_input_names))\n \n-    # Add model-specific video sampling method when applying the template\n-    def apply_chat_template(\n+    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n+    def _load_video_for_model(\n         self,\n-        conversation,\n-        max_frames=None,\n-        target_fps=None,\n-        skip_secs=1,\n-        video_load_backend=\"pyav\",\n-        sample_indices_fn=None,\n-        **kwargs,\n-    ):\n-        max_frames = self.default_max_frames if max_frames is None else max_frames\n-        target_fps = self.default_fps if target_fps is None else target_fps\n+        video: Union[str, \"VideoInput\"],\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+        backend: str = \"opencv\",\n+        skip_secs: int = 0.0,\n+    ) -> np.array:\n+        \"\"\"\n+        Loads `video` to a numpy array.\n+\n+        Args:\n+            video (`str` or `VideoInput`):\n+                The video to convert to the numpy array format. Can be a link to video or local path.\n+            num_frames (`int`, *optional*):\n+                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+            fps (`int`, *optional*):\n+                Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+                If not specified and `num_frames==None`, all frames are sampled.\n+            backend (`str`, *optional*, defaults to `\"opencv\"`):\n+                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n+\n+        Returns:\n+            Tuple[`np.array`, Dict]: A tuple containing:\n+                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+                - Metadata dictionary.\n+        \"\"\"\n+        max_frames = self.default_max_frames if num_frames is None else num_frames\n+        target_fps = self.default_fps if fps is None else fps\n \n         def sample_indices_fn_func(metadata, **fn_kwargs):\n             return smolvlm_sample_indices_fn(\n                 metadata, max_frames=max_frames, target_fps=target_fps, skip_secs=skip_secs, **fn_kwargs\n             )\n \n-        # word of caution- we are blindly overriding a callable kwarg here.\n-        # typed kwargs would be a way to avoid that @molbap\n-        if not sample_indices_fn:\n-            sample_indices_fn = sample_indices_fn_func\n-        return super().apply_chat_template(\n-            conversation, video_load_backend=video_load_backend, sample_indices_fn=sample_indices_fn, **kwargs\n-        )\n+        video, metadata = load_video(video, backend=backend, sample_indices_fn=sample_indices_fn_func)\n+        return video, metadata\n \n \n __all__ = [\"SmolVLMProcessor\"]"
        },
        {
            "sha": "d63eab7938890b50fbab5577b6354f346c7c9b42",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 68,
            "deletions": 26,
            "changes": 94,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -23,7 +23,7 @@\n import typing\n import warnings\n from pathlib import Path\n-from typing import Any, Callable, Dict, List, Optional, TypedDict, Union\n+from typing import Any, Dict, List, Optional, TypedDict, Union\n \n import numpy as np\n import typing_extensions\n@@ -415,7 +415,6 @@ def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n     video_load_backend: Optional[str] = \"pyav\"\n     video_fps: Optional[int] = None\n     sampling_rate: Optional[int] = 16_000\n-    sample_indices_fn: Optional[Callable] = None\n     load_audio_from_video: Optional[bool] = False\n \n \n@@ -435,7 +434,16 @@ class ProcessorChatTemplateKwargs(ChatTemplateLoadKwargs, TokenizerChatTemplateK\n \n class AllKwargsForChatTemplate(\n     TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs, ProcessorChatTemplateKwargs\n-): ...\n+):\n+    processor_kwargs: ProcessingKwargs = {\n+        **ProcessingKwargs.__annotations__,\n+    }\n+    mm_load_kwargs: ChatTemplateLoadKwargs = {\n+        **TextKwargs.__annotations__,\n+    }\n+    template_kwargs: ProcessorChatTemplateKwargs = {\n+        **ProcessorChatTemplateKwargs.__annotations__,\n+    }\n \n \n class ProcessorMixin(PushToHubMixin):\n@@ -1315,19 +1323,20 @@ def apply_chat_template(\n                     \"https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\"\n                 )\n \n-        # Fill two sets of kwargs that should be used by tokenizer's `apply_chat_template`\n-        # and for multimodal data loading. Everything else will be used in `__call__`\n-        tokenizer_template_kwargs = {}\n-        for tokenizer_key in TokenizerChatTemplateKwargs.__annotations__.keys():\n-            default_value = getattr(TokenizerChatTemplateKwargs, tokenizer_key, None)\n-            value = kwargs.pop(tokenizer_key, default_value)\n-            tokenizer_template_kwargs[tokenizer_key] = value\n+        # Fill sets of kwargs that should be used by different parts of template\n+        processed_kwargs = {\n+            \"processor_kwargs\": {},\n+            \"mm_load_kwargs\": {},\n+            \"template_kwargs\": {},\n+        }\n \n-        mm_load_kwargs = {}\n-        for mm_load_key in ChatTemplateLoadKwargs.__annotations__.keys():\n-            default_value = getattr(ChatTemplateLoadKwargs, mm_load_key, None)\n-            value = kwargs.pop(mm_load_key, default_value)\n-            mm_load_kwargs[mm_load_key] = value\n+        for kwarg_type in processed_kwargs:\n+            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__.keys():\n+                kwarg_type_defaults = AllKwargsForChatTemplate.__annotations__[kwarg_type]\n+                default_value = getattr(kwarg_type_defaults, key, None)\n+                value = kwargs.pop(key, default_value)\n+                if value is not None and not isinstance(value, dict):\n+                    processed_kwargs[kwarg_type][key] = value\n \n         if isinstance(conversation, (list, tuple)) and (\n             isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n@@ -1338,8 +1347,9 @@ def apply_chat_template(\n             is_batched = False\n             conversations = [conversation]\n \n-        tokenize = kwargs.pop(\"tokenize\", False)\n-        return_dict = kwargs.pop(\"return_dict\", False)\n+        tokenize = processed_kwargs[\"template_kwargs\"].pop(\"tokenize\", False)\n+        return_dict = processed_kwargs[\"template_kwargs\"].pop(\"return_dict\", False)\n+        mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n \n         if tokenize:\n             batch_images, batch_videos = [], []\n@@ -1382,7 +1392,7 @@ def apply_chat_template(\n \n                     for fname in video_fnames:\n                         if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n-                            video = [np.array(load_image(image_fname)).T for image_fname in fname]\n+                            video = [np.array(load_image(image_fname)) for image_fname in fname]\n                             # create a 4D video because `load_video` always returns a 4D array\n                             video = np.stack(video)\n                             metadata = None\n@@ -1391,12 +1401,13 @@ def apply_chat_template(\n                                 \"If your model uses this metadata during processing, please load the whole video and let the model sample frames instead.\"\n                             )\n                         else:\n-                            video, metadata = load_video(\n+                            # TODO: raushan, should be `self.video_processor.load_video_for_model` when API is added\n+                            video, metadata = self._load_video_for_model(\n                                 fname,\n-                                num_frames=mm_load_kwargs[\"num_frames\"],\n-                                fps=mm_load_kwargs[\"video_fps\"],\n+                                num_frames=mm_load_kwargs.get(\"num_frames\", None),\n+                                fps=mm_load_kwargs.get(\"video_fps\", None),\n                                 backend=mm_load_kwargs[\"video_load_backend\"],\n-                                sample_indices_fn=mm_load_kwargs[\"sample_indices_fn\"],\n+                                **kwargs,\n                             )\n                         videos.append(video)\n                         video_metadata.append(metadata)\n@@ -1415,15 +1426,15 @@ def apply_chat_template(\n                 batch_images=batch_images,\n                 batch_videos=batch_videos,\n                 batch_video_metadata=batch_video_metadata,\n-                **mm_load_kwargs,\n+                **processed_kwargs[\"mm_load_kwargs\"],\n             )\n \n         prompt = self.tokenizer.apply_chat_template(\n             conversations,\n             chat_template=chat_template,\n             tokenize=False,\n             return_dict=False,\n-            **tokenizer_template_kwargs,\n+            **processed_kwargs[\"template_kwargs\"],\n         )\n \n         if not is_batched:\n@@ -1438,21 +1449,52 @@ def apply_chat_template(\n             # without actionable solution for users\n             single_prompt = prompt[0] if is_batched else prompt\n             if self.tokenizer.bos_token is not None and single_prompt.startswith(self.tokenizer.bos_token):\n-                kwargs[\"add_special_tokens\"] = False\n+                processed_kwargs[\"processor_kwargs\"][\"add_special_tokens\"] = False\n \n             out = self(\n                 text=prompt,\n                 images=batch_images if batch_images else None,\n                 videos=batch_videos if batch_videos else None,\n                 audio=batch_audios if batch_audios else None,\n-                **kwargs,\n+                **processed_kwargs[\"processor_kwargs\"],\n             )\n             if return_dict:\n                 return out\n             else:\n                 return out[\"input_ids\"]\n         return prompt\n \n+    # TODO: raushan, has to be public method under `VideoProcessorBase` when API is added\n+    # Keep private so we can simply remove when needed\n+    def _load_video_for_model(\n+        self,\n+        video: Union[str, \"VideoInput\"],\n+        num_frames: Optional[int] = None,\n+        fps: Optional[int] = None,\n+        backend: str = \"opencv\",\n+    ) -> np.array:\n+        \"\"\"\n+        Loads `video` to a numpy array.\n+\n+        Args:\n+            video (`str` or `VideoInput`):\n+                The video to convert to the numpy array format. Can be a link to video or local path.\n+            num_frames (`int`, *optional*):\n+                Number of frames to sample uniformly. If not passed, the whole video is loaded.\n+            fps (`int`, *optional*):\n+                Number of frames to sample per second. Should be passed only when `num_frames=None`.\n+                If not specified and `num_frames==None`, all frames are sampled.\n+            backend (`str`, *optional*, defaults to `\"opencv\"`):\n+                The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n+\n+        Returns:\n+            Tuple[`np.array`, Dict]: A tuple containing:\n+                - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+                - Metadata dictionary.\n+        \"\"\"\n+        video, metadata = load_video(video, num_frames, fps=fps, backend=backend)\n+        return video, metadata\n+\n     def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens=True, **kwargs):\n         \"\"\"\n         Post-process the output of a vlm to decode the text."
        },
        {
            "sha": "ac222e1505862f81c576af84cfe4cd552a60ab0f",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -236,55 +236,6 @@ def test_apply_chat_template(self):\n \"\"\"\n         self.assertEqual(rendered, expected_rendered)\n \n-    # Override as AriaImageProcessor doesn't accept `do_rescale`\n-    def test_image_chat_template_accepts_processing_kwargs(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            padding=\"max_length\",\n-            max_length=50,\n-        )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            truncation=True,\n-            max_length=5,\n-        )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n-\n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            max_image_size=980,\n-            return_tensors=\"np\",\n-        )\n-        self.assertListEqual(list(out_dict[self.images_input_name].shape), [1, 3, 980, 980])\n-\n     # Override as AriaProcessor needs image tokens in prompts\n     def prepare_text_inputs(self, batch_size: Optional[int] = None):\n         if batch_size is None:"
        },
        {
            "sha": "527f83c0bbf3ebc719272b600688692563f23dc4",
            "filename": "tests/models/aya_vision/test_processor_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -79,11 +79,6 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    # todo: yoni, fix this test\n-    @unittest.skip(\"Chat template has long system prompt\")\n-    def test_chat_template_accepts_processing_kwargs(self, **kwargs):\n-        pass\n-\n     # Override as AyaVisionProcessor needs image tokens in prompts\n     def prepare_text_inputs(self, batch_size: Optional[int] = None):\n         if batch_size is None:"
        },
        {
            "sha": "637afebeb265ab9aef27c01d0b07bc30cb730a3d",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 64,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -86,67 +86,3 @@ def test_can_load_various_tokenizers(self):\n             processor = LlavaProcessor.from_pretrained(checkpoint)\n             tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n             self.assertEqual(processor.tokenizer.__class__, tokenizer.__class__)\n-\n-    def test_chat_template(self):\n-        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n-        expected_prompt = \"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    def test_chat_template_dict(self):\n-        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n-        expected_output = [[1, 3148, 1001, 29901, 29871, 32000, 29871, 13, 5618, 338, 4318, 297, 445, 1967, 29973, 319, 1799, 9047, 13566, 29901]]  # fmt: skip\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # add image URL for return dict\n-        messages[0][\"content\"][0] = {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        out_dict_with_image = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n-        )\n-        self.assertListEqual(list(out_dict_with_image.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n-\n-    def test_chat_template_with_continue_final_message(self):\n-        processor = LlavaProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n-        expected_prompt = \"USER: <image>\\nDescribe this image. ASSISTANT: There is a dog and\"\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"Describe this image.\"},\n-                ],\n-            },\n-            {\n-                \"role\": \"assistant\",\n-                \"content\": [\n-                    {\"type\": \"text\", \"text\": \"There is a dog and\"},\n-                ],\n-            },\n-        ]\n-        prompt = processor.apply_chat_template(messages, continue_final_message=True)\n-        self.assertEqual(expected_prompt, prompt)"
        },
        {
            "sha": "e8860a43357808828217c5607b9536aa5b4def5d",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -78,23 +78,6 @@ def test_chat_template_is_saved(self):\n         processor_dict = self.prepare_processor_dict()\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n \n-    def test_chat_template(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n-        expected_prompt = \"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n     def test_image_token_filling(self):\n         processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-vicuna-7b-hf\")\n         processor.patch_size = 14"
        },
        {
            "sha": "f74bbab01a2d2b9a18a5e77365f7bb62bc8c8bab",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 2,
            "deletions": 78,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -18,7 +18,7 @@\n import unittest\n \n from transformers import AutoProcessor, LlamaTokenizerFast, LlavaNextVideoProcessor\n-from transformers.testing_utils import require_av, require_torch, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -28,7 +28,7 @@\n     from transformers import LlavaNextImageProcessor, LlavaNextVideoImageProcessor\n \n if is_torch_available:\n-    import torch\n+    pass\n \n \n @require_vision\n@@ -90,79 +90,3 @@ def test_chat_template_is_saved(self):\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n-\n-    def test_chat_template(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n-        expected_prompt = \"USER: <image>\\nWhat is shown in this image? ASSISTANT:\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    @require_av\n-    def test_chat_template_dict(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"video\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n-        )\n-        expected_output = [[1, 3148, 1001, 29901, 29871, 32000, 13, 5618, 338, 4318, 297, 445, 4863, 29973, 319, 1799, 9047, 13566, 29901]]  # fmt: skip\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # add image URL for return dict\n-        messages[0][\"content\"][0] = {\n-            \"type\": \"video\",\n-            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-        }\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n-        )\n-        self.assertListEqual(list(out_dict_with_video.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n-\n-    @require_torch\n-    @require_av\n-    def test_chat_template_dict_torch(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/LLaVA-NeXT-Video-7B-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"video\",\n-                        \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                ],\n-            },\n-        ]\n-\n-        out_dict_tensors = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-        self.assertListEqual(list(out_dict_tensors.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])\n-        self.assertTrue(isinstance(out_dict_tensors[\"input_ids\"], torch.Tensor))"
        },
        {
            "sha": "9764cd4dcd61bc7edfcc64f9f172bed968b2cee5",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 48,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -16,7 +16,7 @@\n import tempfile\n import unittest\n \n-from transformers.testing_utils import require_av, require_vision\n+from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -93,50 +93,3 @@ def test_chat_template_is_saved(self):\n         # so we check if the same template is loaded\n         processor_dict = self.prepare_processor_dict()\n         self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n-\n-    def test_chat_template(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n-        expected_prompt = \"<|im_start|>user <image>\\nWhat is shown in this image?<|im_end|><|im_start|>assistant\\n\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    @require_av\n-    def test_chat_template_dict(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"video\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n-        expected_output = [[151644, 872, 220, 151647, 198, 3838, 374, 6839, 304, 419, 2766, 30, 151645, 151644, 77091, 198]]  # fmt: skip\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # add image URL for return dict\n-        messages[0][\"content\"][0] = {\n-            \"type\": \"video\",\n-            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-        }\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_dict=True\n-        )\n-        self.assertListEqual(list(out_dict_with_video.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values_videos\"])"
        },
        {
            "sha": "8677eea426f0410aabd3286cd64f3dbd0c06ba9e",
            "filename": "tests/models/mistral3/test_processor_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 71,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processor_mistral3.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -62,77 +62,6 @@ def get_processor(self):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def test_chat_template_accepts_processing_kwargs(self):\n-        # override to use slow image processor to return numpy arrays\n-        processor = self.processor_class.from_pretrained(self.tmpdirname, use_fast=False)\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            padding=\"max_length\",\n-            truncation=True,\n-            max_length=50,\n-        )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            truncation=True,\n-            max_length=5,\n-        )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n-\n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            do_rescale=True,\n-            rescale_factor=-1,\n-            return_tensors=\"np\",\n-        )\n-        self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n-\n-    def test_chat_template(self):\n-        processor = self.processor_class.from_pretrained(self.tmpdirname, use_fast=False)\n-        expected_prompt = \"<s>[SYSTEM_PROMPT][/SYSTEM_PROMPT][INST][IMG]What is shown in this image?[/INST]\"\n-\n-        messages = [\n-            {\n-                \"role\": \"system\",\n-                \"content\": \"\",\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n     def test_image_token_filling(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         # Important to check with non square image"
        },
        {
            "sha": "3f66b9830984a720460b9e92fa0a9ce24fd014e7",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -51,22 +51,6 @@ def setUp(self):\n     def tearDown(self):\n         shutil.rmtree(self.tmpdirname)\n \n-    def test_chat_template(self):\n-        processor = self.processor_class.from_pretrained(self.tmpdirname)\n-        expected_prompt = \"<s>[INST][IMG]What is shown in this image?[/INST]\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)\n-\n     def test_image_token_filling(self):\n         processor = self.processor_class.from_pretrained(self.tmpdirname)\n         # Important to check with non square image"
        },
        {
            "sha": "352456da6e08df72a24d6e2ea4effebef90be056",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 76,
            "deletions": 120,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -17,19 +17,23 @@\n import tempfile\n import unittest\n \n+import numpy as np\n import pytest\n from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n     from transformers import Qwen2_5_VLProcessor, Qwen2VLImageProcessor\n \n+if is_torch_available():\n+    import torch\n+\n \n @require_vision\n @require_torch\n@@ -119,101 +123,97 @@ def test_model_input_names(self):\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n \n-    def test_image_chat_template_single(self):\n+    @require_torch\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 1)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n-        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(self.images_input_name in out_dict)\n-\n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n-    def test_image_chat_template_batched(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        batched_messages = [\n+        batch_messages = [\n             [\n                 {\n                     \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n                 },\n-            ],\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n-                    ],\n-                },\n-            ],\n-        ]\n+            ]\n+        ] * batch_size\n \n-        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 2)\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n \n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n         formatted_prompt_tokenized = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n         )\n-        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None, padding=True).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n         )\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n \n-        # Now test the ability to return dict\n-        batched_messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n         )\n-        batched_messages[1][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n         )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n         out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n         )\n-        self.assertTrue(self.images_input_name in out_dict)\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+        self.assertEqual(len(out_dict[input_name]), batch_size * 19200)\n \n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 90480)\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n \n     @require_av\n-    def test_chat_template_video(self):\n+    def test_apply_chat_template_video_frame_sampling(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n@@ -331,52 +331,7 @@ def test_kwargs_overrides_custom_image_processor_kwargs(self):\n         self.assertEqual(inputs[self.images_input_name].shape[0], 800)\n \n     @require_av\n-    def test_chat_template_video_custom_sampling(self):\n-        \"\"\"\n-        Tests that models can pass their custom callables to sample video indices.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n-            # sample only the first two frame always\n-            return [0, 1]\n-\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            sample_indices_fn=dummy_sample_indices_fn,\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 14400)\n-\n-    @require_av\n-    def test_chat_template_video_special_processing(self):\n+    def test_apply_chat_template_video_special_processing(self):\n         \"\"\"\n         Tests that models can use their own preprocessing to preprocess conversations.\n         \"\"\"\n@@ -433,6 +388,7 @@ def _process_messages_for_chat_template(\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n "
        },
        {
            "sha": "8edf5cea7a7bcd1c9bc4af2bd367f25cbc8f4bbc",
            "filename": "tests/models/qwen2_audio/test_processor_qwen2_audio.py",
            "status": "modified",
            "additions": 1,
            "deletions": 27,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processor_qwen2_audio.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -54,7 +54,7 @@ def tearDownClass(cls):\n     @staticmethod\n     def prepare_processor_dict():\n         return {\n-            \"chat_template\": \"{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or message['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n+            \"chat_template\": \"{% set audio_count = namespace(value=0) %}{% for message in messages %}{% if loop.first and message['role'] != 'system' %}<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n{% endif %}<|im_start|>{{ message['role'] }}\\n{% if message['content'] is string %}{{ message['content'] }}<|im_end|>\\n{% else %}{% for content in message['content'] %}{% if 'audio' in content or 'audio_url' in content or content['type'] == 'audio' %}{% set audio_count.value = audio_count.value + 1 %}Audio {{ audio_count.value }}: <|audio_bos|><|AUDIO|><|audio_eos|>\\n{% elif 'text' in content %}{{ content['text'] }}{% endif %}{% endfor %}<|im_end|>\\n{% endif %}{% endfor %}{% if add_generation_prompt %}<|im_start|>assistant\\n{% endif %}\",\n         }\n \n     # Override as Qwen2AudioProcessor needs audio tokens in prompts\n@@ -159,29 +159,3 @@ def test_chat_template(self):\n \n         formatted_prompt = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n         self.assertEqual(expected_prompt, formatted_prompt)\n-\n-    def test_chat_template_with_continue_final_message(self):\n-        processor = AutoProcessor.from_pretrained(self.checkpoint)\n-        expected_prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|><|AUDIO|><|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of \"  # fmt: skip\n-        messages = [\n-            {\n-                \"role\": \"system\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"audio\",\n-                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n-                ],\n-            },\n-            {\n-                \"role\": \"assistant\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of \"}],\n-            },\n-        ]\n-        prompt = processor.apply_chat_template(messages, continue_final_message=True)\n-        self.assertEqual(expected_prompt, prompt)"
        },
        {
            "sha": "720ba2f09f749473eedc11f162ca023a47b64941",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 76,
            "deletions": 120,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -17,19 +17,23 @@\n import tempfile\n import unittest\n \n+import numpy as np\n import pytest\n from huggingface_hub import hf_hub_download\n \n from transformers import AutoProcessor, Qwen2Tokenizer\n from transformers.testing_utils import require_av, require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n if is_vision_available():\n     from transformers import Qwen2VLImageProcessor, Qwen2VLProcessor\n \n+if is_torch_available():\n+    import torch\n+\n \n @require_vision\n @require_torch\n@@ -116,101 +120,97 @@ def test_model_input_names(self):\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n \n-    def test_image_chat_template_single(self):\n+    @require_torch\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 1)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n-        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(self.images_input_name in out_dict)\n-\n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 71280)\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n-    def test_image_chat_template_batched(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        batched_messages = [\n+        batch_messages = [\n             [\n                 {\n                     \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n                 },\n-            ],\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n-                    ],\n-                },\n-            ],\n-        ]\n+            ]\n+        ] * batch_size\n \n-        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 2)\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n \n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n         formatted_prompt_tokenized = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, padding=True\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n         )\n-        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None, padding=True).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        add_special_tokens = True\n+        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n+            add_special_tokens = False\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n         )\n-        self.assertListEqual(list(out_dict.keys()), [\"input_ids\", \"attention_mask\"])\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n \n-        # Now test the ability to return dict\n-        batched_messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            padding=\"max_length\",\n+            truncation=True,\n+            return_tensors=return_tensors,\n+            max_length=100,\n         )\n-        batched_messages[1][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n+\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n         )\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n+\n         out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+            batch_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n+            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n         )\n-        self.assertTrue(self.images_input_name in out_dict)\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+        self.assertEqual(len(out_dict[input_name]), batch_size * 19200)\n \n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 90480)\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n \n     @require_av\n-    def test_chat_template_video(self):\n+    def test_apply_chat_template_video_frame_sampling(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n@@ -312,52 +312,7 @@ def test_chat_template_video(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 71280)\n \n     @require_av\n-    def test_chat_template_video_custom_sampling(self):\n-        \"\"\"\n-        Tests that models can pass their custom callables to sample video indices.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n-            # sample only the first two frame always\n-            return [0, 1]\n-\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            sample_indices_fn=dummy_sample_indices_fn,\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 14400)\n-\n-    @require_av\n-    def test_chat_template_video_special_processing(self):\n+    def test_apply_chat_template_video_special_processing(self):\n         \"\"\"\n         Tests that models can use their own preprocessing to preprocess conversations.\n         \"\"\"\n@@ -414,6 +369,7 @@ def _process_messages_for_chat_template(\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n "
        },
        {
            "sha": "86d316fd88952d6ff48e6d8444fcb1d2784837c1",
            "filename": "tests/models/shieldgemma2/test_processing_shieldgemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fshieldgemma2%2Ftest_processing_shieldgemma2.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -162,29 +162,14 @@ def test_with_multiple_images(self):\n             self.skipTest(\"Processor has no chat template\")\n \n         images = self.prepare_image_inputs(batch_size=2)\n-        print(images)\n         processed_inputs = processor(images=images)\n         self.assertEqual(len(processed_inputs[self.text_input_name]), 6)\n         self.assertEqual(len(processed_inputs[self.images_input_name]), 6)\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n+    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n     @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_image_chat_template_accepts_processing_kwargs(self):\n-        pass\n-\n-    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n-    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_image_chat_template_batched(self):\n-        pass\n-\n-    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n-    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_image_chat_template_dict_torch(self):\n-        pass\n-\n-    # TODO(ryanmullins): Adapt this test for ShieldGemma 2\n-    @unittest.skip(\"ShieldGemma 2 chat template requires different message structure from parent.\")\n-    def test_image_chat_template_single(self):\n+    def test_apply_chat_template_image(self, batch_size: int, return_tensors: str):\n         pass\n \n     # TODO(ryanmullins): Adapt this test for ShieldGemma 2"
        },
        {
            "sha": "fad6e9489f0699e475e0b08ba9b015fb33a7e993",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -368,12 +368,12 @@ def test_apply_chat_template(self):\n         )\n         self.assertEqual(rendered, expected_rendered)\n \n-    @unittest.skip(reason=\"Broken from common. Fixing TODO @zucchini-nlp @molbap\")\n-    def test_chat_template_video_special_processing(self):\n+    @unittest.skip(reason=\"SmolVLM replaced `type=video` with `type=image` in chat templates\")\n+    def test_apply_chat_template_video_special_processing(self):\n         pass\n \n     @require_av\n-    def test_chat_template_video(self):\n+    def test_apply_chat_template_video_frame_sampling(self):\n         # overriden because SmolVLM has special preprocessing for videos\n         processor = self.get_processor()\n         if processor.chat_template is None:\n@@ -401,11 +401,12 @@ def test_chat_template_video(self):\n             tokenize=True,\n             return_dict=True,\n             num_frames=num_frames,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         # SmolVLM doesn't sample `num_frames` exactly, by uses other sampling method\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 10)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 3)\n \n         # Load with `video_fps` arg\n         video_fps = 1\n@@ -415,6 +416,7 @@ def test_chat_template_video(self):\n             tokenize=True,\n             return_dict=True,\n             video_fps=video_fps,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)"
        },
        {
            "sha": "0ddf569a60f58dce9df65c3b28e2435788b6153d",
            "filename": "tests/models/vipllava/test_processor_vipllava.py",
            "status": "removed",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/10144ff116497878d1d82aabf0a84c16944786f1/tests%2Fmodels%2Fvipllava%2Ftest_processor_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/10144ff116497878d1d82aabf0a84c16944786f1/tests%2Fmodels%2Fvipllava%2Ftest_processor_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_processor_vipllava.py?ref=10144ff116497878d1d82aabf0a84c16944786f1",
            "patch": "@@ -1,41 +0,0 @@\n-# Copyright 2024 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-\n-from transformers.testing_utils import require_vision\n-from transformers.utils import is_vision_available\n-\n-\n-if is_vision_available():\n-    from transformers import AutoProcessor\n-\n-\n-@require_vision\n-class LlavaProcessorTest(unittest.TestCase):\n-    def test_chat_template(self):\n-        processor = AutoProcessor.from_pretrained(\"llava-hf/vip-llava-7b-hf\")\n-        expected_prompt = \"###Human: <image>\\nWhat is shown in this image?###Assistant:\"\n-\n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n-        self.assertEqual(expected_prompt, formatted_prompt)"
        },
        {
            "sha": "f016001c1c2c1c02748261664a27b8321efbbac5",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 116,
            "deletions": 354,
            "changes": 470,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ae8d54b0456867c83e28ecb1fde2e1dcd480e60/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=1ae8d54b0456867c83e28ecb1fde2e1dcd480e60",
            "patch": "@@ -22,6 +22,7 @@\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.processing_utils import Unpack\n@@ -44,6 +45,22 @@\n     import torch\n \n \n+MODALITY_INPUT_DATA = {\n+    \"images\": [\n+        \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+        \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+    ],\n+    \"videos\": [\n+        \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+        [\"https://www.ilankelman.org/stopsigns/australia.jpg\", \"https://www.ilankelman.org/stopsigns/australia.jpg\"],\n+    ],\n+    \"audio\": [\n+        \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+        \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n+    ],\n+}\n+\n+\n def prepare_image_inputs():\n     \"\"\"This function prepares a list of PIL images\"\"\"\n     image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n@@ -729,7 +746,7 @@ def test_prepare_and_validate_optional_call_args(self):\n             )\n \n     def test_chat_template_save_loading(self):\n-        processor = self.get_processor()\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n         signature = inspect.signature(processor.__init__)\n         if \"chat_template\" not in {*signature.parameters.keys()}:\n             self.skipTest(\"Processor doesn't accept chat templates at input\")\n@@ -756,210 +773,133 @@ def test_chat_template_save_loading(self):\n             # the reloaded tokenizer should get the chat template as well\n             self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n \n-    def test_image_chat_template_single(self):\n+    @require_torch\n+    def _test_apply_chat_template(\n+        self,\n+        modality: str,\n+        batch_size: int,\n+        return_tensors: str,\n+        input_name: str,\n+        processor_name: str,\n+        input_data: list[str],\n+    ):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 1)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n-        )\n-        add_special_tokens = True\n-        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n-            add_special_tokens = False\n-        expected_output = processor.tokenizer(\n-            formatted_prompt, return_tensors=None, add_special_tokens=add_special_tokens\n-        ).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(all(key in out_dict for key in [\"input_ids\", \"attention_mask\"]))\n-\n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(self.images_input_name in out_dict)\n-\n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 1)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 1)\n-\n-    def test_image_chat_template_batched(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n+        if processor_name not in self.processor_class.attributes:\n+            self.skipTest(f\"{processor_name} attribute not present in {self.processor_class}\")\n \n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        # some models have only Fast image processor\n+        if getattr(processor, processor_name).__class__.__name__.endswith(\"Fast\"):\n+            return_tensors = \"pt\"\n \n-        batched_messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ],\n+        batch_messages = [\n             [\n                 {\n                     \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What do you see?\"},\n-                    ],\n+                    \"content\": [{\"type\": \"text\", \"text\": \"Describe this.\"}],\n                 },\n-            ],\n-        ]\n+            ]\n+        ] * batch_size\n \n-        formatted_prompt = processor.apply_chat_template(batched_messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 2)\n+        # Test that jinja can be applied\n+        formatted_prompt = processor.apply_chat_template(batch_messages, add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), batch_size)\n \n+        # Test that tokenizing with template and directly with `self.tokenizer` gives same output\n         formatted_prompt_tokenized = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, padding=True, return_tensors=None\n+            batch_messages, add_generation_prompt=True, tokenize=True, return_tensors=return_tensors\n         )\n         add_special_tokens = True\n         if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n             add_special_tokens = False\n-        expected_output = processor.tokenizer(\n-            formatted_prompt,\n-            return_tensors=None,\n-            padding=True,\n-            add_special_tokens=add_special_tokens,\n-        ).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(\n-            batched_messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            padding=True,\n-        )\n-        self.assertTrue(all(key in out_dict for key in [\"input_ids\", \"attention_mask\"]))\n-\n-        # Now test the ability to return dict\n-        batched_messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n-        batched_messages[1][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"}\n-        )\n-        out_dict = processor.apply_chat_template(\n-            batched_messages, add_generation_prompt=True, tokenize=True, return_dict=True, padding=True\n+        tok_output = processor.tokenizer(\n+            formatted_prompt, return_tensors=return_tensors, add_special_tokens=add_special_tokens\n         )\n-        self.assertTrue(self.images_input_name in out_dict)\n-\n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 2)\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 2)\n-        self.assertEqual(len(out_dict[self.images_input_name]), 2)\n-\n-    def test_image_chat_template_accepts_processing_kwargs(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n-\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n+        expected_output = tok_output.input_ids\n+        self.assertListEqual(expected_output.tolist(), formatted_prompt_tokenized.tolist())\n \n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n+        # Test that kwargs passed to processor's `__call__` are actually used\n+        tokenized_prompt_100 = processor.apply_chat_template(\n+            batch_messages,\n             add_generation_prompt=True,\n             tokenize=True,\n             padding=\"max_length\",\n             truncation=True,\n-            max_length=50,\n+            return_tensors=return_tensors,\n+            max_length=100,\n         )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 50)\n+        self.assertEqual(len(tokenized_prompt_100[0]), 100)\n \n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages,\n+        # Test that `return_dict=True` returns text related inputs in the dict\n+        out_dict_text = processor.apply_chat_template(\n+            batch_messages,\n             add_generation_prompt=True,\n             tokenize=True,\n-            truncation=True,\n-            max_length=5,\n+            return_dict=True,\n+            return_tensors=return_tensors,\n         )\n-        self.assertEqual(len(formatted_prompt_tokenized[0]), 5)\n+        self.assertTrue(all(key in out_dict_text for key in [\"input_ids\", \"attention_mask\"]))\n+        self.assertEqual(len(out_dict_text[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict_text[\"attention_mask\"]), batch_size)\n+\n+        # Test that with modality URLs and `return_dict=True`, we get modality inputs in the dict\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx][0][\"content\"] = [batch_messages[idx][0][\"content\"][0], {\"type\": modality, \"url\": url}]\n \n-        # Now test the ability to return dict\n-        messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"}\n-        )\n         out_dict = processor.apply_chat_template(\n-            messages,\n+            batch_messages,\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n-            do_rescale=True,\n-            rescale_factor=-1,\n-            return_tensors=\"np\",\n+            return_tensors=return_tensors,\n+            num_frames=4,  # by default no more than 4 frames, otherwise too slow\n         )\n-        self.assertLessEqual(out_dict[self.images_input_name][0][0].mean(), 0)\n-\n-    @require_torch\n-    def test_image_chat_template_dict_torch(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n+        input_name = getattr(self, input_name)\n+        self.assertTrue(input_name in out_dict)\n+        self.assertEqual(len(out_dict[\"input_ids\"]), batch_size)\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), batch_size)\n+        self.assertEqual(len(out_dict[input_name]), batch_size)\n+\n+        return_tensor_to_type = {\"pt\": torch.Tensor, \"np\": np.ndarray, None: list}\n+        for k in out_dict:\n+            self.assertIsInstance(out_dict[k], return_tensor_to_type[return_tensors])\n+\n+        # Test continue from final message\n+        assistant_message = {\n+            \"role\": \"assistant\",\n+            \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of\"}],\n+        }\n+        for idx, url in enumerate(input_data[:batch_size]):\n+            batch_messages[idx] = batch_messages[idx] + [assistant_message]\n+        continue_prompt = processor.apply_chat_template(batch_messages, continue_final_message=True, tokenize=False)\n+        for prompt in continue_prompt:\n+            self.assertTrue(prompt.endswith(\"It is the sound of\"))  # no `eos` token at the end\n \n-        if \"image_processor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+    @require_av\n+    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n+    def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n+        self._test_apply_chat_template(\n+            \"audio\", batch_size, return_tensors, \"audio_input_name\", \"feature_extracttor\", MODALITY_INPUT_DATA[\"audio\"]\n+        )\n \n-        messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n+    @require_librosa\n+    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n+    def test_apply_chat_template_video(self, batch_size: int, return_tensors: str):\n+        self._test_apply_chat_template(\n+            \"video\", batch_size, return_tensors, \"videos_input_name\", \"video_processor\", MODALITY_INPUT_DATA[\"videos\"]\n+        )\n \n-        out_dict_tensors = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n+    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n+    def test_apply_chat_template_image(self, batch_size: int, return_tensors: str):\n+        self._test_apply_chat_template(\n+            \"image\", batch_size, return_tensors, \"images_input_name\", \"image_processor\", MODALITY_INPUT_DATA[\"images\"]\n         )\n-        self.assertTrue(self.images_input_name in out_dict_tensors)\n-        for k in out_dict_tensors:\n-            self.assertIsInstance(out_dict_tensors[k], torch.Tensor)\n \n-    @require_av\n-    def test_chat_template_video(self):\n+    def test_apply_chat_template_video_frame_sampling(self):\n         processor = self.get_processor()\n+\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n \n@@ -975,44 +915,24 @@ def test_chat_template_video(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"video\"},\n+                        {\n+                            \"type\": \"video\",\n+                            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+                        },\n                         {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n                     ],\n                 },\n             ]\n         ]\n \n-        formatted_prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 1)\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n-        )\n-        add_special_tokens = True\n-        if processor.tokenizer.bos_token is not None and formatted_prompt[0].startswith(processor.tokenizer.bos_token):\n-            add_special_tokens = False\n-        expected_output = processor.tokenizer(\n-            formatted_prompt,\n-            return_tensors=None,\n-            add_special_tokens=add_special_tokens,\n-        ).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(all(key in out_dict for key in [\"input_ids\", \"attention_mask\"]))\n-\n-        # Add video URL for return dict and load with `num_frames` arg\n-        messages[0][0][\"content\"][0] = {\n-            \"type\": \"video\",\n-            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-        }\n         num_frames = 3\n         out_dict_with_video = processor.apply_chat_template(\n             messages,\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n             num_frames=num_frames,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n@@ -1026,6 +946,7 @@ def test_chat_template_video(self):\n             tokenize=True,\n             return_dict=True,\n             video_fps=video_fps,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n@@ -1073,53 +994,7 @@ def test_chat_template_video(self):\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n \n     @require_av\n-    def test_chat_template_video_custom_sampling(self):\n-        \"\"\"\n-        Tests that models can pass their custom callables to sample video indices.\n-        \"\"\"\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        signature = inspect.signature(processor.__call__)\n-        if \"videos\" not in {*signature.parameters.keys()} or (\n-            signature.parameters.get(\"videos\") is not None\n-            and signature.parameters[\"videos\"].annotation == inspect._empty\n-        ):\n-            self.skipTest(\"Processor doesn't accept videos at input\")\n-\n-        video_file_path = hf_hub_download(\n-            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n-        )\n-        messages = [\n-            [\n-                {\n-                    \"role\": \"user\",\n-                    \"content\": [\n-                        {\"type\": \"video\", \"path\": video_file_path},\n-                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n-                    ],\n-                },\n-            ]\n-        ]\n-\n-        def dummy_sample_indices_fn(metadata, **fn_kwargs):\n-            # sample only the first two frame always\n-            return [0, 1]\n-\n-        out_dict_with_video = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            sample_indices_fn=dummy_sample_indices_fn,\n-        )\n-        self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n-\n-    @require_av\n-    def test_chat_template_video_special_processing(self):\n+    def test_apply_chat_template_video_special_processing(self):\n         \"\"\"\n         Tests that models can use their own preprocessing to preprocess conversations.\n         \"\"\"\n@@ -1176,6 +1051,7 @@ def _process_messages_for_chat_template(\n             add_generation_prompt=True,\n             tokenize=True,\n             return_dict=True,\n+            return_tensors=\"np\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n \n@@ -1187,7 +1063,7 @@ def _process_messages_for_chat_template(\n \n     @require_librosa\n     @require_av\n-    def test_audio_chat_template_from_video(self):\n+    def test_chat_template_audio_from_video(self):\n         processor = self.get_processor()\n         if processor.chat_template is None:\n             self.skipTest(\"Processor has no chat template\")\n@@ -1241,124 +1117,10 @@ def test_audio_chat_template_from_video(self):\n             load_audio_from_video=True,\n         )\n         self.assertTrue(self.audio_input_name in out_dict)\n-        self.assertTrue(self.video_input_name in out_dict)\n+        self.assertTrue(self.videos_input_name in out_dict)\n \n         # should always have input_ids and attention_mask\n         self.assertEqual(len(out_dict[\"input_ids\"]), 1)  # batch-size=1\n         self.assertEqual(len(out_dict[\"attention_mask\"]), 1)  # batch-size=1\n         self.assertEqual(len(out_dict[self.audio_input_name]), 2)  # 2 audios in the conversation\n-        self.assertEqual(len(out_dict[self.video_input_name]), 1)  # 1 video in the conversation\n-\n-    @require_librosa\n-    def test_audio_chat_template_single(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n-\n-        messages = [\n-            {\n-                \"role\": \"system\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"audio\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n-                ],\n-            },\n-            {\n-                \"role\": \"assistant\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of glass shattering.\"}],\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"audio\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"How about this one?\"},\n-                ],\n-            },\n-        ]\n-\n-        formatted_prompt = processor.apply_chat_template([messages], add_generation_prompt=True, tokenize=False)\n-        self.assertEqual(len(formatted_prompt), 1)  # batch size=1\n-\n-        formatted_prompt_tokenized = processor.apply_chat_template(\n-            messages, add_generation_prompt=True, tokenize=True, return_tensors=None\n-        )\n-        expected_output = processor.tokenizer(formatted_prompt, return_tensors=None).input_ids\n-        self.assertListEqual(expected_output, formatted_prompt_tokenized)\n-\n-        messages[1][\"content\"][0][\"audio\"] = (\n-            \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n-        )\n-        messages[3][\"content\"][0][\"audio\"] = (\n-            \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n-        )\n-        out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n-        self.assertTrue(self.audio_input_name in out_dict)\n-\n-        # should always have input_ids and attention_mask\n-        self.assertEqual(len(out_dict[\"input_ids\"]), 1)  # batch-size=1\n-        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)  # batch-size=1\n-        self.assertEqual(len(out_dict[self.audio_input_name]), 2)  # 2 audios in the conversation\n-\n-    @require_torch\n-    @require_librosa\n-    def test_audio_chat_template_dict_torch(self):\n-        processor = self.get_processor()\n-        if processor.chat_template is None:\n-            self.skipTest(\"Processor has no chat template\")\n-\n-        if \"feature_extractor\" not in self.processor_class.attributes:\n-            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n-\n-        messages = [\n-            {\n-                \"role\": \"system\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}],\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"audio\",\n-                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"What's that sound?\"},\n-                ],\n-            },\n-            {\n-                \"role\": \"assistant\",\n-                \"content\": [{\"type\": \"text\", \"text\": \"It is the sound of glass shattering.\"}],\n-            },\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\n-                        \"type\": \"audio\",\n-                        \"audio\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n-                    },\n-                    {\"type\": \"text\", \"text\": \"How about this one?\"},\n-                ],\n-            },\n-        ]\n-\n-        out_dict_tensors = processor.apply_chat_template(\n-            messages,\n-            add_generation_prompt=True,\n-            tokenize=True,\n-            return_dict=True,\n-            return_tensors=\"pt\",\n-        )\n-\n-        self.assertTrue(self.audio_input_name in out_dict_tensors)\n-        for k in out_dict_tensors:\n-            self.assertIsInstance(out_dict_tensors[k], torch.Tensor)\n+        self.assertEqual(len(out_dict[self.videos_input_name]), 1)  # 1 video in the conversation"
        }
    ],
    "stats": {
        "total": 1487,
        "additions": 382,
        "deletions": 1105
    }
}