{
    "author": "pglorio",
    "message": "Fix RMSNormGated in Zamba2 (#35943)\n\n* First commit\n\n* Finish model implementation\n\n* First commit\n\n* Finish model implementation\n\n* Register zamba2\n\n* generated modeling and configuration\n\n* generated modeling and configuration\n\n* added hybrid cache\n\n* fix attention_mask in mamba\n\n* dropped unused loras\n\n* fix flash2\n\n* config docstrings\n\n* fix config and fwd pass\n\n* make fixup fixes\n\n* text_modeling_zamba2\n\n* small fixes\n\n* make fixup fixes\n\n* Fix modular model converter\n\n* added inheritances in modular, renamed zamba cache\n\n* modular rebase\n\n* new modular conversion\n\n* fix generated modeling file\n\n* fixed import for Zamba2RMSNormGated\n\n* modular file cleanup\n\n* make fixup and model tests\n\n* dropped inheritance for Zamba2PreTrainedModel\n\n* make fixup and unit tests\n\n* Add inheritance of rope from GemmaRotaryEmbedding\n\n* moved rope to model init\n\n* drop del self.self_attn and del self.feed_forward\n\n* fix tests\n\n* renamed lora -> adapter\n\n* rewrote adapter implementation\n\n* fixed tests\n\n* Fix torch_forward in mamba2 layer\n\n* Fix torch_forward in mamba2 layer\n\n* Fix torch_forward in mamba2 layer\n\n* Dropped adapter in-place sum\n\n* removed rope from attention init\n\n* updated rope\n\n* created get_layers method\n\n* make fixup fix\n\n* make fixup fixes\n\n* make fixup fixes\n\n* update to new attention standard\n\n* update to new attention standard\n\n* make fixup fixes\n\n* minor fixes\n\n* cache_position\n\n* removed cache_position postion_ids use_cache\n\n* remove config from modular\n\n* removed config from modular (2)\n\n* import apply_rotary_pos_emb from llama\n\n* fixed rope_kwargs\n\n* Instantiate cache in Zamba2Model\n\n* fix cache\n\n* fix @slow decorator\n\n* small fix in modular file\n\n* Update docs/source/en/model_doc/zamba2.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* several minor fixes\n\n* inherit mamba2decoder fwd and drop position_ids in mamba\n\n* removed docstrings from modular\n\n* reinstate zamba2 attention decoder fwd\n\n* use regex for tied keys\n\n* Revert \"use regex for tied keys\"\n\nThis reverts commit 9007a522b1f831df6d516a281c0d3fdd20a118f5.\n\n* use regex for tied keys\n\n* add cpu to slow forward tests\n\n* dropped config.use_shared_mlp_adapter\n\n* Update docs/source/en/model_doc/zamba2.md\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* re-convert from modular\n\n* extended Zamba2RMSNormGated to n_groups>1\n\n* removed einops import\n\n* set _supports_sdpa = True\n\n* add use_mem_eff_path flag for fused mamba2 fwd\n\n* added docstring for use_mem_eff_ath flag\n\n---------\n\nCo-authored-by: root <root@node-2.us-southcentral1-a.compute.internal>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "a93b80588b10d567ef7d70143b6581df8e601a8b",
    "files": [
        {
            "sha": "cda81abd430cfe2c3c12a8865c450e64f5f53a44",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=a93b80588b10d567ef7d70143b6581df8e601a8b",
            "patch": "@@ -64,6 +64,8 @@ class Zamba2Config(PretrainedConfig):\n             Whether or not to use bias in the convolution layer of the mixer block.\n         chunk_size (`int`, *optional*, defaults to 256):\n             Size of the chunks that will comprise the sequence.\n+        use_mem_eff_path (`bool`, *optional*, defaults to `False`):\n+            Whether or not to use the fused conv1d and scan in mamba2 layers.\n         add_bias_linear (`bool`, *optional*, defaults to `False`):\n             Flag indicating whether or not to use bias in various layers\n         intermediate_size (`int`, *optional*, defaults to 4 * hidden_size):\n@@ -143,6 +145,7 @@ def __init__(\n         n_mamba_heads=8,\n         use_conv_bias=True,\n         chunk_size=256,\n+        use_mem_eff_path=False,\n         add_bias_linear=False,\n         intermediate_size=None,\n         hidden_act=\"gelu\",\n@@ -231,6 +234,7 @@ def __init__(\n         self.use_cache = use_cache\n         self.num_logits_to_keep = num_logits_to_keep\n         self.hybrid_layer_ids = [index for index, type in enumerate(self.layers_block_type) if type == \"hybrid\"]\n+        self.use_mem_eff_path = use_mem_eff_path\n \n \n __all__ = [\"Zamba2Config\"]"
        },
        {
            "sha": "ce5ee0667ae19da40a8c3babc043d3e22d08baca",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=a93b80588b10d567ef7d70143b6581df8e601a8b",
            "patch": "@@ -62,20 +62,23 @@\n \n \n class Zamba2RMSNormGated(torch.nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n+    def __init__(self, hidden_size, group_size, eps=1e-6):\n         super().__init__()\n         self.weight = nn.Parameter(torch.ones(hidden_size))\n         self.variance_epsilon = eps\n+        self.group_size = group_size\n \n     def forward(self, hidden_states, gate=None):\n         input_dtype = hidden_states.dtype\n         hidden_states = hidden_states.to(torch.float32)\n-\n         if gate is not None:\n             hidden_states = hidden_states * nn.functional.silu(gate.to(torch.float32))\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-\n+        *prefix_dims, last_dim = hidden_states.shape\n+        group_count = last_dim // self.group_size\n+        hidden_states_group = hidden_states.view(*prefix_dims, group_count, self.group_size)\n+        variance = hidden_states_group.pow(2).mean(-1, keepdim=True)\n+        hidden_states_group = hidden_states_group * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = hidden_states_group.view(*prefix_dims, group_count * self.group_size)\n         return self.weight * hidden_states.to(input_dtype)\n \n \n@@ -563,6 +566,7 @@ def __init__(self, config: Zamba2Config, layer_idx: int = None):\n         self.use_conv_bias = config.use_conv_bias\n         self.activation = \"silu\"\n         self.act = nn.SiLU()\n+        self.use_mem_eff_path = config.use_mem_eff_path\n \n         self.n_groups = config.mamba_ngroups\n         self.head_dim = config.mamba_headdim\n@@ -601,7 +605,9 @@ def __init__(self, config: Zamba2Config, layer_idx: int = None):\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n         self.A_log._no_weight_decay = True\n-        self.norm = Zamba2RMSNormGated(self.intermediate_size, eps=1e-5)\n+        self.norm = Zamba2RMSNormGated(\n+            self.intermediate_size, group_size=self.intermediate_size // self.n_groups, eps=1e-5\n+        )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n         self.D._no_weight_decay = True\n \n@@ -685,7 +691,7 @@ def cuda_kernels_forward(\n             else:\n                 input_not_masked = True\n \n-            if self.training and cache_params is None and input_not_masked:\n+            if self.use_mem_eff_path and self.training and cache_params is None and input_not_masked:\n                 out, ssm_state = mamba_split_conv1d_scan_combined(\n                     projected_states,\n                     self.conv1d.weight.squeeze(1),\n@@ -1227,7 +1233,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_flex_attn = True\n-    _supports_sdpa = False\n+    _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache\n     _is_stateful = True\n "
        },
        {
            "sha": "f2074b76f3dad6e084b0d7391cd0f6a698a6ef03",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 6,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a93b80588b10d567ef7d70143b6581df8e601a8b/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=a93b80588b10d567ef7d70143b6581df8e601a8b",
            "patch": "@@ -35,7 +35,7 @@\n     is_mamba_ssm_available,\n )\n from ..llama.modeling_llama import LlamaRotaryEmbedding, apply_rotary_pos_emb\n-from ..mamba2.modeling_mamba2 import MambaRMSNormGated, pad_tensor_by_size, reshape_into_chunks, segment_sum\n+from ..mamba2.modeling_mamba2 import pad_tensor_by_size, reshape_into_chunks, segment_sum\n from ..zamba.modeling_zamba import (\n     ZambaAttention,\n     ZambaAttentionDecoderLayer,\n@@ -70,8 +70,25 @@\n logger = logging.get_logger(__name__)\n \n \n-class Zamba2RMSNormGated(MambaRMSNormGated):\n-    pass\n+class Zamba2RMSNormGated(torch.nn.Module):\n+    def __init__(self, hidden_size, group_size, eps=1e-6):\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+        self.group_size = group_size\n+\n+    def forward(self, hidden_states, gate=None):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        if gate is not None:\n+            hidden_states = hidden_states * nn.functional.silu(gate.to(torch.float32))\n+        *prefix_dims, last_dim = hidden_states.shape\n+        group_count = last_dim // self.group_size\n+        hidden_states_group = hidden_states.view(*prefix_dims, group_count, self.group_size)\n+        variance = hidden_states_group.pow(2).mean(-1, keepdim=True)\n+        hidden_states_group = hidden_states_group * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = hidden_states_group.view(*prefix_dims, group_count * self.group_size)\n+        return self.weight * hidden_states.to(input_dtype)\n \n \n class Zamba2RMSNorm(ZambaRMSNorm):\n@@ -296,6 +313,7 @@ def __init__(self, config: Zamba2Config, layer_idx: int = None):\n         self.use_conv_bias = config.use_conv_bias\n         self.activation = \"silu\"\n         self.act = nn.SiLU()\n+        self.use_mem_eff_path = config.use_mem_eff_path\n \n         self.n_groups = config.mamba_ngroups\n         self.head_dim = config.mamba_headdim\n@@ -334,7 +352,9 @@ def __init__(self, config: Zamba2Config, layer_idx: int = None):\n         A = torch.arange(1, self.num_heads + 1)\n         self.A_log = nn.Parameter(torch.log(A))\n         self.A_log._no_weight_decay = True\n-        self.norm = Zamba2RMSNormGated(self.intermediate_size, eps=1e-5)\n+        self.norm = Zamba2RMSNormGated(\n+            self.intermediate_size, group_size=self.intermediate_size // self.n_groups, eps=1e-5\n+        )\n         self.D = nn.Parameter(torch.ones(self.num_heads))\n         self.D._no_weight_decay = True\n \n@@ -418,7 +438,7 @@ def cuda_kernels_forward(\n             else:\n                 input_not_masked = True\n \n-            if self.training and cache_params is None and input_not_masked:\n+            if self.use_mem_eff_path and self.training and cache_params is None and input_not_masked:\n                 out, ssm_state = mamba_split_conv1d_scan_combined(\n                     projected_states,\n                     self.conv1d.weight.squeeze(1),\n@@ -896,7 +916,7 @@ class Zamba2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_flex_attn = True\n-    _supports_sdpa = False\n+    _supports_sdpa = True\n     _supports_cache_class = True  # Note: only supports Zamba2HybridDynamicCache\n     _is_stateful = True\n "
        }
    ],
    "stats": {
        "total": 58,
        "additions": 44,
        "deletions": 14
    }
}