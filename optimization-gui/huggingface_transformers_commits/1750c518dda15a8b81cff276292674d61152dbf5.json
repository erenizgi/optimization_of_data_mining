{
    "author": "yaswanth19",
    "message": "âœ¨ Add EoMT Model ||  ðŸš¨ Fix Mask2Former loss calculation (#37610)\n\n* Initial Commit\n\n* up\n\n* More changes\n\n* up\n\n* Only mask_logits mismatch\n\n* close enough logits debug later\n\n* fixes\n\n* format\n\n* Add dummy loss\n\n* Close enough processing for semantic seg\n\n* nit\n\n* Added panoptic postprocessor\n\n* refactor\n\n* refactor\n\n* finally fixed panoptic postprocessor\n\n* temp update\n\n* Refactor ForUniversalSegmentation class\n\n* nits and config update\n\n* Few fixes and inference matches\n\n* change mapping\n\n* Added training support but loss slightly off ðŸ¥²\n\n* Loss is matching ðŸ˜€\n\n* update\n\n* Initial tests skelton\n\n* changes\n\n* tests update\n\n* more modular\n\n* initial tests\n\n* updates\n\n* better docstrings\n\n* changes\n\n* proc tests passing :)\n\n* Image processor update\n\n* tiny change\n\n* QOL changes\n\n* Update test w.r.t latest attn refactor\n\n* repo-consistency fixes\n\n* up\n\n* Image proc fix and integration tests :)\n\n* docs update\n\n* integration tests\n\n* fix\n\n* docs update ðŸ¥°\n\n* minor fix\n\n* Happy CI\n\n* fix\n\n* obvious refactoring\n\n* refactoring w.r.t review\n\n* Add fask image proc skelton\n\n* Fast Image proc and cleanups\n\n* Use more modular\n\n* tests update\n\n* Add more tests\n\n* Nit\n\n* QOL updates\n\n* change init_weights to torch default\n\n* add eager func coz of make style\n\n* up\n\n* changes\n\n* typo fix\n\n* Updates\n\n* More deterministic tests\n\n* More modular\n\n* go more modular ðŸš€\n\n* up\n\n* dump\n\n* add supprot for giant ckpts\n\n* overhaul\n\n* modular\n\n* refactor\n\n* instace seg is ready\n\n* cleanup\n\n* forgot this\n\n* docs cleanup\n\n* minor changes\n\n* EoMT - > Eomt\n\n* Happy CI\n\n* remove redundant comment\n\n* Change model references\n\n* final change\n\n* check annealing per block\n\n* My other PR changes ðŸ˜‚\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "1750c518dda15a8b81cff276292674d61152dbf5",
    "files": [
        {
            "sha": "0e5248d8980c63266ef4d7d983e0894e0f5c7ae1",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -737,6 +737,8 @@\n         title: EfficientFormer\n       - local: model_doc/efficientnet\n         title: EfficientNet\n+      - local: model_doc/eomt\n+        title: EoMT\n       - local: model_doc/focalnet\n         title: FocalNet\n       - local: model_doc/glpn"
        },
        {
            "sha": "34842de2101b4d5df3baa7eb98d96ad651cf77ff",
            "filename": "docs/source/en/model_doc/eomt.md",
            "status": "added",
            "additions": 214,
            "deletions": 0,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Feomt.md?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,214 @@\n+<!--Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# EoMT\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+The Encoder-only Mask Transformer (EoMT) model was introduced in the CVPR 2025 Highlight Paper [Your ViT is Secretly an Image Segmentation Model](https://www.tue-mps.org/eomt) by Tommie Kerssies, NiccolÃ² Cavagnero, Alexander Hermans, Narges Norouzi, Giuseppe Averta, Bastian Leibe, Gijs Dubbelman, and Daan de Geus.\n+EoMT reveals Vision Transformers can perform image segmentation efficiently without task-specific components.\n+\n+The abstract from the paper is the following:\n+\n+*Vision Transformers (ViTs) have shown remarkable performance and scalability across various computer vision tasks. To apply single-scale ViTs to image segmentation, existing methods adopt a convolutional adapter to generate multi-scale features, a pixel decoder to fuse these features, and a Transformer decoder that uses the fused features to make predictions. In this paper, we show that the inductive biases introduced by these task-specific components can instead be learned by the ViT itself, given sufficiently large models and extensive pre-training. Based on these findings, we introduce the Encoder-only Mask Transformer (EoMT), which repurposes the plain ViT architecture to conduct image segmentation. With large-scale models and pre-training, EoMT obtains a segmentation accuracy similar to state-of-the-art models that use task-specific components. At the same time, EoMT is significantly faster than these methods due to its architectural simplicity, e.g., up to 4x faster with ViT-L. Across a range of model sizes, EoMT demonstrates an optimal balance between segmentation accuracy and prediction speed, suggesting that compute resources are better spent on scaling the ViT itself rather than adding architectural complexity.*\n+\n+This model was contributed by [Yaswanth Gali](https://huggingface.co/yaswanthgali).\n+The original code can be found [here](https://github.com/tue-mps/eomt).\n+\n+## Architecture Info\n+\n+The `EoMT` model uses a DINOv2-pretrained Vision Transformer with **register tokens** as its backbone. EoMT simplifies the segmentation pipeline by relying solely on the encoder, eliminating the need for task-specific decoders commonly used in prior approaches.\n+\n+Architecturally, EoMT introduces a small set of **learned queries** and a lightweight **mask prediction module**. These queries are injected into the final encoder blocks, enabling **joint attention** between image patches and object queries. During training, **masked attention** is applied to constrain each query to focus on its corresponding regionâ€”effectively mimicking cross-attention. This constraint is gradually phased out via a **mask annealing strategy**, allowing for **efficient, decoder-free inference** without compromising segmentation performance.\n+\n+<div style=\"text-align: center;\">\n+  <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/eomt_architecture.png\"\n+       alt=\"drawing\" width=\"500\"/>\n+</div>\n+\n+\n+The model supports semantic, instance, and panoptic segmentation using a unified architecture and task-specific post-processing.\n+\n+## Usage Examples\n+\n+Use the Hugging Face implementation of EoMT for inference with pre-trained models.\n+\n+### Semantic Segmentation\n+\n+The EoMT model performs semantic segmentation using sliding-window inference. The input image is resized such that the shorter side matches the target input size, then it is split into overlapping crops. Each crop is then passed through the model. After inference, the predicted logits from each crop are stitched back together and rescaled to the original image size to get the final segmentation mask.\n+\n+> **Note:**  \n+> If you want to use a custom target size for **semantic segmentation**, specify it in the following format:  \n+> `{\"shortest_edge\": 512}`  \n+> Notice that `longest_edge` is not provided here â€” this is intentional. For semantic segmentation, images are typically **scaled so that the shortest edge is greater than or equal to the target size** hence longest_edge is not necessary.\n+\n+```python\n+import matplotlib.pyplot as plt\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import EomtForUniversalSegmentation, AutoImageProcessor\n+\n+\n+model_id = \"tue-mps/ade20k_semantic_eomt_large_512\"\n+processor = AutoImageProcessor.from_pretrained(model_id)\n+model = EomtForUniversalSegmentation.from_pretrained(model_id)\n+\n+image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+inputs = processor(\n+    images=image,\n+    return_tensors=\"pt\",\n+)\n+\n+# Remove Patch Offsets from inputs â€” only used later for post-processing.\n+patch_offsets = inputs.pop(\"patch_offsets\")\n+\n+with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+# Prepare the original image size in the format (height, width)\n+original_image_sizes = [(image.height, image.width)]\n+\n+# Post-process the model outputs to get final segmentation prediction\n+preds = processor.post_process_semantic_segmentation(\n+    outputs,\n+    patch_offsets=patch_offsets,\n+    original_image_sizes=original_image_sizes,\n+)\n+\n+# Visualize the segmentation mask\n+plt.imshow(preds[0])\n+plt.axis(\"off\")\n+plt.title(\"Semantic Segmentation\")\n+plt.show()\n+```\n+\n+### Instance Segmentation\n+\n+The EoMT model performs instance segmentation using padded inference. The input image is resized so that the longer side matches the target input size, and the shorter side is zero-padded to form a square. The resulting mask and class logits are combined through post-processing (adapted from Mask2Former) to produce a unified instance segmentation map, along with segment metadata like segment id, class labels and confidence scores.\n+\n+> **Note:**  \n+> To use a custom target size, specify the size as a dictionary in the following format:  \n+> `{\"shortest_edge\": 512, \"longest_edge\": 512}`  \n+> For both instance and panoptic segmentation, input images will be **scaled and padded** to this target size.\n+\n+```python\n+import matplotlib.pyplot as plt\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import EomtForUniversalSegmentation, AutoImageProcessor\n+\n+\n+model_id = \"tue-mps/coco_instance_eomt_large_640\"\n+processor = AutoImageProcessor.from_pretrained(model_id)\n+model = EomtForUniversalSegmentation.from_pretrained(model_id)\n+\n+image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+inputs = processor(\n+    images=image,\n+    return_tensors=\"pt\",\n+)\n+\n+with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+# Prepare the original image size in the format (height, width)\n+original_image_sizes = [(image.height, image.width)]\n+\n+# Post-process the model outputs to get final segmentation prediction\n+preds = processor.post_process_instance_segmentation(\n+    outputs,\n+    original_image_sizes=original_image_sizes,\n+)\n+\n+# Visualize the segmentation mask\n+plt.imshow(preds[0][\"segmentation\"])\n+plt.axis(\"off\")\n+plt.title(\"Instance Segmentation\")\n+plt.show()\n+```\n+\n+### Panoptic Segmentation\n+\n+The EoMT model performs panoptic segmentation using the same padded inference strategy as in instance segmentation. After padding and normalization, the model predicts both thing (instances) and stuff (amorphous regions) classes. The resulting mask and class logits are combined through post-processing (adapted from Mask2Former) to produce a unified panoptic segmentation map, along with segment metadata like segment id, class labels and confidence scores.\n+\n+```python\n+import matplotlib.pyplot as plt\n+import requests\n+import torch\n+from PIL import Image\n+\n+from transformers import EomtForUniversalSegmentation, AutoImageProcessor\n+\n+\n+model_id = \"tue-mps/coco_panoptic_eomt_large_640\"\n+processor = AutoImageProcessor.from_pretrained(model_id)\n+model = EomtForUniversalSegmentation.from_pretrained(model_id)\n+\n+image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+inputs = processor(\n+    images=image,\n+    return_tensors=\"pt\",\n+)\n+\n+with torch.inference_mode():\n+    outputs = model(**inputs)\n+\n+# Prepare the original image size in the format (height, width)\n+original_image_sizes = [(image.height, image.width)]\n+\n+# Post-process the model outputs to get final segmentation prediction\n+preds = processor.post_process_panoptic_segmentation(\n+    outputs,\n+    original_image_sizes=original_image_sizes,\n+)\n+\n+# Visualize the panoptic segmentation mask\n+plt.imshow(preds[0][\"segmentation\"])\n+plt.axis(\"off\")\n+plt.title(\"Panoptic Segmentation\")\n+plt.show()\n+```\n+\n+## EomtImageProcessor\n+\n+[[autodoc]] EomtImageProcessor\n+    - preprocess\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n+## EomtImageProcessorFast\n+\n+[[autodoc]] EomtImageProcessorFast\n+    - preprocess\n+    - post_process_semantic_segmentation\n+    - post_process_instance_segmentation\n+    - post_process_panoptic_segmentation\n+\n+## EomtConfig\n+\n+[[autodoc]] EomtConfig\n+\n+## EomtForUniversalSegmentation\n+\n+[[autodoc]] EomtForUniversalSegmentation\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "36edac4a66c9d3a6d34ec00a4d8a0d7e48c47e16",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -122,6 +122,7 @@\n         (\"emu3\", \"Emu3Config\"),\n         (\"encodec\", \"EncodecConfig\"),\n         (\"encoder-decoder\", \"EncoderDecoderConfig\"),\n+        (\"eomt\", \"EomtConfig\"),\n         (\"ernie\", \"ErnieConfig\"),\n         (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n@@ -501,6 +502,7 @@\n         (\"emu3\", \"Emu3\"),\n         (\"encodec\", \"EnCodec\"),\n         (\"encoder-decoder\", \"Encoder decoder\"),\n+        (\"eomt\", \"EoMT\"),\n         (\"ernie\", \"ERNIE\"),\n         (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),"
        },
        {
            "sha": "4ad74482ebcda2c2f89ff963df68b6afe4584f2b",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -84,6 +84,7 @@\n             (\"dpt\", (\"DPTImageProcessor\", \"DPTImageProcessorFast\")),\n             (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n             (\"efficientnet\", (\"EfficientNetImageProcessor\", \"EfficientNetImageProcessorFast\")),\n+            (\"eomt\", (\"EomtImageProcessor\", \"EomtImageProcessorFast\")),\n             (\"flava\", (\"FlavaImageProcessor\", \"FlavaImageProcessorFast\")),\n             (\"focalnet\", (\"BitImageProcessor\", \"BitImageProcessorFast\")),\n             (\"fuyu\", (\"FuyuImageProcessor\",)),"
        },
        {
            "sha": "bfc09da7e9ff3a71b00225897c8c37f58fb2e56d",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -854,6 +854,7 @@\n     [\n         # Model for Universal Segmentation mapping\n         (\"detr\", \"DetrForSegmentation\"),\n+        (\"eomt\", \"EomtForUniversalSegmentation\"),\n         (\"mask2former\", \"Mask2FormerForUniversalSegmentation\"),\n         (\"maskformer\", \"MaskFormerForInstanceSegmentation\"),\n         (\"oneformer\", \"OneFormerForUniversalSegmentation\"),"
        },
        {
            "sha": "9f4fe6327b312ff5f60ffb08c4b76566bf63f3f9",
            "filename": "src/transformers/models/eomt/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2F__init__.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_eomt import *\n+    from .image_processing_eomt import *\n+    from .image_processing_eomt_fast import *\n+    from .modeling_eomt import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "670250721150e60df3d5da9280197cdad461beef",
            "filename": "src/transformers/models/eomt/configuration_eomt.py",
            "status": "added",
            "additions": 168,
            "deletions": 0,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fconfiguration_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,168 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/eomt/modular_eomt.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_eomt.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class EomtConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`EomtForUniversalSegmentation`]. It is used to instantiate an EoMT model\n+    according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the EoMT\n+    [tue-mps/coco_panoptic_eomt_large_640](https://huggingface.co/tue-mps/coco_panoptic_eomt_large_640)\n+    architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the hidden representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads in each attention layer.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the MLP hidden dimensionality to the hidden size.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings and encoder.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 640):\n+            The size (resolution) of each input image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        layerscale_value (`float`, *optional*, defaults to 1.0):\n+            Initial value for the LayerScale parameter.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            The stochastic depth rate (drop path) used during training.\n+        num_upscale_blocks (`int`, *optional*, defaults to 2):\n+            Number of upsampling blocks used in the decoder or segmentation head.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability applied after attention projection.\n+        use_swiglu_ffn (`bool`, *optional*, defaults to `False`):\n+            Whether to use the SwiGLU feedforward neural network.\n+        num_blocks (`int`, *optional*, defaults to 4):\n+            Number of feature blocks or stages in the architecture.\n+        no_object_weight (`float`, *optional*, defaults to 0.1):\n+            Loss weight for the 'no object' class in panoptic/instance segmentation.\n+        class_weight (`float`, *optional*, defaults to 2.0):\n+            Loss weight for classification targets.\n+        mask_weight (`float`, *optional*, defaults to 5.0):\n+            Loss weight for mask prediction.\n+        dice_weight (`float`, *optional*, defaults to 5.0):\n+            Loss weight for the dice loss component.\n+        train_num_points (`int`, *optional*, defaults to 12544):\n+            Number of points to sample for mask loss computation during training.\n+        oversample_ratio (`float`, *optional*, defaults to 3.0):\n+            Oversampling ratio used in point sampling for mask training.\n+        importance_sample_ratio (`float`, *optional*, defaults to 0.75):\n+            Ratio of points to sample based on importance during training.\n+        num_queries (`int`, *optional*, defaults to 200):\n+            Number of object queries in the Transformer.\n+        num_register_tokens (`int`, *optional*, defaults to 4):\n+            Number of learnable register tokens added to the transformer input.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import EomtConfig, EomtForUniversalSegmentation\n+\n+    >>> # Initialize configuration\n+    >>> config = EomtConfig()\n+\n+    >>> # Initialize model\n+    >>> model = EomtForUniversalSegmentation(config)\n+\n+    >>> # Access config\n+    >>> config = model.config\n+    ```\"\"\"\n+\n+    model_type = \"eomt\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        mlp_ratio=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=640,\n+        patch_size=16,\n+        num_channels=3,\n+        layerscale_value=1.0,\n+        drop_path_rate=0.0,\n+        num_upscale_blocks=2,\n+        attention_dropout=0.0,\n+        use_swiglu_ffn=False,\n+        num_blocks=4,\n+        no_object_weight: float = 0.1,\n+        class_weight: float = 2.0,\n+        mask_weight: float = 5.0,\n+        dice_weight: float = 5.0,\n+        train_num_points: int = 12544,\n+        oversample_ratio: float = 3.0,\n+        importance_sample_ratio: float = 0.75,\n+        num_queries=200,\n+        num_register_tokens=4,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.hidden_act = hidden_act\n+        self.hidden_dropout_prob = hidden_dropout_prob\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+\n+        self.mlp_ratio = mlp_ratio\n+        self.attention_dropout = attention_dropout\n+        self.layerscale_value = layerscale_value\n+        self.drop_path_rate = drop_path_rate\n+        self.num_upscale_blocks = num_upscale_blocks\n+        self.use_swiglu_ffn = use_swiglu_ffn\n+        self.num_blocks = num_blocks\n+        self.no_object_weight = no_object_weight\n+        self.class_weight = class_weight\n+        self.mask_weight = mask_weight\n+        self.dice_weight = dice_weight\n+        self.train_num_points = train_num_points\n+        self.oversample_ratio = oversample_ratio\n+        self.importance_sample_ratio = importance_sample_ratio\n+        self.num_queries = num_queries\n+        self.num_register_tokens = num_register_tokens\n+\n+\n+__all__ = [\"EomtConfig\"]"
        },
        {
            "sha": "6d822c1bfc8697930ca476e366fbbe5743f94410",
            "filename": "src/transformers/models/eomt/convert_eomt_to_hf.py",
            "status": "added",
            "additions": 340,
            "deletions": 0,
            "changes": 340,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,340 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import gc\n+import json\n+import os\n+import re\n+from typing import Optional\n+\n+import torch\n+from accelerate import init_empty_weights\n+from huggingface_hub import snapshot_download\n+\n+from transformers import EomtConfig, EomtForUniversalSegmentation, EomtImageProcessorFast\n+\n+\n+# fmt: off\n+MAPPINGS = {\n+    # Embeddings\n+    r\"network.encoder.backbone.cls_token\"                : r\"embeddings.cls_token\",\n+    r\"network.encoder.backbone.reg_token\"                : r\"embeddings.register_tokens\",\n+    r\"network.encoder.backbone.pos_embed\"                : r\"embeddings.position_embeddings.weight\",\n+    r\"network.encoder.backbone.patch_embed.proj\"         : r\"embeddings.patch_embeddings.projection\",\n+\n+    # Encoder Block\n+    r\"network.encoder.backbone.blocks.(\\d+).norm1\"       : r\"layers.\\1.norm1\",\n+    r\"network.encoder.backbone.blocks.(\\d+).attn.proj\"   : r\"layers.\\1.attention.out_proj\",\n+    r\"network.encoder.backbone.blocks.(\\d+).ls1.gamma\"   : r\"layers.\\1.layer_scale1.lambda1\",\n+    r\"network.encoder.backbone.blocks.(\\d+).norm2\"       : r\"layers.\\1.norm2\",\n+    r\"network.encoder.backbone.blocks.(\\d+).ls2.gamma\"   : r\"layers.\\1.layer_scale2.lambda1\",\n+    r\"network.encoder.backbone.blocks.(\\d+).attn\"        : r\"layers.\\1.attention\",\n+\n+    # Others\n+    r\"network.q.weight\"                                  : r\"query.weight\",\n+    r\"network.class_head\"                                : r\"class_predictor\",\n+    r\"network.upscale.(\\d+).conv1\"                       : r\"upscale_block.block.\\1.conv1\",\n+    r\"network.upscale.(\\d+).conv2\"                       : r\"upscale_block.block.\\1.conv2\",\n+    r\"network.upscale.(\\d+).norm\"                        : r\"upscale_block.block.\\1.layernorm2d\",\n+    r\"network.mask_head.0\"                               : r\"mask_head.fc1\",\n+    r\"network.mask_head.2\"                               : r\"mask_head.fc2\",\n+    r\"network.mask_head.4\"                               : r\"mask_head.fc3\",\n+    r\"network.encoder.backbone.norm\"                     : r\"layernorm\",\n+    r\"network.attn_mask_probs\"                           : r\"attn_mask_probs\",\n+}\n+# fmt: on\n+\n+# Mappings for MLP layers, depending on the type of MLP used in ckpts.\n+MLP_MAPPINGS = {\n+    \"swiglu_ffn\": {\n+        r\"network.encoder.backbone.blocks.(\\d+).mlp.fc1\": r\"layers.\\1.mlp.weights_in\",\n+        r\"network.encoder.backbone.blocks.(\\d+).mlp.fc2\": r\"layers.\\1.mlp.weights_out\",\n+    },\n+    \"vanilla_mlp\": {\n+        r\"network.encoder.backbone.blocks.(\\d+).mlp\": r\"layers.\\1.mlp\",\n+    },\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict):\n+    keys_as_text = \"\\n\".join(state_dict.keys())\n+    new_keys_as_text = keys_as_text\n+    for old, repl in MAPPINGS.items():\n+        if repl is None:\n+            new_keys_as_text = re.sub(old, \"\", new_keys_as_text)\n+        else:\n+            new_keys_as_text = re.sub(old, repl, new_keys_as_text)\n+    output_dict = dict(zip(keys_as_text.split(\"\\n\"), new_keys_as_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def split_qkv_tensor(key, tensor):\n+    \"\"\"Splits a qkv tensor into separate q, k, v tensors and updates the key accordingly.\"\"\"\n+\n+    new_keys = [\"q_proj\", \"k_proj\", \"v_proj\"]\n+    split_size = tensor.shape[0] // 3\n+    split_tensors = torch.split(tensor, split_size, dim=0)\n+\n+    return {key.replace(\"qkv\", new_key): split_tensors[i] for i, new_key in enumerate(new_keys)}\n+\n+\n+def convert_state_dict_to_hf(state_dict):\n+    \"\"\"Convert state dict keys to HF format.\"\"\"\n+    conversion_dict = convert_old_keys_to_new_keys(state_dict)\n+    converted_state_dict = {}\n+\n+    for old_key, new_key in conversion_dict.items():\n+        if new_key:\n+            if \"qkv\" in new_key:  # Detect merged attention keys and split them.\n+                qkv_split_dict = split_qkv_tensor(new_key, state_dict[old_key])\n+                converted_state_dict.update(qkv_split_dict)\n+            else:\n+                converted_state_dict[new_key] = state_dict[old_key]\n+\n+    for i in [\n+        \"network.encoder.pixel_mean\",\n+        \"network.encoder.pixel_std\",\n+    ]:\n+        converted_state_dict.pop(i)\n+\n+    # Embeddings will not have initial dimension\n+    pos_embed_key = \"embeddings.position_embeddings.weight\"\n+    converted_state_dict[pos_embed_key] = converted_state_dict[pos_embed_key].squeeze(0)\n+\n+    return converted_state_dict\n+\n+\n+def ensure_model_downloaded(\n+    repo_id: Optional[str] = None, revision: Optional[str] = None, local_dir: Optional[str] = None\n+) -> str:\n+    \"\"\"\n+    Ensures model files are downloaded locally, downloads them if not.\n+    Returns path to local files.\n+\n+    Args:\n+        repo_id: The Hugging Face model repo ID (required if local_dir not provided)\n+        revision: Optional git revision to use\n+        local_dir: Optional local directory path where model files should be stored/found\n+    \"\"\"\n+    if local_dir is not None:\n+        if os.path.exists(local_dir):\n+            print(f\"Using provided local directory: {local_dir}\")\n+        else:\n+            # Create the local directory if it doesn't exist\n+            os.makedirs(local_dir, exist_ok=True)\n+            print(f\"Created local directory: {local_dir}\")\n+\n+    if repo_id is None:\n+        raise ValueError(\"Either repo_id or local_dir must be provided\")\n+\n+    print(f\"Ensuring {repo_id} (revision: {revision or 'latest'}) is downloaded...\")\n+\n+    try:\n+        # First try to find files locally\n+        download_dir = snapshot_download(repo_id, revision=revision, local_files_only=True, local_dir=local_dir)\n+        print(f\"Found model files locally at {download_dir}\")\n+        return download_dir\n+    except Exception:\n+        # If files not found locally, download them\n+        print(f\"Downloading model files for {repo_id}...\")\n+        download_dir = snapshot_download(repo_id, revision=revision, local_files_only=False, local_dir=local_dir)\n+        print(f\"Downloaded model files to {download_dir}\")\n+        return download_dir\n+\n+\n+def load_model_state_dict(input_path: str) -> dict:\n+    \"\"\"\n+    Load model state dict, handling both single and sharded files.\n+    \"\"\"\n+    index_path = os.path.join(input_path, \"pytorch_model.bin.index.json\")\n+    single_file_path = os.path.join(input_path, \"pytorch_model.bin\")\n+\n+    # Check if we have a sharded model\n+    if os.path.exists(index_path):\n+        print(\"Loading sharded model...\")\n+        state_dict = {}\n+        with open(index_path, \"r\") as f:\n+            index = json.load(f)\n+\n+        # Get unique shard files and load each one only once\n+        unique_shard_files = sorted(set(index[\"weight_map\"].values()))\n+        for shard_file in unique_shard_files:\n+            print(f\"Loading shard {shard_file}...\")\n+            shard_path = os.path.join(input_path, shard_file)\n+            shard_dict = torch.load(shard_path, map_location=\"cpu\")\n+            state_dict.update(shard_dict)\n+\n+        return state_dict\n+\n+    # Single file model\n+    elif os.path.exists(single_file_path):\n+        print(\"Loading single file model...\")\n+        return torch.load(single_file_path, map_location=\"cpu\")\n+\n+    else:\n+        raise ValueError(f\"No model files found in {input_path}\")\n+\n+\n+def convert_model(\n+    repo_id=None,\n+    local_dir=None,\n+    output_dir=None,\n+    output_hub_path=None,\n+    safe_serialization=True,\n+    revision=None,\n+):\n+    \"\"\"Convert and save the model weights, processor, and configuration.\"\"\"\n+    if output_dir is None and output_hub_path is None:\n+        raise ValueError(\"At least one of output_dir or output_hub_path must be specified\")\n+\n+    if repo_id is None and local_dir is None:\n+        raise ValueError(\"Either repo_id or local_dir must be specified\")\n+\n+    # Create output directory if specified\n+    if output_dir:\n+        os.makedirs(output_dir, exist_ok=True)\n+        print(f\"Created/verified output directory: {output_dir}\")\n+\n+    torch.set_default_dtype(torch.float16)\n+\n+    # Download or locate model files\n+    input_path = ensure_model_downloaded(repo_id=repo_id, revision=revision, local_dir=local_dir)\n+\n+    with open(os.path.join(input_path, \"config.json\"), \"r\") as f:\n+        config_data = json.load(f)\n+    # Pop off unwanted keys\n+    _ = config_data.pop(\"backbone\", None)\n+\n+    config = EomtConfig(\n+        **{\n+            **config_data,\n+            \"layerscale_value\": 1e-5,\n+        }\n+    )\n+\n+    if \"semantic\" in repo_id.split(\"_\"):\n+        size = {\"shortest_edge\": config.image_size, \"longest_edge\": None}\n+        do_split_image = True\n+        do_pad = False\n+    else:\n+        size = {\"shortest_edge\": config.image_size, \"longest_edge\": config.image_size}\n+        do_split_image = False\n+        do_pad = True\n+\n+    if \"giant\" in repo_id.split(\"_\"):\n+        config.use_swiglu_ffn = True\n+        config.hidden_size = 1536\n+        config.num_hidden_layers = 40\n+        config.num_attention_heads = 24\n+        # Update MAPPINGS for ckpts depending on the MLP type\n+        MAPPINGS.update(MLP_MAPPINGS[\"swiglu_ffn\"])\n+    else:\n+        MAPPINGS.update(MLP_MAPPINGS[\"vanilla_mlp\"])\n+\n+    processor = EomtImageProcessorFast(size=size, do_split_image=do_split_image, do_pad=do_pad)\n+\n+    # Save the config and processor\n+    if output_dir:\n+        config.save_pretrained(output_dir)\n+        processor.save_pretrained(output_dir)\n+    if output_hub_path:\n+        config.push_to_hub(output_hub_path)\n+        processor.push_to_hub(output_hub_path)\n+\n+    # Initialize model with empty weights\n+    print(\"Creating empty model...\")\n+    with init_empty_weights():\n+        model = EomtForUniversalSegmentation(config)\n+\n+    # Load and convert state dict\n+    print(\"Loading state dict...\")\n+    state_dict = load_model_state_dict(input_path)\n+    state_dict = convert_state_dict_to_hf(state_dict)\n+\n+    # Load converted state dict\n+    print(\"Loading converted weights into model...\")\n+    model.load_state_dict(state_dict, strict=True, assign=True)\n+\n+    # Save the model\n+    if output_dir:\n+        print(f\"Saving model to {output_dir}...\")\n+        model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    if output_hub_path:\n+        print(f\"Pushing model to hub at {output_hub_path}...\")\n+        model.push_to_hub(output_hub_path, safe_serialization=safe_serialization)\n+\n+    del state_dict, model\n+    gc.collect()\n+\n+    # Validate the saved model if saved locally\n+    if output_dir:\n+        print(\"Reloading the local model to check if it's saved correctly...\")\n+        EomtForUniversalSegmentation.from_pretrained(output_dir, device_map=\"auto\")\n+        print(\"Local model reloaded successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--hf_repo_id\",\n+        help=\"HuggingFace Hub repo ID for the model\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--local_dir\",\n+        help=\"Local directory containing the model files\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--revision\",\n+        help=\"Specific revision to download from the Hub\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model locally\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--output_hub_path\",\n+        help=\"Repository ID to push model to hub (e.g. 'username/model-name')\",\n+        default=None,\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        help=\"Whether to save using safetensors\",\n+    )\n+    args = parser.parse_args()\n+\n+    if args.output_dir is None and args.output_hub_path is None:\n+        raise ValueError(\"At least one of --output_dir or --output_hub_path must be specified\")\n+\n+    if args.hf_repo_id is None and args.local_dir is None:\n+        raise ValueError(\"Either --hf_repo_id or --local_dir must be specified\")\n+\n+    convert_model(\n+        repo_id=args.hf_repo_id,\n+        local_dir=args.local_dir,\n+        output_dir=args.output_dir,\n+        output_hub_path=args.output_hub_path,\n+        safe_serialization=args.safe_serialization,\n+        revision=args.revision,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "73fe46034cd57833d81fccec00012e6192d26f35",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "added",
            "additions": 972,
            "deletions": 0,
            "changes": 972,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,972 @@\n+# coding=utf-8\n+# Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for EoMT.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    PaddingMode,\n+    pad,\n+    resize,\n+)\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    make_flat_list_of_images,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    TensorType,\n+    filter_out_non_signature_kwargs,\n+    is_torch_available,\n+    logging,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn.functional as F\n+\n+\n+# Adapted from transformers.models.maskformer.image_processing_maskformer.convert_segmentation_map_to_binary_masks\n+def convert_segmentation_map_to_binary_masks(\n+    segmentation_map: \"np.ndarray\",\n+    instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+    ignore_index: Optional[int] = None,\n+):\n+    if ignore_index is not None:\n+        segmentation_map = np.where(segmentation_map == 0, ignore_index, segmentation_map - 1)\n+\n+    # Get unique ids (class or instance ids based on input)\n+    all_labels = np.unique(segmentation_map)\n+\n+    # Drop background label if applicable\n+    if ignore_index is not None:\n+        all_labels = all_labels[all_labels != ignore_index]\n+\n+    # Generate a binary mask for each object instance\n+    binary_masks = [(segmentation_map == i) for i in all_labels]\n+\n+    # Stack the binary masks\n+    if binary_masks:\n+        binary_masks = np.stack(binary_masks, axis=0)\n+    else:\n+        binary_masks = np.zeros((0, *segmentation_map.shape))\n+\n+    # Convert instance ids to class ids\n+    if instance_id_to_semantic_id is not None:\n+        labels = np.zeros(all_labels.shape[0])\n+\n+        for label in all_labels:\n+            class_id = instance_id_to_semantic_id[label + 1 if ignore_index is not None else label]\n+            labels[all_labels == label] = class_id - 1 if ignore_index is not None else class_id\n+    else:\n+        labels = all_labels\n+\n+    return binary_masks.astype(np.float32), labels.astype(np.int64)\n+\n+\n+def get_size_with_aspect_ratio(image_size, size, max_size=None) -> tuple[int, int]:\n+    \"\"\"\n+    Computes the output image size given the input image size and the desired output size.\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The input image size.\n+        size (`int`):\n+            The desired output size.\n+        max_size (`int`, *optional*):\n+            The maximum allowed output size.\n+    \"\"\"\n+    height, width = image_size\n+    raw_size = None\n+    if max_size is not None:\n+        min_original_size = float(min((height, width)))\n+        max_original_size = float(max((height, width)))\n+        if max_original_size / min_original_size * size > max_size:\n+            raw_size = max_size * min_original_size / max_original_size\n+            size = int(round(raw_size))\n+\n+    if (height <= width and height == size) or (width <= height and width == size):\n+        oh, ow = height, width\n+    elif width < height:\n+        ow = size\n+        if max_size is not None and raw_size is not None:\n+            oh = round(raw_size * height / width)\n+        else:\n+            oh = round(size * height / width)\n+    else:\n+        oh = size\n+        if max_size is not None and raw_size is not None:\n+            ow = round(raw_size * width / height)\n+        else:\n+            ow = round(size * width / height)\n+\n+    return (oh, ow)\n+\n+\n+# Copied from transformers.models.detr.image_processing_detr.remove_low_and_no_objects\n+def remove_low_and_no_objects(masks, scores, labels, object_mask_threshold, num_labels):\n+    \"\"\"\n+    Binarize the given masks using `object_mask_threshold`, it returns the associated values of `masks`, `scores` and\n+    `labels`.\n+\n+    Args:\n+        masks (`torch.Tensor`):\n+            A tensor of shape `(num_queries, height, width)`.\n+        scores (`torch.Tensor`):\n+            A tensor of shape `(num_queries)`.\n+        labels (`torch.Tensor`):\n+            A tensor of shape `(num_queries)`.\n+        object_mask_threshold (`float`):\n+            A number between 0 and 1 used to binarize the masks.\n+    Raises:\n+        `ValueError`: Raised when the first dimension doesn't match in all input tensors.\n+    Returns:\n+        `tuple[`torch.Tensor`, `torch.Tensor`, `torch.Tensor`]`: The `masks`, `scores` and `labels` without the region\n+        < `object_mask_threshold`.\n+    \"\"\"\n+    if not (masks.shape[0] == scores.shape[0] == labels.shape[0]):\n+        raise ValueError(\"mask, scores and labels must have the same shape!\")\n+\n+    to_keep = labels.ne(num_labels) & (scores > object_mask_threshold)\n+\n+    return masks[to_keep], scores[to_keep], labels[to_keep]\n+\n+\n+def check_segment_validity(mask_labels, mask_probs, k, mask_threshold=0.5, overlap_mask_area_threshold=0.8):\n+    # Get the mask associated with the k class\n+    mask_k = mask_labels == k\n+    mask_k_area = mask_k.sum()\n+\n+    # Compute the area of all the stuff in query k\n+    original_mask = mask_probs[k] >= mask_threshold\n+    original_area = original_mask.sum()\n+\n+    final_mask = mask_k & original_mask\n+    final_mask_area = final_mask.sum()\n+\n+    mask_exists = mask_k_area > 0 and original_area > 0 and final_mask_area > 0\n+\n+    if mask_exists:\n+        area_ratio = mask_k_area / original_area\n+        if not area_ratio.item() > overlap_mask_area_threshold:\n+            mask_exists = False\n+\n+    return mask_exists, final_mask\n+\n+\n+def compute_segments(\n+    mask_probs,\n+    pred_scores,\n+    pred_labels,\n+    stuff_classes,\n+    mask_threshold: float = 0.5,\n+    overlap_mask_area_threshold: float = 0.8,\n+    target_size: Optional[tuple[int, int]] = None,\n+):\n+    height = mask_probs.shape[1] if target_size is None else target_size[0]\n+    width = mask_probs.shape[2] if target_size is None else target_size[1]\n+\n+    segmentation = torch.zeros((height, width), dtype=torch.long, device=mask_probs.device) - 1\n+    segments: list[dict] = []\n+\n+    # Compute per-pixel assignment based on weighted mask scores\n+    mask_probs = mask_probs.sigmoid()\n+    mask_labels = (pred_scores[:, None, None] * mask_probs).argmax(0)\n+\n+    # Keep track of instances of each class\n+    current_segment_id = 0\n+    stuff_memory_list: dict[str, int] = {}\n+\n+    for k in range(pred_labels.shape[0]):\n+        pred_class = pred_labels[k].item()\n+\n+        # Check if mask exists and large enough to be a segment\n+        mask_exists, final_mask = check_segment_validity(\n+            mask_labels, mask_probs, k, mask_threshold, overlap_mask_area_threshold\n+        )\n+\n+        if not mask_exists:\n+            continue\n+\n+        if stuff_classes and pred_class in stuff_classes:\n+            if pred_class in stuff_memory_list:\n+                segmentation[final_mask] = stuff_memory_list[pred_class]\n+                continue\n+            else:\n+                stuff_memory_list[pred_class] = current_segment_id\n+\n+        segmentation[final_mask] = current_segment_id\n+        segment_score = round(pred_scores[k].item(), 6)\n+        segments.append(\n+            {\n+                \"id\": current_segment_id,\n+                \"label_id\": pred_class,\n+                \"score\": segment_score,\n+            }\n+        )\n+        current_segment_id += 1\n+    return segmentation, segments\n+\n+\n+def get_target_size(size_dict: dict[str, int]) -> tuple[int, int]:\n+    \"\"\"Returns the height and width from a size dict.\"\"\"\n+    target_height = size_dict[\"shortest_edge\"]\n+    target_width = size_dict.get(\"longest_edge\", None) or target_height\n+\n+    return target_height, target_width\n+\n+\n+class EomtImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a EoMT image processor. The image processor can be used to prepare image(s) and optional targets\n+    for the model.\n+\n+    This image processor inherits from [`BaseImageProcessor`] which contains most of the main methods. Users should\n+    refer to this superclass for more information regarding those methods.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the input to a certain `size`.\n+        size (`int`, *optional*, defaults to 640):\n+            Resize the input to the given size. Only has an effect if `do_resize` is set to `True`. If size is a\n+            sequence like `(width, height)`, output size will be matched to this. If size is an int, smaller edge of\n+            the image will be matched to this number. i.e, if `height > width`, then image will be rescaled to `(size *\n+            height / width, size)`.\n+        resample (`int`, *optional*, defaults to `Resampling.BILINEAR`):\n+            An optional resampling filter. This can be one of `PIL.Image.Resampling.NEAREST`,\n+            `PIL.Image.Resampling.BOX`, `PIL.Image.Resampling.BILINEAR`, `PIL.Image.Resampling.HAMMING`,\n+            `PIL.Image.Resampling.BICUBIC` or `PIL.Image.Resampling.LANCZOS`. Only has an effect if `do_resize` is set\n+            to `True`.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the input to a certain `scale`.\n+        rescale_factor (`float`, *optional*, defaults to `1/ 255`):\n+            Rescale the input by the given factor. Only has an effect if `do_rescale` is set to `True`.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether or not to normalize the input with mean and standard deviation.\n+        do_split_image (`bool`, *optional*, defaults to `False`):\n+            Whether to split the input images into overlapping patches for semantic segmentation. If set to `True`, the\n+            input images will be split into patches of size `size[\"shortest_edge\"]` with an overlap between patches.\n+            Otherwise, the input images will be padded to the target size.\n+        do_pad (`bool`, *optional*, defaults to `False`):\n+            Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+            number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+        image_mean (`int`, *optional*, defaults to `[0.485, 0.456, 0.406]`):\n+            The sequence of means for each channel, to be used when normalizing images. Defaults to the ImageNet mean.\n+        image_std (`int`, *optional*, defaults to `[0.229, 0.224, 0.225]`):\n+            The sequence of standard deviations for each channel, to be used when normalizing images. Defaults to the\n+            ImageNet std.\n+        ignore_index (`int`, *optional*):\n+            Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+            denoted with 0 (background) will be replaced with `ignore_index`.\n+        num_labels (`int`, *optional*):\n+            The number of labels in the segmentation map.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_normalize: bool = True,\n+        do_split_image: bool = False,\n+        do_pad: bool = False,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        ignore_index: Optional[int] = None,\n+        num_labels: Optional[int] = None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        size = size if size is not None else {\"shortest_edge\": 640, \"longest_edge\": 640}\n+        size = get_size_dict(size, default_to_square=False)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.do_split_image = do_split_image\n+        self.do_pad = do_pad\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.ignore_index = ignore_index\n+        self.num_labels = num_labels\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: dict,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        data_format=None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n+        resized to keep the input aspect ratio.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                Resampling filter to use when resiizing the image.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the image. If not provided, it will be the same as the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+        \"\"\"\n+        image_size = get_image_size(image)\n+        output_size = get_size_with_aspect_ratio(image_size, size[\"shortest_edge\"], size[\"longest_edge\"])\n+\n+        image = resize(\n+            image=image,\n+            size=output_size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            return_numpy=True,\n+            **kwargs,\n+        )\n+\n+        return image\n+\n+    def _split_image(self, image: ImageInput, size: dict, image_index: int) -> tuple[list, list]:\n+        \"\"\"Slices an image into overlapping patches for semantic segmentation.\"\"\"\n+\n+        patches, patch_offsets = [], []\n+\n+        image_size = get_image_size(image)\n+        patch_size = size[\"shortest_edge\"]\n+\n+        longer_side = max(image_size)\n+        num_patches = math.ceil(longer_side / patch_size)\n+        total_overlap = num_patches * patch_size - longer_side\n+        overlap_per_patch = total_overlap / (num_patches - 1) if num_patches > 1 else 0\n+\n+        for i in range(num_patches):\n+            start = int(i * (patch_size - overlap_per_patch))\n+            end = start + patch_size\n+\n+            if image_size[0] > image_size[1]:\n+                patch = image[:, start:end, :]\n+            else:\n+                patch = image[:, :, start:end]\n+\n+            patches.append(patch)\n+            patch_offsets.append([image_index, start, end])\n+\n+        return patches, patch_offsets\n+\n+    def _pad(self, image: ImageInput, size: dict) -> np.ndarray:\n+        \"\"\"Pads the image to the target size using zero padding.\"\"\"\n+        height, width = get_image_size(image)\n+\n+        target_height, target_width = get_target_size(size)\n+        pad_h = max(0, target_height - height)\n+        pad_w = max(0, target_width - width)\n+\n+        padding = ((0, pad_h), (0, pad_w))\n+\n+        # Channel axis is last; default padding format is compatible\n+        padded_image = pad(image=image, padding=padding, mode=PaddingMode.CONSTANT, constant_values=0.0)\n+        return padded_image\n+\n+    def _preprocess_images(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_split_image: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Preprocesses a batch of images.\"\"\"\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_resize:\n+            images = [\n+                self.resize(\n+                    image,\n+                    size=size,\n+                    resample=resample,\n+                    data_format=data_format,\n+                    input_data_format=input_data_format,\n+                )\n+                for image in images\n+            ]\n+\n+        processed_images, patch_offsets = [], []\n+\n+        if do_split_image:\n+            for idx, img in enumerate(images):\n+                patches, offsets = self._split_image(img, size, idx)\n+                processed_images.extend(patches)\n+                patch_offsets.extend(offsets)\n+\n+            images = processed_images\n+\n+        if do_pad:\n+            images = [self._pad(img, size) for img in images]\n+\n+        if do_rescale:\n+            images = [self.rescale(img, scale=rescale_factor, input_data_format=input_data_format) for img in images]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(\n+                    image,\n+                    mean=image_mean,\n+                    std=image_std,\n+                    input_data_format=input_data_format,\n+                )\n+                for image in images\n+            ]\n+\n+        return images, patch_offsets\n+\n+    def _preprocess_mask(\n+        self,\n+        segmentation_map: ImageInput,\n+        do_resize: Optional[bool] = False,\n+        do_pad: Optional[bool] = False,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        data_format: Union[str, ChannelDimension] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Preprocesses a single mask.\"\"\"\n+        # Add channel dimension if missing - needed for certain transformations\n+        if segmentation_map.ndim == 2:\n+            added_channel_dim = True\n+            segmentation_map = segmentation_map[None, ...]\n+            input_data_format = ChannelDimension.FIRST\n+        else:\n+            added_channel_dim = False\n+            if input_data_format is None:\n+                input_data_format = infer_channel_dimension_format(segmentation_map)\n+\n+        if do_resize:\n+            segmentation_map = self.resize(\n+                segmentation_map,\n+                size=size,\n+                resample=resample,\n+                data_format=data_format,\n+            )\n+\n+        if do_pad:\n+            segmentation_map = self._pad(segmentation_map, size)\n+\n+        # Remove extra channel dimension if added for processing\n+        if added_channel_dim:\n+            segmentation_map = segmentation_map.squeeze(0)\n+        return torch.from_numpy(segmentation_map)\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+        do_split_image: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        ignore_index: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocesses images or a batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The corresponding semantic segmentation maps with the pixel-wise annotations.\n+            instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+                A mapping between object instance ids and class ids.\n+            do_split_image (`bool`, *optional*, defaults to `self.do_split_image`):\n+                Whether to split the input images into overlapping patches for semantic segmentation.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the input images.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Target size as a dictionary with `\"shortest_edge\"` and `\"longest_edge\"` keys.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use when resizing.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the input images by `rescale_factor`.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Factor to scale image pixel values.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the input images.\n+            do_pad (`bool`, *optional*, defaults to `False`):\n+                Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+                number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Mean for normalization. Single value or list for each channel.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Standard deviation for normalization. Single value or list for each channel.\n+            ignore_index (`int`, *optional*):\n+                Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+                denoted with 0 (background) will be replaced with `ignore_index`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be `\"pt\"`, `\"tf\"`, `\"np\"`, or `\"jax\"`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                Channel format of the output image. Either `\"channels_first\"` or `\"channels_last\"`.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                Channel format of the input image.\n+        \"\"\"\n+\n+        do_split_image = do_split_image if do_split_image is not None else self.do_split_image\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        ignore_index = ignore_index if ignore_index is not None else self.ignore_index\n+\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        pixel_values_list, patch_offsets = self._preprocess_images(\n+            images=images,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            do_split_image=do_split_image,\n+            do_pad=do_pad,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+        )\n+\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+            segmentation_maps = [to_numpy_array(mask) for mask in segmentation_maps]\n+\n+            segmentation_maps = [\n+                self._preprocess_mask(\n+                    segmentation_map,\n+                    do_resize=do_resize,\n+                    do_pad=do_pad,\n+                    size=size,\n+                    resample=PILImageResampling.NEAREST,\n+                    data_format=data_format,\n+                    input_data_format=input_data_format,\n+                )\n+                for segmentation_map in segmentation_maps\n+            ]\n+\n+        encoded_inputs = self.encode_inputs(\n+            pixel_values_list,\n+            segmentation_maps,\n+            instance_id_to_semantic_id,\n+            ignore_index,\n+            return_tensors,\n+            input_data_format=data_format,\n+        )\n+\n+        if do_split_image and patch_offsets:\n+            encoded_inputs[\"patch_offsets\"] = patch_offsets\n+\n+        return encoded_inputs\n+\n+    def encode_inputs(\n+        self,\n+        pixel_values_list: list[ImageInput],\n+        segmentation_maps: ImageInput = None,\n+        instance_id_to_semantic_id: Optional[Union[list[dict[int, int]], dict[int, int]]] = None,\n+        ignore_index: Optional[int] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"\n+        Pad images up to the largest image in a batch and create a corresponding `pixel_mask`.\n+\n+        EoMT addresses semantic segmentation with a mask classification paradigm, thus input segmentation maps\n+        will be converted to lists of binary masks and their respective labels. Let's see an example, assuming\n+        `segmentation_maps = [[2,6,7,9]]`, the output will contain `mask_labels =\n+        [[1,0,0,0],[0,1,0,0],[0,0,1,0],[0,0,0,1]]` (four binary masks) and `class_labels = [2,6,7,9]`, the labels for\n+        each mask.\n+\n+        Args:\n+            pixel_values_list (`List[ImageInput]`):\n+                List of images (pixel values) to be padded. Each image should be a tensor of shape `(channels, height,\n+                width)`.\n+\n+            segmentation_maps (`ImageInput`, *optional*):\n+                The corresponding semantic segmentation maps with the pixel-wise annotations.\n+\n+             (`bool`, *optional*, defaults to `True`):\n+                Whether or not to pad images up to the largest image in a batch and create a pixel mask.\n+\n+                If left to the default, will return a pixel mask that is:\n+\n+                - 1 for pixels that are real (i.e. **not masked**),\n+                - 0 for pixels that are padding (i.e. **masked**).\n+\n+            instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+                A mapping between object instance ids and class ids. If passed, `segmentation_maps` is treated as an\n+                instance segmentation map where each pixel represents an instance id. Can be provided as a single\n+                dictionary with a global/dataset-level mapping or as a list of dictionaries (one per image), to map\n+                instance ids in each image separately.\n+\n+            return_tensors (`str` or [`~file_utils.TensorType`], *optional*):\n+                If set, will return tensors instead of NumPy arrays. If set to `'pt'`, return PyTorch `torch.Tensor`\n+                objects.\n+\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **pixel_values** -- Pixel values to be fed to a model.\n+            - **mask_labels** -- Optional list of mask labels of shape `(labels, height, width)` to be fed to a model\n+              (when `annotations` are provided).\n+            - **class_labels** -- Optional list of class labels of shape `(labels)` to be fed to a model (when\n+              `annotations` are provided). They identify the labels of `mask_labels`, e.g. the label of\n+              `mask_labels[i][j]` if `class_labels[i][j]`.\n+        \"\"\"\n+        ignore_index = self.ignore_index if ignore_index is None else ignore_index\n+\n+        pixel_values_list = [to_numpy_array(pixel_values) for pixel_values in pixel_values_list]\n+\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(pixel_values_list[0])\n+\n+        encoded_inputs = BatchFeature({\"pixel_values\": pixel_values_list}, tensor_type=return_tensors)\n+\n+        if segmentation_maps is not None:\n+            mask_labels = []\n+            class_labels = []\n+            # Convert to list of binary masks and labels\n+            for idx, segmentation_map in enumerate(segmentation_maps):\n+                segmentation_map = to_numpy_array(segmentation_map)\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+                # Use instance2class_id mapping per image\n+                masks, classes = convert_segmentation_map_to_binary_masks(\n+                    segmentation_map,\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                )\n+\n+                mask_labels.append(torch.from_numpy(masks))\n+                class_labels.append(torch.from_numpy(classes))\n+\n+            # we cannot batch them since they don't share a common class size\n+            encoded_inputs[\"mask_labels\"] = mask_labels\n+            encoded_inputs[\"class_labels\"] = class_labels\n+\n+        return encoded_inputs\n+\n+    def merge_image_patches(\n+        self,\n+        segmentation_logits: torch.Tensor,\n+        patch_offsets: list[tuple[int, int, int]],\n+        original_image_sizes: list[tuple[int, int]],\n+        size: dict[str, int],\n+    ) -> list[torch.Tensor]:\n+        \"\"\"\n+        Reconstructs full-size semantic segmentation logits from patch predictions.\n+\n+        Args:\n+            segmentation_logits (`torch.Tensor`):\n+                A tensor of shape `(num_patches, num_classes, patch_height, patch_width)` representing predicted logits\n+                for each image patch.\n+            patch_offsets (`List[Tuple[int, int, int]]`):\n+                A list of tuples where each tuple contains:\n+                - `image_index` (int): Index of the original image this patch belongs to.\n+                - `start` (int): Start pixel index of the patch along the long dimension (height or width).\n+                - `end` (int): End pixel index of the patch along the long dimension.\n+            original_image_sizes (`List[Tuple[int, int]]`):\n+                List of original (height, width) dimensions for each image before preprocessing.\n+            size (`Dict[str, int]`):\n+                A size dict which was used to resize.\n+        \"\"\"\n+        num_classes = segmentation_logits.shape[1]\n+        aggregated_logits = []\n+        patch_counts = []\n+\n+        for image_size in original_image_sizes:\n+            height, width = get_size_with_aspect_ratio(image_size, size[\"shortest_edge\"], size[\"longest_edge\"])\n+            aggregated_logits.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n+            patch_counts.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n+\n+        # Stitch patches back into full-sized logit maps\n+        for patch_idx, (image_idx, patch_start, patch_end) in enumerate(patch_offsets):\n+            if original_image_sizes[image_idx][0] > original_image_sizes[image_idx][1]:\n+                aggregated_logits[image_idx][:, patch_start:patch_end, :] += segmentation_logits[patch_idx]\n+                patch_counts[image_idx][:, patch_start:patch_end, :] += 1\n+            else:\n+                aggregated_logits[image_idx][:, :, patch_start:patch_end] += segmentation_logits[patch_idx]\n+                patch_counts[image_idx][:, :, patch_start:patch_end] += 1\n+\n+        # Normalize and resize logits to original image size\n+        reconstructed_logits = []\n+        for idx, (logit_sum, count) in enumerate(zip(aggregated_logits, patch_counts)):\n+            averaged_logits = logit_sum / count.clamp(min=1)\n+            resized_logits = F.interpolate(\n+                averaged_logits[None, ...],\n+                size=original_image_sizes[idx],\n+                mode=\"bilinear\",\n+                align_corners=False,\n+            )[0]\n+\n+            reconstructed_logits.append(resized_logits)\n+\n+        return reconstructed_logits\n+\n+    def unpad_image(\n+        self,\n+        segmentation_logits: torch.Tensor,\n+        original_image_sizes: list[tuple[int, int]],\n+        size: dict[str, int],\n+    ) -> list[torch.Tensor]:\n+        \"\"\"Restores panoptic segmentation logits to their original image resolutions.\"\"\"\n+\n+        resized_logits = []\n+\n+        for idx, original_size in enumerate(original_image_sizes):\n+            target_height, target_width = get_size_with_aspect_ratio(\n+                original_size, size[\"shortest_edge\"], size[\"longest_edge\"]\n+            )\n+            cropped_logits = segmentation_logits[idx][:, :target_height, :target_width]\n+            upsampled_logits = F.interpolate(\n+                cropped_logits[None, ...], size=original_size, mode=\"bilinear\", align_corners=False\n+            )[0]\n+            resized_logits.append(upsampled_logits)\n+        return resized_logits\n+\n+    def post_process_semantic_segmentation(\n+        self,\n+        outputs,\n+        patch_offsets: list[tuple[int, int, int]],\n+        original_image_sizes: list[tuple[int, int]],\n+        size: Optional[dict[str, int]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = F.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        segmentation_logits = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+\n+        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, original_image_sizes, size)\n+\n+        preds = torch.stack(output_logits).argmax(dim=1)\n+        return preds\n+\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        original_image_sizes: list[tuple[int, int]],\n+        threshold: float = 0.8,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        stuff_classes: Optional[list[int]] = None,\n+        size: Optional[dict[str, int]] = None,\n+    ):\n+        \"\"\"Post-processes model outputs into final panoptic segmentation prediction.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = F.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        pred_scores_batch, pred_labels_batch = class_queries_logits.softmax(dim=-1).max(-1)\n+\n+        results: list = []\n+\n+        for i in range(batch_size):\n+            mask_probs, pred_scores, pred_labels = remove_low_and_no_objects(\n+                mask_probs_batch[i], pred_scores_batch[i], pred_labels_batch[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs.shape[0] <= 0:\n+                height, width = original_image_sizes[i] if original_image_sizes is not None else mask_probs.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs,\n+                pred_scores=pred_scores,\n+                pred_labels=pred_labels,\n+                stuff_classes=stuff_classes,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                target_size=original_image_sizes[i] if original_image_sizes is not None else None,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        original_image_sizes: list[tuple[int, int]],\n+        threshold: float = 0.5,\n+        size: Optional[dict[str, int]] = None,\n+    ):\n+        \"\"\"Post-processes model outputs into Instance Segmentation Predictions.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        class_queries_logits = outputs.class_queries_logits\n+        masks_queries_logits = outputs.masks_queries_logits\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = F.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+\n+        device = masks_queries_logits.device\n+        batch_size = class_queries_logits.shape[0]\n+        num_queries = class_queries_logits.shape[-2]\n+\n+        results = []\n+\n+        for i in range(batch_size):\n+            mask_pred = mask_probs_batch[i]\n+            mask_class = class_queries_logits[i]\n+\n+            # Remove the null class `[..., :-1]`\n+            scores, pred_classes = mask_class.softmax(dim=-1)[..., :-1].max(-1)\n+            pred_masks = (mask_pred > 0).float()\n+\n+            # Calculate average mask prob\n+            mask_scores = (mask_pred.sigmoid().flatten(1) * pred_masks.flatten(1)).sum(1) / (\n+                pred_masks.flatten(1).sum(1) + 1e-6\n+            )\n+            pred_scores = scores * mask_scores\n+\n+            segmentation = torch.zeros(original_image_sizes[i], device=device) - 1\n+\n+            instance_maps, segments = [], []\n+            current_segment_id = 0\n+            for j in range(num_queries):\n+                score = pred_scores[j].item()\n+\n+                if not torch.all(pred_masks[j] == 0) and score >= threshold:\n+                    segmentation[pred_masks[j] == 1] = current_segment_id\n+                    segments.append(\n+                        {\n+                            \"id\": current_segment_id,\n+                            \"label_id\": pred_classes[j].item(),\n+                            \"score\": round(score, 6),\n+                        }\n+                    )\n+                    current_segment_id += 1\n+                    instance_maps.append(pred_masks[j])\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"EomtImageProcessor\"]"
        },
        {
            "sha": "04b53c418db6293cca5136567a0d41cb58d5fbc5",
            "filename": "src/transformers/models/eomt/image_processing_eomt_fast.py",
            "status": "added",
            "additions": 580,
            "deletions": 0,
            "changes": 580,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt_fast.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,580 @@\n+# coding=utf-8\n+# Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for EoMT.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    make_list_of_images,\n+    pil_torch_interpolation_mapping,\n+    validate_kwargs,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    auto_docstring,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+from .image_processing_eomt import (\n+    compute_segments,\n+    convert_segmentation_map_to_binary_masks,\n+    get_size_with_aspect_ratio,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class EomtImageProcessorFastKwargs(DefaultFastImageProcessorKwargs):\n+    \"\"\"\n+    do_split_image (`bool`, *optional*, defaults to `False`):\n+            Whether to split the input images into overlapping patches for semantic segmentation. If set to `True`, the\n+            input images will be split into patches of size `size[\"shortest_edge\"]` with an overlap between patches.\n+            Otherwise, the input images will be padded to the target size.\n+    do_pad (`bool`, *optional*, defaults to `False`):\n+            Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n+            number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n+    ignore_index (`int`, *optional*):\n+            Label to be assigned to background pixels in segmentation maps. If provided, segmentation map pixels\n+            denoted with 0 (background) will be replaced with `ignore_index`.\n+    \"\"\"\n+\n+    do_split_image: bool\n+    do_pad: bool\n+    ignore_index: Optional[int] = None\n+\n+\n+def get_target_size(size_dict: dict[str, int]) -> tuple[int, int]:\n+    \"\"\"Returns the height and width from a size dict.\"\"\"\n+    target_height = size_dict[\"shortest_edge\"]\n+    target_width = size_dict[\"longest_edge\"] or target_height\n+\n+    return target_height, target_width\n+\n+\n+def reorder_patches_and_offsets(\n+    patches: list[torch.Tensor], offsets: list[list[int]]\n+) -> tuple[list[torch.Tensor], list[list[int]]]:\n+    \"\"\"Sorts patches and offsets according to the original image index.\"\"\"\n+\n+    combined = list(zip(offsets, patches))\n+    combined.sort(key=lambda x: x[0][0])\n+    sorted_offsets, sorted_patches = zip(*combined)\n+\n+    return list(sorted_patches), list(sorted_offsets)\n+\n+\n+@auto_docstring\n+class EomtImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    size = {\"shortest_edge\": 640, \"longest_edge\": 640}\n+    default_to_square = False\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_split_image = False\n+    do_pad = False\n+    ignore_index = None\n+    valid_kwargs = EomtImageProcessorFastKwargs\n+\n+    def __init__(self, **kwargs: Unpack[EomtImageProcessorFastKwargs]):\n+        super().__init__(**kwargs)\n+\n+    def _split_image(self, images: torch.Tensor, size: dict, image_indices: int) -> tuple[list, list]:\n+        \"\"\"Slices an image into overlapping patches for semantic segmentation.\"\"\"\n+\n+        patches, patch_offsets = [], []\n+\n+        _, _, height, width = images.shape\n+        patch_size = size[\"shortest_edge\"]\n+\n+        longer_side = max(height, width)\n+        num_patches = math.ceil(longer_side / patch_size)\n+        total_overlap = num_patches * patch_size - longer_side\n+        overlap_per_patch = total_overlap / (num_patches - 1) if num_patches > 1 else 0\n+\n+        for i in range(num_patches):\n+            start = int(i * (patch_size - overlap_per_patch))\n+            end = start + patch_size\n+\n+            if height > width:\n+                batch_patch = images[:, :, start:end, :]\n+            else:\n+                batch_patch = images[:, :, :, start:end]\n+\n+            for batch_idx, single in enumerate(torch.unbind(batch_patch, dim=0)):\n+                patches.append(single)\n+                patch_offsets.append([image_indices[batch_idx], start, end])\n+\n+        return patches, patch_offsets\n+\n+    def _pad(self, images: torch.Tensor, size: dict) -> torch.Tensor:\n+        \"\"\"Pads the image to the target size using zero padding.\"\"\"\n+        _, _, height, width = images.shape\n+\n+        target_height, target_width = get_target_size(size)\n+        pad_h = max(0, target_height - height)\n+        pad_w = max(0, target_width - width)\n+        padding = (0, pad_w, 0, pad_h)\n+\n+        padded_images = torch.nn.functional.pad(images, padding, mode=\"constant\", value=0.0)\n+        return padded_images\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_split_image: bool,\n+        do_pad: bool,\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        disable_grouping: Optional[bool],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ):\n+        \"\"\"Preprocesses the input images and masks if provided.\"\"\"\n+        processed_images, patch_offsets = [], []\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+                resized_images_grouped[shape] = stacked_images\n+        images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for batched resizing, Needed in case do_resize is False.\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            original_indices = [\n+                original_idx for original_idx, (img_shape, _) in grouped_images_index.items() if img_shape == shape\n+            ]\n+\n+            if do_split_image:\n+                patches, offsets = self._split_image(stacked_images, size, original_indices)\n+                processed_images.extend(patches)\n+                patch_offsets.extend(offsets)\n+\n+            if do_pad:\n+                stacked_images = self._pad(stacked_images, size)\n+                processed_images_grouped[shape] = stacked_images\n+\n+        if do_split_image:\n+            images, patch_offsets = reorder_patches_and_offsets(processed_images, patch_offsets)\n+\n+        if do_pad:\n+            images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+\n+        for shape, stacked_images in grouped_images.items():\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        processed_images = torch.stack(images, dim=0) if return_tensors else images\n+\n+        return processed_images, patch_offsets\n+\n+    def _preprocess_images(self, images, **kwargs):\n+        \"\"\"Preprocesses the input images.\"\"\"\n+        return self._preprocess(images, **kwargs)\n+\n+    def _preprocess_masks(self, segmentation_maps: list[torch.Tensor], **kwargs):\n+        \"\"\"Preprocesses segmentation maps.\"\"\"\n+        processed_segmentation_maps = []\n+        for segmentation_map in segmentation_maps:\n+            segmentation_map = self._process_image(\n+                segmentation_map, do_convert_rgb=False, input_data_format=ChannelDimension.FIRST\n+            )\n+\n+            if segmentation_map.ndim == 2:\n+                segmentation_map = segmentation_map[None, ...]\n+\n+            processed_segmentation_maps.append(segmentation_map)\n+\n+        kwargs[\"do_normalize\"] = False\n+        kwargs[\"do_rescale\"] = False\n+        kwargs[\"input_data_format\"] = ChannelDimension.FIRST\n+\n+        # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n+        kwargs[\"interpolation\"] = pil_torch_interpolation_mapping[PILImageResampling.NEAREST]\n+\n+        processed_segmentation_maps, _ = self._preprocess(images=processed_segmentation_maps, **kwargs)\n+        processed_segmentation_maps = processed_segmentation_maps.squeeze(1)\n+        processed_segmentation_maps = processed_segmentation_maps.to(torch.int64)\n+\n+        return processed_segmentation_maps\n+\n+    @auto_docstring\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        segmentation_maps: Optional[list[torch.Tensor]] = None,\n+        instance_id_to_semantic_id: Optional[dict[int, int]] = None,\n+        **kwargs: Unpack[EomtImageProcessorFastKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to preprocess for corresponding images.\n+        instance_id_to_semantic_id (`List[Dict[int, int]]` or `Dict[int, int]`, *optional*):\n+            A mapping between object instance ids and class ids.\n+        \"\"\"\n+        # args are not validated, but their order in the `preprocess` and `_preprocess` signatures must be the same\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_kwargs_names)\n+        # Set default kwargs from self. This ensures that if a kwarg is not provided\n+        # by the user, it gets its default value from the instance, or is set to None.\n+        for kwarg_name in self._valid_kwargs_names:\n+            kwargs.setdefault(kwarg_name, getattr(self, kwarg_name, None))\n+\n+        # Extract parameters that are only used for preparing the input images\n+        do_convert_rgb = kwargs.pop(\"do_convert_rgb\")\n+        input_data_format = kwargs.pop(\"input_data_format\")\n+        device = kwargs.pop(\"device\")\n+        # Prepare input images\n+        images = self._prepare_input_images(\n+            images=images, do_convert_rgb=do_convert_rgb, input_data_format=input_data_format, device=device\n+        )\n+        # Prepare segmentation maps\n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(images=segmentation_maps, expected_ndims=2)\n+\n+        # Update kwargs that need further processing before being validated\n+        kwargs = self._further_process_kwargs(**kwargs)\n+\n+        # Validate kwargs\n+        self._validate_preprocess_kwargs(**kwargs)\n+\n+        # torch resize uses interpolation instead of resample\n+        resample = kwargs.pop(\"resample\")\n+\n+        # Check if resample is an int before checking if it's an instance of PILImageResampling\n+        # because if pillow < 9.1.0, resample is an int and PILImageResampling is a module.\n+        # Checking PILImageResampling will fail with error `TypeError: isinstance() arg 2 must be a type or tuple of types`.\n+        kwargs[\"interpolation\"] = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (int, PILImageResampling)) else resample\n+        )\n+\n+        # Pop kwargs that are not needed in _preprocess\n+        kwargs.pop(\"default_to_square\")\n+        kwargs.pop(\"data_format\")\n+\n+        ignore_index = kwargs.pop(\"ignore_index\", None)\n+\n+        processed_images, patch_offsets = self._preprocess_images(images=images, **kwargs)\n+\n+        outputs = BatchFeature({\"pixel_values\": processed_images})\n+\n+        mask_labels, class_labels = [], []\n+        if segmentation_maps is not None:\n+            segmentation_maps = self._preprocess_masks(segmentation_maps=segmentation_maps, **kwargs)\n+            # Convert to list of binary masks and labels\n+            for idx, segmentation_map in enumerate(segmentation_maps):\n+                if isinstance(instance_id_to_semantic_id, list):\n+                    instance_id = instance_id_to_semantic_id[idx]\n+                else:\n+                    instance_id = instance_id_to_semantic_id\n+                # Use instance2class_id mapping per image\n+                masks, classes = convert_segmentation_map_to_binary_masks(\n+                    segmentation_map,\n+                    instance_id,\n+                    ignore_index=ignore_index,\n+                )\n+\n+                mask_labels.append(torch.from_numpy(masks))\n+                class_labels.append(torch.from_numpy(classes))\n+\n+            # we cannot batch them since they don't share a common class size\n+            outputs[\"mask_labels\"] = mask_labels\n+            outputs[\"class_labels\"] = class_labels\n+\n+        if patch_offsets:\n+            outputs[\"patch_offsets\"] = patch_offsets\n+\n+        return outputs\n+\n+    def merge_image_patches(\n+        self,\n+        segmentation_logits: torch.Tensor,\n+        patch_offsets: list[tuple[int, int, int]],\n+        original_image_sizes: list[tuple[int, int]],\n+        size: dict[str, int],\n+    ) -> list[torch.Tensor]:\n+        \"\"\"\n+        Reconstructs full-size semantic segmentation logits from patch predictions.\n+\n+        Args:\n+            segmentation_logits (`torch.Tensor`):\n+                A tensor of shape `(num_patches, num_classes, patch_height, patch_width)` representing predicted logits\n+                for each image patch.\n+            patch_offsets (`List[Tuple[int, int, int]]`):\n+                A list of tuples where each tuple contains:\n+                - `image_index` (int): Index of the original image this patch belongs to.\n+                - `start` (int): Start pixel index of the patch along the long dimension (height or width).\n+                - `end` (int): End pixel index of the patch along the long dimension.\n+            original_image_sizes (`List[Tuple[int, int]]`):\n+                List of original (height, width) dimensions for each image before preprocessing.\n+            size (`Dict[str, int]`):\n+                A size dict which was used to resize.\n+        \"\"\"\n+        num_classes = segmentation_logits.shape[1]\n+        aggregated_logits = []\n+        patch_counts = []\n+\n+        for image_size in original_image_sizes:\n+            height, width = get_size_with_aspect_ratio(image_size, size[\"shortest_edge\"], size[\"longest_edge\"])\n+            aggregated_logits.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n+            patch_counts.append(torch.zeros((num_classes, height, width), device=segmentation_logits.device))\n+\n+        # Stitch patches back into full-sized logit maps\n+        for patch_idx, (image_idx, patch_start, patch_end) in enumerate(patch_offsets):\n+            if original_image_sizes[image_idx][0] > original_image_sizes[image_idx][1]:\n+                aggregated_logits[image_idx][:, patch_start:patch_end, :] += segmentation_logits[patch_idx]\n+                patch_counts[image_idx][:, patch_start:patch_end, :] += 1\n+            else:\n+                aggregated_logits[image_idx][:, :, patch_start:patch_end] += segmentation_logits[patch_idx]\n+                patch_counts[image_idx][:, :, patch_start:patch_end] += 1\n+\n+        # Normalize and resize logits to original image size\n+        reconstructed_logits = []\n+        for idx, (logit_sum, count) in enumerate(zip(aggregated_logits, patch_counts)):\n+            averaged_logits = logit_sum / count.clamp(min=1)\n+            resized_logits = torch.nn.functional.interpolate(\n+                averaged_logits[None, ...],\n+                size=original_image_sizes[idx],\n+                mode=\"bilinear\",\n+                align_corners=False,\n+            )[0]\n+\n+            reconstructed_logits.append(resized_logits)\n+\n+        return reconstructed_logits\n+\n+    def unpad_image(\n+        self,\n+        segmentation_logits: torch.Tensor,\n+        original_image_sizes: list[tuple[int, int]],\n+        size: dict[str, int],\n+    ) -> list[torch.Tensor]:\n+        \"\"\"Restores panoptic segmentation logits to their original image resolutions.\"\"\"\n+\n+        resized_logits = []\n+\n+        for idx, original_size in enumerate(original_image_sizes):\n+            target_height, target_width = get_size_with_aspect_ratio(\n+                original_size, size[\"shortest_edge\"], size[\"longest_edge\"]\n+            )\n+            cropped_logits = segmentation_logits[idx][:, :target_height, :target_width]\n+            upsampled_logits = torch.nn.functional.interpolate(\n+                cropped_logits[None, ...], size=original_size, mode=\"bilinear\", align_corners=False\n+            )[0]\n+            resized_logits.append(upsampled_logits)\n+        return resized_logits\n+\n+    def post_process_semantic_segmentation(\n+        self,\n+        outputs,\n+        patch_offsets: list[tuple[int, int, int]],\n+        original_image_sizes: list[tuple[int, int]],\n+        size: Optional[dict[str, int]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Post-processes model outputs into final semantic segmentation prediction.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        segmentation_logits = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+\n+        output_logits = self.merge_image_patches(segmentation_logits, patch_offsets, original_image_sizes, size)\n+\n+        preds = torch.stack(output_logits).argmax(dim=1)\n+        return preds\n+\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        original_image_sizes: list[tuple[int, int]],\n+        threshold: float = 0.8,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        stuff_classes: Optional[list[int]] = None,\n+        size: Optional[dict[str, int]] = None,\n+    ):\n+        \"\"\"Post-processes model outputs into final panoptic segmentation prediction.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        masks_queries_logits = outputs.masks_queries_logits  # [batch_size, num_queries, height, width]\n+        class_queries_logits = outputs.class_queries_logits  # [batch_size, num_queries, num_classes+1]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+        pred_scores_batch, pred_labels_batch = class_queries_logits.softmax(dim=-1).max(-1)\n+\n+        results: list = []\n+\n+        for i in range(batch_size):\n+            mask_probs, pred_scores, pred_labels = remove_low_and_no_objects(\n+                mask_probs_batch[i], pred_scores_batch[i], pred_labels_batch[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs.shape[0] <= 0:\n+                height, width = original_image_sizes[i] if original_image_sizes is not None else mask_probs.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs,\n+                pred_scores=pred_scores,\n+                pred_labels=pred_labels,\n+                stuff_classes=stuff_classes,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                target_size=original_image_sizes[i] if original_image_sizes is not None else None,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        original_image_sizes: list[tuple[int, int]],\n+        threshold: float = 0.8,\n+        size: Optional[dict[str, int]] = None,\n+    ):\n+        \"\"\"Post-processes model outputs into Instance Segmentation Predictions.\"\"\"\n+\n+        size = size if size is not None else self.size\n+\n+        masks_queries_logits = outputs.masks_queries_logits\n+        class_queries_logits = outputs.class_queries_logits\n+\n+        output_size = get_target_size(size)\n+        masks_queries_logits = torch.nn.functional.interpolate(\n+            masks_queries_logits,\n+            size=output_size,\n+            mode=\"bilinear\",\n+        )\n+\n+        mask_probs_batch = self.unpad_image(masks_queries_logits, original_image_sizes, size)\n+\n+        device = masks_queries_logits.device\n+        batch_size = class_queries_logits.shape[0]\n+        num_queries = class_queries_logits.shape[-2]\n+\n+        results = []\n+\n+        for i in range(batch_size):\n+            mask_pred = mask_probs_batch[i]\n+            mask_class = class_queries_logits[i]\n+\n+            # Remove the null class `[..., :-1]`\n+            scores, pred_classes = mask_class.softmax(dim=-1)[..., :-1].max(-1)\n+            pred_masks = (mask_pred > 0).float()\n+\n+            # Calculate average mask prob\n+            mask_scores = (mask_pred.sigmoid().flatten(1) * pred_masks.flatten(1)).sum(1) / (\n+                pred_masks.flatten(1).sum(1) + 1e-6\n+            )\n+            pred_scores = scores * mask_scores\n+\n+            segmentation = torch.zeros(original_image_sizes[i], device=device) - 1\n+\n+            instance_maps, segments = [], []\n+            current_segment_id = 0\n+            for j in range(num_queries):\n+                score = pred_scores[j].item()\n+\n+                if not torch.all(pred_masks[j] == 0) and score >= threshold:\n+                    segmentation[pred_masks[j] == 1] = current_segment_id\n+                    segments.append(\n+                        {\n+                            \"id\": current_segment_id,\n+                            \"label_id\": pred_classes[j].item(),\n+                            \"score\": round(score, 6),\n+                        }\n+                    )\n+                    current_segment_id += 1\n+                    instance_maps.append(pred_masks[j])\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"EomtImageProcessorFast\"]"
        },
        {
            "sha": "bbdd11e1f58f9de0c413a47036259521047789a4",
            "filename": "src/transformers/models/eomt/modeling_eomt.py",
            "status": "added",
            "additions": 1242,
            "deletions": 0,
            "changes": 1242,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodeling_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,1242 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/eomt/modular_eomt.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_eomt.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import collections.abc\n+import math\n+from dataclasses import dataclass\n+from typing import Callable, Optional, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...file_utils import ModelOutput, is_scipy_available, requires_backends\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...utils import auto_docstring, can_return_tuple, is_accelerate_available\n+from .configuration_eomt import EomtConfig\n+\n+\n+if is_scipy_available():\n+    from scipy.optimize import linear_sum_assignment\n+\n+if is_accelerate_available():\n+    from accelerate import PartialState\n+    from accelerate.utils import reduce\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class for outputs of [`EomtForUniversalSegmentationOutput`].\n+\n+    This output can be directly passed to [`~EomtImageProcessor.post_process_semantic_segmentation`] or\n+    [`~EomtImageProcessor.post_process_instance_segmentation`] or\n+    [`~EomtImageProcessor.post_process_panoptic_segmentation`] to compute final segmentation maps. Please, see\n+    [`~EomtImageProcessor] for details regarding usage.\n+    \"\"\"\n+)\n+class EomtForUniversalSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.Tensor`, *optional*):\n+        The computed loss, returned when labels are present.\n+    class_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n+        query. Note the `+ 1` is needed because we incorporate the null class.\n+    masks_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n+        query.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last layer.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states all layers of the model.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    class_queries_logits: Optional[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+# Adapted from https://github.com/facebookresearch/detectron2/blob/main/projects/PointRend/point_rend/point_features.py\n+def sample_point(\n+    input_features: torch.Tensor, point_coordinates: torch.Tensor, add_dim=False, **kwargs\n+) -> torch.Tensor:\n+    \"\"\"\n+    A wrapper around `torch.nn.functional.grid_sample` to support 3D point_coordinates tensors.\n+\n+    Args:\n+        input_features (`torch.Tensor` of shape (batch_size, channels, height, width)):\n+            A tensor that contains features map on a height * width grid\n+        point_coordinates (`torch.Tensor` of shape (batch_size, num_points, 2) or (batch_size, grid_height, grid_width,:\n+        2)):\n+            A tensor that contains [0, 1] * [0, 1] normalized point coordinates\n+        add_dim (`bool`):\n+            boolean value to keep track of added dimension\n+\n+    Returns:\n+        point_features (`torch.Tensor` of shape (batch_size, channels, num_points) or (batch_size, channels,\n+        height_grid, width_grid):\n+            A tensor that contains features for points in `point_coordinates`.\n+    \"\"\"\n+    if point_coordinates.dim() == 3:\n+        add_dim = True\n+        point_coordinates = point_coordinates.unsqueeze(2)\n+\n+    # use nn.function.grid_sample to get features for points in `point_coordinates` via bilinear interpolation\n+    point_features = torch.nn.functional.grid_sample(input_features, 2.0 * point_coordinates - 1.0, **kwargs)\n+    if add_dim:\n+        point_features = point_features.squeeze(3)\n+\n+    return point_features\n+\n+\n+def pair_wise_dice_loss(inputs: Tensor, labels: Tensor) -> Tensor:\n+    \"\"\"\n+    A pair wise version of the dice loss, see `dice_loss` for usage.\n+\n+    Args:\n+        inputs (`torch.Tensor`):\n+            A tensor representing a mask\n+        labels (`torch.Tensor`):\n+            A tensor with the same shape as inputs. Stores the binary classification labels for each element in inputs\n+            (0 for the negative class and 1 for the positive class).\n+\n+    Returns:\n+        `torch.Tensor`: The computed loss between each pairs.\n+    \"\"\"\n+    inputs = inputs.sigmoid().flatten(1)\n+    numerator = 2 * torch.matmul(inputs, labels.T)\n+    # using broadcasting to get a [num_queries, NUM_CLASSES] matrix\n+    denominator = inputs.sum(-1)[:, None] + labels.sum(-1)[None, :]\n+    loss = 1 - (numerator + 1) / (denominator + 1)\n+    return loss\n+\n+\n+def pair_wise_sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor) -> torch.Tensor:\n+    r\"\"\"\n+    A pair wise version of the cross entropy loss, see `sigmoid_cross_entropy_loss` for usage.\n+\n+    Args:\n+        inputs (`torch.Tensor`):\n+            A tensor representing a mask.\n+        labels (`torch.Tensor`):\n+            A tensor with the same shape as inputs. Stores the binary classification labels for each element in inputs\n+            (0 for the negative class and 1 for the positive class).\n+\n+    Returns:\n+        loss (`torch.Tensor`): The computed loss between each pairs.\n+    \"\"\"\n+\n+    height_and_width = inputs.shape[1]\n+\n+    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n+    cross_entropy_loss_pos = criterion(inputs, torch.ones_like(inputs))\n+    cross_entropy_loss_neg = criterion(inputs, torch.zeros_like(inputs))\n+\n+    loss_pos = torch.matmul(cross_entropy_loss_pos / height_and_width, labels.T)\n+    loss_neg = torch.matmul(cross_entropy_loss_neg / height_and_width, (1 - labels).T)\n+    loss = loss_pos + loss_neg\n+    return loss\n+\n+\n+# Adapted from https://github.com/facebookresearch/Eomt/blob/main/eomt/modeling/matcher.py\n+class EomtHungarianMatcher(nn.Module):\n+    \"\"\"This class computes an assignment between the labels and the predictions of the network.\n+\n+    For efficiency reasons, the labels don't include the no_object. Because of this, in general, there are more\n+    predictions than labels. In this case, we do a 1-to-1 matching of the best predictions, while the others are\n+    un-matched (and thus treated as non-objects).\n+    \"\"\"\n+\n+    def __init__(\n+        self, cost_class: float = 1.0, cost_mask: float = 1.0, cost_dice: float = 1.0, num_points: int = 12544\n+    ):\n+        \"\"\"Creates the matcher\n+\n+        Params:\n+            cost_class (`float`, *optional*, defaults to 1.0):\n+                Relative weight of the classification error in the matching cost.\n+            cost_mask (`float`, *optional*,  defaults to 1.0):\n+                This is the relative weight of the focal loss of the binary mask in the matching cost.\n+            cost_dice (`float`, *optional*, defaults to 1.0):\n+                This is the relative weight of the dice loss of the binary mask in the matching cost.\n+            num_points (`int`, *optional*, defaults to 12544):\n+                No. of points to sample on which the mask loss will be calculated. The same set of K points are\n+                uniformly sampled for all prediction and ground truth masks to construct the cost matrix for bipartite\n+                matching.\n+        \"\"\"\n+        super().__init__()\n+        if cost_class == 0 and cost_mask == 0 and cost_dice == 0:\n+            raise ValueError(\"All costs can't be 0\")\n+\n+        self.num_points = num_points\n+        self.cost_class = cost_class\n+        self.cost_mask = cost_mask\n+        self.cost_dice = cost_dice\n+\n+    @torch.no_grad()\n+    def forward(\n+        self,\n+        masks_queries_logits: torch.Tensor,\n+        class_queries_logits: torch.Tensor,\n+        mask_labels: torch.Tensor,\n+        class_labels: torch.Tensor,\n+    ) -> list[tuple[Tensor]]:\n+        \"\"\"\n+        Params:\n+            masks_queries_logits (`torch.Tensor`):\n+                A tensor of dim `batch_size, num_queries, num_labels` with the classification logits.\n+            class_queries_logits (`torch.Tensor`):\n+                A tensor of dim `batch_size, num_queries, height, width` with the predicted masks.\n+            class_labels (`torch.Tensor`):\n+                A tensor of dim `num_target_boxes` (where num_target_boxes is the number of ground-truth objects in the\n+                target) containing the class labels.\n+            mask_labels (`torch.Tensor`):\n+                A tensor of dim `num_target_boxes, height, width` containing the target masks.\n+\n+        Returns:\n+            matched_indices (`list[tuple[Tensor]]`): A list of size batch_size, containing tuples of (index_i, index_j)\n+            where:\n+                - index_i is the indices of the selected predictions (in order)\n+                - index_j is the indices of the corresponding selected labels (in order)\n+            For each batch element, it holds:\n+                len(index_i) = len(index_j) = min(num_queries, num_target_boxes).\n+        \"\"\"\n+        indices: list[tuple[np.array]] = []\n+\n+        # iterate through batch size\n+        batch_size = masks_queries_logits.shape[0]\n+        for i in range(batch_size):\n+            pred_probs = class_queries_logits[i].softmax(-1)\n+            pred_mask = masks_queries_logits[i]\n+\n+            # Compute the classification cost. Contrary to the loss, we don't use the NLL, but approximate it in 1 - proba[target class]. The 1 is a constant that doesn't change the matching, it can be omitted.\n+            cost_class = -pred_probs[:, class_labels[i]]\n+            target_mask = mask_labels[i].to(pred_mask)\n+            target_mask = target_mask[:, None]\n+            pred_mask = pred_mask[:, None]\n+\n+            # Sample ground truth and predicted masks\n+            point_coordinates = torch.rand(1, self.num_points, 2, device=pred_mask.device)\n+\n+            target_coordinates = point_coordinates.repeat(target_mask.shape[0], 1, 1)\n+            target_mask = sample_point(target_mask, target_coordinates, align_corners=False).squeeze(1)\n+\n+            pred_coordinates = point_coordinates.repeat(pred_mask.shape[0], 1, 1)\n+            pred_mask = sample_point(pred_mask, pred_coordinates, align_corners=False).squeeze(1)\n+\n+            # compute the cross entropy loss between each mask pairs -> shape (num_queries, num_labels)\n+            cost_mask = pair_wise_sigmoid_cross_entropy_loss(pred_mask, target_mask)\n+            # Compute the dice loss between each mask pairs -> shape (num_queries, num_labels)\n+            cost_dice = pair_wise_dice_loss(pred_mask, target_mask)\n+            # final cost matrix\n+            cost_matrix = self.cost_mask * cost_mask + self.cost_class * cost_class + self.cost_dice * cost_dice\n+            # eliminate infinite values in cost_matrix to avoid the error ``ValueError: cost matrix is infeasible``\n+            cost_matrix = torch.minimum(cost_matrix, torch.tensor(1e10))\n+            cost_matrix = torch.maximum(cost_matrix, torch.tensor(-1e10))\n+            cost_matrix = torch.nan_to_num(cost_matrix, 0)\n+            # do the assignment using the hungarian algorithm in scipy\n+            assigned_indices: tuple[np.array] = linear_sum_assignment(cost_matrix.cpu())\n+            indices.append(assigned_indices)\n+\n+        # It could be stacked in one tensor\n+        matched_indices = [\n+            (torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices\n+        ]\n+        return matched_indices\n+\n+\n+def dice_loss(inputs: Tensor, labels: Tensor, num_masks: int) -> Tensor:\n+    r\"\"\"\n+    Compute the DICE loss, similar to generalized IOU for masks as follows:\n+\n+    $$ \\mathcal{L}_{\\text{dice}(x, y) = 1 - \\frac{2 * x \\cap y }{x \\cup y + 1}} $$\n+\n+    In practice, since `labels` is a binary mask, (only 0s and 1s), dice can be computed as follow\n+\n+    $$ \\mathcal{L}_{\\text{dice}(x, y) = 1 - \\frac{2 * x * y }{x + y + 1}} $$\n+\n+    Args:\n+        inputs (`torch.Tensor`):\n+            A tensor representing a mask.\n+        labels (`torch.Tensor`):\n+            A tensor with the same shape as inputs. Stores the binary classification labels for each element in inputs\n+            (0 for the negative class and 1 for the positive class).\n+        num_masks (`int`):\n+            The number of masks present in the current batch, used for normalization.\n+\n+    Returns:\n+        `torch.Tensor`: The computed loss.\n+    \"\"\"\n+    probs = inputs.sigmoid().flatten(1)\n+    numerator = 2 * (probs * labels).sum(-1)\n+    denominator = probs.sum(-1) + labels.sum(-1)\n+    loss = 1 - (numerator + 1) / (denominator + 1)\n+    loss = loss.sum() / num_masks\n+    return loss\n+\n+\n+def sigmoid_cross_entropy_loss(inputs: torch.Tensor, labels: torch.Tensor, num_masks: int) -> torch.Tensor:\n+    r\"\"\"\n+    Args:\n+        inputs (`torch.Tensor`):\n+            A float tensor of arbitrary shape.\n+        labels (`torch.Tensor`):\n+            A tensor with the same shape as inputs. Stores the binary classification labels for each element in inputs\n+            (0 for the negative class and 1 for the positive class).\n+\n+    Returns:\n+        loss (`torch.Tensor`): The computed loss.\n+    \"\"\"\n+    criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n+    cross_entropy_loss = criterion(inputs, labels)\n+\n+    loss = cross_entropy_loss.mean(1).sum() / num_masks\n+    return loss\n+\n+\n+# Adapted from https://github.com/facebookresearch/Eomt/blob/main/eomt/modeling/criterion.py\n+class EomtLoss(nn.Module):\n+    def __init__(self, config: EomtConfig, weight_dict: dict[str, float]):\n+        \"\"\"\n+        The Eomt Loss. The loss is computed very similar to DETR. The process happens in two steps: 1) we\n+        compute hungarian assignment between ground truth masks and the outputs of the model 2) we supervise each pair\n+        of matched ground-truth / prediction (supervise class and mask)\n+\n+        Args:\n+            config (`EomtConfig`):\n+                The configuration for Eomt model also containing loss calculation specific parameters.\n+            weight_dict (`dict[str, float]`):\n+                A dictionary of weights to be applied to the different losses.\n+        \"\"\"\n+        super().__init__()\n+        requires_backends(self, [\"scipy\"])\n+        self.num_labels = config.num_labels\n+        self.weight_dict = weight_dict\n+\n+        # Weight to apply to the null class\n+        self.eos_coef = config.no_object_weight\n+        empty_weight = torch.ones(self.num_labels + 1)\n+        empty_weight[-1] = self.eos_coef\n+        self.register_buffer(\"empty_weight\", empty_weight)\n+\n+        # pointwise mask loss parameters\n+        self.num_points = config.train_num_points\n+        self.oversample_ratio = config.oversample_ratio\n+        self.importance_sample_ratio = config.importance_sample_ratio\n+\n+        self.matcher = EomtHungarianMatcher(\n+            cost_class=config.class_weight,\n+            cost_dice=config.dice_weight,\n+            cost_mask=config.mask_weight,\n+            num_points=self.num_points,\n+        )\n+\n+    def _max_by_axis(self, sizes: list[list[int]]) -> list[int]:\n+        maxes = sizes[0]\n+        for sublist in sizes[1:]:\n+            for index, item in enumerate(sublist):\n+                maxes[index] = max(maxes[index], item)\n+        return maxes\n+\n+    # Adapted from nested_tensor_from_tensor_list() in original implementation\n+    def _pad_images_to_max_in_batch(self, tensors: list[Tensor]) -> tuple[Tensor, Tensor]:\n+        # get the maximum size in the batch\n+        max_size = self._max_by_axis([list(tensor.shape) for tensor in tensors])\n+        # compute final size\n+        batch_shape = [len(tensors)] + max_size\n+        batch_size, _, height, width = batch_shape\n+        dtype = tensors[0].dtype\n+        device = tensors[0].device\n+        padded_tensors = torch.zeros(batch_shape, dtype=dtype, device=device)\n+        padding_masks = torch.ones((batch_size, height, width), dtype=torch.bool, device=device)\n+        # pad the tensors to the size of the biggest one\n+        for tensor, padded_tensor, padding_mask in zip(tensors, padded_tensors, padding_masks):\n+            padded_tensor[: tensor.shape[0], : tensor.shape[1], : tensor.shape[2]].copy_(tensor)\n+            padding_mask[: tensor.shape[1], : tensor.shape[2]] = False\n+\n+        return padded_tensors, padding_masks\n+\n+    def loss_labels(\n+        self, class_queries_logits: Tensor, class_labels: list[Tensor], indices: tuple[np.array]\n+    ) -> dict[str, Tensor]:\n+        \"\"\"Compute the losses related to the labels using cross entropy.\n+\n+        Args:\n+            class_queries_logits (`torch.Tensor`):\n+                A tensor of shape `batch_size, num_queries, num_labels`\n+            class_labels (`list[torch.Tensor]`):\n+                List of class labels of shape `(labels)`.\n+            indices (`tuple[np.array])`:\n+                The indices computed by the Hungarian matcher.\n+\n+        Returns:\n+            `dict[str, Tensor]`: A dict of `torch.Tensor` containing the following key:\n+            - **loss_cross_entropy** -- The loss computed using cross entropy on the predicted and ground truth labels.\n+        \"\"\"\n+        pred_logits = class_queries_logits\n+        batch_size, num_queries, _ = pred_logits.shape\n+        criterion = nn.CrossEntropyLoss(weight=self.empty_weight)\n+        idx = self._get_predictions_permutation_indices(indices)  # shape of (batch_size, num_queries)\n+        target_classes_o = torch.cat(\n+            [target[j] for target, (_, j) in zip(class_labels, indices)]\n+        )  # shape of (batch_size, num_queries)\n+        target_classes = torch.full(\n+            (batch_size, num_queries), fill_value=self.num_labels, dtype=torch.int64, device=pred_logits.device\n+        )\n+        target_classes[idx] = target_classes_o\n+        # Permute target_classes (batch_size, num_queries, num_labels) -> (batch_size, num_labels, num_queries)\n+        pred_logits_transposed = pred_logits.transpose(1, 2)\n+        loss_ce = criterion(pred_logits_transposed, target_classes)\n+        losses = {\"loss_cross_entropy\": loss_ce}\n+        return losses\n+\n+    def loss_masks(\n+        self,\n+        masks_queries_logits: torch.Tensor,\n+        mask_labels: list[torch.Tensor],\n+        indices: tuple[np.array],\n+        num_masks: int,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"Compute the losses related to the masks using sigmoid_cross_entropy_loss and dice loss.\n+\n+        Args:\n+            masks_queries_logits (`torch.Tensor`):\n+                A tensor of shape `(batch_size, num_queries, height, width)`.\n+            mask_labels (`torch.Tensor`):\n+                List of mask labels of shape `(labels, height, width)`.\n+            indices (`tuple[np.array])`:\n+                The indices computed by the Hungarian matcher.\n+            num_masks (`int)`:\n+                The number of masks, used for normalization.\n+\n+        Returns:\n+            losses (`dict[str, Tensor]`): A dict of `torch.Tensor` containing two keys:\n+            - **loss_mask** -- The loss computed using sigmoid cross entropy loss on the predicted and ground truth.\n+              masks.\n+            - **loss_dice** -- The loss computed using dice loss on the predicted on the predicted and ground truth,\n+              masks.\n+        \"\"\"\n+        src_idx = self._get_predictions_permutation_indices(indices)\n+        tgt_idx = self._get_targets_permutation_indices(indices)\n+        # shape (batch_size * num_queries, height, width)\n+        pred_masks = masks_queries_logits[src_idx]\n+        # shape (batch_size, num_queries, height, width)\n+        # pad all and stack the targets to the num_labels dimension\n+        target_masks, _ = self._pad_images_to_max_in_batch(mask_labels)\n+        target_masks = target_masks[tgt_idx]\n+\n+        # No need to upsample predictions as we are using normalized coordinates\n+        pred_masks = pred_masks[:, None]\n+        target_masks = target_masks[:, None]\n+\n+        # Sample point coordinates\n+        with torch.no_grad():\n+            point_coordinates = self.sample_points_using_uncertainty(\n+                pred_masks,\n+                lambda logits: self.calculate_uncertainty(logits),\n+                self.num_points,\n+                self.oversample_ratio,\n+                self.importance_sample_ratio,\n+            )\n+\n+            point_labels = sample_point(target_masks, point_coordinates, align_corners=False).squeeze(1)\n+\n+        point_logits = sample_point(pred_masks, point_coordinates, align_corners=False).squeeze(1)\n+\n+        losses = {\n+            \"loss_mask\": sigmoid_cross_entropy_loss(point_logits, point_labels, num_masks),\n+            \"loss_dice\": dice_loss(point_logits, point_labels, num_masks),\n+        }\n+\n+        del pred_masks\n+        del target_masks\n+        return losses\n+\n+    def _get_predictions_permutation_indices(self, indices):\n+        # Permute predictions following indices\n+        batch_indices = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n+        predictions_indices = torch.cat([src for (src, _) in indices])\n+        return batch_indices, predictions_indices\n+\n+    def _get_targets_permutation_indices(self, indices):\n+        # Permute labels following indices\n+        batch_indices = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n+        target_indices = torch.cat([tgt for (_, tgt) in indices])\n+        return batch_indices, target_indices\n+\n+    def calculate_uncertainty(self, logits: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        In Eomt paper, uncertainty is estimated as L1 distance between 0.0 and the logit prediction in 'logits'\n+        for the foreground class in `classes`.\n+\n+        Args:\n+            logits (`torch.Tensor`):\n+            A tensor of shape (R, 1, ...) for class-specific or class-agnostic, where R is the total number of predicted masks in all images and C is:\n+            the number of foreground classes. The values are logits.\n+\n+        Returns:\n+            scores (`torch.Tensor`): A tensor of shape (R, 1, ...) that contains uncertainty scores with the most\n+            uncertain locations having the highest uncertainty score.\n+        \"\"\"\n+        uncertainty_scores = -(torch.abs(logits))\n+        return uncertainty_scores\n+\n+    def sample_points_using_uncertainty(\n+        self,\n+        logits: torch.Tensor,\n+        uncertainty_function,\n+        num_points: int,\n+        oversample_ratio: int,\n+        importance_sample_ratio: float,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        This function is meant for sampling points in [0, 1] * [0, 1] coordinate space based on their uncertainty. The\n+        uncertainty is calculated for each point using the passed `uncertainty function` that takes points logit\n+        prediction as input.\n+\n+        Args:\n+            logits (`float`):\n+                Logit predictions for P points.\n+            uncertainty_function:\n+                A function that takes logit predictions for P points and returns their uncertainties.\n+            num_points (`int`):\n+                The number of points P to sample.\n+            oversample_ratio (`int`):\n+                Oversampling parameter.\n+            importance_sample_ratio (`float`):\n+                Ratio of points that are sampled via importance sampling.\n+\n+        Returns:\n+            point_coordinates (`torch.Tensor`):\n+                Coordinates for P sampled points.\n+        \"\"\"\n+\n+        num_boxes = logits.shape[0]\n+        num_points_sampled = int(num_points * oversample_ratio)\n+\n+        # Get random point coordinates\n+        point_coordinates = torch.rand(num_boxes, num_points_sampled, 2, device=logits.device)\n+        # Get sampled prediction value for the point coordinates\n+        point_logits = sample_point(logits, point_coordinates, align_corners=False)\n+        # Calculate the uncertainties based on the sampled prediction values of the points\n+        point_uncertainties = uncertainty_function(point_logits)\n+\n+        num_uncertain_points = int(importance_sample_ratio * num_points)\n+        num_random_points = num_points - num_uncertain_points\n+\n+        idx = torch.topk(point_uncertainties[:, 0, :], k=num_uncertain_points, dim=1)[1]\n+        shift = num_points_sampled * torch.arange(num_boxes, dtype=torch.long, device=logits.device)\n+        idx += shift[:, None]\n+        point_coordinates = point_coordinates.view(-1, 2)[idx.view(-1), :].view(num_boxes, num_uncertain_points, 2)\n+\n+        if num_random_points > 0:\n+            point_coordinates = torch.cat(\n+                [point_coordinates, torch.rand(num_boxes, num_random_points, 2, device=logits.device)],\n+                dim=1,\n+            )\n+        return point_coordinates\n+\n+    def forward(\n+        self,\n+        masks_queries_logits: torch.Tensor,\n+        class_queries_logits: torch.Tensor,\n+        mask_labels: list[torch.Tensor],\n+        class_labels: list[torch.Tensor],\n+        auxiliary_predictions: Optional[dict[str, torch.Tensor]] = None,\n+    ) -> dict[str, torch.Tensor]:\n+        \"\"\"\n+        This performs the loss computation.\n+\n+        Args:\n+            masks_queries_logits (`torch.Tensor`):\n+                A tensor of shape `(batch_size, num_queries, height, width)`.\n+            class_queries_logits (`torch.Tensor`):\n+                A tensor of shape `(batch_size, num_queries, num_labels)`.\n+            mask_labels (`torch.Tensor`):\n+                List of mask labels of shape `(labels, height, width)`.\n+            class_labels (`list[torch.Tensor]`):\n+                List of class labels of shape `(labels)`.\n+            auxiliary_predictions (`dict[str, torch.Tensor]`, *optional*):\n+                if `use_auxiliary_loss` was set to `true` in [`EomtConfig`], then it contains the logits from\n+                the inner layers of the EomtMaskedAttentionDecoder.\n+\n+        Returns:\n+            losses (`dict[str, Tensor]`): A dict of `torch.Tensor` containing three keys:\n+            - **loss_cross_entropy** -- The loss computed using cross entropy on the predicted and ground truth labels.\n+            - **loss_mask** -- The loss computed using sigmoid cross_entropy loss on the predicted and ground truth\n+              masks.\n+            - **loss_dice** -- The loss computed using dice loss on the predicted on the predicted and ground truth\n+              masks.\n+            if `use_auxiliary_loss` was set to `true` in [`EomtConfig`], the dictionary contains additional\n+            losses for each auxiliary predictions.\n+        \"\"\"\n+\n+        # retrieve the matching between the outputs of the last layer and the labels\n+        indices = self.matcher(masks_queries_logits, class_queries_logits, mask_labels, class_labels)\n+        # compute the average number of target masks for normalization purposes\n+        num_masks = self.get_num_masks(class_labels, device=class_labels[0].device)\n+        # get all the losses\n+        losses: dict[str, Tensor] = {\n+            **self.loss_masks(masks_queries_logits, mask_labels, indices, num_masks),\n+            **self.loss_labels(class_queries_logits, class_labels, indices),\n+        }\n+        # in case of auxiliary losses, we repeat this process with the output of each intermediate layer.\n+        if auxiliary_predictions is not None:\n+            for idx, aux_outputs in enumerate(auxiliary_predictions):\n+                masks_queries_logits = aux_outputs[\"masks_queries_logits\"]\n+                class_queries_logits = aux_outputs[\"class_queries_logits\"]\n+                loss_dict = self.forward(masks_queries_logits, class_queries_logits, mask_labels, class_labels)\n+                loss_dict = {f\"{key}_{idx}\": value for key, value in loss_dict.items()}\n+                losses.update(loss_dict)\n+\n+        return losses\n+\n+    def get_num_masks(self, class_labels: torch.Tensor, device: torch.device) -> torch.Tensor:\n+        \"\"\"\n+        Computes the average number of target masks across the batch, for normalization purposes.\n+        \"\"\"\n+        num_masks = sum([len(classes) for classes in class_labels])\n+        num_masks = torch.as_tensor(num_masks, dtype=torch.float, device=device)\n+        world_size = 1\n+        if is_accelerate_available():\n+            if PartialState._shared_state != {}:\n+                num_masks = reduce(num_masks)\n+                world_size = PartialState().num_processes\n+\n+        num_masks = torch.clamp(num_masks / world_size, min=1)\n+        return num_masks\n+\n+\n+class EomtPatchEmbeddings(nn.Module):\n+    \"\"\"\n+    This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n+    `hidden_states` (patch embeddings) of shape `(batch_size, seq_length, hidden_size)` to be consumed by a\n+    Transformer.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        image_size, patch_size = config.image_size, config.patch_size\n+        num_channels, hidden_size = config.num_channels, config.hidden_size\n+\n+        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n+        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.num_patches = num_patches\n+\n+        self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        num_channels = pixel_values.shape[1]\n+        if num_channels != self.num_channels:\n+            raise ValueError(\n+                \"Make sure that the channel dimension of the pixel values match with the one set in the configuration.\"\n+                f\" Expected {self.num_channels} but got {num_channels}.\"\n+            )\n+        embeddings = self.projection(pixel_values).flatten(2).transpose(1, 2)\n+        return embeddings\n+\n+\n+class EomtEmbeddings(nn.Module):\n+    \"\"\"\n+    Construct the CLS token, mask token, position and patch embeddings.\n+    \"\"\"\n+\n+    def __init__(self, config: EomtConfig) -> None:\n+        super().__init__()\n+\n+        self.config = config\n+        self.patch_size = config.patch_size\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.zeros(1, config.num_register_tokens, config.hidden_size))\n+\n+        self.patch_embeddings = EomtPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.num_prefix_tokens = 1 + config.num_register_tokens  # 1 for [CLS]\n+        self.position_embeddings = nn.Embedding(num_patches, config.hidden_size)\n+        self.register_buffer(\"position_ids\", torch.arange(num_patches).expand((1, -1)), persistent=False)\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, _, _, _ = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        register_tokens = self.register_tokens.expand(batch_size, -1, -1)\n+\n+        embeddings = embeddings + self.position_embeddings(self.position_ids)\n+        embeddings = torch.cat([cls_tokens, register_tokens, embeddings], dim=1)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class EomtAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class EomtLayerScale(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        self.lambda1 = nn.Parameter(config.layerscale_value * torch.ones(config.hidden_size))\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        return hidden_state * self.lambda1\n+\n+\n+def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n+    \"\"\"\n+    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n+\n+    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n+    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n+    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n+    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n+    argument.\n+    \"\"\"\n+    if drop_prob == 0.0 or not training:\n+        return input\n+    keep_prob = 1 - drop_prob\n+    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n+    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n+    random_tensor.floor_()  # binarize\n+    output = input.div(keep_prob) * random_tensor\n+    return output\n+\n+\n+class EomtDropPath(nn.Module):\n+    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n+\n+    def __init__(self, drop_prob: Optional[float] = None) -> None:\n+        super().__init__()\n+        self.drop_prob = drop_prob\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        return drop_path(hidden_states, self.drop_prob, self.training)\n+\n+    def extra_repr(self) -> str:\n+        return f\"p={self.drop_prob}\"\n+\n+\n+class EomtMLP(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        self.fc1 = nn.Linear(in_features, hidden_features, bias=True)\n+        if isinstance(config.hidden_act, str):\n+            self.activation = ACT2FN[config.hidden_act]\n+        else:\n+            self.activation = config.hidden_act\n+        self.fc2 = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.fc1(hidden_state)\n+        hidden_state = self.activation(hidden_state)\n+        hidden_state = self.fc2(hidden_state)\n+        return hidden_state\n+\n+\n+class EomtSwiGLUFFN(nn.Module):\n+    def __init__(self, config) -> None:\n+        super().__init__()\n+        in_features = out_features = config.hidden_size\n+        hidden_features = int(config.hidden_size * config.mlp_ratio)\n+        hidden_features = (int(hidden_features * 2 / 3) + 7) // 8 * 8\n+\n+        self.weights_in = nn.Linear(in_features, 2 * hidden_features, bias=True)\n+        self.weights_out = nn.Linear(hidden_features, out_features, bias=True)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = self.weights_in(hidden_state)\n+        x1, x2 = hidden_state.chunk(2, dim=-1)\n+        hidden = nn.functional.silu(x1) * x2\n+        return self.weights_out(hidden)\n+\n+\n+class EomtLayer(GradientCheckpointingLayer):\n+    \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n+\n+    def __init__(self, config: EomtConfig) -> None:\n+        super().__init__()\n+\n+        self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.attention = EomtAttention(config)\n+        self.layer_scale1 = EomtLayerScale(config)\n+        self.drop_path = EomtDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n+\n+        self.norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        if config.use_swiglu_ffn:\n+            self.mlp = EomtSwiGLUFFN(config)\n+        else:\n+            self.mlp = EomtMLP(config)\n+        self.layer_scale2 = EomtLayerScale(config)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:\n+        self_attention_outputs = self.attention(\n+            self.norm1(hidden_states),  # in Eomt, layernorm is applied before self-attention\n+            head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        attention_output = self_attention_outputs[0]\n+\n+        attention_output = self.layer_scale1(attention_output)\n+        outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n+\n+        # first residual connection\n+        hidden_states = self.drop_path(attention_output) + hidden_states\n+\n+        # in Eomt, layernorm is also applied after self-attention\n+        layer_output = self.norm2(hidden_states)\n+        layer_output = self.mlp(layer_output)\n+        layer_output = self.layer_scale2(layer_output)\n+\n+        # second residual connection\n+        layer_output = self.drop_path(layer_output) + hidden_states\n+\n+        outputs = (layer_output,) + outputs\n+\n+        return outputs\n+\n+\n+class EomtLayerNorm2d(nn.LayerNorm):\n+    def __init__(self, num_channels, eps=1e-6, affine=True):\n+        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = hidden_state.permute(0, 2, 3, 1)\n+        hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)\n+        hidden_state = hidden_state.permute(0, 3, 1, 2)\n+        return hidden_state\n+\n+\n+class EomtScaleLayer(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.conv1 = nn.ConvTranspose2d(hidden_size, hidden_size, kernel_size=2, stride=2)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.conv2 = nn.Conv2d(\n+            hidden_size,\n+            hidden_size,\n+            kernel_size=3,\n+            padding=1,\n+            groups=hidden_size,\n+            bias=False,\n+        )\n+\n+        self.layernorm2d = EomtLayerNorm2d(hidden_size)\n+\n+    def forward(self, hidden_states: torch.tensor) -> torch.Tensor:\n+        hidden_states = self.conv1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layernorm2d(hidden_states)\n+        return hidden_states\n+\n+\n+class EomtScaleBlock(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+        self.num_blocks = config.num_upscale_blocks\n+        self.block = nn.ModuleList([EomtScaleLayer(config) for _ in range(self.num_blocks)])\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        for block in self.block:\n+            hidden_states = block(hidden_states)\n+        return hidden_states\n+\n+\n+class EomtMaskHead(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+\n+        hidden_size = config.hidden_size\n+        self.fc1 = nn.Linear(hidden_size, hidden_size)\n+        self.fc2 = nn.Linear(hidden_size, hidden_size)\n+        self.fc3 = nn.Linear(hidden_size, hidden_size)\n+        self.activation = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.activation(self.fc1(hidden_states))\n+        hidden_states = self.activation(self.fc2(hidden_states))\n+        hidden_states = self.fc3(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class EomtPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = EomtConfig\n+    base_model_prefix = \"eomt\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = False\n+    _no_split_modules = [\"EomtMLP\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n+                nn.init.uniform_(module.bias, -bound, bound)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=1)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, EomtLayerScale):\n+            if hasattr(module, \"lambda1\"):\n+                module.lambda1.data.fill_(self.config.layerscale_value)\n+        elif isinstance(module, EomtEmbeddings):\n+            module.cls_token.data = nn.init.trunc_normal_(\n+                module.cls_token.data.to(torch.float32), mean=0.0, std=std\n+            ).to(module.cls_token.dtype)\n+            module.register_tokens.data.zero_()\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The EoMT Model with head on top for instance/semantic/panoptic segmentation.\n+    \"\"\"\n+)\n+class EomtForUniversalSegmentation(EomtPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: EomtConfig) -> None:\n+        super().__init__(config)\n+        self.config = config\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.embeddings = EomtEmbeddings(config)\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.query = nn.Embedding(config.num_queries, config.hidden_size)\n+        self.layers = nn.ModuleList([EomtLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+        self.upscale_block = EomtScaleBlock(config)\n+        self.mask_head = EomtMaskHead(config)\n+\n+        self.class_predictor = nn.Linear(config.hidden_size, config.num_labels + 1)\n+\n+        self.grid_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+        self.weight_dict: dict[str, float] = {\n+            \"loss_cross_entropy\": config.class_weight,\n+            \"loss_mask\": config.mask_weight,\n+            \"loss_dice\": config.dice_weight,\n+        }\n+\n+        self.criterion = EomtLoss(config=config, weight_dict=self.weight_dict)\n+\n+        self.register_buffer(\"attn_mask_probs\", torch.ones(config.num_blocks))\n+\n+        self.post_init()\n+\n+    def get_loss_dict(\n+        self,\n+        masks_queries_logits: Tensor,\n+        class_queries_logits: Tensor,\n+        mask_labels: Tensor,\n+        class_labels: Tensor,\n+        auxiliary_predictions: dict[str, Tensor],\n+    ) -> dict[str, Tensor]:\n+        loss_dict: dict[str, Tensor] = self.criterion(\n+            masks_queries_logits=masks_queries_logits,\n+            class_queries_logits=class_queries_logits,\n+            mask_labels=mask_labels,\n+            class_labels=class_labels,\n+            auxiliary_predictions=auxiliary_predictions,\n+        )\n+\n+        # weight each loss by `self.weight_dict[<LOSS_NAME>]` including auxiliary losses\n+        for key, weight in self.weight_dict.items():\n+            for loss_key, loss in loss_dict.items():\n+                if key in loss_key:\n+                    loss *= weight\n+\n+        return loss_dict\n+\n+    def get_loss(self, loss_dict: dict[str, Tensor]) -> Tensor:\n+        return sum(loss_dict.values())\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: Tensor,\n+        mask_labels: Optional[list[Tensor]] = None,\n+        class_labels: Optional[list[Tensor]] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+    ) -> EomtForUniversalSegmentationOutput:\n+        r\"\"\"\n+        mask_labels (`List[torch.Tensor]`, *optional*):\n+            List of mask labels of shape `(num_labels, height, width)` to be fed to a model\n+        class_labels (`List[torch.LongTensor]`, *optional*):\n+            list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n+            labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n+        \"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        masks_queries_logits_per_layer, class_queries_logits_per_layer = (), ()\n+        attention_mask = None\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        for idx, layer_module in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if idx == self.num_hidden_layers - self.config.num_blocks:\n+                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1)\n+                hidden_states = torch.cat((query, hidden_states), dim=1)\n+\n+            if idx >= self.num_hidden_layers - self.config.num_blocks and (\n+                self.training or self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks] > 0\n+            ):\n+                norm_hidden_states = self.layernorm(hidden_states)\n+                masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)\n+\n+                masks_queries_logits_per_layer += (masks_queries_logits,)\n+                class_queries_logits_per_layer += (class_queries_logits,)\n+\n+                attention_mask = torch.ones(\n+                    hidden_states.shape[0],\n+                    hidden_states.shape[1],\n+                    hidden_states.shape[1],\n+                    device=hidden_states.device,\n+                    dtype=torch.bool,\n+                )\n+\n+                interpolated_logits = F.interpolate(masks_queries_logits, size=self.grid_size, mode=\"bilinear\")\n+                interpolated_logits = interpolated_logits.view(\n+                    interpolated_logits.size(0), interpolated_logits.size(1), -1\n+                )\n+\n+                num_query_tokens = self.config.num_queries\n+                encoder_start_tokens = num_query_tokens + self.embeddings.num_prefix_tokens\n+\n+                # Set attention mask for queries to focus on encoder tokens based on interpolated logits\n+                attention_mask[:, :num_query_tokens, encoder_start_tokens:] = interpolated_logits > 0\n+\n+                # Disable attention mask for random query tokens.\n+                attention_mask = self._disable_attention_mask(\n+                    attention_mask,\n+                    prob=self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks],\n+                    num_query_tokens=num_query_tokens,\n+                    encoder_start_tokens=encoder_start_tokens,\n+                    device=attention_mask.device,\n+                )\n+\n+                # Expand attention mask to 4d mask.\n+                attention_mask = attention_mask[:, None, ...].expand(-1, self.config.num_attention_heads, -1, -1)\n+                attention_mask = attention_mask.float().masked_fill(~attention_mask, -1e9)\n+\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions += (layer_outputs[1],)\n+\n+        sequence_output = self.layernorm(hidden_states)\n+        if output_hidden_states:\n+            all_hidden_states += (sequence_output,)\n+\n+        masks_queries_logits, class_queries_logits = self.predict(sequence_output)\n+        masks_queries_logits_per_layer += (masks_queries_logits,)\n+        class_queries_logits_per_layer += (class_queries_logits,)\n+\n+        loss = None\n+        if mask_labels is not None and class_labels is not None:\n+            loss = 0.0\n+            for masks_queries_logits, class_queries_logits in zip(\n+                masks_queries_logits_per_layer, class_queries_logits_per_layer\n+            ):\n+                loss_dict = self.get_loss_dict(\n+                    masks_queries_logits=masks_queries_logits,\n+                    class_queries_logits=class_queries_logits,\n+                    mask_labels=mask_labels,\n+                    class_labels=class_labels,\n+                    auxiliary_predictions=None,\n+                )\n+                loss += self.get_loss(loss_dict)\n+\n+        return EomtForUniversalSegmentationOutput(\n+            loss=loss,\n+            masks_queries_logits=masks_queries_logits,\n+            class_queries_logits=class_queries_logits,\n+            last_hidden_state=sequence_output,\n+            hidden_states=all_hidden_states,\n+            attentions=all_attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    def predict(self, logits: torch.Tensor):\n+        query_tokens = logits[:, : self.config.num_queries, :]\n+        class_logits = self.class_predictor(query_tokens)\n+\n+        prefix_tokens = logits[:, self.config.num_queries + self.embeddings.num_prefix_tokens :, :]\n+        prefix_tokens = prefix_tokens.transpose(1, 2)\n+\n+        prefix_tokens = prefix_tokens.reshape(prefix_tokens.shape[0], -1, *self.grid_size)\n+\n+        query_tokens = self.mask_head(query_tokens)\n+        prefix_tokens = self.upscale_block(prefix_tokens)\n+\n+        mask_logits = torch.einsum(\"bqc, bchw -> bqhw\", query_tokens, prefix_tokens)\n+\n+        return mask_logits, class_logits\n+\n+    @staticmethod\n+    def _disable_attention_mask(attn_mask, prob, num_query_tokens, encoder_start_tokens, device):\n+        if prob < 1:\n+            # Generate random queries to disable based on the probs\n+            random_queries = torch.rand(attn_mask.shape[0], num_query_tokens, device=device) > prob\n+\n+            # Disable attention to the query tokens, considering the prefix tokens\n+            attn_mask[:, :num_query_tokens, encoder_start_tokens:][random_queries] = 1\n+\n+        return attn_mask\n+\n+\n+__all__ = [\"EomtPreTrainedModel\", \"EomtForUniversalSegmentation\"]"
        },
        {
            "sha": "fc82836e4bee5a86cd0c67b9c2760e7bce298171",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "added",
            "additions": 588,
            "deletions": 0,
            "changes": 588,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,588 @@\n+# coding=utf-8\n+# Copyright 2025 Mobile Perception Systems Lab at TU/e and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch EoMT model.\"\"\"\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+\n+from ...activations import ACT2FN\n+from ...file_utils import (\n+    ModelOutput,\n+)\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n+from ..dinov2.modeling_dinov2 import (\n+    Dinov2Embeddings,\n+    Dinov2Layer,\n+    Dinov2LayerScale,\n+    Dinov2PatchEmbeddings,\n+)\n+from ..mask2former.modeling_mask2former import Mask2FormerForUniversalSegmentation, Mask2FormerLoss\n+from ..siglip.modeling_siglip import SiglipAttention\n+from ..vit.configuration_vit import ViTConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class EomtConfig(ViTConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`EomtForUniversalSegmentation`]. It is used to instantiate an EoMT model\n+    according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the EoMT\n+    [tue-mps/coco_panoptic_eomt_large_640](https://huggingface.co/tue-mps/coco_panoptic_eomt_large_640)\n+    architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimensionality of the hidden representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads in each attention layer.\n+        mlp_ratio (`int`, *optional*, defaults to 4):\n+            Ratio of the MLP hidden dimensionality to the hidden size.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder.\n+        hidden_dropout_prob (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings and encoder.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        image_size (`int`, *optional*, defaults to 640):\n+            The size (resolution) of each input image.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        layerscale_value (`float`, *optional*, defaults to 1.0):\n+            Initial value for the LayerScale parameter.\n+        drop_path_rate (`float`, *optional*, defaults to 0.0):\n+            The stochastic depth rate (drop path) used during training.\n+        num_upscale_blocks (`int`, *optional*, defaults to 2):\n+            Number of upsampling blocks used in the decoder or segmentation head.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability applied after attention projection.\n+        use_swiglu_ffn (`bool`, *optional*, defaults to `False`):\n+            Whether to use the SwiGLU feedforward neural network.\n+        num_blocks (`int`, *optional*, defaults to 4):\n+            Number of feature blocks or stages in the architecture.\n+        no_object_weight (`float`, *optional*, defaults to 0.1):\n+            Loss weight for the 'no object' class in panoptic/instance segmentation.\n+        class_weight (`float`, *optional*, defaults to 2.0):\n+            Loss weight for classification targets.\n+        mask_weight (`float`, *optional*, defaults to 5.0):\n+            Loss weight for mask prediction.\n+        dice_weight (`float`, *optional*, defaults to 5.0):\n+            Loss weight for the dice loss component.\n+        train_num_points (`int`, *optional*, defaults to 12544):\n+            Number of points to sample for mask loss computation during training.\n+        oversample_ratio (`float`, *optional*, defaults to 3.0):\n+            Oversampling ratio used in point sampling for mask training.\n+        importance_sample_ratio (`float`, *optional*, defaults to 0.75):\n+            Ratio of points to sample based on importance during training.\n+        num_queries (`int`, *optional*, defaults to 200):\n+            Number of object queries in the Transformer.\n+        num_register_tokens (`int`, *optional*, defaults to 4):\n+            Number of learnable register tokens added to the transformer input.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import EomtConfig, EomtForUniversalSegmentation\n+\n+    >>> # Initialize configuration\n+    >>> config = EomtConfig()\n+\n+    >>> # Initialize model\n+    >>> model = EomtForUniversalSegmentation(config)\n+\n+    >>> # Access config\n+    >>> config = model.config\n+    ```\"\"\"\n+\n+    model_type = \"eomt\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        mlp_ratio=4,\n+        hidden_act=\"gelu\",\n+        hidden_dropout_prob=0.0,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-6,\n+        image_size=640,\n+        patch_size=16,\n+        num_channels=3,\n+        layerscale_value=1.0,\n+        drop_path_rate=0.0,\n+        num_upscale_blocks=2,\n+        attention_dropout=0.0,\n+        use_swiglu_ffn=False,\n+        num_blocks=4,\n+        no_object_weight: float = 0.1,\n+        class_weight: float = 2.0,\n+        mask_weight: float = 5.0,\n+        dice_weight: float = 5.0,\n+        train_num_points: int = 12544,\n+        oversample_ratio: float = 3.0,\n+        importance_sample_ratio: float = 0.75,\n+        num_queries=200,\n+        num_register_tokens=4,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            hidden_size=hidden_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            hidden_dropout_prob=hidden_dropout_prob,\n+            hidden_act=hidden_act,\n+            initializer_range=initializer_range,\n+            layer_norm_eps=layer_norm_eps,\n+            image_size=image_size,\n+            patch_size=patch_size,\n+            num_channels=num_channels,\n+            **kwargs,\n+        )\n+\n+        del self.intermediate_size\n+        del self.qkv_bias\n+        del self.pooler_act\n+        del self.pooler_output_size\n+        del self.encoder_stride\n+        del self.attention_probs_dropout_prob\n+\n+        self.mlp_ratio = mlp_ratio\n+        self.attention_dropout = attention_dropout\n+        self.layerscale_value = layerscale_value\n+        self.drop_path_rate = drop_path_rate\n+        self.num_upscale_blocks = num_upscale_blocks\n+        self.use_swiglu_ffn = use_swiglu_ffn\n+        self.num_blocks = num_blocks\n+        self.no_object_weight = no_object_weight\n+        self.class_weight = class_weight\n+        self.mask_weight = mask_weight\n+        self.dice_weight = dice_weight\n+        self.train_num_points = train_num_points\n+        self.oversample_ratio = oversample_ratio\n+        self.importance_sample_ratio = importance_sample_ratio\n+        self.num_queries = num_queries\n+        self.num_register_tokens = num_register_tokens\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Class for outputs of [`EomtForUniversalSegmentationOutput`].\n+\n+    This output can be directly passed to [`~EomtImageProcessor.post_process_semantic_segmentation`] or\n+    [`~EomtImageProcessor.post_process_instance_segmentation`] or\n+    [`~EomtImageProcessor.post_process_panoptic_segmentation`] to compute final segmentation maps. Please, see\n+    [`~EomtImageProcessor] for details regarding usage.\n+    \"\"\"\n+)\n+class EomtForUniversalSegmentationOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.Tensor`, *optional*):\n+        The computed loss, returned when labels are present.\n+    class_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, num_labels + 1)` representing the proposed classes for each\n+        query. Note the `+ 1` is needed because we incorporate the null class.\n+    masks_queries_logits (`torch.FloatTensor`):\n+        A tensor of shape `(batch_size, num_queries, height, width)` representing the proposed masks for each\n+        query.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+        Last hidden states (final feature map) of the last layer.\n+    hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+        Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+        shape `(batch_size, sequence_length, hidden_size)`. Hidden-states all layers of the model.\n+    attentions (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `tuple(torch.FloatTensor)` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Self and Cross Attentions weights from transformer decoder.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    class_queries_logits: Optional[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+\n+\n+class EomtLoss(Mask2FormerLoss):\n+    pass\n+\n+\n+class EomtPatchEmbeddings(Dinov2PatchEmbeddings):\n+    pass\n+\n+\n+class EomtEmbeddings(Dinov2Embeddings, nn.Module):\n+    def __init__(self, config: EomtConfig) -> None:\n+        Dinov2Embeddings().__init__()\n+\n+        self.config = config\n+        self.patch_size = config.patch_size\n+\n+        self.cls_token = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.register_tokens = nn.Parameter(torch.zeros(1, config.num_register_tokens, config.hidden_size))\n+\n+        self.patch_embeddings = EomtPatchEmbeddings(config)\n+        num_patches = self.patch_embeddings.num_patches\n+        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.num_prefix_tokens = 1 + config.num_register_tokens  # 1 for [CLS]\n+        self.position_embeddings = nn.Embedding(num_patches, config.hidden_size)\n+        self.register_buffer(\"position_ids\", torch.arange(num_patches).expand((1, -1)), persistent=False)\n+\n+    def interpolate_pos_encoding(self):\n+        raise AttributeError(\"Not needed for Eomt Model\")\n+\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n+        batch_size, _, _, _ = pixel_values.shape\n+        target_dtype = self.patch_embeddings.projection.weight.dtype\n+        embeddings = self.patch_embeddings(pixel_values.to(dtype=target_dtype))\n+\n+        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n+        register_tokens = self.register_tokens.expand(batch_size, -1, -1)\n+\n+        embeddings = embeddings + self.position_embeddings(self.position_ids)\n+        embeddings = torch.cat([cls_tokens, register_tokens, embeddings], dim=1)\n+\n+        embeddings = self.dropout(embeddings)\n+\n+        return embeddings\n+\n+\n+class EomtAttention(SiglipAttention):\n+    pass\n+\n+\n+class EomtLayerScale(Dinov2LayerScale):\n+    pass\n+\n+\n+class EomtLayer(Dinov2Layer):\n+    pass\n+\n+\n+class EomtLayerNorm2d(nn.LayerNorm):\n+    def __init__(self, num_channels, eps=1e-6, affine=True):\n+        super().__init__(num_channels, eps=eps, elementwise_affine=affine)\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        hidden_state = hidden_state.permute(0, 2, 3, 1)\n+        hidden_state = F.layer_norm(hidden_state, self.normalized_shape, self.weight, self.bias, self.eps)\n+        hidden_state = hidden_state.permute(0, 3, 1, 2)\n+        return hidden_state\n+\n+\n+class EomtScaleLayer(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+        hidden_size = config.hidden_size\n+        self.conv1 = nn.ConvTranspose2d(hidden_size, hidden_size, kernel_size=2, stride=2)\n+        self.activation = ACT2FN[config.hidden_act]\n+        self.conv2 = nn.Conv2d(\n+            hidden_size,\n+            hidden_size,\n+            kernel_size=3,\n+            padding=1,\n+            groups=hidden_size,\n+            bias=False,\n+        )\n+\n+        self.layernorm2d = EomtLayerNorm2d(hidden_size)\n+\n+    def forward(self, hidden_states: torch.tensor) -> torch.Tensor:\n+        hidden_states = self.conv1(hidden_states)\n+        hidden_states = self.activation(hidden_states)\n+        hidden_states = self.conv2(hidden_states)\n+        hidden_states = self.layernorm2d(hidden_states)\n+        return hidden_states\n+\n+\n+class EomtScaleBlock(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+        self.num_blocks = config.num_upscale_blocks\n+        self.block = nn.ModuleList([EomtScaleLayer(config) for _ in range(self.num_blocks)])\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        for block in self.block:\n+            hidden_states = block(hidden_states)\n+        return hidden_states\n+\n+\n+class EomtMaskHead(nn.Module):\n+    def __init__(self, config: EomtConfig):\n+        super().__init__()\n+\n+        hidden_size = config.hidden_size\n+        self.fc1 = nn.Linear(hidden_size, hidden_size)\n+        self.fc2 = nn.Linear(hidden_size, hidden_size)\n+        self.fc3 = nn.Linear(hidden_size, hidden_size)\n+        self.activation = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.activation(self.fc1(hidden_states))\n+        hidden_states = self.activation(self.fc2(hidden_states))\n+        hidden_states = self.fc3(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class EomtPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = EomtConfig\n+    base_model_prefix = \"eomt\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = False\n+    _no_split_modules = [\"EomtMLP\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n+\n+    def _init_weights(self, module: nn.Module) -> None:\n+        std = self.config.initializer_range\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.ConvTranspose2d)):\n+            nn.init.kaiming_uniform_(module.weight, a=math.sqrt(5))\n+            if module.bias is not None:\n+                fan_in, _ = nn.init._calculate_fan_in_and_fan_out(module.weight)\n+                bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0\n+                nn.init.uniform_(module.bias, -bound, bound)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=1)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, EomtLayerScale):\n+            if hasattr(module, \"lambda1\"):\n+                module.lambda1.data.fill_(self.config.layerscale_value)\n+        elif isinstance(module, EomtEmbeddings):\n+            module.cls_token.data = nn.init.trunc_normal_(\n+                module.cls_token.data.to(torch.float32), mean=0.0, std=std\n+            ).to(module.cls_token.dtype)\n+            module.register_tokens.data.zero_()\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The EoMT Model with head on top for instance/semantic/panoptic segmentation.\n+    \"\"\"\n+)\n+class EomtForUniversalSegmentation(Mask2FormerForUniversalSegmentation, nn.Module):\n+    def __init__(self, config: EomtConfig) -> None:\n+        nn.Module().__init__(config)\n+        self.config = config\n+        self.num_hidden_layers = config.num_hidden_layers\n+        self.embeddings = EomtEmbeddings(config)\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+\n+        self.query = nn.Embedding(config.num_queries, config.hidden_size)\n+        self.layers = nn.ModuleList([EomtLayer(config) for _ in range(config.num_hidden_layers)])\n+\n+        self.upscale_block = EomtScaleBlock(config)\n+        self.mask_head = EomtMaskHead(config)\n+\n+        self.class_predictor = nn.Linear(config.hidden_size, config.num_labels + 1)\n+\n+        self.grid_size = (config.image_size // config.patch_size, config.image_size // config.patch_size)\n+        self.weight_dict: dict[str, float] = {\n+            \"loss_cross_entropy\": config.class_weight,\n+            \"loss_mask\": config.mask_weight,\n+            \"loss_dice\": config.dice_weight,\n+        }\n+\n+        self.criterion = EomtLoss(config=config, weight_dict=self.weight_dict)\n+\n+        self.register_buffer(\"attn_mask_probs\", torch.ones(config.num_blocks))\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings.patch_embeddings\n+\n+    def get_auxiliary_logits(self):\n+        raise AttributeError(\"Note needed for Eomt Model.\")\n+\n+    def predict(self, logits: torch.Tensor):\n+        query_tokens = logits[:, : self.config.num_queries, :]\n+        class_logits = self.class_predictor(query_tokens)\n+\n+        prefix_tokens = logits[:, self.config.num_queries + self.embeddings.num_prefix_tokens :, :]\n+        prefix_tokens = prefix_tokens.transpose(1, 2)\n+\n+        prefix_tokens = prefix_tokens.reshape(prefix_tokens.shape[0], -1, *self.grid_size)\n+\n+        query_tokens = self.mask_head(query_tokens)\n+        prefix_tokens = self.upscale_block(prefix_tokens)\n+\n+        mask_logits = torch.einsum(\"bqc, bchw -> bqhw\", query_tokens, prefix_tokens)\n+\n+        return mask_logits, class_logits\n+\n+    @staticmethod\n+    def _disable_attention_mask(attn_mask, prob, num_query_tokens, encoder_start_tokens, device):\n+        if prob < 1:\n+            # Generate random queries to disable based on the probs\n+            random_queries = torch.rand(attn_mask.shape[0], num_query_tokens, device=device) > prob\n+\n+            # Disable attention to the query tokens, considering the prefix tokens\n+            attn_mask[:, :num_query_tokens, encoder_start_tokens:][random_queries] = 1\n+\n+        return attn_mask\n+\n+    @auto_docstring\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        pixel_values: Tensor,\n+        mask_labels: Optional[list[Tensor]] = None,\n+        class_labels: Optional[list[Tensor]] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+    ):\n+        r\"\"\"\n+        mask_labels (`List[torch.Tensor]`, *optional*):\n+            List of mask labels of shape `(num_labels, height, width)` to be fed to a model\n+        class_labels (`List[torch.LongTensor]`, *optional*):\n+            list of target class labels of shape `(num_labels, height, width)` to be fed to a model. They identify the\n+            labels of `mask_labels`, e.g. the label of `mask_labels[i][j]` if `class_labels[i][j]`.\n+        \"\"\"\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+\n+        all_hidden_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        masks_queries_logits_per_layer, class_queries_logits_per_layer = (), ()\n+        attention_mask = None\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values)\n+\n+        for idx, layer_module in enumerate(self.layers):\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            if idx == self.num_hidden_layers - self.config.num_blocks:\n+                query = self.query.weight[None, :, :].expand(hidden_states.shape[0], -1, -1)\n+                hidden_states = torch.cat((query, hidden_states), dim=1)\n+\n+            if idx >= self.num_hidden_layers - self.config.num_blocks and (\n+                self.training or self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks] > 0\n+            ):\n+                norm_hidden_states = self.layernorm(hidden_states)\n+                masks_queries_logits, class_queries_logits = self.predict(norm_hidden_states)\n+\n+                masks_queries_logits_per_layer += (masks_queries_logits,)\n+                class_queries_logits_per_layer += (class_queries_logits,)\n+\n+                attention_mask = torch.ones(\n+                    hidden_states.shape[0],\n+                    hidden_states.shape[1],\n+                    hidden_states.shape[1],\n+                    device=hidden_states.device,\n+                    dtype=torch.bool,\n+                )\n+\n+                interpolated_logits = F.interpolate(masks_queries_logits, size=self.grid_size, mode=\"bilinear\")\n+                interpolated_logits = interpolated_logits.view(\n+                    interpolated_logits.size(0), interpolated_logits.size(1), -1\n+                )\n+\n+                num_query_tokens = self.config.num_queries\n+                encoder_start_tokens = num_query_tokens + self.embeddings.num_prefix_tokens\n+\n+                # Set attention mask for queries to focus on encoder tokens based on interpolated logits\n+                attention_mask[:, :num_query_tokens, encoder_start_tokens:] = interpolated_logits > 0\n+\n+                # Disable attention mask for random query tokens.\n+                attention_mask = self._disable_attention_mask(\n+                    attention_mask,\n+                    prob=self.attn_mask_probs[idx - self.num_hidden_layers + self.config.num_blocks],\n+                    num_query_tokens=num_query_tokens,\n+                    encoder_start_tokens=encoder_start_tokens,\n+                    device=attention_mask.device,\n+                )\n+\n+                # Expand attention mask to 4d mask.\n+                attention_mask = attention_mask[:, None, ...].expand(-1, self.config.num_attention_heads, -1, -1)\n+                attention_mask = attention_mask.float().masked_fill(~attention_mask, -1e9)\n+\n+            layer_outputs = layer_module(hidden_states, attention_mask, output_attentions)\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions += (layer_outputs[1],)\n+\n+        sequence_output = self.layernorm(hidden_states)\n+        if output_hidden_states:\n+            all_hidden_states += (sequence_output,)\n+\n+        masks_queries_logits, class_queries_logits = self.predict(sequence_output)\n+        masks_queries_logits_per_layer += (masks_queries_logits,)\n+        class_queries_logits_per_layer += (class_queries_logits,)\n+\n+        loss = None\n+        if mask_labels is not None and class_labels is not None:\n+            loss = 0.0\n+            for masks_queries_logits, class_queries_logits in zip(\n+                masks_queries_logits_per_layer, class_queries_logits_per_layer\n+            ):\n+                loss_dict = self.get_loss_dict(\n+                    masks_queries_logits=masks_queries_logits,\n+                    class_queries_logits=class_queries_logits,\n+                    mask_labels=mask_labels,\n+                    class_labels=class_labels,\n+                    auxiliary_predictions=None,\n+                )\n+                loss += self.get_loss(loss_dict)\n+\n+        return EomtForUniversalSegmentationOutput(\n+            loss=loss,\n+            masks_queries_logits=masks_queries_logits,\n+            class_queries_logits=class_queries_logits,\n+            last_hidden_state=sequence_output,\n+            hidden_states=all_hidden_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+__all__ = [\"EomtConfig\", \"EomtPreTrainedModel\", \"EomtForUniversalSegmentation\"]"
        },
        {
            "sha": "fbcf33f48600db37c62b1cd57a25142575179b28",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -512,7 +512,7 @@ def __init__(self, config: Mask2FormerConfig, weight_dict: dict[str, float]):\n         self.importance_sample_ratio = config.importance_sample_ratio\n \n         self.matcher = Mask2FormerHungarianMatcher(\n-            cost_class=1.0,\n+            cost_class=config.class_weight,\n             cost_dice=config.dice_weight,\n             cost_mask=config.mask_weight,\n             num_points=self.num_points,"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/eomt/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2F__init__.py?ref=1750c518dda15a8b81cff276292674d61152dbf5"
        },
        {
            "sha": "6d449453de6c90ae018574e3ae632635fc8b4dda",
            "filename": "tests/models/eomt/test_image_processing_eomt.py",
            "status": "added",
            "additions": 308,
            "deletions": 0,
            "changes": 308,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,308 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch EoMT Image Processor.\"\"\"\n+\n+import unittest\n+\n+import numpy as np\n+import requests\n+from datasets import load_dataset\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import EomtImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import EomtImageProcessorFast\n+    from transformers.models.eomt.modeling_eomt import EomtForUniversalSegmentationOutput\n+\n+\n+class EomtImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        min_resolution=30,\n+        max_resolution=400,\n+        size=None,\n+        do_resize=True,\n+        do_pad=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        num_labels=10,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.do_pad = do_pad\n+        self.size = size if size is not None else {\"shortest_edge\": 18, \"longest_edge\": 18}\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        # for the post_process_functions\n+        self.batch_size = 2\n+        self.num_queries = 3\n+        self.num_classes = 2\n+        self.height = 18\n+        self.width = 18\n+        self.num_labels = num_labels\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_pad\": self.do_pad,\n+            \"num_labels\": self.num_labels,\n+        }\n+\n+    def prepare_fake_eomt_outputs(self, batch_size):\n+        return EomtForUniversalSegmentationOutput(\n+            masks_queries_logits=torch.randn((batch_size, self.num_queries, self.height, self.width)),\n+            class_queries_logits=torch.randn((batch_size, self.num_queries, self.num_classes + 1)),\n+        )\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+def prepare_semantic_single_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    example = ds[0]\n+    return example[\"image\"], example[\"map\"]\n+\n+\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\")\n+    return list(ds[\"image\"][:2]), list(ds[\"map\"][:2])\n+\n+\n+@require_torch\n+@require_vision\n+class EomtImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = EomtImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = EomtImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = EomtImageProcessingTester(self)\n+        self.model_id = \"tue-mps/coco_panoptic_eomt_large_640\"\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 18})\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (2, 3, 18, 18)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    @unittest.skip(reason=\"Not supported\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_call_pil(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+        for image in image_inputs:\n+            self.assertIsInstance(image, Image.Image)\n+\n+        # Test Non batched input\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 3, 18, 18)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        # Test batched\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (2, 3, 18, 18)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        for image in image_inputs:\n+            self.assertIsInstance(image, torch.Tensor)\n+\n+        encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (1, 3, 18, 18)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+        encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = (2, 3, 18, 18)\n+        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image, dummy_map = prepare_semantic_single_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        image_encoding_slow = image_processor_slow(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+        image_encoding_fast = image_processor_fast(dummy_image, segmentation_maps=dummy_map, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(image_encoding_slow.pixel_values, image_encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(image_encoding_slow.pixel_values - image_encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+        # Lets check whether 99.9% of mask_labels values match or not.\n+        match_ratio = (image_encoding_slow.mask_labels[0] == image_encoding_fast.mask_labels[0]).float().mean().item()\n+        self.assertGreaterEqual(match_ratio, 0.999, \"Mask labels do not match between slow and fast image processor.\")\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images, dummy_maps = prepare_semantic_batch_inputs()\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, segmentation_maps=dummy_maps, return_tensors=\"pt\")\n+\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n+\n+        for idx in range(len(dummy_maps)):\n+            match_ratio = (encoding_slow.mask_labels[idx] == encoding_fast.mask_labels[idx]).float().mean().item()\n+            self.assertGreaterEqual(\n+                match_ratio, 0.999, \"Mask labels do not match between slow and fast image processors.\"\n+            )\n+\n+    def test_post_process_semantic_segmentation(self):\n+        processor = self.image_processing_class(**self.image_processor_dict)\n+        # Set longest_edge to None to test for semantic segmentatiom.\n+        processor.size = {\"shortest_edge\": 18, \"longest_edge\": None}\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, do_split_image=True, return_tensors=\"pt\")\n+        patch_offsets = inputs.pop(\"patch_offsets\")\n+\n+        original_sizes = [image.size[::-1]]\n+\n+        # For semantic segmentation, the BS of output is 2 coz, two patches are created for the image.\n+        outputs = self.image_processor_tester.prepare_fake_eomt_outputs(inputs[\"pixel_values\"].shape[0])\n+        segmentation = processor.post_process_semantic_segmentation(outputs, patch_offsets, original_sizes)\n+\n+        self.assertEqual(segmentation[0].shape, (image.height, image.width))\n+\n+    def test_post_process_panoptic_segmentation(self):\n+        processor = self.image_processing_class(**self.image_processor_dict)\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        original_sizes = [image.size[::-1], image.size[::-1]]\n+\n+        # lets test for batched input of 2\n+        outputs = self.image_processor_tester.prepare_fake_eomt_outputs(2)\n+        segmentation = processor.post_process_panoptic_segmentation(outputs, original_sizes)\n+\n+        self.assertTrue(len(segmentation) == 2)\n+        for el in segmentation:\n+            self.assertTrue(\"segmentation\" in el)\n+            self.assertTrue(\"segments_info\" in el)\n+            self.assertEqual(type(el[\"segments_info\"]), list)\n+            self.assertEqual(el[\"segmentation\"].shape, (image.height, image.width))\n+\n+    def test_post_process_instance_segmentation(self):\n+        processor = self.image_processing_class(**self.image_processor_dict)\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        original_sizes = [image.size[::-1], image.size[::-1]]\n+\n+        # lets test for batched input of 2\n+        outputs = self.image_processor_tester.prepare_fake_eomt_outputs(2)\n+        segmentation = processor.post_process_instance_segmentation(outputs, original_sizes)\n+\n+        self.assertTrue(len(segmentation) == 2)\n+        for el in segmentation:\n+            self.assertTrue(\"segmentation\" in el)\n+            self.assertTrue(\"segments_info\" in el)\n+            self.assertEqual(type(el[\"segments_info\"]), list)\n+            self.assertEqual(el[\"segmentation\"].shape, (image.height, image.width))"
        },
        {
            "sha": "c52603025061cd8c1fa1af7802c9811bdbc7a3ba",
            "filename": "tests/models/eomt/test_modeling_eomt.py",
            "status": "added",
            "additions": 475,
            "deletions": 0,
            "changes": 475,
            "blob_url": "https://github.com/huggingface/transformers/blob/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1750c518dda15a8b81cff276292674d61152dbf5/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_modeling_eomt.py?ref=1750c518dda15a8b81cff276292674d61152dbf5",
            "patch": "@@ -0,0 +1,475 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch EoMT model.\"\"\"\n+\n+import unittest\n+\n+import requests\n+\n+from transformers import AutoImageProcessor, EomtConfig, EomtForUniversalSegmentation\n+from transformers.testing_utils import require_torch, require_torch_accelerator, require_torch_fp16, slow, torch_device\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class EomtForUniversalSegmentationTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=2,\n+        is_training=True,\n+        image_size=40,\n+        patch_size=2,\n+        num_queries=5,\n+        num_register_tokens=19,\n+        num_labels=4,\n+        hidden_size=8,\n+        num_attention_heads=2,\n+        num_hidden_layers=4,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.is_training = is_training\n+        self.num_queries = num_queries\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_labels = num_labels\n+        self.hidden_size = hidden_size\n+        self.num_attention_heads = num_attention_heads\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_register_tokens = num_register_tokens\n+\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def get_config(self):\n+        config = {\n+            \"image_size\": self.image_size,\n+            \"patch_size\": self.patch_size,\n+            \"num_labels\": self.num_labels,\n+            \"hidden_size\": self.hidden_size,\n+            \"num_attention_heads\": self.num_attention_heads,\n+            \"num_hidden_layers\": self.num_hidden_layers,\n+            \"num_register_tokens\": self.num_register_tokens,\n+            \"num_queries\": self.num_queries,\n+            \"num_blocks\": 1,\n+        }\n+        return EomtConfig(**config)\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, 3, self.image_size, self.image_size]).to(torch_device)\n+\n+        mask_labels = (\n+            torch.rand([self.batch_size, self.num_labels, self.image_size, self.image_size], device=torch_device) > 0.5\n+        ).float()\n+        class_labels = (torch.rand((self.batch_size, self.num_labels), device=torch_device) > 0.5).long()\n+\n+        config = self.get_config()\n+        return config, pixel_values, mask_labels, class_labels\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, mask_labels, class_labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+    def prepare_config_and_inputs_for_training(self):\n+        config, pixel_values, mask_labels, class_labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values, \"mask_labels\": mask_labels, \"class_labels\": class_labels}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class EomtForUniversalSegmentationTest(ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (EomtForUniversalSegmentation,) if is_torch_available() else ()\n+    is_encoder_decoder = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+    test_torch_exportable = False\n+\n+    def setUp(self):\n+        self.model_tester = EomtForUniversalSegmentationTester(self)\n+        self.config_tester = ConfigTester(self, config_class=EomtConfig, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model_with_labels(self):\n+        size = (self.model_tester.image_size,) * 2\n+        inputs = {\n+            \"pixel_values\": torch.randn((2, 3, *size), device=torch_device),\n+            \"mask_labels\": torch.randn((2, 10, *size), device=torch_device),\n+            \"class_labels\": torch.zeros(2, 10, device=torch_device).long(),\n+        }\n+        config = self.model_tester.get_config()\n+\n+        model = EomtForUniversalSegmentation(config).to(torch_device)\n+        outputs = model(**inputs)\n+        self.assertTrue(outputs.loss is not None)\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class._from_config(config, attn_implementation=\"eager\")\n+            config = model.config\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            # Check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.attentions\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n+\n+            out_len = len(outputs)\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            added_hidden_states = 1\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.attentions\n+            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    @unittest.skip(reason=\"EoMT does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EoMT does not have a get_input_embeddings method\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EoMT is not a generative model\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"EoMT does not use token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    def test_training(self):\n+        if not self.model_tester.is_training:\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_training()\n+            config.return_dict = True\n+\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.train()\n+            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+            loss = model(**inputs).loss\n+            loss.backward()\n+\n+    def test_initialization(self):\n+        # Apart from the below params, all other parameters are initialized using kaiming uniform.\n+        non_uniform_init_parms = [\n+            \"layernorm.bias\",\n+            \"layernorm.weight\",\n+            \"norm1.bias\",\n+            \"norm1.weight\",\n+            \"norm2.bias\",\n+            \"norm2.weight\",\n+            \"layer_scale1.lambda1\",\n+            \"layer_scale2.lambda1\",\n+            \"register_tokens\",\n+            \"cls_token\",\n+        ]\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if any(x in name for x in non_uniform_init_parms):\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertTrue(\n+                            -1.0 <= ((param.data.mean() * 1e9).round() / 1e9).item() <= 1.0,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+\n+@require_torch\n+class EomtForUniversalSegmentationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.model_id = \"tue-mps/coco_panoptic_eomt_large_640\"\n+\n+    @slow\n+    def test_inference(self):\n+        model = EomtForUniversalSegmentation.from_pretrained(self.model_id, device_map=\"auto\")\n+        processor = AutoImageProcessor.from_pretrained(self.model_id)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.inference_mode():\n+            outputs = model(**inputs)\n+\n+        self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 134))\n+        self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n+\n+        # fmt: off\n+        EXPECTED_SLICE = torch.tensor([\n+            [ 13.2540,   8.9279,   8.6631,  12.3760,  10.1429],\n+            [ -3.4815, -36.4630, -45.5604, -46.8404, -37.5099],\n+            [ -6.8689, -44.4206, -62.7591, -59.2928, -47.7035],\n+            [ -2.9380, -42.0659, -57.4382, -55.1537, -43.5142],\n+            [ -8.4387, -38.5275, -53.1383, -47.0064, -38.9667],\n+        ]).to(model.device)\n+        # fmt: on\n+\n+        output_slice = outputs.masks_queries_logits[0, 0, :5, :5]\n+        torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n+        # fmt: off\n+        EXPECTED_SLICE = torch.tensor([\n+            [-0.6977, -6.4907, -4.1178, -6.5554, -6.6529],\n+            [-0.3650, -6.6560, -4.0143, -6.5776, -6.5879],\n+            [-0.8820, -6.7175, -3.5334, -6.8569, -6.2415],\n+            [ 0.4502, -5.3911, -3.0232, -5.9411, -6.3243],\n+            [ 0.3157, -5.6321, -2.6716, -5.5740, -5.5607],\n+        ]).to(model.device)\n+        # fmt: on\n+\n+        output_slice = outputs.class_queries_logits[0, :5, :5]\n+        torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n+    @require_torch_accelerator\n+    @require_torch_fp16\n+    @slow\n+    def test_inference_fp16(self):\n+        model = EomtForUniversalSegmentation.from_pretrained(\n+            self.model_id, torch_dtype=torch.float16, device_map=\"auto\"\n+        )\n+        processor = AutoImageProcessor.from_pretrained(self.model_id)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.inference_mode():\n+            outputs = model(**inputs)\n+\n+        self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 134))\n+        self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n+\n+    @slow\n+    def test_semantic_segmentation_inference(self):\n+        model_id = \"tue-mps/ade20k_semantic_eomt_large_512\"\n+        model = EomtForUniversalSegmentation.from_pretrained(model_id, device_map=\"auto\")\n+        processor = AutoImageProcessor.from_pretrained(model_id)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+        patch_offsets = inputs.pop(\"patch_offsets\", None)\n+\n+        with torch.inference_mode():\n+            outputs = model(**inputs)\n+\n+        self.assertTrue(outputs.class_queries_logits.shape == (2, 100, 151))\n+        self.assertTrue(outputs.masks_queries_logits.shape == (2, 100, 128, 128))\n+\n+        preds = processor.post_process_semantic_segmentation(\n+            outputs, original_image_sizes=[(image.size[1], image.size[0])], patch_offsets=patch_offsets\n+        )\n+\n+        self.assertTrue(preds.shape[1:] == (image.size[1], image.size[0]))\n+\n+        # fmt: off\n+        EXPECTED_SLICE = torch.tensor([\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39],\n+            [39, 39, 39, 39, 39, 39, 39, 39, 39, 39]\n+        ], device=model.device)\n+        # fmt: on\n+\n+        output_slice = preds[0, :10, :10]\n+        torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n+    @slow\n+    def test_panoptic_segmentation_inference(self):\n+        model = EomtForUniversalSegmentation.from_pretrained(self.model_id, device_map=\"auto\")\n+        processor = AutoImageProcessor.from_pretrained(self.model_id)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.inference_mode():\n+            outputs = model(**inputs)\n+\n+        self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 134))\n+        self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n+\n+        preds = processor.post_process_panoptic_segmentation(\n+            outputs, original_image_sizes=[(image.size[1], image.size[0])]\n+        )[0]\n+        segmentation, segments_info = preds[\"segmentation\"], preds[\"segments_info\"]\n+\n+        # fmt: off\n+        EXPECTED_SLICE = torch.tensor([\n+            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n+            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n+            [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1],\n+            [-1, -1, -1, -1, -1,  2,  2,  2,  2,  2],\n+            [-1, -1, -1,  2,  2,  2,  2,  2,  2,  2],\n+            [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n+            [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n+            [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n+            [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2],\n+            [ 2,  2,  2,  2,  2,  2,  2,  2,  2,  2]\n+        ], device=model.device)\n+\n+        EXPECTED_SEGMENTS_INFO = [\n+            {\"id\": 0, \"label_id\": 15, \"score\": 0.99935},\n+            {\"id\": 1, \"label_id\": 15, \"score\": 0.998688},\n+            {\"id\": 2, \"label_id\": 57, \"score\": 0.954325},\n+            {\"id\": 3, \"label_id\": 65, \"score\": 0.997285},\n+            {\"id\": 4, \"label_id\": 65, \"score\": 0.99711}\n+        ]\n+        # fmt: on\n+\n+        output_slice = segmentation[:10, :10]\n+        torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+        for actual, expected in zip(segments_info, EXPECTED_SEGMENTS_INFO):\n+            self.assertEqual(actual[\"id\"], expected[\"id\"])\n+            self.assertEqual(actual[\"label_id\"], expected[\"label_id\"])\n+            self.assertAlmostEqual(actual[\"score\"], expected[\"score\"], delta=1e-3)\n+\n+    @slow\n+    def test_instance_segmentation_inference(self):\n+        model_id = \"tue-mps/coco_instance_eomt_large_640\"\n+        model = EomtForUniversalSegmentation.from_pretrained(model_id, device_map=\"auto\")\n+        processor = AutoImageProcessor.from_pretrained(model_id)\n+\n+        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+\n+        inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n+\n+        with torch.inference_mode():\n+            outputs = model(**inputs)\n+\n+        self.assertTrue(outputs.class_queries_logits.shape == (1, 200, 81))\n+        self.assertTrue(outputs.masks_queries_logits.shape == (1, 200, 160, 160))\n+\n+        preds = processor.post_process_instance_segmentation(\n+            outputs, original_image_sizes=[(image.size[1], image.size[0])]\n+        )[0]\n+        segmentation, segments_info = preds[\"segmentation\"], preds[\"segments_info\"]\n+\n+        # fmt: off\n+        EXPECTED_SLICE = torch.tensor([\n+            [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n+            [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n+            [-1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n+            [-1., -1., -1.,  0.,  0.,  1.,  1.,  1.,  1.,  1.],\n+            [ 0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n+            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n+            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n+            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n+            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n+            [ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.]\n+        ], device=model.device)\n+\n+        EXPECTED_SEGMENTS_INFO = [\n+            {'id': 0, 'label_id': 57, 'score': 0.871247},\n+            {'id': 1, 'label_id': 57, 'score': 0.821225},\n+            {'id': 2, 'label_id': 15, 'score': 0.976252},\n+            {'id': 3, 'label_id': 65, 'score': 0.972960},\n+            {'id': 4, 'label_id': 65, 'score': 0.981109},\n+            {'id': 5, 'label_id': 15, 'score': 0.972689}\n+        ]\n+        # fmt: on\n+\n+        output_slice = segmentation[:10, :10]\n+        torch.testing.assert_close(output_slice, EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+        for actual, expected in zip(segments_info, EXPECTED_SEGMENTS_INFO):\n+            self.assertEqual(actual[\"id\"], expected[\"id\"])\n+            self.assertEqual(actual[\"label_id\"], expected[\"label_id\"])\n+            self.assertAlmostEqual(actual[\"score\"], expected[\"score\"], delta=1e-3)"
        }
    ],
    "stats": {
        "total": 4924,
        "additions": 4923,
        "deletions": 1
    }
}