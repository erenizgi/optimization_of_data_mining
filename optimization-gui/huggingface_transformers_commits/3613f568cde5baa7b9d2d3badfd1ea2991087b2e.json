{
    "author": "Cyrilvallez",
    "message": "Add default TP plan for all models with backend support (#35870)\n\n* Add some tp plans!\n\n* More tp plans!\n\n* Add it in the comment\n\n* style\n\n* Update configuration_mixtral.py\n\n* Update configuration_phi.py\n\n* update the layout according to special archs\n\n* fix mixtral\n\n* style\n\n* trigger CIs\n\n* trigger CIs\n\n* CIs\n\n* olmo2\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
    "files": [
        {
            "sha": "254e4c9fc465d7f6e995b3daf242f5e6d8c25952",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -1294,7 +1294,7 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n \n     # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n     # In practice, it means that they support attention interface functions, fully pass the kwargs\n-    # through all modules up to the Attention layer, and can slice logits with Tensor\n+    # through all modules up to the Attention layer, can slice logits with Tensor, and have a default TP plan\n     _supports_attention_backend = False\n \n     @property"
        },
        {
            "sha": "dc9ca2cb4dd0d86ae85967ac61c8ea85d4ac414d",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -139,6 +139,15 @@ class CohereConfig(PretrainedConfig):\n \n     model_type = \"cohere\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "88d3265eadfe0255a9810903c4fef3501ea0047d",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -139,6 +139,15 @@ class Cohere2Config(PretrainedConfig):\n \n     model_type = \"cohere2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "0e9ad69176cfa0820484a515f72c3d8f759508b8",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -163,6 +163,15 @@ class Cohere2Config(PretrainedConfig):\n \n     model_type = \"cohere2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b8470e92fb3220c434904cc894248653ffb18ffc",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -93,6 +93,15 @@ class GemmaConfig(PretrainedConfig):\n \n     model_type = \"gemma\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2e3f7a4edf4d520585d1b7549c95cbd9332c37ba",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -117,6 +117,15 @@ class GemmaConfig(PretrainedConfig):\n \n     model_type = \"gemma\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "3dfbd6a107e9d55220e83efce97e2dbd86a37efd",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -97,6 +97,15 @@ class Gemma2Config(PretrainedConfig):\n \n     model_type = \"gemma2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f3908b203a5818df4666319c69f6f42e1e639984",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -123,6 +123,15 @@ class Gemma2Config(PretrainedConfig):\n \n     model_type = \"gemma2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "553b71cf234db78bca22d17d73894c20452ea423",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -85,6 +85,14 @@ class GlmConfig(PretrainedConfig):\n \n     model_type = \"glm\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n+        \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "73c2638a88c2ba9690fe5f9b24460fc8a10603a0",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -86,6 +86,15 @@ class HeliumConfig(PretrainedConfig):\n \n     model_type = \"helium\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "c3f7ec8e4cc17c393fab7679006cb0579aa83313",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -109,6 +109,16 @@ class MixtralConfig(PretrainedConfig):\n \n     model_type = \"mixtral\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.block_sparse_moe.gate\": \"colwise_rep\",  # we need to replicate here to correctly route experts\n+        \"layers.*.block_sparse_moe.experts.*.w1\": \"colwise\",\n+        \"layers.*.block_sparse_moe.experts.*.w2\": \"rowwise\",\n+        \"layers.*.block_sparse_moe.experts.*.w3\": \"colwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d80910e8456b1f7569e35462327bc34cf1aa6c69",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -106,6 +106,15 @@ class OlmoConfig(PretrainedConfig):\n \n     model_type = \"olmo\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "ce434f5416081c7f8b07172845c0220139a9e8bc",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -89,6 +89,15 @@ class Olmo2Config(PretrainedConfig):\n \n     model_type = \"olmo2\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "04c7f0f486bde353002c2cbe75bcf095c76ccd28",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -100,6 +100,15 @@ class Olmo2Config(OlmoConfig):\n     \"\"\"\n \n     model_type = \"olmo2\"\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2733d77ff67432a09178fa0c372e8f0e6036b570",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -138,6 +138,14 @@ class PhiConfig(PretrainedConfig):\n \n     model_type = \"phi\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.dense\": \"rowwise\",\n+        \"layers.*.mlp.fc1\": \"colwise\",\n+        \"layers.*.mlp.fc2\": \"rowwise\",\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "6fe6e1cdfca874f96251ef8e277574f2663f7702",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -107,6 +107,12 @@ class Phi3Config(PretrainedConfig):\n \n     model_type = \"phi3\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.qkv_proj\": \"colwise_rep\",  # we need to replicate here due to the slicing of qkv\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the slicing of qkv\n+        \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n+        \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "1bd878a9488f8a2c518c7f45da12322bdd91b13f",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3613f568cde5baa7b9d2d3badfd1ea2991087b2e/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=3613f568cde5baa7b9d2d3badfd1ea2991087b2e",
            "patch": "@@ -358,5 +358,7 @@ def translate_to_torch_parallel_style(style: str):\n         return RowwiseParallel()\n     elif style == \"colwise_rep\":\n         return ColwiseParallel(output_layouts=Replicate())\n+    elif style == \"rowwise_rep\":\n+        return RowwiseParallel(input_layouts=Replicate())\n     else:\n         raise ValueError(f\"Unsupported parallel style value: {style}\")"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 134,
        "deletions": 1
    }
}