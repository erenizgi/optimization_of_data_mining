{
    "author": "zucchini-nlp",
    "message": "[mllama] fix loading and inference (#38223)\n\nfix loading",
    "sha": "2edb0e4b4dda8172d5628ca7497a4125f28bf6fc",
    "files": [
        {
            "sha": "f1e634d911501a9d1a09e6ffaa2acb58f3ed239e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2edb0e4b4dda8172d5628ca7497a4125f28bf6fc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2edb0e4b4dda8172d5628ca7497a4125f28bf6fc/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=2edb0e4b4dda8172d5628ca7497a4125f28bf6fc",
            "patch": "@@ -486,8 +486,6 @@ def forward(\n             value_states = self.v_proj(cross_attention_states)\n             key_states = key_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n             value_states = value_states.view(bsz, -1, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-            key_states = repeat_kv(key_states, self.num_key_value_groups)\n-            value_states = repeat_kv(value_states, self.num_key_value_groups)\n \n             key_states = self.k_norm(key_states)\n             if past_key_value is not None:\n@@ -850,7 +848,7 @@ def forward(self, x, position_ids):\n @auto_docstring\n class MllamaPreTrainedModel(PreTrainedModel):\n     config_class = MllamaConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"MllamaVisionEncoderLayer\","
        }
    ],
    "stats": {
        "total": 4,
        "additions": 1,
        "deletions": 3
    }
}