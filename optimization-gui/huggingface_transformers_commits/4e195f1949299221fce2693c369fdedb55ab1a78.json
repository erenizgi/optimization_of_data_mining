{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ Allow `check_model_inputs` in core VLMs (#40342)\n\n* allow `check_model_inputs` in core VLMs\n\n* address comments\n\n* fix style\n\n* why this didnt fail prev?\n\n* chec for Noneness instead\n\n* batch update vlms\n\n* fix some tests\n\n* fix copies\n\n* oops delete\n\n* fix efficientloftr\n\n* fix copies\n\n* i am stupid, fix idefics\n\n* fix GC\n\n* return type and other comments\n\n* we shouldn't manually change attention anymore\n\n* fix style\n\n* fix copies\n\n* fix the test",
    "sha": "4e195f1949299221fce2693c369fdedb55ab1a78",
    "files": [
        {
            "sha": "e20797feead92e85bb6fd061889ce21ab88f2199",
            "filename": "src/transformers/models/aimv2/modeling_aimv2.py",
            "status": "modified",
            "additions": 35,
            "deletions": 95,
            "changes": 130,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodeling_aimv2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -34,7 +34,10 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n from .configuration_aimv2 import Aimv2Config, Aimv2TextConfig, Aimv2VisionConfig\n \n \n@@ -300,17 +303,17 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         norm_hidden_states = self.rms_norm1(hidden_states)\n-        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+        attn_output, _ = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask, **kwargs)\n \n         hidden_states = hidden_states + attn_output\n         norm_hidden_states = self.rms_norm2(hidden_states)\n         mlp_output = self.ffn(norm_hidden_states)\n \n         hidden_states = hidden_states + mlp_output\n-        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+        return hidden_states\n \n \n class Aimv2Encoder(nn.Module):\n@@ -329,68 +332,22 @@ def __init__(self, config: Aimv2Config):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class Aimv2AttentionPoolingHead(nn.Module):\n@@ -464,6 +421,10 @@ def _init_weights(self, module):\n class Aimv2VisionModel(Aimv2PreTrainedModel):\n     config: Aimv2VisionConfig\n     main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Aimv2EncoderLayer,\n+        \"attentions\": Aimv2Attention,\n+    }\n \n     def __init__(self, config: Aimv2VisionConfig):\n         super().__init__(config)\n@@ -482,14 +443,14 @@ def __init__(self, config: Aimv2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embed\n \n-    @can_return_tuple\n+    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -511,29 +472,21 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled features\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(pixel_values)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n         pooler_output = self.head(last_hidden_state) if self.use_head else None\n \n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -545,6 +498,11 @@ def forward(\n class Aimv2TextModel(Aimv2PreTrainedModel):\n     main_input_name = \"input_ids\"\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Aimv2EncoderLayer,\n+        \"attentions\": Aimv2Attention,\n+    }\n+\n     def __init__(self, config: Aimv2TextConfig):\n         super().__init__(config)\n         self.config = config\n@@ -562,20 +520,14 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.embeddings.token_embedding = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(input_ids)\n         batch_size, seq_len, _ = hidden_states.shape\n \n@@ -594,11 +546,10 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n         # Get pooled output\n@@ -610,8 +561,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -733,8 +682,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Aimv2Output:\n         r\"\"\"\n         Examples:\n@@ -758,23 +706,15 @@ def forward(\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         image_embeds = vision_outputs.pooler_output"
        },
        {
            "sha": "0bf1cdd346a0b4e86f995cccd6610f93959f5150",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 44,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -24,12 +24,16 @@\n \n from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPooling\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n )\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import CLIPModel, CLIPTextEmbeddings, _get_vector_norm\n from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n from ..siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n@@ -373,17 +377,17 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         norm_hidden_states = self.rms_norm1(hidden_states)\n-        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+        attn_output, _ = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask, **kwargs)\n \n         hidden_states = hidden_states + attn_output\n         norm_hidden_states = self.rms_norm2(hidden_states)\n         mlp_output = self.ffn(norm_hidden_states)\n \n         hidden_states = hidden_states + mlp_output\n-        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+        return hidden_states\n \n \n class Aimv2Encoder(SiglipEncoder):\n@@ -461,6 +465,10 @@ def _init_weights(self, module):\n class Aimv2VisionModel(Aimv2PreTrainedModel):\n     config: Aimv2VisionConfig\n     main_input_name = \"pixel_values\"\n+    _can_record_outputs = {\n+        \"hidden_states\": Aimv2EncoderLayer,\n+        \"attentions\": Aimv2Attention,\n+    }\n \n     def __init__(self, config: Aimv2VisionConfig):\n         super().__init__(config)\n@@ -479,14 +487,14 @@ def __init__(self, config: Aimv2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embed\n \n-    @can_return_tuple\n+    @deprecate_kwarg(\"attention_mask\", version=\"v4.58.0\")\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -508,29 +516,21 @@ def forward(\n         >>> last_hidden_state = outputs.last_hidden_state\n         >>> pooled_output = outputs.pooler_output  # pooled features\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(pixel_values)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n         pooler_output = self.head(last_hidden_state) if self.use_head else None\n \n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -542,6 +542,11 @@ def forward(\n class Aimv2TextModel(Aimv2PreTrainedModel):\n     main_input_name = \"input_ids\"\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Aimv2EncoderLayer,\n+        \"attentions\": Aimv2Attention,\n+    }\n+\n     def __init__(self, config: Aimv2TextConfig):\n         super().__init__(config)\n         self.config = config\n@@ -559,20 +564,14 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.embeddings.token_embedding = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(input_ids)\n         batch_size, seq_len, _ = hidden_states.shape\n \n@@ -591,11 +590,10 @@ def forward(\n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n         # Get pooled output\n@@ -607,8 +605,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -641,8 +637,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Aimv2Output:\n         r\"\"\"\n         Examples:\n@@ -666,23 +661,15 @@ def forward(\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         image_embeds = vision_outputs.pooler_output"
        },
        {
            "sha": "8b98548d0bfec8e7bbdd449f51364b12a5b044eb",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -545,13 +545,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "5c2cd95dff01d53502523ae8a831b134a74804ea",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -28,7 +28,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n@@ -280,7 +279,7 @@ def forward(\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n@@ -403,10 +402,6 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n@@ -448,11 +443,6 @@ def forward(\n         >>> gen_tokens = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.3)\n         >>> processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -471,10 +461,6 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n             **kwargs,"
        },
        {
            "sha": "f3f75b3e2dbd5e21c0034c9f2c0e51cce0e6cab6",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -30,7 +30,6 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, logging\n from ...utils.generic import check_model_inputs\n@@ -177,7 +176,7 @@ def forward(\n         vision_feature_select_strategy: Optional[str] = None,\n         use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, AyaVisionModelOutputWithPast]:\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n@@ -237,10 +236,6 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n@@ -292,10 +287,6 @@ def forward(\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n             labels=labels,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             image_sizes=image_sizes,"
        },
        {
            "sha": "c3b5821601fb32a8fd136d7cb3ded1dbe58d4b90",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 54,
            "deletions": 192,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -28,7 +28,9 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, logging, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.generic import check_model_inputs\n from .configuration_blip import BlipConfig, BlipTextConfig, BlipVisionConfig\n from .modeling_blip_text import BlipTextLMHeadModel, BlipTextModel\n \n@@ -327,8 +329,8 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n         bsz, tgt_len, embed_dim = hidden_states.size()\n@@ -363,9 +365,7 @@ def forward(\n \n         output = self.projection(context_layer)\n \n-        outputs = (output, attention_probs) if output_attentions else (output, None)\n-\n-        return outputs\n+        return output, attention_probs\n \n \n # Copied from transformers.models.clip.modeling_clip.CLIPMLP with CLIP->Blip\n@@ -393,29 +393,20 @@ def __init__(self, config: BlipConfig):\n         self.mlp = BlipMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             head_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + residual\n         residual = hidden_states\n@@ -424,12 +415,7 @@ def forward(\n \n         hidden_states = hidden_states + residual\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -486,72 +472,31 @@ def __init__(self, config: BlipConfig):\n         self.layers = nn.ModuleList([BlipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Embedded representation of the inputs. Should be float, not int tokens.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class BlipVisionModel(BlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config: BlipVisionConfig\n+    _can_record_outputs = {\n+        \"hidden_states\": BlipEncoderLayer,\n+        \"attentions\": BlipAttention,\n+    }\n \n     def __init__(self, config: BlipVisionConfig):\n         super().__init__(config)\n@@ -564,47 +509,33 @@ def __init__(self, config: BlipVisionConfig):\n \n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n     def get_input_embeddings(self):\n@@ -667,7 +598,6 @@ def get_text_features(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -685,13 +615,10 @@ def get_text_features(\n         >>> inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n         >>> text_features = model.get_text_features(**inputs)\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         text_outputs = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            return_dict=return_dict,\n         )\n \n         pooled_output = text_outputs[1]\n@@ -703,7 +630,6 @@ def get_text_features(\n     def get_image_features(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -728,11 +654,9 @@ def get_image_features(\n \n         >>> image_features = model.get_image_features(**inputs)\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n@@ -747,7 +671,6 @@ def get_multimodal_features(\n         input_ids: Optional[torch.LongTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n     ) -> torch.FloatTensor:\n         r\"\"\"\n@@ -771,12 +694,8 @@ def get_multimodal_features(\n \n         >>> multimodal_features = model.get_multimodal_features(**inputs)\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=True,\n-            output_hidden_states=True,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n         )\n \n@@ -788,14 +707,14 @@ def get_multimodal_features(\n             attention_mask=attention_mask,\n             encoder_hidden_states=image_embeds,\n             encoder_attention_mask=image_atts,\n-            return_dict=return_dict,\n         )\n \n         pooled_output = text_outputs[1]  # pooled_output\n         multimodal_features = self.text_projection(pooled_output)\n \n         return multimodal_features\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -804,10 +723,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BlipOutput]:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -834,34 +751,23 @@ def forward(\n         >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n         >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n         ```\"\"\"\n-        # Use BLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n         text_outputs = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        image_embeds = vision_outputs[1]\n+        image_embeds = vision_outputs.pooler_output\n         image_embeds = self.visual_projection(image_embeds)\n \n-        text_embeds = text_outputs[1]\n+        text_embeds = text_outputs.pooler_output\n         text_embeds = self.text_projection(text_embeds)\n \n         # normalized features\n@@ -878,10 +784,6 @@ def forward(\n         if return_loss:\n             loss = blip_loss(logits_per_text)\n \n-        if not return_dict:\n-            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n-            return ((loss,) + output) if loss is not None else output\n-\n         return BlipOutput(\n             loss=loss,\n             logits_per_image=logits_per_image,\n@@ -925,17 +827,16 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_decoder.set_input_embeddings(value)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -957,36 +858,23 @@ def forward(\n         >>> outputs = model(**inputs)\n         ```\"\"\"\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n-        image_embeds = vision_outputs[0]\n+        image_embeds = vision_outputs.last_hidden_state\n \n         outputs = self.text_decoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             encoder_hidden_states=image_embeds,\n             labels=labels,\n-            return_dict=return_dict,\n             reduction=\"mean\",\n+            **kwargs,\n         )\n \n-        if not return_dict:\n-            outputs = (outputs[0], outputs[1]) if labels is not None else (outputs[0],)\n-            outputs += (image_embeds, vision_outputs[0]) + vision_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return BlipForConditionalGenerationModelOutput(\n             loss=outputs.loss,\n             logits=outputs.logits,\n@@ -1105,6 +993,7 @@ def get_input_embeddings(self):\n         # This will return shared embeddings if they are shared else specific to encoder.\n         return self.text_encoder.get_input_embeddings()\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1113,11 +1002,9 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -1158,56 +1045,44 @@ def forward(\n                 \" are using the model for inference make sure that `decoder_input_ids` is passed or call `generate`\"\n             )\n \n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n-        image_embeds = vision_outputs[0]\n+        image_embeds = vision_outputs.last_hidden_state\n         image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n \n         question_embeds = self.text_encoder(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             encoder_hidden_states=image_embeds,\n             encoder_attention_mask=image_attention_mask,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n         if labels is not None and decoder_input_ids is None:\n             # labels are already shifted right, see: https://github.com/huggingface/transformers/pull/23153\n             decoder_input_ids = labels\n \n-        question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n+        question_embeds = question_embeds[0]\n \n         answer_output = self.text_decoder(\n             input_ids=decoder_input_ids,\n             attention_mask=decoder_attention_mask,\n             encoder_hidden_states=question_embeds,\n             encoder_attention_mask=attention_mask,\n             labels=labels,\n-            return_dict=return_dict,\n             reduction=\"mean\",\n+            **kwargs,\n         )\n \n         if labels is not None:\n-            decoder_loss = answer_output.loss.mean() if return_dict else answer_output[0].mean()\n+            decoder_loss = answer_output.loss.mean()\n         else:\n             decoder_loss = None\n \n-        if not return_dict:\n-            outputs = (decoder_loss, image_embeds, vision_outputs[0]) + vision_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return BlipTextVisionModelOutput(\n             loss=decoder_loss,\n             image_embeds=image_embeds,\n@@ -1348,17 +1223,16 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.text_encoder.set_input_embeddings(value)\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n         pixel_values: torch.FloatTensor,\n         use_itm_head: Optional[bool] = True,\n         attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BlipTextVisionModelOutput]:\n         r\"\"\"\n         use_itm_head (`bool`, *optional*, defaults to `True`):\n@@ -1382,21 +1256,13 @@ def forward(\n         >>> outputs = model(**inputs)\n         ```\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n-        image_embeds = vision_outputs[0]\n+        image_embeds = vision_outputs.last_hidden_state\n         image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long)\n \n         if use_itm_head:\n@@ -1405,28 +1271,24 @@ def forward(\n                 attention_mask=attention_mask,\n                 encoder_hidden_states=image_embeds,\n                 encoder_attention_mask=image_atts,\n-                return_dict=return_dict,\n+                **kwargs,\n             )\n-            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n+            question_embeds = question_embeds.last_hidden_state\n \n             output = self.itm_head(question_embeds[:, 0, :])\n         else:\n             question_embeds = self.text_encoder(\n                 input_ids=input_ids,\n                 attention_mask=attention_mask,\n-                return_dict=return_dict,\n+                **kwargs,\n             )\n-            question_embeds = question_embeds[0] if not return_dict else question_embeds.last_hidden_state\n+            question_embeds = question_embeds.last_hidden_state\n \n             image_feat = normalize(self.vision_proj(image_embeds[:, 0, :]), dim=-1)\n             text_feat = normalize(self.text_proj(question_embeds[:, 0, :]), dim=-1)\n \n             output = image_feat @ text_feat.t()\n \n-        if not return_dict:\n-            outputs = (output, vision_outputs[0]) + vision_outputs[2:] + (question_embeds,)\n-            return tuple(output for output in outputs if output is not None)\n-\n         return BlipImageTextMatchingModelOutput(\n             itm_score=output,\n             last_hidden_state=vision_outputs.last_hidden_state,"
        },
        {
            "sha": "6a488fba8f5a7695874193ebd9d0525db060fb13",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 78,
            "deletions": 269,
            "changes": 347,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -42,10 +42,12 @@\n     ModelOutput,\n     TransformersKwargs,\n     auto_docstring,\n+    can_return_tuple,\n     filter_out_non_signature_kwargs,\n     logging,\n     torch_int,\n )\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n \n@@ -303,7 +305,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -320,13 +321,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -342,8 +337,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.projection(attn_output)\n \n-        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n-        return outputs\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipMLP\n@@ -372,29 +366,20 @@ def __init__(self, config: Blip2Config):\n         self.mlp = Blip2MLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             head_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + residual\n         residual = hidden_states\n@@ -403,12 +388,7 @@ def forward(\n \n         hidden_states = hidden_states + residual\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -477,74 +457,33 @@ def __init__(self, config: Blip2Config):\n         self.layers = nn.ModuleList([Blip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Embedded representation of the inputs. Should be float, not int tokens.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n @auto_docstring\n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->Blip2, BLIP->BLIP_2\n class Blip2VisionModel(Blip2PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config: Blip2VisionConfig\n+    _can_record_outputs = {\n+        \"hidden_states\": Blip2EncoderLayer,\n+        \"attentions\": Blip2Attention,\n+    }\n \n     def __init__(self, config: Blip2VisionConfig):\n         super().__init__(config)\n@@ -557,47 +496,33 @@ def __init__(self, config: Blip2VisionConfig):\n \n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n     def get_input_embeddings(self):\n@@ -657,7 +582,7 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n@@ -722,15 +647,10 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (\n-            (\n-                context_layer,\n-                attention_probs,\n-            )\n-            if output_attentions\n-            else (context_layer,)\n+        return (\n+            context_layer,\n+            attention_probs,\n         )\n-        return outputs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->Blip2QFormer\n@@ -780,19 +700,18 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.attention(\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attn_output, hidden_states)\n+        return attention_output\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->Blip2QFormer\n@@ -855,35 +774,30 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        self_attention_outputs = self.attention(\n+        attention_output = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n \n             if self.has_cross_attention:\n                 if encoder_hidden_states is None:\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n-                cross_attention_outputs = self.crossattention(\n+                query_attention_output = self.crossattention(\n                     hidden_states=query_attention_output,\n                     attention_mask=attention_mask,\n                     head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n+                    **kwargs,\n                 )\n-                query_attention_output = cross_attention_outputs[0]\n-                # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -907,9 +821,7 @@ def forward(\n                 self.seq_len_dim,\n                 attention_output,\n             )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -931,64 +843,33 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @can_return_tuple\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict=True,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n                 query_length=query_length,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if query_length > 0 and layer_module.has_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n         )\n \n \n@@ -1049,6 +930,16 @@ class Blip2QFormerModel(Blip2PreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Blip2QFormerLayer,\n+        \"attentions\": [\n+            OutputRecorder(Blip2QFormerMultiHeadAttention, index=1, layer_name=\".attention\"),\n+        ],\n+        \"cross_attentions\": [\n+            OutputRecorder(Blip2QFormerMultiHeadAttention, index=1, layer_name=\".crossattention\"),\n+        ],\n+    }\n+\n     def __init__(self, config: Blip2QFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1117,6 +1008,7 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -1126,9 +1018,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n         query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n@@ -1138,12 +1028,6 @@ def forward(\n             Length of the query, usually based on the number of query tokens.\n             If no value is provided, query_length will be inferred by the query_embeds.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         query_length = (\n             query_length if query_length is not None else query_embeds.shape[1] if query_embeds is not None else 0\n         )\n@@ -1194,30 +1078,21 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             query_length=query_length,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = sequence_output[:, 0, :]\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n \n@@ -1477,6 +1352,7 @@ def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch\n         special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         return special_image_mask\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1485,21 +1361,11 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`Blip2Processor`]. See [`Blip2Processor.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n@@ -1528,16 +1394,13 @@ def forward(\n \n         >>> outputs = model(**inputs)\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # step 1: forward the images through the vision encoder,\n         # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n         image_embeds = vision_outputs[0]\n \n@@ -1549,9 +1412,7 @@ def forward(\n             query_embeds=query_tokens,\n             encoder_hidden_states=image_embeds,\n             encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n         query_output = query_outputs[0]\n \n@@ -1577,12 +1438,9 @@ def forward(\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n                 **kwargs,\n             )\n-            logits = outputs.logits if return_dict else outputs[0]\n+            logits = outputs[0]\n             loss = None\n             # we compute the loss here since we need to take into account the sequence length of the query embeds\n             if labels is not None:\n@@ -1602,19 +1460,12 @@ def forward(\n                 attention_mask=attention_mask,\n                 decoder_input_ids=decoder_input_ids,\n                 decoder_attention_mask=decoder_attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n+                return_dict=True,\n                 **kwargs,\n             )\n             loss = outputs.loss\n             logits = outputs.logits\n-            outputs = outputs.to_tuple() if not return_dict else outputs\n-\n-        if not return_dict:\n-            output = (logits, vision_outputs, query_outputs, outputs)\n-            return ((loss,) + output) if loss is not None else output\n \n         return Blip2ForConditionalGenerationModelOutput(\n             loss=loss,\n@@ -1650,15 +1501,14 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Blip2TextModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -1684,7 +1534,6 @@ def forward(\n         >>> print(text_embeds.shape)\n         torch.Size([2, 7, 256])\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         query_embeds = self.embeddings(\n             input_ids=input_ids,\n@@ -1695,21 +1544,15 @@ def forward(\n             query_embeds=query_embeds,\n             query_length=0,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        pooled_output = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+        pooled_output = text_outputs[0]\n         pooled_output = pooled_output.to(dtype=self.text_projection.weight.dtype)\n \n         text_embeds = self.text_projection(pooled_output)\n         text_embeds = nn.functional.normalize(text_embeds, dim=-1)\n \n-        if not return_dict:\n-            outputs = (text_embeds, text_outputs[0]) + text_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return Blip2TextModelOutput(\n             text_embeds=text_embeds,\n             last_hidden_state=text_outputs.last_hidden_state,\n@@ -1741,13 +1584,12 @@ def __init__(self, config: Blip2Config):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Blip2VisionModelOutput]:\n         r\"\"\"\n         Examples:\n@@ -1776,41 +1618,27 @@ def forward(\n         >>> print(image_embeds.shape)\n         torch.Size([1, 32, 256])\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        pooled_output = vision_outputs[0] if not return_dict else vision_outputs.last_hidden_state\n-\n+        pooled_output = vision_outputs[0]\n         image_attention_mask = torch.ones(pooled_output.size()[:-1], dtype=torch.long, device=pooled_output.device)\n-\n         query_tokens = self.query_tokens.expand(pooled_output.shape[0], -1, -1)\n \n         query_outputs = self.qformer(\n             query_embeds=query_tokens,\n             encoder_hidden_states=pooled_output,\n             encoder_attention_mask=image_attention_mask,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n+        embeds = query_outputs[0]\n         embeds = embeds.to(dtype=self.vision_projection.weight.dtype)\n         image_embeds = self.vision_projection(embeds)\n         image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n \n-        if not return_dict:\n-            outputs = (image_embeds, vision_outputs[0]) + vision_outputs[2:]\n-            return tuple(output for output in outputs if output is not None)\n-\n         return Blip2VisionModelOutput(\n             image_embeds=image_embeds,\n             last_hidden_state=vision_outputs.last_hidden_state,\n@@ -1967,6 +1795,7 @@ def get_placeholder_mask(self, input_ids: torch.LongTensor, inputs_embeds: torch\n         special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         return special_image_mask\n \n+    @can_return_tuple\n     @auto_docstring\n     def forward(\n         self,\n@@ -1976,12 +1805,8 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Blip2ForConditionalGenerationModelOutput]:\n         r\"\"\"\n@@ -2057,14 +1882,10 @@ def forward(\n         >>> print(generated_text)\n         two\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         language_model_inputs, vision_outputs, query_outputs = self.get_image_features(\n             pixel_values, interpolate_pos_encoding=interpolate_pos_encoding, return_dict=True\n         )\n-        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n-        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -2081,13 +1902,9 @@ def forward(\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n-            logits = outputs.logits if return_dict else outputs[0]\n+            logits = outputs[0]\n             loss = None\n             # we compute the loss here since we need to take into account the sequence length of the query embeds\n             if labels is not None:\n@@ -2102,25 +1919,17 @@ def forward(\n \n                 loss = loss_fct(shift_logits.view(-1, self.config.text_config.vocab_size), shift_labels.view(-1))\n         else:\n+            kwargs[\"return_dict\"] = True\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n                 decoder_input_ids=decoder_input_ids,\n                 decoder_attention_mask=decoder_attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n             loss = outputs.loss\n             logits = outputs.logits\n-            outputs = outputs.to_tuple() if not return_dict else outputs\n-\n-        if not return_dict:\n-            output = (logits, vision_outputs, query_outputs, outputs)\n-            return ((loss,) + output) if loss is not None else output\n \n         return Blip2ForConditionalGenerationModelOutput(\n             loss=loss,"
        },
        {
            "sha": "5a1a38913fc7362aca6c30d7d0e96c4f5f9e69c4",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -329,13 +329,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "f652b2309a67821cb2afc4fb46de4b480fb60820",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -332,13 +332,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "0f126353e98084fbfbcd17c3264f6c3b1878b158",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -658,13 +658,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "a07bd06205ac53957ecbc5bff6c8743854837ac0",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -33,7 +33,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -608,7 +607,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, GotOcr2ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "470afc8cbab12939491034a631175b4f67333db6",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -21,7 +21,6 @@\n \n from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -334,7 +333,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, GotOcr2ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "c86262b95b7a86b5fcf7fda559beb7eb30b430ee",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 43,
            "deletions": 268,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -30,25 +30,19 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...masking_utils import create_causal_mask\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PretrainedConfig, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_idefics import IdeficsConfig\n from .perceiver import IdeficsPerceiverResampler\n from .vision import IdeficsVisionEmbeddings, IdeficsVisionTransformer\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -582,11 +576,9 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n         # if key_value_states are provided this layer is used as a cross-attention layer\n         is_cross_attention = self.is_cross_attention or key_value_states is not None\n \n@@ -624,13 +616,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -646,9 +632,6 @@ def forward(\n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n \n-        if output_attentions:\n-            attn_weights = None\n-\n         return attn_output, attn_weights\n \n \n@@ -674,43 +657,26 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n         self.dropout = config.dropout\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[tuple[torch.Tensor]] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n-        \"\"\"\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n-            output_attentions=output_attentions,\n-            use_cache=use_cache,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -724,12 +690,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class IdeficsGatedCrossAttentionLayer(GradientCheckpointingLayer):\n@@ -800,34 +761,25 @@ def __init__(self, config: IdeficsConfig, layer_idx: Optional[int] = None):\n             raise ValueError(\"Alpha parameters not initialized correctly!\")\n \n     @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         image_hidden_states: Optional[torch.Tensor] = None,\n         image_attention_mask: Optional[torch.Tensor] = None,\n         cross_attention_gate: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n         past_key_values: Optional[tuple[torch.Tensor]] = None,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            image_attention_mask (`torch.FloatTensor`, *optional*): image attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-            cross_attention_gate (`torch.FloatTensor`, *optional*):\n-                gate of size `(batch, seq_len)` used to zero-out cross-attention output for tokens attending no images.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            use_cache (`bool`, *optional*):\n-                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n-                (see `past_key_values`).\n-            past_key_values (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        image_hidden_states (`torch.FloatTensor`):\n+            Input to the layer of shape `(batch, seq_len, embed_dim)`\n+        image_attention_mask (`torch.FloatTensor`, *optional*):\n+            image attention mask of size\n+            `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+        cross_attention_gate (`torch.FloatTensor`, *optional*):\n+            gate of size `(batch, seq_len)` used to zero-out cross-attention output for tokens attending no images.\n         \"\"\"\n         if image_hidden_states is None:\n             raise ValueError(\n@@ -848,11 +800,10 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights = self.cross_attn(\n+        hidden_states, _ = self.cross_attn(\n             hidden_states=hidden_states,\n             key_value_states=image_hidden_states,\n             attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n             **kwargs,\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.config, training=self.training)\n@@ -867,12 +818,7 @@ def forward(\n         hidden_states = nn.functional.dropout(hidden_states, p=self.config, training=self.training)\n         hidden_states = residual + self.act_dense(self.alpha_dense) * hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -887,6 +833,11 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _can_compile_fullgraph = False  # IDEFICS cannot compile due to dynamic control flow when checking inputs\n     _supports_attention_backend = True\n \n+    _can_record_outputs = {\n+        \"hidden_states\": IdeficsDecoderLayer,\n+        \"attentions\": OutputRecorder(IdeficsAttention, index=1, layer_name=\"self_attn\"),\n+    }\n+\n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only\n         # inference and fine-tuning - so the proper init weights code has been removed - the m4 code\n@@ -997,7 +948,7 @@ def freeze_text_layers(self, module_exceptions=[]):\n     def freeze_vision_layers(self, module_exceptions=[]):\n         freeze_model(self.vision_model, module_exceptions=module_exceptions)\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -1011,12 +962,9 @@ def forward(\n         perceiver_embeddings: Optional[torch.FloatTensor] = None,\n         image_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, IdeficsBaseModelOutputWithPast]:\n         r\"\"\"\n         image_encoder_embeddings (`torch.FloatTensor`, *optional*):\n@@ -1028,30 +976,12 @@ def forward(\n         \"\"\"\n         device = input_ids.device if input_ids is not None else inputs_embeds.device\n \n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training and use_cache:\n-            logger.warning_once(\n-                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n-            )\n-            use_cache = False\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n@@ -1137,192 +1067,49 @@ def forward(\n                 (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n             )\n \n-        attention_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         hidden_states = inputs_embeds\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-\n         for idx, decoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n             # TODO(ls): Add cross attention values to respective lists\n             if idx % self.cross_layer_interval == 0:\n                 cross_attn_block = self.gated_cross_attn_layers[idx // self.cross_layer_interval]\n-                outputs = cross_attn_block(\n+                hidden_states = cross_attn_block(\n                     hidden_states,\n-                    attention_mask,\n+                    causal_mask,\n                     image_hidden_states,\n                     image_attention_mask=image_attention_mask,\n                     cross_attention_gate=cross_attention_gate,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n                     past_key_values=None,  # not implemented\n                     **kwargs,\n                 )\n-                hidden_states = outputs[0]\n \n-            layer_outputs = decoder_layer(\n+            hidden_states = decoder_layer(\n                 hidden_states,\n-                attention_mask=attention_mask,\n+                attention_mask=causal_mask,\n                 position_ids=position_ids,\n                 past_key_values=past_key_values,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n                 cache_position=cache_position,\n                 **kwargs,\n             )\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n \n         hidden_states = self.norm(hidden_states)\n-\n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n-\n         image_hidden_states = image_hidden_states.view(batch_size, num_images, image_seq_len, image_hidden_size)\n \n         return IdeficsBaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=past_key_values,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n             image_hidden_states=image_hidden_states,\n+            past_key_values=past_key_values,\n         )\n \n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class IdeficsForVisionText2Text(IdeficsPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n@@ -1378,10 +1165,7 @@ def forward(\n         image_attention_mask: Optional[torch.Tensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, IdeficsCausalLMOutputWithPast]:\n@@ -1422,13 +1206,6 @@ def forward(\n         >>> generate_ids = model.generate(**inputs, max_new_tokens=6)\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True)\n         ```\"\"\"\n-\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.model(\n             input_ids=input_ids,\n@@ -1441,8 +1218,6 @@ def forward(\n             perceiver_embeddings=perceiver_embeddings,\n             image_attention_mask=image_attention_mask,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n             cache_position=cache_position,"
        },
        {
            "sha": "72521761d9d1786ec297bc539238aa6560f84176",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -243,13 +243,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "249883eb1c85b7d306b8bfd797c4b0aa37d360bf",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 37,
            "deletions": 144,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -32,6 +32,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_idefics2 import Idefics2Config, Idefics2PerceiverConfig, Idefics2VisionConfig\n \n@@ -335,30 +336,21 @@ def __init__(self, config: Idefics2VisionConfig):\n         self.mlp = Idefics2VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -367,12 +359,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoder with Siglip->Idefics2\n@@ -392,68 +379,22 @@ def __init__(self, config: Idefics2Config):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n @auto_docstring\n@@ -503,6 +444,10 @@ class Idefics2VisionTransformer(Idefics2PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Idefics2EncoderLayer,\n+        \"attentions\": Idefics2VisionAttention,\n+    }\n \n     def __init__(self, config: Idefics2VisionConfig):\n         super().__init__(config)\n@@ -519,25 +464,18 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings = value\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n         r\"\"\"\n         patch_attention_mask (`torch.BoolTensor` of shape `(batch_size, num_patches_height, num_patches_width)`, *optional*):\n             The attention mask for the patches.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n             patch_size = self.config.patch_size\n@@ -561,25 +499,16 @@ def forward(\n         elif self.config._attn_implementation != \"flash_attention_2\":\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=patch_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n-        if not return_dict:\n-            return (last_hidden_state,) + encoder_outputs[1:]\n-\n-        return BaseModelOutput(\n-            last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=last_hidden_state)\n \n \n # Copied from transformers.models.llama.modeling_llama.repeat_kv\n@@ -645,9 +574,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"\n         Runs Perceiver Self-Attention, with special (context, latents) appended along the `seq` dimension!\n \n@@ -680,13 +608,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -697,15 +619,13 @@ def forward(\n             is_causal=self.is_causal,\n             scaling=self.scaling,\n             dropout=0.0 if not self.training else self.attention_dropout,\n+            **kwargs,\n         )\n \n         attn_output = attn_output.reshape(bsz, q_len, self.num_heads * self.head_dim)\n         attn_output = self.o_proj(attn_output)\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_values\n+        return attn_output, attn_weights\n \n \n class Idefics2PerceiverLayer(nn.Module):\n@@ -735,10 +655,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        use_cache: Optional[bool] = False,\n-        **kwargs,\n-    ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         \"\"\"\n         Args:\n             latents (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n@@ -758,10 +676,11 @@ def forward(\n         latents = self.input_latents_norm(latents)\n         context = self.input_context_norm(context)\n \n-        latents, self_attn_weights, present_key_value = self.self_attn(\n+        latents, _ = self.self_attn(\n             latents=latents,\n             context=context,\n             attention_mask=attention_mask,\n+            **kwargs,\n         )\n         latents = residual + latents\n         residual = latents\n@@ -770,15 +689,7 @@ def forward(\n         latents = self.mlp(latents)\n         latents = residual + latents\n \n-        outputs = (latents,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights,)\n-\n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n-        return outputs\n+        return latents\n \n \n @auto_docstring(\n@@ -812,6 +723,7 @@ def forward(\n         self,\n         context: torch.Tensor,\n         attention_mask: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.Tensor:\n         r\"\"\"\n         context (`torch.FloatTensor` of shape `(batch, seq_len, embed_dim)`):\n@@ -832,18 +744,14 @@ def forward(\n \n         compressed_context = latents\n         for perceiver_layer in self.layers:\n-            layer_outputs = perceiver_layer(\n+            compressed_context = perceiver_layer(\n                 compressed_context,\n                 context,\n                 attention_mask=attention_mask,\n                 position_ids=None,\n-                past_key_values=None,\n-                output_attentions=False,\n-                use_cache=False,\n+                **kwargs,\n             )\n \n-            compressed_context = layer_outputs[0]\n-\n         compressed_context = self.norm(compressed_context)\n \n         return compressed_context\n@@ -1020,10 +928,7 @@ def forward(\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,\n         image_hidden_states: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, Idefics2BaseModelOutputWithPast]:\n         r\"\"\"\n@@ -1032,12 +937,6 @@ def forward(\n         image_hidden_states (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)`):\n             The hidden states of the image encoder after modality projection and perceiver resampling.\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        use_cache = use_cache if use_cache is not None else self.config.use_cache\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if self.training and self.text_model.gradient_checkpointing and use_cache:\n             logger.warning_once(\n@@ -1053,10 +952,6 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n-        if not isinstance(past_key_values, (type(None), Cache)):\n-            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n-\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache(config=self.config)\n \n@@ -1080,16 +975,14 @@ def forward(\n                 image_hidden_states=image_hidden_states,\n             )\n \n+        kwargs[\"return_dict\"] = True\n         outputs = self.text_model(\n             inputs_embeds=inputs_embeds,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             cache_position=cache_position,\n-            return_dict=True,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "a9fb0e54339e683d802eedc88bf08f398106d8f6",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 17,
            "deletions": 88,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -31,6 +31,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_idefics3 import Idefics3Config, Idefics3VisionConfig\n \n@@ -294,30 +295,21 @@ def __init__(self, config: Idefics3VisionConfig):\n         self.mlp = Idefics3VisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoderLayer.forward\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -326,12 +318,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n # Copied from transformers.models.siglip.modeling_siglip.SiglipEncoder with Siglip->Idefics3\n@@ -351,68 +338,22 @@ def __init__(self, config: Idefics3Config):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n             layer_outputs = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n+            hidden_states = layer_outputs\n \n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n # Copied from transformers.models.llama.modeling_llama.repeat_kv\n@@ -513,6 +454,10 @@ class Idefics3VisionTransformer(Idefics3PreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Idefics3EncoderLayer,\n+        \"attentions\": Idefics3VisionAttention,\n+    }\n \n     def __init__(self, config: Idefics3VisionConfig):\n         super().__init__(config)\n@@ -531,20 +476,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings = value\n \n+    @check_model_inputs\n     def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n             patch_size = self.patch_size\n@@ -568,24 +505,16 @@ def forward(\n         elif not torch.any(~patch_attention_mask):\n             patch_attention_mask = None\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=patch_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n-        if not return_dict:\n-            return (last_hidden_state,) + encoder_outputs[1:]\n-\n         return BaseModelOutput(\n             last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "af039a508c5da7c4653137f2cccb7a5620641bd4",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 67,
            "deletions": 250,
            "changes": 317,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -36,6 +36,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n \n@@ -220,7 +221,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -237,13 +237,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -259,8 +253,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.projection(attn_output)\n \n-        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n-        return outputs\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipMLP\n@@ -289,29 +282,20 @@ def __init__(self, config: InstructBlipConfig):\n         self.mlp = InstructBlipMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             head_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + residual\n         residual = hidden_states\n@@ -320,12 +304,7 @@ def forward(\n \n         hidden_states = hidden_states + residual\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -384,73 +363,32 @@ def __init__(self, config: InstructBlipConfig):\n         self.layers = nn.ModuleList([InstructBlipEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Embedded representation of the inputs. Should be float, not int tokens.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n # Copied from transformers.models.blip.modeling_blip.BlipVisionModel with Blip->InstructBlip, BLIP->INSTRUCTBLIP\n class InstructBlipVisionModel(InstructBlipPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config: InstructBlipVisionConfig\n+    _can_record_outputs = {\n+        \"hidden_states\": InstructBlipEncoderLayer,\n+        \"attentions\": InstructBlipAttention,\n+    }\n \n     def __init__(self, config: InstructBlipVisionConfig):\n         super().__init__(config)\n@@ -463,47 +401,33 @@ def __init__(self, config: InstructBlipVisionConfig):\n \n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n     def get_input_embeddings(self):\n@@ -563,7 +487,7 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n@@ -629,9 +553,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput with Bert->InstructBlipQFormer\n@@ -682,19 +604,18 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.attention(\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attn_output, hidden_states)\n+        return attention_output\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate with Bert->InstructBlipQFormer\n@@ -756,35 +677,30 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        self_attention_outputs = self.attention(\n+        attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n \n             if self.has_cross_attention:\n                 if encoder_hidden_states is None:\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n-                cross_attention_outputs = self.crossattention(\n+                query_attention_output = self.crossattention(\n                     query_attention_output,\n                     attention_mask=attention_mask,\n                     head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n+                    **kwargs,\n                 )\n-                query_attention_output = cross_attention_outputs[0]\n-                # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -808,9 +724,7 @@ def forward(\n                 self.seq_len_dim,\n                 attention_output,\n             )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -833,64 +747,33 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @can_return_tuple\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict=True,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n                 query_length=query_length,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if query_length > 0 and layer_module.has_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n         )\n \n \n@@ -956,6 +839,16 @@ class InstructBlipQFormerModel(InstructBlipPreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    _can_record_outputs = {\n+        \"hidden_states\": InstructBlipQFormerLayer,\n+        \"attentions\": [\n+            OutputRecorder(InstructBlipQFormerMultiHeadAttention, index=1, layer_name=\".attention\"),\n+        ],\n+        \"cross_attentions\": [\n+            OutputRecorder(InstructBlipQFormerMultiHeadAttention, index=1, layer_name=\".crossattention\"),\n+        ],\n+    }\n+\n     def __init__(self, config: InstructBlipQFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -1023,6 +916,8 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @check_model_inputs\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -1032,35 +927,13 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of:\n-            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n-            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n-            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n-            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n-            `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n+        query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n+            Hidden states to be used in the attention computation. If cross-attention,\n+            will be used for the query (i.e., key and value will use the encoder_hidden_states).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if input_ids is None and query_embeds is None:\n             raise ValueError(\"You have to specify query_embeds when input_ids is None\")\n \n@@ -1109,30 +982,21 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             query_length=query_length,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = sequence_output[:, 0, :]\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n \n@@ -1222,11 +1086,7 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[tuple, InstructBlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n@@ -1245,30 +1105,19 @@ def forward(\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipProcessor`]. See [`InstructBlipProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n \n             Only relevant in case an encoder-decoder language model (like T5) is used.\n         \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         # step 1: forward the images through the vision encoder,\n         # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n         vision_outputs = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n         image_embeds = vision_outputs[0]\n \n@@ -1287,9 +1136,7 @@ def forward(\n             query_embeds=query_tokens,\n             encoder_hidden_states=image_embeds,\n             encoder_attention_mask=image_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n         query_output = query_outputs[0][:, : query_tokens.size(1), :]\n \n@@ -1308,10 +1155,6 @@ def forward(\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n         else:\n@@ -1320,10 +1163,6 @@ def forward(\n                 attention_mask=attention_mask,\n                 decoder_input_ids=decoder_input_ids,\n                 decoder_attention_mask=decoder_attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n \n@@ -1497,12 +1336,8 @@ def forward(\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n-        use_cache: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InstructBlipForConditionalGenerationModelOutput]:\n         r\"\"\"\n@@ -1521,14 +1356,6 @@ def forward(\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipProcessor`]. See [`InstructBlipProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default.\n@@ -1573,7 +1400,6 @@ def forward(\n         >>> print(generated_text)\n         The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV, which is parked in the middle of a busy city street. This is an unconventional approach to ironing clothes, as it requires the man to balance himself and his ironing equipment on top of the vehicle while navigating through traffic. Additionally, the presence of taxis and other vehicles in the scene further emphasizes the unusual nature of this situation.\n         ```\"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         language_model_inputs, vision_outputs, query_outputs = self.get_image_features(\n             pixel_values,\n@@ -1582,8 +1408,6 @@ def forward(\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=True,\n         )\n-        vision_outputs = vision_outputs.to_tuple() if not return_dict else vision_outputs\n-        query_outputs = query_outputs.to_tuple() if not return_dict else query_outputs\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n@@ -1599,34 +1423,27 @@ def forward(\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n-            logits = outputs.logits if return_dict else outputs[0]\n+            logits = outputs[0]\n             loss = None\n             if labels is not None:\n                 loss = self.loss_function(\n                     logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n                 )\n \n         else:\n+            kwargs[\"return_dict\"] = True\n             outputs = self.language_model(\n                 inputs_embeds=inputs_embeds,\n                 attention_mask=attention_mask,\n                 decoder_input_ids=decoder_input_ids,\n                 decoder_attention_mask=decoder_attention_mask,\n-                output_attentions=output_attentions,\n-                output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n                 labels=labels,\n-                use_cache=use_cache,\n                 **kwargs,\n             )\n-            loss = outputs.loss if return_dict else outputs[0]\n-            logits = outputs.logits if return_dict else outputs[1]\n+            loss = outputs.loss\n+            logits = outputs.logits\n \n         return InstructBlipForConditionalGenerationModelOutput(\n             loss=loss,"
        },
        {
            "sha": "863e22e82b17f8f9077d319b5698b22d4addc741",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 61,
            "deletions": 205,
            "changes": 266,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -40,6 +40,7 @@\n from ...processing_utils import Unpack\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblipvideo import (\n     InstructBlipVideoConfig,\n@@ -229,7 +230,6 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n         **kwargs,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -246,13 +246,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -268,8 +262,7 @@ def forward(\n         attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n         attn_output = self.projection(attn_output)\n \n-        outputs = (attn_output, attn_weights) if output_attentions else (attn_output, None)\n-        return outputs\n+        return attn_output, attn_weights\n \n \n class InstructBlipVideoMLP(nn.Module):\n@@ -296,29 +289,20 @@ def __init__(self, config: InstructBlipVideoConfig):\n         self.mlp = InstructBlipVideoMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n-                `(config.encoder_attention_heads,)`.\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             head_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = hidden_states + residual\n         residual = hidden_states\n@@ -327,12 +311,7 @@ def forward(\n \n         hidden_states = hidden_states + residual\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class InstructBlipVideoEncoder(nn.Module):\n@@ -351,72 +330,31 @@ def __init__(self, config: InstructBlipVideoConfig):\n         self.layers = nn.ModuleList([InstructBlipVideoEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Embedded representation of the inputs. Should be float, not int tokens.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n-        for idx, encoder_layer in enumerate(self.layers):\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask=attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config: InstructBlipVideoVisionConfig\n+    _can_record_outputs = {\n+        \"hidden_states\": InstructBlipVideoEncoderLayer,\n+        \"attentions\": InstructBlipVideoAttention,\n+    }\n \n     def __init__(self, config: InstructBlipVideoVisionConfig):\n         super().__init__(config)\n@@ -429,47 +367,33 @@ def __init__(self, config: InstructBlipVideoVisionConfig):\n \n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n     def get_input_embeddings(self):\n@@ -529,7 +453,7 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         # If this is instantiated as a cross-attention module, the keys\n         # and values come from an encoder; the attention mask needs to be\n@@ -595,9 +519,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(*new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n+        return context_layer, attention_probs\n \n \n class InstructBlipVideoQFormerSelfOutput(nn.Module):\n@@ -646,19 +568,18 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        self_outputs = self.attention(\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        attn_output, _ = self.attention(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attn_output, hidden_states)\n+        return attention_output\n \n \n class InstructBlipVideoQFormerIntermediate(nn.Module):\n@@ -718,35 +639,30 @@ def forward(\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        self_attention_outputs = self.attention(\n+        attention_output = self.attention(\n             hidden_states,\n             attention_mask=attention_mask,\n             head_mask=head_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n-        attention_output = self_attention_outputs[0]\n-        outputs = self_attention_outputs[1:]\n \n         if query_length > 0:\n             query_attention_output = attention_output[:, :query_length, :]\n \n             if self.has_cross_attention:\n                 if encoder_hidden_states is None:\n                     raise ValueError(\"encoder_hidden_states must be given for cross-attention layers\")\n-                cross_attention_outputs = self.crossattention(\n+                query_attention_output = self.crossattention(\n                     query_attention_output,\n                     attention_mask=attention_mask,\n                     head_mask=head_mask,\n                     encoder_hidden_states=encoder_hidden_states,\n                     encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n+                    **kwargs,\n                 )\n-                query_attention_output = cross_attention_outputs[0]\n-                # add cross attentions if we output attention weights\n-                outputs = outputs + cross_attention_outputs[1:]\n \n             layer_output = apply_chunking_to_forward(\n                 self.feed_forward_chunk_query,\n@@ -770,9 +686,7 @@ def forward(\n                 self.seq_len_dim,\n                 attention_output,\n             )\n-        outputs = (layer_output,) + outputs\n-\n-        return outputs\n+        return layer_output\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -794,64 +708,33 @@ def __init__(self, config):\n         )\n         self.gradient_checkpointing = False\n \n+    @can_return_tuple\n     def forward(\n         self,\n         hidden_states,\n         attention_mask=None,\n         head_mask=None,\n         encoder_hidden_states=None,\n         encoder_attention_mask=None,\n-        output_attentions=False,\n-        output_hidden_states=False,\n-        return_dict=True,\n         query_length=0,\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-        all_cross_attentions = () if output_attentions else None\n-\n         for i in range(self.config.num_hidden_layers):\n             layer_module = self.layer[i]\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n-\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n-            layer_outputs = layer_module(\n+            hidden_states = layer_module(\n                 hidden_states,\n                 attention_mask,\n                 layer_head_mask,\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n                 query_length=query_length,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n-                if query_length > 0 and layer_module.has_cross_attention:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[2],)\n-\n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    all_hidden_states,\n-                    all_self_attentions,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-            cross_attentions=all_cross_attentions,\n         )\n \n \n@@ -917,6 +800,16 @@ class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     _supports_sdpa = False\n     _supports_flex_attn = False\n \n+    _can_record_outputs = {\n+        \"hidden_states\": InstructBlipVideoQFormerLayer,\n+        \"attentions\": [\n+            OutputRecorder(InstructBlipVideoQFormerMultiHeadAttention, index=1, layer_name=\".attention\"),\n+        ],\n+        \"cross_attentions\": [\n+            OutputRecorder(InstructBlipVideoQFormerMultiHeadAttention, index=1, layer_name=\".crossattention\"),\n+        ],\n+    }\n+\n     def __init__(self, config: InstructBlipVideoQFormerConfig):\n         super().__init__(config)\n         self.config = config\n@@ -984,6 +877,8 @@ def get_extended_attention_mask(\n         extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n         return extended_attention_mask\n \n+    @check_model_inputs\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor,\n@@ -993,35 +888,13 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n         r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of:\n-            shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n-            value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n-            used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n-            value states given to this model) of shape `(batch_size, 1)` instead of all `decoder_input_ids` of shape\n-            `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n+        query_embeds (`torch.FloatTensor`  of shape `(batch_size, sequence_length, hidden_size)`):\n+            Hidden states to be used in the attention computation. If cross-attention,\n+            will be used for the query (i.e., key and value will use the encoder_hidden_states).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if input_ids is None and query_embeds is None:\n             raise ValueError(\"You have to specify query_embeds when input_ids is None\")\n \n@@ -1070,30 +943,21 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             embedding_output,\n             attention_mask=extended_attention_mask,\n             head_mask=head_mask,\n             encoder_hidden_states=encoder_hidden_states,\n             encoder_attention_mask=encoder_extended_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n             query_length=query_length,\n+            **kwargs,\n         )\n-        sequence_output = encoder_outputs[0]\n+        sequence_output = encoder_outputs.last_hidden_state\n         pooled_output = sequence_output[:, 0, :]\n \n-        if not return_dict:\n-            return (sequence_output, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPoolingAndCrossAttentions(\n             last_hidden_state=sequence_output,\n             pooler_output=pooled_output,\n-            past_key_values=encoder_outputs.past_key_values,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-            cross_attentions=encoder_outputs.cross_attentions,\n         )\n \n \n@@ -1241,14 +1105,6 @@ def forward(\n             - 0 for tokens that are **masked**.\n \n             [What are attention masks?](../glossary#attention-mask)\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of input sequence tokens in the vocabulary of the language model. Input tokens can optionally be\n-            provided to serve as text prompt, which the language model can continue.\n-\n-            Indices can be obtained using [`InstructBlipVideoProcessor`]. See [`InstructBlipVideoProcessor.__call__`] for\n-            details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n         decoder_attention_mask (`torch.BoolTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n             Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n             be used by default."
        },
        {
            "sha": "b9450b4bfd36d9ec0d138026b7fcc415992f931d",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 94,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -31,12 +31,12 @@\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, torch_int\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n@@ -124,8 +124,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size, seq_len, _ = hidden_states.size()\n \n@@ -160,34 +159,7 @@ def forward(\n         output = self.projection_layer(attn_output)\n         output = self.projection_dropout(output)\n \n-        outputs = (output, attn_weights) if output_attentions else (output, None)\n-        return outputs\n-\n-\n-@auto_docstring\n-class InternVLVisionPreTrainedModel(PreTrainedModel):\n-    config: InternVLVisionConfig\n-    base_model_prefix = \"internvl_vision\"\n-    main_input_name = \"pixel_values\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"InternVLVisionLayer\"]\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        super()._init_weights(module)\n-        if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.data.zero_()\n-            if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n-            if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n-        elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+        return output, attn_weights\n \n \n @dataclass\n@@ -376,11 +348,9 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        attention_output, attention_weights = self.attention(\n+        attention_output, _ = self.attention(\n             self.layernorm_before(hidden_states),  # in InternVLVision, layernorm is applied before self-attention\n-            output_attentions=output_attentions,\n         )\n \n         attention_output = self.lambda_1 * attention_output\n@@ -400,7 +370,7 @@ def forward(\n         # second residual connection\n         layer_output = layer_output + hidden_states\n \n-        return layer_output, attention_weights\n+        return layer_output\n \n \n class InternVLVisionEncoder(nn.Module):\n@@ -410,35 +380,48 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n         self.layer = nn.ModuleList([InternVLVisionLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n     ) -> Union[tuple, BaseModelOutput]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-\n-        for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n+        for layer_module in self.layer:\n+            hidden_states = layer_module(hidden_states)\n \n-            layer_outputs = layer_module(hidden_states, output_attentions)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n \n-            hidden_states = layer_outputs[0]\n \n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+@auto_docstring\n+class InternVLVisionPreTrainedModel(PreTrainedModel):\n+    config: InternVLVisionConfig\n+    base_model_prefix = \"internvl_vision\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"InternVLVisionLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n+    _can_record_outputs = {\n+        \"hidden_states\": InternVLVisionLayer,\n+        \"attentions\": InternVLVisionAttention,\n+    }\n \n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-        )\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n+        if isinstance(module, InternVLVisionEmbeddings):\n+            module.cls_token.data.zero_()\n+            if module.mask_token is not None:\n+                module.mask_token.data.zero_()\n+            if module.position_embeddings is not None:\n+                module.position_embeddings.data.zero_()\n+        elif isinstance(module, InternVLVisionLayer):\n+            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring\n@@ -466,25 +449,14 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n+        encoder_outputs = self.encoder(embedding_output)\n         sequence_output = encoder_outputs[0]\n         sequence_output = self.layernorm(sequence_output)\n \n@@ -668,18 +640,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InternVLModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -712,10 +675,6 @@ def forward(\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -870,10 +829,6 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n@@ -914,11 +869,6 @@ def forward(\n         >>> print(processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True))\n         The images depict the Statue of Liberty and the Golden Gate Bridge.\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -937,10 +887,6 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n             **kwargs,"
        },
        {
            "sha": "823adc3904dbacb641ab7522ddce12a4dfa78dc8",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 41,
            "deletions": 82,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -24,12 +24,12 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.generic import check_model_inputs\n from ..clip.modeling_clip import CLIPMLP\n from ..janus.modeling_janus import JanusVisionAttention\n from ..llama.modeling_llama import LlamaRMSNorm\n@@ -93,8 +93,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[torch.Tensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ):\n         batch_size, seq_len, _ = hidden_states.size()\n \n@@ -129,34 +128,7 @@ def forward(\n         output = self.projection_layer(attn_output)\n         output = self.projection_dropout(output)\n \n-        outputs = (output, attn_weights) if output_attentions else (output, None)\n-        return outputs\n-\n-\n-@auto_docstring\n-class InternVLVisionPreTrainedModel(PreTrainedModel):\n-    config: InternVLVisionConfig\n-    base_model_prefix = \"internvl_vision\"\n-    main_input_name = \"pixel_values\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"InternVLVisionLayer\"]\n-    _supports_sdpa = True\n-    _supports_flash_attn = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        super()._init_weights(module)\n-        if isinstance(module, InternVLVisionEmbeddings):\n-            module.cls_token.data.zero_()\n-            if module.mask_token is not None:\n-                module.mask_token.data.zero_()\n-            if module.position_embeddings is not None:\n-                module.position_embeddings.data.zero_()\n-        elif isinstance(module, InternVLVisionLayer):\n-            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n-            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n+        return output, attn_weights\n \n \n @dataclass\n@@ -334,11 +306,9 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        output_attentions: bool = False,\n     ) -> Union[tuple[torch.Tensor], tuple[torch.Tensor, torch.Tensor]]:\n-        attention_output, attention_weights = self.attention(\n+        attention_output, _ = self.attention(\n             self.layernorm_before(hidden_states),  # in InternVLVision, layernorm is applied before self-attention\n-            output_attentions=output_attentions,\n         )\n \n         attention_output = self.lambda_1 * attention_output\n@@ -358,7 +328,7 @@ def forward(\n         # second residual connection\n         layer_output = layer_output + hidden_states\n \n-        return layer_output, attention_weights\n+        return layer_output\n \n \n class InternVLVisionEncoder(nn.Module):\n@@ -368,35 +338,48 @@ def __init__(self, config: InternVLVisionConfig) -> None:\n         self.layer = nn.ModuleList([InternVLVisionLayer(config) for i in range(config.num_hidden_layers)])\n         self.gradient_checkpointing = False\n \n-    @can_return_tuple\n+    @check_model_inputs\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        output_attentions: bool = False,\n-        output_hidden_states: bool = False,\n     ) -> Union[tuple, BaseModelOutput]:\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attentions = () if output_attentions else None\n-\n-        for i, layer_module in enumerate(self.layer):\n-            if output_hidden_states:\n-                all_hidden_states = all_hidden_states + (hidden_states,)\n+        for layer_module in self.layer:\n+            hidden_states = layer_module(hidden_states)\n \n-            layer_outputs = layer_module(hidden_states, output_attentions)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n \n-            hidden_states = layer_outputs[0]\n \n-            if output_attentions:\n-                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n+@auto_docstring\n+class InternVLVisionPreTrainedModel(PreTrainedModel):\n+    config: InternVLVisionConfig\n+    base_model_prefix = \"internvl_vision\"\n+    main_input_name = \"pixel_values\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"InternVLVisionLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n-        if output_hidden_states:\n-            all_hidden_states = all_hidden_states + (hidden_states,)\n+    _can_record_outputs = {\n+        \"hidden_states\": InternVLVisionLayer,\n+        \"attentions\": InternVLVisionAttention,\n+    }\n \n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attentions,\n-        )\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        super()._init_weights(module)\n+        if isinstance(module, InternVLVisionEmbeddings):\n+            module.cls_token.data.zero_()\n+            if module.mask_token is not None:\n+                module.mask_token.data.zero_()\n+            if module.position_embeddings is not None:\n+                module.position_embeddings.data.zero_()\n+        elif isinstance(module, InternVLVisionLayer):\n+            module.lambda_1.data.fill_(self.config.layer_scale_init_value)\n+            module.lambda_2.data.fill_(self.config.layer_scale_init_value)\n \n \n @auto_docstring\n@@ -424,25 +407,14 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n     ) -> Union[tuple, InternVLVisionModelOutputWithPooling]:\n         r\"\"\"\n         bool_masked_pos (`torch.BoolTensor` of shape `(batch_size, num_patches)`, *optional*):\n             Boolean masked positions. Indicates which patches are masked (1) and which aren't (0).\n         \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n \n-        encoder_outputs = self.encoder(\n-            embedding_output,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n+        encoder_outputs = self.encoder(embedding_output)\n         sequence_output = encoder_outputs[0]\n         sequence_output = self.layernorm(sequence_output)\n \n@@ -584,18 +556,9 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, InternVLModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -628,10 +591,6 @@ def forward(\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )"
        },
        {
            "sha": "84b78831d625ea34184f0009ec797c550f1d7ac4",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -40,12 +40,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import (\n-    TensorType,\n-    filter_out_non_signature_kwargs,\n-    is_vision_available,\n-    logging,\n-)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n if is_vision_available():"
        },
        {
            "sha": "64b285f84c3c4722e28cd6266a2b72714830fa40",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 142,
            "deletions": 86,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -42,6 +42,7 @@\n     logging,\n     torch_int,\n )\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_janus import JanusConfig, JanusVisionConfig, JanusVQVAEConfig\n \n@@ -373,29 +374,20 @@ def __init__(self, config: JanusVisionConfig):\n         self.mlp = JanusVisionMLP(config)\n         self.config = config\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -404,12 +396,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class JanusVisionEncoder(nn.Module):\n@@ -428,74 +415,157 @@ def __init__(self, config: JanusVisionConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n \n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n+class JanusAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.is_causal = False\n+        self.attention_dropout = config.attention_dropout\n+\n+        # small tweak here compared to CLIP, no bias here\n+        self.qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, bias=False)\n+\n+        if config.qkv_bias:\n+            q_bias = nn.Parameter(torch.zeros(self.embed_dim))\n+            v_bias = nn.Parameter(torch.zeros(self.embed_dim))\n+        else:\n+            q_bias = None\n+            v_bias = None\n+\n+        if q_bias is not None:\n+            qkv_bias = torch.cat((q_bias, torch.zeros_like(v_bias, requires_grad=False), v_bias))\n+            self.qkv.bias = nn.Parameter(qkv_bias)\n \n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n+        self.projection = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        head_mask: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        bsz, tgt_len, embed_dim = hidden_states.size()\n+\n+        mixed_qkv = self.qkv(hidden_states)\n+\n+        mixed_qkv = mixed_qkv.reshape(bsz, tgt_len, 3, self.num_heads, embed_dim // self.num_heads).permute(\n+            2, 0, 3, 1, 4\n         )\n+        query_states, key_states, value_states = mixed_qkv[0], mixed_qkv[1], mixed_qkv[2]\n+\n+        attention_interface: Callable = eager_attention_forward\n+\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask=None,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scale,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.projection(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class JanusMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+class JanusEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: JanusConfig):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = JanusAttention(config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = JanusMLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            head_mask=attention_mask,\n+            **kwargs,\n+        )\n+        hidden_states = hidden_states + residual\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+\n+        hidden_states = hidden_states + residual\n+\n+        return hidden_states\n \n \n @auto_docstring\n class JanusVisionModel(JanusPreTrainedModel):\n     main_input_name = \"pixel_values\"\n     config: JanusVisionConfig\n+    _can_record_outputs = {\n+        \"hidden_states\": JanusEncoderLayer,\n+        \"attentions\": JanusAttention,\n+    }\n \n     def __init__(self, config: JanusVisionConfig):\n         super().__init__(config)\n@@ -508,47 +578,33 @@ def __init__(self, config: JanusVisionConfig):\n \n         self.post_init()\n \n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         if pixel_values is None:\n             raise ValueError(\"You have to specify pixel_values\")\n \n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n         pooled_output = last_hidden_state[:, 0, :]\n         pooled_output = self.post_layernorm(pooled_output)\n \n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n     def get_input_embeddings(self):"
        },
        {
            "sha": "ee77fd701c1694f772e2c83c77ac336b8475edf1",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -344,13 +344,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -768,13 +762,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "783712660279667a40b5b783770408b68e961d24",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 28,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -24,7 +24,6 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n@@ -254,19 +253,10 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, LlavaModelOutputWithPast]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -300,10 +290,6 @@ def forward(\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             **kwargs,\n         )\n@@ -392,10 +378,6 @@ def forward(\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n         labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         image_sizes: Optional[torch.Tensor] = None,\n@@ -428,11 +410,6 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a busy city street with a stop sign prominently displayed\"\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n         vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n@@ -451,10 +428,6 @@ def forward(\n             inputs_embeds=inputs_embeds,\n             vision_feature_layer=vision_feature_layer,\n             vision_feature_select_strategy=vision_feature_select_strategy,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n             cache_position=cache_position,\n             image_sizes=image_sizes,\n             **kwargs,"
        },
        {
            "sha": "cf1c66beb0652268fc91a3ef61f6692165e3413f",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -23,16 +23,12 @@\n     auto_docstring,\n     can_return_tuple,\n     filter_out_non_signature_kwargs,\n-    logging,\n     torch_int,\n )\n from ...utils.generic import check_model_inputs\n from .configuration_metaclip_2 import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MetaClip2TextEmbeddings(nn.Module):\n     def __init__(self, config: MetaClip2TextConfig):\n         super().__init__()\n@@ -233,13 +229,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "644f6523d111b221a1bb927b5672f9b908876010",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -29,7 +29,6 @@\n from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n@@ -303,7 +302,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "0277568d2b001035c3898a8cc8add85312db2ce7",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -20,7 +20,6 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import logging\n from ..llava.modeling_llava import (\n@@ -177,7 +176,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         image_sizes: torch.Tensor = None,\n-        **kwargs: Unpack[FlashAttentionKwargs],\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> Union[tuple, Mistral3ModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = ("
        },
        {
            "sha": "6f06a2214768b8efa136a1bc286e9c5dc5204fbb",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -175,13 +175,7 @@ def forward(\n         attention_interface: Callable = eager_attention_forward\n \n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "8390f66ff39b9fe9e8910ef524be7847c7cdc3f8",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 79,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -32,7 +32,8 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_ovis2 import Ovis2Config, Ovis2VisionConfig\n \n@@ -333,17 +334,17 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n         norm_hidden_states = self.rms_norm1(hidden_states)\n-        attn_output, attn_weights = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask)\n+        attn_output, _ = self.attention(hidden_states=norm_hidden_states, attention_mask=attention_mask, **kwargs)\n \n         hidden_states = hidden_states + attn_output\n         norm_hidden_states = self.rms_norm2(hidden_states)\n         mlp_output = self.ffn(norm_hidden_states)\n \n         hidden_states = hidden_states + mlp_output\n-        return (hidden_states, attn_weights) if output_attentions else (hidden_states, None)\n+        return hidden_states\n \n \n class Ovis2VisionEncoder(nn.Module):\n@@ -363,67 +364,18 @@ def __init__(self, config: Ovis2VisionConfig):\n \n     # Ignore copy\n     @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n-                hidden_states,\n-                attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n+            hidden_states = encoder_layer(hidden_states, attention_mask, **kwargs)\n \n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class Ovis2VisionTransformer(nn.Module):\n@@ -440,32 +392,20 @@ def forward(\n         self,\n         pixel_values,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(pixel_values)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n-        return BaseModelOutput(\n-            last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=last_hidden_state)\n \n \n class Ovis2VisualEmbeddingTable(nn.Embedding):\n@@ -516,10 +456,9 @@ def __init__(self, config: Ovis2VisionConfig):\n         )\n         self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> tuple[torch.Tensor, torch.Tensor]:\n-        outputs = self.transformer(pixel_values)\n-        last_hidden_state = outputs.last_hidden_state\n-\n+    def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n+        outputs = self.transformer(pixel_values, **kwargs)\n+        last_hidden_state = outputs[0]\n         if self.config.hidden_stride > 1:\n             num_images, seq_len, hidden_dim = last_hidden_state.shape\n             hidden_stride = self.config.hidden_stride"
        },
        {
            "sha": "4163d62be5e558613429ed6bbabd476fd1898958",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 22,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -22,7 +22,8 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ..aimv2.modeling_aimv2 import Aimv2Attention, Aimv2EncoderLayer\n from ..auto import AutoModel\n from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n@@ -90,6 +91,20 @@ def __init__(self, config: Ovis2VisionConfig):\n         super().__init__(config)\n         self.layers = nn.ModuleList([Ovis2VisionEncoderLayer(config) for _ in range(config.num_hidden_layers)])\n \n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutput:\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            hidden_states = encoder_layer(hidden_states, attention_mask, **kwargs)\n+\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n+\n \n class Ovis2VisionTransformer(nn.Module):\n     def __init__(self, config: Ovis2VisionConfig):\n@@ -105,32 +120,20 @@ def forward(\n         self,\n         pixel_values,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n     ):\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(pixel_values)\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=True,\n+            **kwargs,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.rms_norm(last_hidden_state)\n \n-        return BaseModelOutput(\n-            last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=last_hidden_state)\n \n \n class Ovis2VisualEmbeddingTable(nn.Embedding):\n@@ -171,10 +174,9 @@ def __init__(self, config: Ovis2VisionConfig):\n         )\n         self.head_norm = nn.LayerNorm(self.vocab_size - self.num_visual_indicator_tokens)\n \n-    def forward(self, pixel_values: torch.FloatTensor) -> tuple[torch.Tensor, torch.Tensor]:\n-        outputs = self.transformer(pixel_values)\n-        last_hidden_state = outputs.last_hidden_state\n-\n+    def forward(self, pixel_values: torch.FloatTensor, **kwargs) -> tuple[torch.Tensor, torch.Tensor]:\n+        outputs = self.transformer(pixel_values, **kwargs)\n+        last_hidden_state = outputs[0]\n         if self.config.hidden_stride > 1:\n             num_images, seq_len, hidden_dim = last_hidden_state.shape\n             hidden_stride = self.config.hidden_stride"
        },
        {
            "sha": "349f2e02e2f2562007700827829907b9b9e8ba27",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 20,
            "deletions": 83,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -109,7 +109,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         input_shape = hidden_states.shape[:-1]\n@@ -148,29 +148,20 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Phi4MultimodalVisionMLP(config)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -179,12 +170,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Phi4MultimodalVisionEncoder(nn.Module):\n@@ -205,68 +191,22 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n def _trunc_normal_(tensor, mean, std, a, b):\n@@ -376,6 +316,11 @@ class Phi4MultimodalVisionPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Phi4MultimodalVisionEncoderLayer,\n+        \"attentions\": Phi4MultimodalVisionAttention,\n+    }\n+\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Phi4MultimodalVisionEmbeddings):\n@@ -543,18 +488,13 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embedding\n \n+    @check_model_inputs\n     def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n             patch_attention_mask = torch.ones(\n@@ -585,8 +525,7 @@ def forward(\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -600,8 +539,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "ea226e4e1981bdfa22ff8a7cc5210e42543c6540",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 9,
            "deletions": 12,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -489,7 +489,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n         input_shape = hidden_states.shape[:-1]\n@@ -544,6 +544,11 @@ class Phi4MultimodalVisionPreTrainedModel(SiglipPreTrainedModel):\n     _supports_sdpa = True\n     _supports_flex_attn = True\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Phi4MultimodalVisionEncoderLayer,\n+        \"attentions\": Phi4MultimodalVisionAttention,\n+    }\n+\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Phi4MultimodalVisionEmbeddings):\n@@ -667,18 +672,13 @@ def __init__(self, config: Phi4MultimodalVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.embeddings.patch_embedding\n \n+    @check_model_inputs\n     def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n             patch_attention_mask = torch.ones(\n@@ -709,8 +709,7 @@ def forward(\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -724,8 +723,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "564c118fccb90bda30d276642ea0f9aea385db3c",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -206,13 +206,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         # Since we use packing, if flash_attention_2 is selected we rely on position_ids\n         if self.config._attn_implementation == \"flash_attention_2\":"
        },
        {
            "sha": "0720560a87a9fe6d36bfc9c573e2b133d6916cf2",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 45,
            "deletions": 128,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -30,7 +30,16 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs, torch_int\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    ModelOutput,\n+    TransformersKwargs,\n+    auto_docstring,\n+    can_return_tuple,\n+    filter_out_non_signature_kwargs,\n+    torch_int,\n+)\n+from ...utils.generic import check_model_inputs\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n \n@@ -428,29 +437,20 @@ def __init__(self, config: Union[SiglipVisionConfig, SiglipTextConfig]):\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = SiglipMLP(config)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -459,12 +459,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n @auto_docstring\n@@ -484,6 +479,11 @@ class SiglipPreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    _can_record_outputs = {\n+        \"hidden_states\": SiglipEncoderLayer,\n+        \"attentions\": SiglipAttention,\n+    }\n+\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, SiglipVisionEmbeddings):\n@@ -548,68 +548,22 @@ def __init__(self, config: SiglipConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class SiglipTextTransformer(nn.Module):\n@@ -630,14 +584,8 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -658,8 +606,7 @@ def forward(\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -672,8 +619,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -697,15 +642,14 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -728,8 +672,7 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -751,21 +694,14 @@ def __init__(self, config: SiglipVisionConfig):\n     def forward(\n         self,\n         pixel_values,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n \n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -776,8 +712,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooler_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -825,14 +759,13 @@ def __init__(self, config: SiglipVisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -857,9 +790,8 @@ def forward(\n \n         return self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n \n@@ -941,6 +873,7 @@ def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> torch.FloatTensor:\n         r\"\"\"\n         Returns:\n@@ -968,11 +901,13 @@ def get_image_features(\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n         pooled_output = vision_outputs.pooler_output\n \n         return pooled_output\n \n+    # NOTE: SiglipModel uses Pretrained backbones, so we don't need to add `check_model_inputs` here\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -982,9 +917,8 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         return_loss: Optional[bool] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> SiglipOutput:\n         r\"\"\"\n         return_loss (`bool`, *optional*):\n@@ -1016,25 +950,17 @@ def forward(\n         >>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n         31.9% that image 0 is 'a photo of 2 cats'\n         ```\"\"\"\n-        # Use SigLIP model's config for some fields (if specified) instead of those of vision & text components.\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values=pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n         text_outputs: BaseModelOutputWithPooling = self.text_model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         image_embeds = vision_outputs.pooler_output\n@@ -1099,15 +1025,14 @@ def __init__(self, config: SiglipConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.Tensor] = None,\n         labels: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> ImageClassifierOutput:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -1140,16 +1065,10 @@ def forward(\n         >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n         Predicted class: LABEL_1\n         ```\"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         outputs: BaseModelOutputWithPooling = self.vision_model(\n             pixel_values,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n+            **kwargs,\n         )\n \n         sequence_output = outputs.last_hidden_state\n@@ -1187,8 +1106,6 @@ def forward(\n         return ImageClassifierOutput(\n             loss=loss,\n             logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "9cac8a35f6f6ce1ded86eecfccc5a1cae7573573",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 90,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -35,7 +35,9 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, filter_out_non_signature_kwargs\n+from ...utils.generic import check_model_inputs\n from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n \n \n@@ -322,29 +324,20 @@ def __init__(self, config: Union[Siglip2VisionConfig, Siglip2TextConfig]):\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n         self.mlp = Siglip2MLP(config)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -353,12 +346,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class Siglip2Encoder(nn.Module):\n@@ -377,68 +365,22 @@ def __init__(self, config: Siglip2Config):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n-    @can_return_tuple\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n-\n-            layer_outputs = encoder_layer(\n+            hidden_states = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n+                **kwargs,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n-\n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states,\n-            hidden_states=encoder_states,\n-            attentions=all_attentions,\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n class Siglip2VisionTransformer(nn.Module):\n@@ -613,6 +555,11 @@ class Siglip2PreTrainedModel(PreTrainedModel):\n     _supports_flex_attn = True\n     _supports_attention_backend = True\n \n+    _can_record_outputs = {\n+        \"hidden_states\": Siglip2EncoderLayer,\n+        \"attentions\": Siglip2Attention,\n+    }\n+\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         if isinstance(module, Siglip2VisionEmbeddings):\n@@ -718,14 +665,8 @@ def forward(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n         if input_ids is None:\n             raise ValueError(\"You have to specify input_ids\")\n \n@@ -746,8 +687,7 @@ def forward(\n         encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n         last_hidden_state = encoder_outputs.last_hidden_state\n@@ -760,8 +700,6 @@ def forward(\n         return BaseModelOutputWithPooling(\n             last_hidden_state=last_hidden_state,\n             pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n \n@@ -785,15 +723,14 @@ def get_input_embeddings(self) -> nn.Module:\n     def set_input_embeddings(self, value):\n         self.text_model.embeddings.token_embedding = value\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n         Examples:\n@@ -816,8 +753,7 @@ def forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n+            **kwargs,\n         )\n \n \n@@ -872,7 +808,7 @@ def __init__(self, config: Siglip2VisionConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,\n@@ -1034,6 +970,7 @@ def get_image_features(\n \n         return pooled_output\n \n+    # NOTE: Siglip2Model uses Pretrained backbones, so we don't need to add `check_model_inputs` here\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1167,7 +1104,7 @@ def __init__(self, config: Siglip2Config) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @can_return_tuple\n+    @check_model_inputs\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "812b4df5d9c4f3884d6e3a7418c590f71aa3f9da",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 17,
            "deletions": 88,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -40,6 +40,7 @@\n     can_return_tuple,\n     logging,\n )\n+from ...utils.generic import check_model_inputs\n from ..auto import AutoModel\n from .configuration_smolvlm import SmolVLMConfig, SmolVLMVisionConfig\n \n@@ -273,29 +274,20 @@ def __init__(self, config: SmolVLMVisionConfig):\n         self.mlp = SmolVLMVisionMLP(config)\n         self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n \n+    @auto_docstring\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.FloatTensor]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor`):\n-                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n-            attention_mask (`torch.FloatTensor`):\n-                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n-            output_attentions (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-        \"\"\"\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.FloatTensor:\n         residual = hidden_states\n \n         hidden_states = self.layer_norm1(hidden_states)\n-        hidden_states, attn_weights = self.self_attn(\n+        hidden_states, _ = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n-            output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -304,12 +296,7 @@ def forward(\n         hidden_states = self.mlp(hidden_states)\n         hidden_states = residual + hidden_states\n \n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return hidden_states\n \n \n class SmolVLMEncoder(nn.Module):\n@@ -328,68 +315,22 @@ def __init__(self, config: SmolVLMConfig):\n         self.gradient_checkpointing = False\n \n     # Ignore copy\n+    @auto_docstring\n     def forward(\n         self,\n         inputs_embeds,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n-            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-                - 1 for tokens that are **not masked**,\n-                - 0 for tokens that are **masked**.\n-\n-                [What are attention masks?](../glossary#attention-mask)\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        encoder_states = () if output_hidden_states else None\n-        all_attentions = () if output_attentions else None\n-\n         hidden_states = inputs_embeds\n         for encoder_layer in self.layers:\n-            if output_hidden_states:\n-                encoder_states = encoder_states + (hidden_states,)\n             layer_outputs = encoder_layer(\n                 hidden_states,\n                 attention_mask,\n-                output_attentions=output_attentions,\n             )\n \n-            hidden_states = layer_outputs[0]\n-\n-            if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[1],)\n+            hidden_states = layer_outputs\n \n-        if output_hidden_states:\n-            encoder_states = encoder_states + (hidden_states,)\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n-        return BaseModelOutput(\n-            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n-        )\n+        return BaseModelOutput(last_hidden_state=hidden_states)\n \n \n @auto_docstring(\n@@ -402,6 +343,10 @@ class SmolVLMVisionTransformer(SmolVLMPreTrainedModel):\n     _supports_sdpa = True\n     _supports_flash_attn = True\n     _supports_flex_attn = True\n+    _can_record_outputs = {\n+        \"hidden_states\": SmolVLMEncoderLayer,\n+        \"attentions\": SmolVLMVisionAttention,\n+    }\n \n     def __init__(self, config: SmolVLMVisionConfig):\n         super().__init__(config)\n@@ -418,20 +363,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings = value\n \n+    @check_model_inputs\n     def forward(\n         self,\n         pixel_values,\n         patch_attention_mask: Optional[torch.BoolTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n     ) -> Union[tuple, BaseModelOutput]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n         batch_size = pixel_values.size(0)\n         if patch_attention_mask is None:\n             patch_size = self.patch_size\n@@ -455,24 +392,16 @@ def forward(\n         elif not torch.any(~patch_attention_mask):\n             patch_attention_mask = None\n \n-        encoder_outputs = self.encoder(\n+        encoder_outputs: BaseModelOutput = self.encoder(\n             inputs_embeds=hidden_states,\n             attention_mask=patch_attention_mask,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n         )\n \n-        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = encoder_outputs.last_hidden_state\n         last_hidden_state = self.post_layernorm(last_hidden_state)\n \n-        if not return_dict:\n-            return (last_hidden_state,) + encoder_outputs[1:]\n-\n         return BaseModelOutput(\n             last_hidden_state=last_hidden_state,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "bde505b4ea54cf1514ca22ac316b03068b9c69ee",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -324,13 +324,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         context_layer, attention_probs = attention_interface(\n             self,\n@@ -771,13 +765,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -846,13 +834,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "5a4b478ceef9544f05510d3b902ff9aa9e287038",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -299,13 +299,7 @@ def forward(\n \n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "76d36a74518835eb083bc915407bd954d542eea5",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -1007,6 +1007,12 @@ def wrapper(self, *args, **kwargs):\n             )\n             for k in capture_flags\n         }\n+\n+        # We let cross attentions to be saved separately because some models add `cross-attn` layer\n+        # when certain condtions are met. Let's output cross attention if attentions are requested (for BC)\n+        if \"output_attentions\" in recordable_keys:\n+            recordable_keys[\"output_cross_attentions\"] = recordable_keys[\"output_attentions\"]\n+\n         collected_outputs = defaultdict(tuple)\n         monkey_patched_layers = []\n \n@@ -1084,10 +1090,11 @@ def wrapped_forward(*args, **kwargs):\n         # Inject collected outputs into model output\n         for key in collected_outputs:\n             if key == \"hidden_states\":\n-                collected_outputs[key] = collected_outputs[key][:-1]\n                 if hasattr(outputs, \"vision_hidden_states\"):\n+                    collected_outputs[key] = collected_outputs[key][:-1]\n                     collected_outputs[key] += (outputs.vision_hidden_states,)\n                 elif hasattr(outputs, \"last_hidden_state\"):\n+                    collected_outputs[key] = collected_outputs[key][:-1]\n                     collected_outputs[key] += (outputs.last_hidden_state,)\n \n                 outputs[key] = collected_outputs[key]"
        },
        {
            "sha": "adc63c536b285a03f107b889348868c0043fce32",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -1132,6 +1132,7 @@ def prepare_config_and_inputs_for_common(self):\n \n     def create_and_check_model(self, config, input_ids, attention_mask):\n         model = Blip2TextModelWithProjection(config=config)\n+        model.set_attn_implementation(\"eager\")\n         model.to(torch_device)\n         model.eval()\n         with torch.no_grad():\n@@ -1289,6 +1290,7 @@ def prepare_config_and_inputs_for_common(self):\n     def create_and_check_model(self, config, pixel_values):\n         model = Blip2VisionModelWithProjection(config=config)\n         model.to(torch_device)\n+        model.set_attn_implementation(\"eager\")\n         model.eval()\n         with torch.no_grad():\n             result = model(pixel_values, output_attentions=True, output_hidden_states=True)"
        },
        {
            "sha": "bbc0c398061886741de68304193df91ce1e504a5",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -211,7 +211,7 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             hidden_states = outputs.hidden_states\n \n-            expected_num_hidden_states = len(self.model_tester.stage_num_blocks)\n+            expected_num_hidden_states = len(self.model_tester.stage_num_blocks) + 1\n             self.assertEqual(len(hidden_states), expected_num_hidden_states)\n \n             self.assertListEqual("
        },
        {
            "sha": "407f1dd6b99b07000ff5365dc380e75ced20aacf",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -538,8 +538,8 @@ def test_attention_outputs(self):\n             with torch.no_grad():\n                 outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n             attentions = outputs.attentions\n-            # IDEFICS does not support outputting attention score because it uses SDPA under the hood\n-            self.assertTrue(attentions[0] is None)\n+            self.assertFalse(attentions[0] is None)\n+            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n             out_len = len(outputs)\n \n             # Check attention is always last and order is fine\n@@ -556,8 +556,7 @@ def test_attention_outputs(self):\n             self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n \n             self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n-            # IDEFICS does not support outputting attention score because it uses SDPA under the hood\n-            self.assertTrue(self_attentions[0] is None)\n+            self.assertFalse(self_attentions[0] is None)\n \n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):"
        },
        {
            "sha": "aef0d98bbdfaa27b90c54490f68f398e43e4e1e3",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4e195f1949299221fce2693c369fdedb55ab1a78/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py?ref=4e195f1949299221fce2693c369fdedb55ab1a78",
            "patch": "@@ -134,6 +134,9 @@ def check_save_load(self, text_config, input_ids, attention_mask, vision_config,\n     def check_vision_text_output_attention(\n         self, text_config, input_ids, attention_mask, vision_config, pixel_values=None, **kwargs\n     ):\n+        # The backbones don't support dynamic attention setting, so we manually change it. FIXME; when bert is refactored\n+        vision_config._attn_implementation = \"eager\"\n+        text_config._attn_implementation = \"eager\"\n         vision_model, text_model = self.get_vision_text_model(vision_config, text_config)\n         model = VisionTextDualEncoderModel(vision_model=vision_model, text_model=text_model)\n         model.to(torch_device)"
        }
    ],
    "stats": {
        "total": 3339,
        "additions": 845,
        "deletions": 2494
    }
}