{
    "author": "yao-matrix",
    "message": "fix asr pipeline ut failures (#41275)\n\n* fix asr pipeline ut failures\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* make style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "55d63e86eac12c127ad420ec5b18c99eaf75ae62",
    "files": [
        {
            "sha": "3235b869a5e4fdd03bea0edd99ef4c5e36e4f295",
            "filename": "tests/pipelines/test_pipelines_automatic_speech_recognition.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55d63e86eac12c127ad420ec5b18c99eaf75ae62/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55d63e86eac12c127ad420ec5b18c99eaf75ae62/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_automatic_speech_recognition.py?ref=55d63e86eac12c127ad420ec5b18c99eaf75ae62",
            "patch": "@@ -615,7 +615,8 @@ def test_torch_whisper_batched(self):\n             {\"text\": \" Nor is Mr. Quilters' manner less interesting than his matter.\"},\n         ]\n \n-        output = speech_recognizer(ds[\"audio\"], batch_size=2)\n+        audio_arrays = [x.get_all_samples().data for x in ds[\"audio\"]]\n+        output = speech_recognizer(audio_arrays, batch_size=2)\n         self.assertEqual(output, EXPECTED_OUTPUT)\n \n     @slow\n@@ -1763,11 +1764,11 @@ def test_pipeline_assisted_generation(self):\n         pipe = pipeline(\"automatic-speech-recognition\", model=model, assistant_model=model)\n \n         # We can run the pipeline\n-        prompt = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")[\"audio\"]\n-        _ = pipe(prompt)\n+        prompt = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:1]\")[0][\"audio\"]\n+        _ = pipe(prompt, generate_kwargs={\"num_beams\": 1})\n \n         # It is running assisted generation under the hood (e.g. flags incompatible with assisted gen will crash)\n-        with self.assertRaises(ValueError):\n+        with self.assertRaises(TypeError):\n             _ = pipe(prompt, generate_kwargs={\"num_beams\": 2})\n \n     @require_torch"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}