{
    "author": "3outeille",
    "message": "fix `Dtensor` and `tensor` mismatch (#42906)\n\n* begin Moe test tensor parallel\n\n* create tiny moe model + fix test tensor parallel Moe\n\neaeaae\n\n* create tiny moe model + fix test tensor parallel Moe\n\neaeaae\n\nfix tensor parallel MoE test\nfix tensor parallel MoE test\n\n* fix backward pass test in tensor parallel for Dense model (#42811)\n\n* fix\n\n* linting\n\n* use mixtral instead for testing\n\n* fix dtensor and tensor mismatch\n\n* linting\n\n* checkout test tensor parallel to be like main\n\n* avoid hack and create class instead",
    "sha": "b1a2fba1c47e14390f17998218addf20c329e010",
    "files": [
        {
            "sha": "7d93a7a5ff98dfb062afe1267cef70a0c795ff71",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 35,
            "deletions": 8,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/b1a2fba1c47e14390f17998218addf20c329e010/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b1a2fba1c47e14390f17998218addf20c329e010/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=b1a2fba1c47e14390f17998218addf20c329e010",
            "patch": "@@ -758,6 +758,15 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n+class LocalColwiseParallel(ColwiseParallel):\n+    \"\"\"\n+    Colwise parallel with use_dtensor=False for local tensor operations.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(use_dtensor=False, **kwargs)\n+\n+\n class RowwiseParallel(TensorParallelLayer):\n     \"\"\"\n     Partition a compatible nn.Module in a row-wise fashion. Currently supports nn.Linear and nn.Embedding.\n@@ -782,7 +791,7 @@ def __init__(\n         input_layouts: Placement | None = None,\n         output_layouts: Placement | None = None,\n         use_local_output: bool = True,\n-        use_dtensor=True,\n+        use_dtensor: bool = True,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -913,6 +922,24 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n+class LocalRowwiseParallel(RowwiseParallel):\n+    \"\"\"\n+    Rowwise parallel with use_dtensor=False for local tensor operations.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(use_dtensor=False, **kwargs)\n+\n+\n+class LocalPackedRowwiseParallel(PackedRowwiseParallel):\n+    \"\"\"\n+    Packed rowwise parallel with use_dtensor=False for local tensor operations.\n+    \"\"\"\n+\n+    def __init__(self, **kwargs):\n+        super().__init__(use_dtensor=False, **kwargs)\n+\n+\n class SequenceParallel(TensorParallelLayer):\n     \"\"\"\n     SequenceParallel replicates a compatible ``nn.Module`` parameters and runs the sharded computation with\n@@ -1064,10 +1091,10 @@ class RouterParallel(TensorParallelLayer):\n     Allows to reshape the router scores to support running expert parallel.\n     \"\"\"\n \n-    def __init__(self, *args, **kwargs):\n+    def __init__(self, use_dtensor: bool = False, *args, **kwargs):\n         super().__init__(**kwargs)\n         self.args = args\n-        self.use_dtensor = False\n+        self.use_dtensor = use_dtensor\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n@@ -1118,7 +1145,7 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n                 f\"The number of experts must be divisible by number of ep_size: {mod.num_experts} % {ep_size} != 0\"\n             )\n         num_local_experts = mod.num_experts // ep_size\n-        router_scores, router_indices = outputs\n+        router_logits, router_scores, router_indices = outputs\n         router_scores = router_scores[:, ep_rank * num_local_experts : (ep_rank + 1) * num_local_experts]\n         router_indices = router_indices.masked_fill((router_indices // num_local_experts) != ep_rank, -1)\n         # As -1 % 1 is 0, we can only use mask fill when num_local_experts is 1\n@@ -1129,7 +1156,7 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         router_indices = router_indices.masked_fill(\n             router_indices == -1, num_local_experts\n         )  # masking class for one hot\n-        return router_scores, router_indices\n+        return router_logits, router_scores, router_indices\n \n     def shard_tensor(\n         self,\n@@ -1171,11 +1198,11 @@ class ParallelInterface(GeneralInterface):\n             \"rowwise\": RowwiseParallel(),\n             \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n             \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n-            \"local_colwise\": ColwiseParallel(use_dtensor=False),\n-            \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n+            \"local_colwise\": LocalColwiseParallel(),\n+            \"local_rowwise\": LocalRowwiseParallel(),\n             \"local\": IsolatedParallel(),\n             \"gather\": GatherParallel(),\n-            \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n+            \"local_packed_rowwise\": LocalPackedRowwiseParallel(),\n             \"sequence_parallel\": SequenceParallel(),\n             \"replicate\": ReplicateParallel(),\n             \"grouped_gemm\": GroupedGemmParallel(),"
        }
    ],
    "stats": {
        "total": 43,
        "additions": 35,
        "deletions": 8
    }
}