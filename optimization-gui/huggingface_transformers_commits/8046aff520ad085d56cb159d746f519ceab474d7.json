{
    "author": "dvrogozh",
    "message": "tests/roformer: fix couple roformer tests on gpus (#38570)\n\nFix \"RuntimeError: Expected all tensors to be on the same device,\nbut found at least two devices, cuda:0 and cpu\" error running the\nfollowing roformer tests on GPUs (CUDA or XPU):\n\n```\ntests/models/roformer/test_modeling_roformer.py::RoFormerSinusoidalPositionalEmbeddingTest::test_basic\ntests/models/roformer/test_modeling_roformer.py::RoFormerSelfAttentionRotaryPositionEmbeddingTest::test_apply_rotary_position_embeddings\n```\n\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "8046aff520ad085d56cb159d746f519ceab474d7",
    "files": [
        {
            "sha": "dde8b72c49d690ba22122a88197003f8281af7c8",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8046aff520ad085d56cb159d746f519ceab474d7/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8046aff520ad085d56cb159d746f519ceab474d7/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=8046aff520ad085d56cb159d746f519ceab474d7",
            "patch": "@@ -513,8 +513,9 @@ class RoFormerSinusoidalPositionalEmbeddingTest(unittest.TestCase):\n \n     def test_basic(self):\n         input_ids = torch.tensor([[4, 10]], dtype=torch.long, device=torch_device)\n-        emb1 = RoFormerSinusoidalPositionalEmbedding(num_positions=6, embedding_dim=6).to(torch_device)\n+        emb1 = RoFormerSinusoidalPositionalEmbedding(num_positions=6, embedding_dim=6)\n         emb1._init_weight()\n+        emb1 = emb1.to(torch_device)\n         emb = emb1(input_ids.shape)\n         desired_weights = torch.tensor(\n             [[0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000], [0.8415, 0.0464, 0.0022, 0.5403, 0.9989, 1.0000]]\n@@ -554,8 +555,9 @@ def test_apply_rotary_position_embeddings(self):\n         key_layer = (\n             -torch.arange(2 * 12 * 16 * 64, dtype=torch.float, device=torch_device).reshape(2, 12, 16, 64) / 100\n         ).to(torch_device)\n-        embed_positions = RoFormerSinusoidalPositionalEmbedding(num_positions=32, embedding_dim=64).to(torch_device)\n+        embed_positions = RoFormerSinusoidalPositionalEmbedding(num_positions=32, embedding_dim=64)\n         embed_positions._init_weight()\n+        embed_positions = embed_positions.to(torch_device)\n         sinusoidal_pos = embed_positions([2, 16, 768])[None, None, :, :]\n \n         query_layer, key_layer = RoFormerSelfAttention.apply_rotary_position_embeddings("
        }
    ],
    "stats": {
        "total": 6,
        "additions": 4,
        "deletions": 2
    }
}