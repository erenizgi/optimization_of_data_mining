{
    "author": "ankke",
    "message": "Make projection LayerNorm configurable in LFM2-VL (#43087)\n\n* Make projection LayerNorm configurable\n\n* CI fix",
    "sha": "3c2517727ce28a30f5044e01663ee204deb1cdbe",
    "files": [
        {
            "sha": "c38737ad4df0104aef5db6bd483ccbf48b84d065",
            "filename": "src/transformers/models/lfm2_vl/configuration_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fconfiguration_lfm2_vl.py?ref=3c2517727ce28a30f5044e01663ee204deb1cdbe",
            "patch": "@@ -46,6 +46,8 @@ class Lfm2VlConfig(PreTrainedConfig):\n             The hidden size of the multimodal projector.\n         projector_bias (`bool`, *optional*, defaults to `True`):\n             Whether to use bias in the multimodal projector.\n+        projector_use_layernorm (`bool`, *optional*, defaults to `True`):\n+            Whether to use layernorm in the multimodal projector.\n         downsample_factor (`int`, *optional*, defaults to 2):\n             The downsample_factor factor of the vision backbone.\n     \"\"\"\n@@ -61,13 +63,15 @@ def __init__(\n         projector_hidden_act=\"gelu\",\n         projector_hidden_size=2560,\n         projector_bias=True,\n+        projector_use_layernorm=True,\n         downsample_factor=2,\n         **kwargs,\n     ):\n         self.image_token_id = image_token_id\n         self.projector_hidden_act = projector_hidden_act\n         self.projector_hidden_size = projector_hidden_size\n         self.projector_bias = projector_bias\n+        self.projector_use_layernorm = projector_use_layernorm\n         self.downsample_factor = downsample_factor\n \n         if isinstance(vision_config, dict):"
        },
        {
            "sha": "c83dba6b3934604d82242ea231082659e6204953",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=3c2517727ce28a30f5044e01663ee204deb1cdbe",
            "patch": "@@ -41,7 +41,8 @@ def __init__(self, config: Lfm2VlConfig):\n         super().__init__()\n         in_channels = config.vision_config.hidden_size * (config.downsample_factor**2)\n         self.factor = config.downsample_factor\n-        self.layer_norm = nn.LayerNorm(in_channels)\n+        self.use_layer_norm = config.projector_use_layernorm\n+        self.layer_norm = nn.LayerNorm(in_channels) if config.projector_use_layernorm else None\n         self.linear_1 = nn.Linear(\n             in_channels,\n             config.projector_hidden_size,\n@@ -56,7 +57,8 @@ def __init__(self, config: Lfm2VlConfig):\n \n     def forward(self, image_features: torch.Tensor):\n         image_features = self.pixel_unshuffle(image_features)\n-        image_features = self.layer_norm(image_features)\n+        if self.use_layer_norm:\n+            image_features = self.layer_norm(image_features)\n         hidden_states = self.linear_1(image_features)\n         hidden_states = self.act(hidden_states)\n         hidden_states = self.linear_2(hidden_states)"
        },
        {
            "sha": "1ba900c5c32b05a55572a56858e1dc64e2d3af5a",
            "filename": "src/transformers/models/lfm2_vl/modular_lfm2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c2517727ce28a30f5044e01663ee204deb1cdbe/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodular_lfm2_vl.py?ref=3c2517727ce28a30f5044e01663ee204deb1cdbe",
            "patch": "@@ -41,7 +41,8 @@ def __init__(self, config: Lfm2VlConfig):\n         super().__init__()\n         in_channels = config.vision_config.hidden_size * (config.downsample_factor**2)\n         self.factor = config.downsample_factor\n-        self.layer_norm = nn.LayerNorm(in_channels)\n+        self.use_layer_norm = config.projector_use_layernorm\n+        self.layer_norm = nn.LayerNorm(in_channels) if config.projector_use_layernorm else None\n         self.linear_1 = nn.Linear(\n             in_channels,\n             config.projector_hidden_size,\n@@ -56,7 +57,8 @@ def __init__(self, config: Lfm2VlConfig):\n \n     def forward(self, image_features: torch.Tensor):\n         image_features = self.pixel_unshuffle(image_features)\n-        image_features = self.layer_norm(image_features)\n+        if self.use_layer_norm:\n+            image_features = self.layer_norm(image_features)\n         hidden_states = self.linear_1(image_features)\n         hidden_states = self.act(hidden_states)\n         hidden_states = self.linear_2(hidden_states)"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 12,
        "deletions": 4
    }
}