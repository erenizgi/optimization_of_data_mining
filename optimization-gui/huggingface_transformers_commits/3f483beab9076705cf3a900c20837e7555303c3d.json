{
    "author": "ArthurZucker",
    "message": "[`PixtralLarge`] Update Pixtral conversion script to support large format! (#34801)\n\n* update conversion script\n\n* update for bias again\n\n* remove pdv\n\n* use my dir\n\n* Update how we initialize the tokenizer\n\n* Convert in bfloat16\n\n* Undo that one again\n\n* fix config dump\n\n* .to() was broken for BatchMixFeature\n\n* quick debug breakpoint\n\n* put the breakpoint in the right place\n\n* Add a config flag for the multimodal projector bias\n\n* Add a config flag for the multimodal projector bias\n\n* Conversion script can load chat templates\n\n* Indent config for comparison\n\n* Stop clobbering the config\n\n* Re-enable the config clobber\n\n* Get rid of the config manual save - it has no effect!\n\n* Handle adapter bias correctly\n\n* Default vision transformer activation to silu\n\n* Remove legacy processing path\n\n* One commit with all the debug breakpoints before I delete them all, in case I need to revert\n\n* Update conversion\n\n* Remove vLLM debugging instrumentation\n\n* Drop xformers\n\n* Remove debug enumerates\n\n* make fixup\n\n* make fixup\n\n* Break copied from in pixtral\n\n* Propagate multimodal_projector_bias change\n\n* Propagate multimodal_projector_bias change\n\n* Remove debug device .to()\n\n* Restore attention weights output\n\n* Fix Pixtral test\n\n* Drop image_seq_length\n\n* Drop image_seq_length\n\n* Put the legacy processing code back\n\n* Add the bias option to the llava_next_video config\n\n* Add the bias option to the llava_next_video config\n\n* Make certain args required in converter\n\n* Make certain args required in converter\n\n* typo\n\n* make fixup\n\n* Reverting some dtype changes since it seems to work without them\n\n---------\n\nCo-authored-by: arthur@huggingface.co <arthur@ip-26-0-166-244.ec2.internal>\nCo-authored-by: Matt <rocketknight1@gmail.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "3f483beab9076705cf3a900c20837e7555303c3d",
    "files": [
        {
            "sha": "58bf40d6ce339dac45e647f605cf75de81a68c47",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -50,6 +50,8 @@ class LlavaConfig(PretrainedConfig):\n             The index of the layer to select the vision feature.\n         image_seq_length (`int`, *optional*, defaults to 576):\n             Sequence length of one image embedding.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n \n     Example:\n \n@@ -85,6 +87,7 @@ def __init__(\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-2,\n         image_seq_length=576,\n+        multimodal_projector_bias=True,\n         **kwargs,\n     ):\n         self.ignore_index = ignore_index\n@@ -127,6 +130,7 @@ def __init__(\n             text_config = CONFIG_MAPPING[\"llama\"]()\n \n         self.text_config = text_config\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         super().__init__(**kwargs)\n "
        },
        {
            "sha": "3d9bc339fd29a42ca705f2008c581d821dc0642e",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -86,10 +86,13 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n class LlavaMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaConfig):\n         super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n         self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n \n     def forward(self, image_features):\n         hidden_states = self.linear_1(image_features)"
        },
        {
            "sha": "6cb76c5b9d15c05f8519bd77a6fa9002b373f5cb",
            "filename": "src/transformers/models/llava_next/configuration_llava_next.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconfiguration_llava_next.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -55,6 +55,8 @@ class LlavaNextConfig(PretrainedConfig):\n             Whether the model's input and output word embeddings should be tied.\n         image_seq_length (`int`, *optional*, defaults to 576):\n             Sequence length of one image embedding.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n \n     Example:\n \n@@ -92,12 +94,14 @@ def __init__(\n         image_grid_pinpoints=None,\n         tie_word_embeddings=False,\n         image_seq_length=576,\n+        multimodal_projector_bias=True,\n         **kwargs,\n     ):\n         self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n         self.image_seq_length = image_seq_length\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError("
        },
        {
            "sha": "71e46389892822c64f5f59a27cd1ad65775a1cc8",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -194,10 +194,13 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n class LlavaNextMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextConfig):\n         super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n         self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n \n     def forward(self, image_features):\n         hidden_states = self.linear_1(image_features)"
        },
        {
            "sha": "77089ed0f365ca720971d2696f4d7d03310003ed",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -44,6 +44,8 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n             The activation function used by the multimodal projector.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n@@ -95,6 +97,7 @@ def __init__(\n         ignore_index=-100,\n         image_token_index=32001,\n         projector_hidden_act=\"gelu\",\n+        multimodal_projector_bias=True,\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-2,\n         image_grid_pinpoints=None,\n@@ -114,6 +117,7 @@ def __init__(\n         self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError("
        },
        {
            "sha": "b1ae26aaac80e031cd24a1a1086a32a8f4a5343b",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -179,10 +179,13 @@ def _init_weights(self, module):\n class LlavaNextVideoMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaNextVideoConfig):\n         super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n         self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n \n     def forward(self, image_features):\n         hidden_states = self.linear_1(image_features)"
        },
        {
            "sha": "89975a745b790ce20a543a700f5cf8d03286e791",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -58,6 +58,8 @@ class LlavaNextVideoConfig(PretrainedConfig):\n             The image token index to encode the image prompt.\n         projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n             The activation function used by the multimodal projector.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n         vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n@@ -109,6 +111,7 @@ def __init__(\n         ignore_index=-100,\n         image_token_index=32001,\n         projector_hidden_act=\"gelu\",\n+        multimodal_projector_bias=True,\n         vision_feature_select_strategy=\"default\",\n         vision_feature_layer=-2,\n         image_grid_pinpoints=None,\n@@ -128,6 +131,7 @@ def __init__(\n         self.ignore_index = ignore_index\n         self.image_token_index = image_token_index\n         self.projector_hidden_act = projector_hidden_act\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError("
        },
        {
            "sha": "504e8a7878be40f0a2123fddb45dea1eb7433ad7",
            "filename": "src/transformers/models/llava_onevision/configuration_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconfiguration_llava_onevision.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -58,6 +58,8 @@ class LlavaOnevisionConfig(PretrainedConfig):\n             of the form `(height, width)`.\n         tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n             Whether the model's input and output word embeddings should be tied.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n \n     Example:\n \n@@ -95,11 +97,13 @@ def __init__(\n         vision_aspect_ratio=\"anyres_max_9\",\n         image_grid_pinpoints=None,\n         tie_word_embeddings=False,\n+        multimodal_projector_bias=True,\n         **kwargs,\n     ):\n         self.image_token_index = image_token_index\n         self.video_token_index = video_token_index\n         self.projector_hidden_act = projector_hidden_act\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         if vision_feature_select_strategy not in [\"default\", \"full\"]:\n             raise ValueError("
        },
        {
            "sha": "7bc88ec95ab359ab4a684f7fac75faef5d99fe8e",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -201,10 +201,13 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n class LlavaOnevisionMultiModalProjector(nn.Module):\n     def __init__(self, config: LlavaOnevisionConfig):\n         super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n         self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n \n     def forward(self, image_features):\n         hidden_states = self.linear_1(image_features)"
        },
        {
            "sha": "a8b3ae5024ccb11d113cf09d59b67ee1b8fb7de5",
            "filename": "src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py",
            "status": "modified",
            "additions": 91,
            "deletions": 57,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -12,6 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import argparse\n+import json\n+import os\n \n import regex as re\n import torch\n@@ -27,7 +29,6 @@\n     PixtralImageProcessor,\n     PixtralProcessor,\n     PixtralVisionConfig,\n-    PreTrainedTokenizerFast,\n )\n from transformers.convert_slow_tokenizer import bytes_to_unicode\n \n@@ -156,29 +157,18 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n-def convert_mistral_tokenizer():\n-    model_name = \"mistralai/Pixtral-12B-2409\"\n+def convert_mistral_tokenizer(model_file):\n+    from transformers import LlamaTokenizer\n \n-    tokenizer = MistralTokenizer.from_model(model_name)\n-\n-    vocab = tokenizer.instruct_tokenizer.tokenizer._tekken_token2id_nospecial\n-    all_special = [\n-        token.value if hasattr(token, \"value\") else token\n-        for token in tokenizer.instruct_tokenizer.tokenizer._all_special_tokens\n-    ]\n-    specials_tokens = {token: all_special.index(token) for token in all_special}\n-    specials_tokens.update(vocab)\n-    vocab = specials_tokens\n-\n-    tokenizer = PreTrainedTokenizerFast(\n-        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted(),\n-        bos_token=\"<s>\",\n-        unk_token=\"<unk>\",\n-        eos_token=\"</s>\",\n-    )\n-    tokenizer.model_input_names = [\"input_ids\", \"attention_mask\"]\n-\n-    return tokenizer\n+    mistral_tokenizer = MistralTokenizer.from_file(model_file)\n+    vocab = mistral_tokenizer.instruct_tokenizer.tokenizer.vocab()\n+    control_token_ids = mistral_tokenizer.instruct_tokenizer.tokenizer._control_tokens\n+    all_special = [vocab[id] for id in control_token_ids]\n+    hf_tokenizer = LlamaTokenizer(model_file)\n+    # Do I need to exclude tokens that are already special?\n+    hf_tokenizer.add_special_tokens({\"additional_special_tokens\": all_special})\n+    hf_tokenizer.model_input_names = [\"input_ids\", \"attention_mask\"]\n+    return hf_tokenizer\n \n \n def permute_for_rope(value, n_heads, config):\n@@ -187,7 +177,7 @@ def permute_for_rope(value, n_heads, config):\n     return value.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n \n \n-def convert_dictionnary(original_state_dict, vision_config, text_config):\n+def convert_dictionary(original_state_dict, vision_config, text_config):\n     new_dict = {}\n \n     all_keys = \"\\n\" + \"\\n\".join(original_state_dict.keys())\n@@ -208,7 +198,6 @@ def convert_dictionnary(original_state_dict, vision_config, text_config):\n                 num_attention_heads = _config.num_attention_heads\n             if \"k_proj\" in new_key:\n                 num_attention_heads = _config.num_key_value_heads\n-            # convert the text model (basically mistral model)\n \n         if \"q_proj\" in new_key or \"k_proj\" in new_key:\n             value = permute_for_rope(value, num_attention_heads, _config)\n@@ -217,68 +206,113 @@ def convert_dictionnary(original_state_dict, vision_config, text_config):\n     return new_dict\n \n \n-def convert_mistral_model(input_dir, output_dir):\n-    text_config = MistralConfig(\n-        attention_dropout=0.0,\n-        bos_token_id=1,\n-        eos_token_id=2,\n-        head_dim=128,\n-        hidden_act=\"silu\",\n-        hidden_size=5120,\n-        initializer_range=0.02,\n-        intermediate_size=14336,\n-        max_position_embeddings=1024000,\n-        model_type=\"mistral\",\n-        num_attention_heads=32,\n-        num_hidden_layers=40,\n-        num_key_value_heads=8,\n-        rms_norm_eps=1e-05,\n-        rope_theta=1000000000.0,\n-        sliding_window=None,\n-        tie_word_embeddings=False,\n-        vocab_size=131072,\n-    )\n+MISTRAL_CONFIG_MAPPING = {\n+    \"dim\": \"hidden_size\",\n+    \"hidden_dim\": \"intermediate_size\",\n+    \"n_kv_heads\": \"num_key_value_heads\",\n+    \"n_heads\": \"num_attention_heads\",\n+    \"n_layers\": \"num_hidden_layers\",\n+}\n+\n \n-    vision_config = PixtralVisionConfig()\n+def convert_mistral_model(input_dir, output_dir):\n+    vision_config = {}\n+    if os.path.isfile(f\"{input_dir}/params.json\"):\n+        with open(f\"{input_dir}/params.json\") as f:\n+            param_json = json.load(f)\n+        vision_config = param_json.pop(\"vision_encoder\")\n+        for k, v in MISTRAL_CONFIG_MAPPING.items():\n+            value = param_json.pop(k)\n+            param_json[v] = value\n+        if \"hidden_act\" not in vision_config:\n+            vision_config[\"hidden_act\"] = \"silu\"\n+        text_config = MistralConfig(\n+            **param_json,\n+            hidden_act=\"silu\",\n+            sliding_window=None,\n+            tie_word_embeddings=False,\n+            is_composition=True,\n+            rms_norm_eps=1e-5,\n+        )\n+    else:\n+        text_config = MistralConfig(\n+            attention_dropout=0.0,\n+            bos_token_id=1,\n+            eos_token_id=2,\n+            head_dim=128,\n+            hidden_act=\"silu\",\n+            hidden_size=5120,\n+            initializer_range=0.02,\n+            intermediate_size=14336,\n+            max_position_embeddings=1024000,\n+            model_type=\"mistral\",\n+            num_attention_heads=32,\n+            num_hidden_layers=40,\n+            num_key_value_heads=8,\n+            rms_norm_eps=1e-05,\n+            rope_theta=1000000000.0,\n+            sliding_window=None,\n+            tie_word_embeddings=False,\n+            vocab_size=131072,\n+        )\n+    adapter_bias = vision_config.pop(\"adapter_bias\", True)\n+    vision_config = PixtralVisionConfig(**vision_config)\n     config = LlavaConfig(\n         vision_config,\n         text_config,\n         vision_feature_layer=-1,\n         image_token_index=10,\n         vision_feature_select_strategy=\"full\",\n         image_seq_length=1,\n+        multimodal_projector_bias=adapter_bias,\n     )\n     config.architectures = [\"LlavaForConditionalGeneration\"]\n     config.save_pretrained(output_dir)\n-\n-    original_state_dict = safe_load_file(f\"{input_dir}/consolidated.safetensors\")\n-    new_dict = convert_dictionnary(original_state_dict, vision_config, text_config)\n-\n+    full_original_state_dict = {}\n+    safetensors_files = sorted([file for file in os.listdir(input_dir) if file.endswith(\".safetensors\")])\n+    if len(safetensors_files) == 1:\n+        full_original_state_dict = safe_load_file(f\"{input_dir}/consolidated.safetensors\")\n+    else:\n+        for file in safetensors_files:\n+            loaded_dict = safe_load_file(f\"{input_dir}/{file}\")\n+            full_original_state_dict.update(loaded_dict)\n+\n+    new_dict = convert_dictionary(full_original_state_dict, vision_config, text_config)\n     with torch.device(\"meta\"):\n         model = LlavaForConditionalGeneration(config)\n     model.load_state_dict(new_dict, strict=True, assign=True)\n-\n     model.save_pretrained(output_dir)\n \n-    tokenizer = convert_mistral_tokenizer()\n-    image_processor = PixtralImageProcessor()\n-    processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor, image_token=\"[IMG]\")\n-    processor.save_pretrained(output_dir)\n-\n \n def main():\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--input_dir\",\n         help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n+        required=True,\n     )\n     parser.add_argument(\n         \"--output_dir\",\n         help=\"Location to write HF model and tokenizer\",\n+        required=True,\n+    )\n+    parser.add_argument(\n+        \"--tokenizer_file\", help=\"Location of the specific tokenizer model file to use.\", required=True\n+    )\n+    parser.add_argument(\n+        \"--chat_template_file\",\n+        help=\"Optional file containing a raw chat template. Will be set as the processor's chat template.\",\n+        required=False,\n     )\n \n     args = parser.parse_args()\n     convert_mistral_model(args.input_dir, args.output_dir)\n+    tokenizer = convert_mistral_tokenizer(args.tokenizer_file)\n+    image_processor = PixtralImageProcessor()\n+    processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor, image_token=\"[IMG]\")\n+    if args.chat_template_file:\n+        processor.chat_template = open(args.chat_template_file).read()\n+    processor.save_pretrained(args.output_dir)\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "6d83e0c464714847228acb2580e86b72835a8af4",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 17,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -37,7 +37,7 @@\n     validate_kwargs,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_torch_device, is_torch_dtype, is_torch_tensor, is_vision_available, logging\n+from ...utils import TensorType, is_torch_device, is_torch_dtype, is_vision_available, logging\n from ...utils.import_utils import requires_backends\n \n \n@@ -63,10 +63,24 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n         Returns:\n             [`BatchFeature`]: The same instance after modification.\n         \"\"\"\n+\n+        def _recursive_to(obj, device, *args, **kwargs):\n+            # Lists can be nested, so keep digging until we hit tensors\n+            if isinstance(obj, list):\n+                return [_recursive_to(o, device, *args, **kwargs) for o in obj]\n+            # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n+            elif isinstance(obj, torch.Tensor) and torch.is_floating_point(obj):\n+                # cast and send to device\n+                return obj.to(*args, **kwargs)\n+            elif isinstance(obj, torch.Tensor) and device is not None:\n+                # only send to device, don't cast\n+                return obj.to(device=device)\n+            else:\n+                return obj\n+\n         requires_backends(self, [\"torch\"])\n         import torch  # noqa\n \n-        new_data = {}\n         device = kwargs.get(\"device\")\n         # Check if the args are a device or a dtype\n         if device is None and len(args) > 0:\n@@ -80,21 +94,8 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n             else:\n                 # it's something else\n                 raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n-        # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n-        for k, v in self.items():\n-            # check if v is a floating point\n-            if isinstance(v, list):\n-                new_data[k] = [\n-                    element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n-                ]\n-            elif isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n-                # cast and send to device\n-                new_data[k] = v.to(*args, **kwargs)\n-            elif isinstance(v, torch.Tensor) and device is not None:\n-                new_data[k] = v.to(device=device)\n-            else:\n-                new_data[k] = v\n-        self.data = new_data\n+\n+        self.data = {k: _recursive_to(v, device, *args, **kwargs) for k, v in self.data.items()}\n         return self\n \n "
        },
        {
            "sha": "905eef22ca3d0005387431fc31c03c636c73a3fe",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -126,7 +126,6 @@ def rotate_half(x):\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -491,18 +490,20 @@ def forward(\n                 all tokens of all images of shape (N_toks, D)\n         \"\"\"\n         # pass images through initial convolution independently\n-        patch_embeds_list = [self.patch_conv(img.unsqueeze(0).to(self.dtype)) for img in pixel_values]\n+        if len(pixel_values) > 1:\n+            raise ValueError(\"Batching/padding not supported yet!\")\n+        patch_embeds_list = [self.patch_conv(img.to(self.dtype)) for sample in pixel_values for img in sample]\n \n         # flatten to a single sequence\n-        patch_embeds = torch.cat([p.flatten(2).permute(0, 2, 1) for p in patch_embeds_list], dim=1)\n+        patch_embeds = torch.cat([p.flatten(1).T for p in patch_embeds_list], dim=0).unsqueeze(0)\n         patch_embeds = self.ln_pre(patch_embeds)\n-\n         # positional embeddings\n         position_ids = position_ids_in_meshgrid(\n             patch_embeds_list, max_width=self.config.image_size // self.config.patch_size\n         ).to(self.device)\n \n         position_embedding = self.patch_positional_embedding(patch_embeds, position_ids)\n+\n         attention_mask = generate_block_attention_mask(\n             [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n         )"
        },
        {
            "sha": "e60151130ae02fb46ffd657b55ff6489ff36d1cf",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 31,
            "deletions": 21,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_utils import ImageInput, is_valid_image, load_image\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_device, is_torch_dtype, is_torch_tensor, logging, requires_backends\n+from ...utils import is_torch_device, is_torch_dtype, logging, requires_backends\n \n \n logger = logging.get_logger(__name__)\n@@ -66,10 +66,24 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n         Returns:\n             [`BatchFeature`]: The same instance after modification.\n         \"\"\"\n+\n+        def _recursive_to(obj, device, *args, **kwargs):\n+            # Lists can be nested, so keep digging until we hit tensors\n+            if isinstance(obj, list):\n+                return [_recursive_to(o, device, *args, **kwargs) for o in obj]\n+            # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n+            elif isinstance(obj, torch.Tensor) and torch.is_floating_point(obj):\n+                # cast and send to device\n+                return obj.to(*args, **kwargs)\n+            elif isinstance(obj, torch.Tensor) and device is not None:\n+                # only send to device, don't cast\n+                return obj.to(device=device)\n+            else:\n+                return obj\n+\n         requires_backends(self, [\"torch\"])\n         import torch  # noqa\n \n-        new_data = {}\n         device = kwargs.get(\"device\")\n         # Check if the args are a device or a dtype\n         if device is None and len(args) > 0:\n@@ -83,21 +97,8 @@ def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n             else:\n                 # it's something else\n                 raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n-        # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n-        for k, v in self.items():\n-            # check if v is a floating point\n-            if isinstance(v, list):\n-                new_data[k] = [\n-                    element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n-                ]\n-            elif isinstance(v, torch.Tensor) and torch.is_floating_point(v):\n-                # cast and send to device\n-                new_data[k] = v.to(*args, **kwargs)\n-            elif isinstance(v, torch.Tensor) and device is not None:\n-                new_data[k] = v.to(device=device)\n-            else:\n-                new_data[k] = v\n-        self.data = new_data\n+\n+        self.data = {k: _recursive_to(v, device, *args, **kwargs) for k, v in self.data.items()}\n         return self\n \n \n@@ -204,12 +205,21 @@ def __call__(\n \n         if images is not None:\n             if is_image_or_image_url(images):\n-                images = [[images]]\n-            elif isinstance(images, list) and is_image_or_image_url(images[0]):\n-                if isinstance(text, list):\n-                    images = [[im] for im in images]\n+                if isinstance(text, str) or isinstance(text, list) and len(text) == 1:\n+                    # If there's a single sample, the image must belong to it\n+                    images = [[images]]\n                 else:\n+                    raise ValueError(\n+                        \"You have supplied multiple text samples, but `images` is not a nested list. When processing multiple samples, `images` should be a list of lists of images, one list per sample.\"\n+                    )\n+            elif isinstance(images, list) and is_image_or_image_url(images[0]):\n+                if isinstance(text, str) or isinstance(text, list) and len(text) == 1:\n+                    # If there's a single sample, all images must belong to it\n                     images = [images]\n+                else:\n+                    raise ValueError(\n+                        \"You have supplied multiple text samples, but `images` is not a nested list. When processing multiple samples, `images` should be a list of lists of images, one list per sample.\"\n+                    )\n             elif isinstance(images, list) and isinstance(images[0], list) and is_image_or_image_url(images[0][0]):\n                 pass\n             else:"
        },
        {
            "sha": "2342e16da4a2e771d97a2c2b60296869882e7e5e",
            "filename": "src/transformers/models/video_llava/configuration_video_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fconfiguration_video_llava.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -55,6 +55,8 @@ class VideoLlavaConfig(PretrainedConfig):\n             Sequence length of one image embedding.\n         video_seq_length (`int`, *optional*, defaults to 2056):\n             Sequence length of one video embedding.\n+        multimodal_projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to use bias in the multimodal projector.\n \n     Example:\n \n@@ -92,6 +94,7 @@ def __init__(\n         vision_feature_layer=-2,\n         image_seq_length=256,\n         video_seq_length=2056,\n+        multimodal_projector_bias=True,\n         **kwargs,\n     ):\n         self.ignore_index = ignore_index\n@@ -102,6 +105,7 @@ def __init__(\n         self.vision_feature_layer = vision_feature_layer\n         self.image_seq_length = image_seq_length\n         self.video_seq_length = video_seq_length\n+        self.multimodal_projector_bias = multimodal_projector_bias\n \n         self.vision_config = vision_config\n "
        },
        {
            "sha": "aeff4ad1d0c29d1cc390c1e9401f2f671bc64295",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -88,10 +88,13 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n class VideoLlavaMultiModalProjector(nn.Module):\n     def __init__(self, config: VideoLlavaConfig):\n         super().__init__()\n-\n-        self.linear_1 = nn.Linear(config.vision_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n         self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n \n     def forward(self, image_features):\n         hidden_states = self.linear_1(image_features)"
        },
        {
            "sha": "d224c531241fa72b3e349be330206d5f11389840",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3f483beab9076705cf3a900c20837e7555303c3d/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3f483beab9076705cf3a900c20837e7555303c3d/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=3f483beab9076705cf3a900c20837e7555303c3d",
            "patch": "@@ -253,7 +253,7 @@ def test_processor_returns_full_length_batches(self):\n             \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n         ] * 5\n         processor.tokenizer.pad_token = \"</s>\"\n-        image_inputs = [self.image_0] * 5\n+        image_inputs = [[self.image_0]] * 5\n \n         # Make small for checking image token expansion\n         processor.image_processor.size = {\"longest_edge\": 30}"
        }
    ],
    "stats": {
        "total": 315,
        "additions": 200,
        "deletions": 115
    }
}