{
    "author": "SunMarc",
    "message": "fix regression (#42569)\n\n* fix regression\n\n* fix\n\n* fix",
    "sha": "53d2bf6dab1c0ab09ed23889eab43ae92c1ebcd2",
    "files": [
        {
            "sha": "a0acc36715a88b12286587e39ee2817516b5577d",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/53d2bf6dab1c0ab09ed23889eab43ae92c1ebcd2/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/53d2bf6dab1c0ab09ed23889eab43ae92c1ebcd2/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=53d2bf6dab1c0ab09ed23889eab43ae92c1ebcd2",
            "patch": "@@ -316,21 +316,17 @@ def __init__(\n     ):\n         super().__init__(in_features, out_features)\n \n-        # If block size, is not passed, it means that we are doing per-tensor quantization\n-        if block_size is not None:\n-            self.block_size = block_size\n-        else:\n-            self.block_size = (out_features, in_features)\n-\n+        # If block size is None, it means that we are doing per-tensor quantization\n+        self.block_size = block_size\n         self.activation_scheme = activation_scheme\n \n         self.weight = torch.nn.Parameter(torch.empty(out_features, in_features, dtype=dtype))\n-        scale_out_features = (out_features + block_size[0] - 1) // block_size[0]\n-        scale_in_features = (in_features + block_size[1] - 1) // block_size[1]\n \n-        if scale_out_features * scale_in_features == 1:\n+        if self.block_size is None:\n             self.weight_scale_inv = nn.Parameter(torch.tensor(1.0, dtype=torch.float32))\n         else:\n+            scale_out_features = (out_features + self.block_size[0] - 1) // self.block_size[0]\n+            scale_in_features = (in_features + self.block_size[1] - 1) // self.block_size[1]\n             self.weight_scale_inv = nn.Parameter(\n                 torch.empty(scale_out_features, scale_in_features, dtype=torch.float32)\n             )"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 5,
        "deletions": 9
    }
}