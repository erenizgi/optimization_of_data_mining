{
    "author": "vasqu",
    "message": "[`Attn`] Allow dynamic causality in SDPA via Kwargs (#41692)\n\n* is causal as kwarg\n\n* Update src/transformers/integrations/sdpa_attention.py\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* fix comment\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "7e204ad121e729cac59ea213356c68527cac0a7e",
    "files": [
        {
            "sha": "eab702914d00f8c9ecc7d1974337996a6f237944",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e204ad121e729cac59ea213356c68527cac0a7e/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e204ad121e729cac59ea213356c68527cac0a7e/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=7e204ad121e729cac59ea213356c68527cac0a7e",
            "patch": "@@ -34,6 +34,7 @@ def flash_attention_forward(\n     scaling: Optional[float] = None,\n     sliding_window: Optional[int] = None,\n     softcap: Optional[float] = None,\n+    is_causal: Optional[bool] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n     if kwargs.get(\"output_attentions\", False):\n@@ -64,9 +65,7 @@ def flash_attention_forward(\n     target_dtype = get_target_dtype(query, module)\n \n     # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n-    is_causal = kwargs.pop(\"is_causal\", None)\n-    if is_causal is None:\n-        is_causal = module.is_causal\n+    is_causal = is_causal if is_causal is not None else module.is_causal\n \n     attn_output = _flash_attention_forward(\n         query,"
        },
        {
            "sha": "0526d135d0c2d81414439944c8edb717c47ac6a9",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e204ad121e729cac59ea213356c68527cac0a7e/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e204ad121e729cac59ea213356c68527cac0a7e/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=7e204ad121e729cac59ea213356c68527cac0a7e",
            "patch": "@@ -68,13 +68,20 @@ def sdpa_attention_forward(\n     if attention_mask is not None and attention_mask.ndim == 4:\n         attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n \n-    # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-    # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-    # Note that it is important to check first for the shape, otherwise compile will fail with `argument 'is_causal' must be bool, not SymBool`\n-    if is_causal is None:\n-        # The last condition is for encoder (decoder) models which specify this by passing their own `is_causal` flag\n-        # This is mainly due to those models having mixed implementations for encoder, decoder, and encoder-decoder attns\n-        is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n+    # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n+    is_causal = is_causal if is_causal is not None else getattr(module, \"is_causal\", True)\n+\n+    # SDPA's Flash Attention (and cuDNN) kernels rely on the `is_causal` flag. However, there are certain conditions:\n+    # - Not in decoding phase (otherwise we want full attention on the single query token)\n+    # - Attention mask is not to be provided (even if it is a causal pattern)\n+    # - Internally, we marked this as compatible with causal, i.e. it is a decoder attention type\n+    #\n+    # Quirks on the conditionals:\n+    # - We avoid inline passing this to the SDPA function directly to support both torch.compile's dynamic shapes and\n+    #   full graph options. Otherwise, dynamic shapes are prevented from compiling.\n+    # - It is important to check first for the shape, otherwise compile will fail with\n+    #   `argument 'is_causal' must be bool, not SymBool`.\n+    is_causal = query.shape[2] > 1 and attention_mask is None and is_causal\n \n     # Shapes (e.g. query.shape[2]) are tensors during jit tracing, resulting in `is_causal` being a tensor.\n     # We convert it to a bool for the SDPA kernel that only accepts bools."
        }
    ],
    "stats": {
        "total": 26,
        "additions": 16,
        "deletions": 10
    }
}