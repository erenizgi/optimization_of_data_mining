{
    "author": "vasqu",
    "message": "[`Docs`] Fix build on other langs (#42485)\n\n* fix\n\n* albert missed ouch",
    "sha": "a4a7008ee046b0ff6e38cd389080e1a3982b994f",
    "files": [
        {
            "sha": "ba0bfef33e6d1eef7148f5c694cb7b0be7eba81b",
            "filename": "docs/source/ja/model_doc/albert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -60,9 +60,7 @@ ALBERT„É¢„Éá„É´„ÅØ„ÄÅ„Äå[ALBERT: A Lite BERT for Self-supervised Learning of Lan\n ## AlbertTokenizer\n \n [[autodoc]] AlbertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## AlbertTokenizerFast"
        },
        {
            "sha": "4bb582966277709bab57c82455f776128a5e3825",
            "filename": "docs/source/ja/model_doc/bert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -133,11 +133,12 @@ BERT „ÇíÂßã„ÇÅ„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•„Éã„ÉÜ„Ç£\n ## BertTokenizer\n \n [[autodoc]] BertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n+## BertTokenizerLegacy\n+\n+[[autodoc]] BertTokenizerLegacy\n \n ## BertTokenizerFast\n "
        },
        {
            "sha": "46f39cc6610448c6b5d9ef81073e76d6f305fad4",
            "filename": "docs/source/ja/model_doc/big_bird.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -72,9 +72,7 @@ BigBird „ÅØ„ÄÅË≥™ÂïèÂøúÁ≠î„ÇÑË¶ÅÁ¥Ñ„Å™„Å©„ÅÆ„Åï„Åæ„Åñ„Åæ„Å™ NLP „Çø„Çπ„ÇØ„ÅÆ„Éë\n ## BigBirdTokenizer\n \n [[autodoc]] BigBirdTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## BigBirdTokenizerFast"
        },
        {
            "sha": "93ceda1b7da012597ffe35d8b15e923b5bd8de6b",
            "filename": "docs/source/ja/model_doc/blenderbot-small.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot-small.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -61,9 +61,7 @@ Blender „ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà „É¢„Éá„É´„ÅØ„ÄÅ[Recipes for building an open-domai\n ## BlenderbotSmallTokenizer\n \n [[autodoc]] BlenderbotSmallTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## BlenderbotSmallTokenizerFast"
        },
        {
            "sha": "a9842b65d4194982bed27ac257ad61c8a1dfd79e",
            "filename": "docs/source/ja/model_doc/blenderbot.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fblenderbot.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -81,12 +81,10 @@ Blender „ÉÅ„É£„ÉÉ„Éà„Éú„ÉÉ„Éà „É¢„Éá„É´„ÅØ„ÄÅ[Recipes for building an open-domai\n ## BlenderbotTokenizer\n \n [[autodoc]] BlenderbotTokenizer\n-    - build_inputs_with_special_tokens\n \n ## BlenderbotTokenizerFast\n \n [[autodoc]] BlenderbotTokenizerFast\n-    - build_inputs_with_special_tokens\n \n ## BlenderbotModel\n "
        },
        {
            "sha": "47ee047f1e74d2e45ba77db4d7c0b0c1abec238f",
            "filename": "docs/source/ja/model_doc/camembert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -60,9 +60,7 @@ Bi-direction Encoders for Transformers (BERT) „ÅÆ„Éï„É©„É≥„ÇπË™ûÁâà„Åß„ÅÇ„Çã Cam\n ## CamembertTokenizer\n \n [[autodoc]] CamembertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CamembertTokenizerFast"
        },
        {
            "sha": "7de3be2ce294194cfc0fe880034d25609d16ac4b",
            "filename": "docs/source/ja/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fclip.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -118,9 +118,7 @@ CLIP „Çí‰Ωø„ÅÑÂßã„ÇÅ„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•„Éã\n ## CLIPTokenizer\n \n [[autodoc]] CLIPTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CLIPTokenizerFast"
        },
        {
            "sha": "fc1b8a6ff5ec0ff8f5847f6daf286aec37328294",
            "filename": "docs/source/ja/model_doc/code_llama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcode_llama.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -111,16 +111,12 @@ LLaMA „Éà„Éº„ÇØ„Éä„Ç§„Ç∂„Éº„ÅØ„ÄÅ[sentencepiece](https://github.com/google/sente\n ## CodeLlamaTokenizer\n \n [[autodoc]] CodeLlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CodeLlamaTokenizerFast\n \n [[autodoc]] CodeLlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary"
        },
        {
            "sha": "738fc021995d695fc65fe9c9221c2f4c02911ec6",
            "filename": "docs/source/ja/model_doc/convbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -66,9 +66,7 @@ ConvBERT „Éà„É¨„Éº„Éã„É≥„Ç∞„ÅÆ„Éí„É≥„Éà„ÅØ BERT „ÅÆ„Éí„É≥„Éà„Å®‰ºº„Å¶„ÅÑ„Åæ„Åô\n ## ConvBertTokenizer\n \n [[autodoc]] ConvBertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## ConvBertTokenizerFast"
        },
        {
            "sha": "81927e5464cfe07473b5b4c215658393f9712758",
            "filename": "docs/source/ja/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -74,16 +74,12 @@ v2 „ÅÆÊñ∞Ê©üËÉΩ:\n ## DebertaV2Tokenizer\n \n [[autodoc]] DebertaV2Tokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaV2TokenizerFast\n \n [[autodoc]] DebertaV2TokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n \n ## DebertaV2Model"
        },
        {
            "sha": "58bc1ed214e58739e07160566645c639c3ae4ab9",
            "filename": "docs/source/ja/model_doc/deberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -83,16 +83,12 @@ DeBERTa „Çí‰Ωø„ÅÑÂßã„ÇÅ„Çã„ÅÆ„Å´ÂΩπÁ´ã„Å§ÂÖ¨Âºè Hugging Face „Åä„Çà„Å≥„Ç≥„Éü„É•\n ## DebertaTokenizer\n \n [[autodoc]] DebertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaTokenizerFast\n \n [[autodoc]] DebertaTokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n \n ## DebertaModel"
        },
        {
            "sha": "753a810c16bb723415a528de09f374161fa75d24",
            "filename": "docs/source/ko/model_doc/albert.md",
            "status": "modified",
            "additions": 15,
            "deletions": 7,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Falbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -153,7 +153,9 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n ## AlbertTokenizer[[alberttokenizer]]\n \n-[[autodoc]] AlbertTokenizer - build_inputs_with_special_tokens - get_special_tokens_mask - create_token_type_ids_from_sequences - save_vocabulary\n+[[autodoc]] AlbertTokenizer \n+    - get_special_tokens_mask \n+    - save_vocabulary\n \n ## AlbertTokenizerFast[[alberttokenizerfast]]\n \n@@ -166,29 +168,35 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n ## AlbertModel[[albertmodel]]\n \n-[[autodoc]] AlbertModel - forward\n+[[autodoc]] AlbertModel \n+    - forward\n \n ## AlbertForPreTraining[[albertforpretraining]]\n \n-[[autodoc]] AlbertForPreTraining - forward\n+[[autodoc]] AlbertForPreTraining \n+    - forward\n \n ## AlbertForMaskedLM[[albertformaskedlm]]\n \n-[[autodoc]] AlbertForMaskedLM - forward\n+[[autodoc]] AlbertForMaskedLM \n+    - forward\n \n ## AlbertForSequenceClassification[[albertforsequenceclassification]]\n \n-[[autodoc]] AlbertForSequenceClassification - forward\n+[[autodoc]] AlbertForSequenceClassification \n+    - forward\n \n ## AlbertForMultipleChoice[[albertformultiplechoice]]\n \n [[autodoc]] AlbertForMultipleChoice\n \n ## AlbertForTokenClassification[[albertfortokenclassification]]\n \n-[[autodoc]] AlbertForTokenClassification - forward\n+[[autodoc]] AlbertForTokenClassification \n+    - forward\n \n ## AlbertForQuestionAnswering[[albertforquestionanswering]]\n \n-[[autodoc]] AlbertForQuestionAnswering - forward\n+[[autodoc]] AlbertForQuestionAnswering \n+    - forward\n "
        },
        {
            "sha": "3e14b795752264ccd04ed3dae0228271939c1af3",
            "filename": "docs/source/ko/model_doc/bert.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -159,11 +159,12 @@ BERTÎ•º ÏãúÏûëÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Hugging FaceÏôÄ community ÏûêÎ£å Î™©\n ## BertTokenizer\n \n [[autodoc]] BertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n+## BertTokenizerLegacy\n+\n+[[autodoc]] BertTokenizerLegacy\n \n ## BertTokenizerFast\n "
        },
        {
            "sha": "7461cd45ee53ae262fd46be841c767c6da700e05",
            "filename": "docs/source/ko/model_doc/big_bird.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fbig_bird.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -104,9 +104,7 @@ print(f\"The predicted token is: {predicted_token}\")\n ## BigBirdTokenizer[[bigbirdtokenizer]]\n \n [[autodoc]] BigBirdTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## BigBirdTokenizerFast[[bigbirdtokenizerfast]]"
        },
        {
            "sha": "f2bb6bd57f966c1c44accb4403a0eb19dd01d2cd",
            "filename": "docs/source/ko/model_doc/clip.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fclip.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -211,9 +211,7 @@ CLIPÏùÑ ÏãúÏûëÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Hugging FaceÏôÄ community ÏûêÎ£å Î™©\n ## CLIPTokenizer[[transformers.CLIPTokenizer]]\n \n [[autodoc]] CLIPTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CLIPTokenizerFast[[transformers.CLIPTokenizerFast]]"
        },
        {
            "sha": "b8435bec5a2500b080174f2c0906df0ceed1672c",
            "filename": "docs/source/ko/model_doc/code_llama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fcode_llama.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -165,16 +165,12 @@ visualizer(\"\"\"def func(a, b):\n ## CodeLlamaTokenizer\n \n [[autodoc]] CodeLlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CodeLlamaTokenizerFast\n \n [[autodoc]] CodeLlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary"
        },
        {
            "sha": "ae6b0de8fc7868c98d44b7129ff1764b615aa3da",
            "filename": "docs/source/ko/model_doc/codegen.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fcodegen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fcodegen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fcodegen.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -76,7 +76,6 @@ hello_world()\n ## CodeGenTokenizer\n \n [[autodoc]] CodeGenTokenizer\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## CodeGenTokenizerFast"
        },
        {
            "sha": "2bc1b934764cc0bd3ec941957d8eec701cb97c62",
            "filename": "docs/source/ko/model_doc/convbert.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fconvbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -56,9 +56,7 @@ ConvBERT ÌõàÎ†® ÌåÅÏùÄ BERTÏôÄ Ïú†ÏÇ¨Ìï©ÎãàÎã§. ÏÇ¨Ïö© ÌåÅÏùÄ [BERT Î¨∏ÏÑú](bert\n ## ConvBertTokenizer [[transformers.ConvBertTokenizer]]\n \n [[autodoc]] ConvBertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## ConvBertTokenizerFast [[transformers.ConvBertTokenizerFast]]"
        },
        {
            "sha": "5194cc481d20f34e47df693f64a0ed584f150812",
            "filename": "docs/source/ko/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta-v2.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -56,16 +56,12 @@ v2Ïùò ÏÉàÎ°úÏö¥ Ï†ê:\n ## DebertaV2Tokenizer\n \n [[autodoc]] DebertaV2Tokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaV2TokenizerFast\n \n [[autodoc]] DebertaV2TokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n \n ## DebertaV2Model"
        },
        {
            "sha": "5fcac9fee06df31b0413186350df9a88997a7484",
            "filename": "docs/source/ko/model_doc/deberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -71,16 +71,12 @@ DeBERTaÎ•º ÏãúÏûëÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ ÎêòÎäî Hugging FaceÏôÄ community ÏûêÎ£å \n ## DebertaTokenizer[[transformers.DebertaTokenizer]]\n \n [[autodoc]] DebertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## DebertaTokenizerFast[[transformers.DebertaTokenizerFast]]\n \n [[autodoc]] DebertaTokenizerFast\n-    - build_inputs_with_special_tokens\n-    - create_token_type_ids_from_sequences\n \n \n ## DebertaModel[[transformers.DebertaModel]]"
        },
        {
            "sha": "9203ce8f3a92df26fc7417df74eb0ce07e93a288",
            "filename": "docs/source/ko/model_doc/llama.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -86,17 +86,13 @@ LLaMAÎ•º ÏãúÏûëÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ Îê† Hugging Face Î∞è Ïª§ÎÆ§ÎãàÌã∞(üåéÎ°ú \n ## LlamaTokenizer [[llamatokenizer]]\n \n [[autodoc]] LlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## LlamaTokenizerFast [[llamatokenizerfast]]\n \n [[autodoc]] LlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary\n "
        },
        {
            "sha": "c61193ba5c2687e1e78d71be0e097ebab15dd6b1",
            "filename": "docs/source/ko/model_doc/llama2.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fllama2.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -98,17 +98,13 @@ LLaMA2Î•º ÏãúÏûëÌïòÎäî Îç∞ ÎèÑÏõÄÏù¥ Îê† Hugging FaceÏùò Í≥µÏãù Î∞è Ïª§ÎÆ§ÎãàÌã∞\n ## LlamaTokenizer [[llamatokenizer]]\n \n [[autodoc]] LlamaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## LlamaTokenizerFast [[llamatokenizerfast]]\n \n [[autodoc]] LlamaTokenizerFast\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - update_post_processor\n     - save_vocabulary\n "
        },
        {
            "sha": "fe9ea309fc28c2c7a9cf189ea3b9171532fad1c2",
            "filename": "docs/source/ko/model_doc/roberta.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Froberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Froberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Froberta.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -102,15 +102,12 @@ RoBERTaÎ•º Ï≤òÏùå Îã§Î£∞ Îïå ÎèÑÏõÄÏù¥ ÎêòÎäî Hugging Face Í≥µÏãù ÏûêÎ£åÏôÄ Ïª§\n ## RobertaTokenizer\n \n [[autodoc]] RobertaTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## RobertaTokenizerFast\n \n [[autodoc]] RobertaTokenizerFast\n-    - build_inputs_with_special_tokens\n \n \n ## RobertaModel"
        },
        {
            "sha": "d24f8c4c0189d1eab19082525156bbb117d1f5e7",
            "filename": "docs/source/ko/model_doc/whisper.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fwhisper.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fko%2Fmodel_doc%2Fwhisper.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fwhisper.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -54,18 +54,14 @@ python src/transformers/models/whisper/convert_openai_to_hf.py --checkpoint_path\n \n [[autodoc]] WhisperTokenizer\n     - set_prefix_tokens\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## WhisperTokenizerFast [[whispertokenizerfast]]\n \n [[autodoc]] WhisperTokenizerFast\n     - set_prefix_tokens\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n ## WhisperFeatureExtractor [[whisperfeatureextractor]]"
        },
        {
            "sha": "0a0fd7187c65cc1a0d203e2ebe7a1b3c33c47cbf",
            "filename": "docs/source/zh/model_doc/bert.md",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a4a7008ee046b0ff6e38cd389080e1a3982b994f/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmodel_doc%2Fbert.md?ref=a4a7008ee046b0ff6e38cd389080e1a3982b994f",
            "patch": "@@ -101,11 +101,13 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n ## BertTokenizer\n \n [[autodoc]] BertTokenizer\n-    - build_inputs_with_special_tokens\n     - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n     - save_vocabulary\n \n+## BertTokenizerLegacy\n+\n+[[autodoc]] BertTokenizerLegacy\n+\n ## BertTokenizerFast\n \n [[autodoc]] BertTokenizerFast"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 25,
        "deletions": 73
    }
}