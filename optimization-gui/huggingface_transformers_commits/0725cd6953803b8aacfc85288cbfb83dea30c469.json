{
    "author": "Cyrilvallez",
    "message": "Remove deprecated classes in modeling_utils.py (#38919)\n\n* remove deprecated classes\n\n* style",
    "sha": "0725cd6953803b8aacfc85288cbfb83dea30c469",
    "files": [
        {
            "sha": "ce4cdab8b865139e44cf14afdae2d83e30ca9488",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 452,
            "changes": 452,
            "blob_url": "https://github.com/huggingface/transformers/blob/0725cd6953803b8aacfc85288cbfb83dea30c469/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0725cd6953803b8aacfc85288cbfb83dea30c469/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0725cd6953803b8aacfc85288cbfb83dea30c469",
            "patch": "@@ -29,7 +29,6 @@\n from collections import defaultdict\n from concurrent.futures import ThreadPoolExecutor, as_completed\n from contextlib import contextmanager\n-from dataclasses import dataclass\n from enum import Enum\n from functools import partial, wraps\n from threading import Thread\n@@ -41,7 +40,6 @@\n from packaging import version\n from torch import Tensor, nn\n from torch.distributions import constraints\n-from torch.nn import CrossEntropyLoss, Identity\n from torch.utils.checkpoint import checkpoint\n \n from transformers.utils import is_torchao_available\n@@ -50,7 +48,6 @@\n if is_torchao_available():\n     from torchao.quantization import Int4WeightOnlyConfig\n \n-from .activations import get_activation\n from .configuration_utils import PretrainedConfig\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n@@ -98,7 +95,6 @@\n     WEIGHTS_INDEX_NAME,\n     WEIGHTS_NAME,\n     ContextManagers,\n-    ModelOutput,\n     PushToHubMixin,\n     cached_file,\n     check_torch_load_is_safe,\n@@ -123,7 +119,6 @@\n     is_torch_xla_available,\n     is_torch_xpu_available,\n     logging,\n-    replace_return_docstrings,\n     strtobool,\n )\n from .utils.generic import GeneralInterface\n@@ -5624,453 +5619,6 @@ def get_parameter_or_buffer(self, target: str):\n     )\n \n \n-class PoolerStartLogits(nn.Module):\n-    \"\"\"\n-    Compute SQuAD start logits from sequence hidden states.\n-\n-    Args:\n-        config ([`PretrainedConfig`]):\n-            The config used by the model, will be used to grab the `hidden_size` of the model.\n-    \"\"\"\n-\n-    def __init__(self, config: PretrainedConfig):\n-        super().__init__()\n-        self.dense = nn.Linear(config.hidden_size, 1)\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `PoolerStartLogits` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMPoolerStartLogits`.\"\n-        )\n-\n-    def forward(\n-        self, hidden_states: torch.FloatTensor, p_mask: Optional[torch.FloatTensor] = None\n-    ) -> torch.FloatTensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n-                The final hidden states of the model.\n-            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n-                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n-                should be masked.\n-\n-        Returns:\n-            `torch.FloatTensor`: The start logits for SQuAD.\n-        \"\"\"\n-        x = self.dense(hidden_states).squeeze(-1)\n-\n-        if p_mask is not None:\n-            if get_parameter_dtype(self) == torch.float16:\n-                x = x * (1 - p_mask) - 65500 * p_mask\n-            else:\n-                x = x * (1 - p_mask) - 1e30 * p_mask\n-\n-        return x\n-\n-\n-class PoolerEndLogits(nn.Module):\n-    \"\"\"\n-    Compute SQuAD end logits from sequence hidden states.\n-\n-    Args:\n-        config ([`PretrainedConfig`]):\n-            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n-            to use.\n-    \"\"\"\n-\n-    def __init__(self, config: PretrainedConfig):\n-        super().__init__()\n-        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n-        self.activation = nn.Tanh()\n-        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.dense_1 = nn.Linear(config.hidden_size, 1)\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `PoolerEndLogits` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMPoolerEndLogits`.\"\n-        )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        start_states: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        p_mask: Optional[torch.FloatTensor] = None,\n-    ) -> torch.FloatTensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n-                The final hidden states of the model.\n-            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n-                The hidden states of the first tokens for the labeled span.\n-            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                The position of the first token for the labeled span.\n-            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n-                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n-                should be masked.\n-\n-        <Tip>\n-\n-        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n-        `start_states`.\n-\n-        </Tip>\n-\n-        Returns:\n-            `torch.FloatTensor`: The end logits for SQuAD.\n-        \"\"\"\n-        assert start_states is not None or start_positions is not None, (\n-            \"One of start_states, start_positions should be not None\"\n-        )\n-        if start_positions is not None:\n-            slen, hsz = hidden_states.shape[-2:]\n-            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n-            start_states = hidden_states.gather(-2, start_positions)  # shape (bsz, 1, hsz)\n-            start_states = start_states.expand(-1, slen, -1)  # shape (bsz, slen, hsz)\n-\n-        x = self.dense_0(torch.cat([hidden_states, start_states], dim=-1))\n-        x = self.activation(x)\n-        x = self.LayerNorm(x)\n-        x = self.dense_1(x).squeeze(-1)\n-\n-        if p_mask is not None:\n-            if get_parameter_dtype(self) == torch.float16:\n-                x = x * (1 - p_mask) - 65500 * p_mask\n-            else:\n-                x = x * (1 - p_mask) - 1e30 * p_mask\n-\n-        return x\n-\n-\n-class PoolerAnswerClass(nn.Module):\n-    \"\"\"\n-    Compute SQuAD 2.0 answer class from classification and start tokens hidden states.\n-\n-    Args:\n-        config ([`PretrainedConfig`]):\n-            The config used by the model, will be used to grab the `hidden_size` of the model.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.dense_0 = nn.Linear(config.hidden_size * 2, config.hidden_size)\n-        self.activation = nn.Tanh()\n-        self.dense_1 = nn.Linear(config.hidden_size, 1, bias=False)\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `PoolerAnswerClass` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMPoolerAnswerClass`.\"\n-        )\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        start_states: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        cls_index: Optional[torch.LongTensor] = None,\n-    ) -> torch.FloatTensor:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n-                The final hidden states of the model.\n-            start_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`, *optional*):\n-                The hidden states of the first tokens for the labeled span.\n-            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                The position of the first token for the labeled span.\n-            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n-\n-        <Tip>\n-\n-        One of `start_states` or `start_positions` should be not `None`. If both are set, `start_positions` overrides\n-        `start_states`.\n-\n-        </Tip>\n-\n-        Returns:\n-            `torch.FloatTensor`: The SQuAD 2.0 answer class.\n-        \"\"\"\n-        # No dependency on end_feature so that we can obtain one single `cls_logits` for each sample.\n-        hsz = hidden_states.shape[-1]\n-        assert start_states is not None or start_positions is not None, (\n-            \"One of start_states, start_positions should be not None\"\n-        )\n-        if start_positions is not None:\n-            start_positions = start_positions[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n-            start_states = hidden_states.gather(-2, start_positions).squeeze(-2)  # shape (bsz, hsz)\n-\n-        if cls_index is not None:\n-            cls_index = cls_index[:, None, None].expand(-1, -1, hsz)  # shape (bsz, 1, hsz)\n-            cls_token_state = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, hsz)\n-        else:\n-            cls_token_state = hidden_states[:, -1, :]  # shape (bsz, hsz)\n-\n-        x = self.dense_0(torch.cat([start_states, cls_token_state], dim=-1))\n-        x = self.activation(x)\n-        x = self.dense_1(x).squeeze(-1)\n-\n-        return x\n-\n-\n-@dataclass\n-class SquadHeadOutput(ModelOutput):\n-    \"\"\"\n-    Base class for outputs of question answering models using a [`~modeling_utils.SQuADHead`].\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned if both `start_positions` and `end_positions` are provided):\n-            Classification loss as the sum of start token, end token (and is_impossible if provided) classification\n-            losses.\n-        start_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top config.start_n_top start token possibilities (beam-search).\n-        start_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top config.start_n_top start token possibilities (beam-search).\n-        end_top_log_probs (`torch.FloatTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the top `config.start_n_top * config.end_n_top` end token possibilities\n-            (beam-search).\n-        end_top_index (`torch.LongTensor` of shape `(batch_size, config.start_n_top * config.end_n_top)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Indices for the top `config.start_n_top * config.end_n_top` end token possibilities (beam-search).\n-        cls_logits (`torch.FloatTensor` of shape `(batch_size,)`, *optional*, returned if `start_positions` or `end_positions` is not provided):\n-            Log probabilities for the `is_impossible` label of the answers.\n-\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    start_top_log_probs: Optional[torch.FloatTensor] = None\n-    start_top_index: Optional[torch.LongTensor] = None\n-    end_top_log_probs: Optional[torch.FloatTensor] = None\n-    end_top_index: Optional[torch.LongTensor] = None\n-    cls_logits: Optional[torch.FloatTensor] = None\n-\n-    def __post_init__(self):\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `SquadHeadOutput` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMSquadHeadOutput`.\"\n-        )\n-\n-\n-class SQuADHead(nn.Module):\n-    r\"\"\"\n-    A SQuAD head inspired by XLNet.\n-\n-    Args:\n-        config ([`PretrainedConfig`]):\n-            The config used by the model, will be used to grab the `hidden_size` of the model and the `layer_norm_eps`\n-            to use.\n-    \"\"\"\n-\n-    def __init__(self, config):\n-        super().__init__()\n-        self.start_n_top = config.start_n_top\n-        self.end_n_top = config.end_n_top\n-\n-        self.start_logits = PoolerStartLogits(config)\n-        self.end_logits = PoolerEndLogits(config)\n-        self.answer_class = PoolerAnswerClass(config)\n-\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `SQuADHead` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMSQuADHead`.\"\n-        )\n-\n-    @replace_return_docstrings(output_type=SquadHeadOutput, config_class=PretrainedConfig)\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        cls_index: Optional[torch.LongTensor] = None,\n-        is_impossible: Optional[torch.LongTensor] = None,\n-        p_mask: Optional[torch.FloatTensor] = None,\n-        return_dict: bool = False,\n-    ) -> Union[SquadHeadOutput, tuple[torch.FloatTensor]]:\n-        \"\"\"\n-        Args:\n-            hidden_states (`torch.FloatTensor` of shape `(batch_size, seq_len, hidden_size)`):\n-                Final hidden states of the model on the sequence tokens.\n-            start_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Positions of the first token for the labeled span.\n-            end_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Positions of the last token for the labeled span.\n-            cls_index (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Position of the CLS token for each sentence in the batch. If `None`, takes the last token.\n-            is_impossible (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-                Whether the question has a possible answer in the paragraph or not.\n-            p_mask (`torch.FloatTensor` of shape `(batch_size, seq_len)`, *optional*):\n-                Mask for tokens at invalid position, such as query and special symbols (PAD, SEP, CLS). 1.0 means token\n-                should be masked.\n-            return_dict (`bool`, *optional*, defaults to `False`):\n-                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\n-        Returns:\n-        \"\"\"\n-        start_logits = self.start_logits(hidden_states, p_mask=p_mask)\n-\n-        if start_positions is not None and end_positions is not None:\n-            # If we are on multi-GPU, let's remove the dimension added by batch splitting\n-            for x in (start_positions, end_positions, cls_index, is_impossible):\n-                if x is not None and x.dim() > 1:\n-                    x.squeeze_(-1)\n-\n-            # during training, compute the end logits based on the ground truth of the start position\n-            end_logits = self.end_logits(hidden_states, start_positions=start_positions, p_mask=p_mask)\n-\n-            loss_fct = CrossEntropyLoss()\n-            start_loss = loss_fct(start_logits, start_positions)\n-            end_loss = loss_fct(end_logits, end_positions)\n-            total_loss = (start_loss + end_loss) / 2\n-\n-            if cls_index is not None and is_impossible is not None:\n-                # Predict answerability from the representation of CLS and START\n-                cls_logits = self.answer_class(hidden_states, start_positions=start_positions, cls_index=cls_index)\n-                loss_fct_cls = nn.BCEWithLogitsLoss()\n-                cls_loss = loss_fct_cls(cls_logits, is_impossible)\n-\n-                # note(zhiliny): by default multiply the loss by 0.5 so that the scale is comparable to start_loss and end_loss\n-                total_loss += cls_loss * 0.5\n-\n-            return SquadHeadOutput(loss=total_loss) if return_dict else (total_loss,)\n-\n-        else:\n-            # during inference, compute the end logits based on beam search\n-            bsz, slen, hsz = hidden_states.size()\n-            start_log_probs = nn.functional.softmax(start_logits, dim=-1)  # shape (bsz, slen)\n-\n-            start_top_log_probs, start_top_index = torch.topk(\n-                start_log_probs, self.start_n_top, dim=-1\n-            )  # shape (bsz, start_n_top)\n-            start_top_index_exp = start_top_index.unsqueeze(-1).expand(-1, -1, hsz)  # shape (bsz, start_n_top, hsz)\n-            start_states = torch.gather(hidden_states, -2, start_top_index_exp)  # shape (bsz, start_n_top, hsz)\n-            start_states = start_states.unsqueeze(1).expand(-1, slen, -1, -1)  # shape (bsz, slen, start_n_top, hsz)\n-\n-            hidden_states_expanded = hidden_states.unsqueeze(2).expand_as(\n-                start_states\n-            )  # shape (bsz, slen, start_n_top, hsz)\n-            p_mask = p_mask.unsqueeze(-1) if p_mask is not None else None\n-            end_logits = self.end_logits(hidden_states_expanded, start_states=start_states, p_mask=p_mask)\n-            end_log_probs = nn.functional.softmax(end_logits, dim=1)  # shape (bsz, slen, start_n_top)\n-\n-            end_top_log_probs, end_top_index = torch.topk(\n-                end_log_probs, self.end_n_top, dim=1\n-            )  # shape (bsz, end_n_top, start_n_top)\n-            end_top_log_probs = end_top_log_probs.view(-1, self.start_n_top * self.end_n_top)\n-            end_top_index = end_top_index.view(-1, self.start_n_top * self.end_n_top)\n-\n-            start_states = torch.einsum(\"blh,bl->bh\", hidden_states, start_log_probs)\n-            cls_logits = self.answer_class(hidden_states, start_states=start_states, cls_index=cls_index)\n-\n-            if not return_dict:\n-                return (start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits)\n-            else:\n-                return SquadHeadOutput(\n-                    start_top_log_probs=start_top_log_probs,\n-                    start_top_index=start_top_index,\n-                    end_top_log_probs=end_top_log_probs,\n-                    end_top_index=end_top_index,\n-                    cls_logits=cls_logits,\n-                )\n-\n-\n-class SequenceSummary(nn.Module):\n-    r\"\"\"\n-    Compute a single vector summary of a sequence hidden states.\n-\n-    Args:\n-        config ([`PretrainedConfig`]):\n-            The config used by the model. Relevant arguments in the config class of the model are (refer to the actual\n-            config class of your model for the default values it uses):\n-\n-            - **summary_type** (`str`) -- The method to use to make this summary. Accepted values are:\n-\n-                - `\"last\"` -- Take the last token hidden state (like XLNet)\n-                - `\"first\"` -- Take the first token hidden state (like Bert)\n-                - `\"mean\"` -- Take the mean of all tokens hidden states\n-                - `\"cls_index\"` -- Supply a Tensor of classification token position (GPT/GPT-2)\n-                - `\"attn\"` -- Not implemented now, use multi-head attention\n-\n-            - **summary_use_proj** (`bool`) -- Add a projection after the vector extraction.\n-            - **summary_proj_to_labels** (`bool`) -- If `True`, the projection outputs to `config.num_labels` classes\n-              (otherwise to `config.hidden_size`).\n-            - **summary_activation** (`Optional[str]`) -- Set to `\"tanh\"` to add a tanh activation to the output,\n-              another string or `None` will add no activation.\n-            - **summary_first_dropout** (`float`) -- Optional dropout probability before the projection and activation.\n-            - **summary_last_dropout** (`float`)-- Optional dropout probability after the projection and activation.\n-    \"\"\"\n-\n-    def __init__(self, config: PretrainedConfig):\n-        super().__init__()\n-\n-        self.summary_type = getattr(config, \"summary_type\", \"last\")\n-        if self.summary_type == \"attn\":\n-            # We should use a standard multi-head attention module with absolute positional embedding for that.\n-            # Cf. https://github.com/zihangdai/xlnet/blob/master/modeling.py#L253-L276\n-            # We can probably just use the multi-head attention module of PyTorch >=1.1.0\n-            raise NotImplementedError\n-\n-        self.summary = Identity()\n-        if hasattr(config, \"summary_use_proj\") and config.summary_use_proj:\n-            if hasattr(config, \"summary_proj_to_labels\") and config.summary_proj_to_labels and config.num_labels > 0:\n-                num_classes = config.num_labels\n-            else:\n-                num_classes = config.hidden_size\n-            self.summary = nn.Linear(config.hidden_size, num_classes)\n-\n-        activation_string = getattr(config, \"summary_activation\", None)\n-        self.activation: Callable = get_activation(activation_string) if activation_string else Identity()\n-\n-        self.first_dropout = Identity()\n-        if hasattr(config, \"summary_first_dropout\") and config.summary_first_dropout > 0:\n-            self.first_dropout = nn.Dropout(config.summary_first_dropout)\n-\n-        self.last_dropout = Identity()\n-        if hasattr(config, \"summary_last_dropout\") and config.summary_last_dropout > 0:\n-            self.last_dropout = nn.Dropout(config.summary_last_dropout)\n-\n-        logger.warning_once(\n-            \"[DEPRECATION WARNING] `SequenceSummary` is deprecated and will be removed in v4.53. \"\n-            \"Please use model-specific class, e.g. `XLMSequenceSummary`.\"\n-        )\n-\n-    def forward(\n-        self, hidden_states: torch.FloatTensor, cls_index: Optional[torch.LongTensor] = None\n-    ) -> torch.FloatTensor:\n-        \"\"\"\n-        Compute a single vector summary of a sequence hidden states.\n-\n-        Args:\n-            hidden_states (`torch.FloatTensor` of shape `[batch_size, seq_len, hidden_size]`):\n-                The hidden states of the last layer.\n-            cls_index (`torch.LongTensor` of shape `[batch_size]` or `[batch_size, ...]` where ... are optional leading dimensions of `hidden_states`, *optional*):\n-                Used if `summary_type == \"cls_index\"` and takes the last token of the sequence as classification token.\n-\n-        Returns:\n-            `torch.FloatTensor`: The summary of the sequence hidden states.\n-        \"\"\"\n-        if self.summary_type == \"last\":\n-            output = hidden_states[:, -1]\n-        elif self.summary_type == \"first\":\n-            output = hidden_states[:, 0]\n-        elif self.summary_type == \"mean\":\n-            output = hidden_states.mean(dim=1)\n-        elif self.summary_type == \"cls_index\":\n-            if cls_index is None:\n-                cls_index = torch.full_like(\n-                    hidden_states[..., :1, :],\n-                    hidden_states.shape[-2] - 1,\n-                    dtype=torch.long,\n-                )\n-            else:\n-                cls_index = cls_index.unsqueeze(-1).unsqueeze(-1)\n-                cls_index = cls_index.expand((-1,) * (cls_index.dim() - 1) + (hidden_states.size(-1),))\n-            # shape of cls_index: (bsz, XX, 1, hidden_size) where XX are optional leading dim of hidden_states\n-            output = hidden_states.gather(-2, cls_index).squeeze(-2)  # shape (bsz, XX, hidden_size)\n-        elif self.summary_type == \"attn\":\n-            raise NotImplementedError\n-\n-        output = self.first_dropout(output)\n-        output = self.summary(output)\n-        output = self.activation(output)\n-        output = self.last_dropout(output)\n-\n-        return output\n-\n-\n def unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n     \"\"\"\n     Recursively unwraps a model from potential containers (as used in distributed training)."
        }
    ],
    "stats": {
        "total": 452,
        "additions": 0,
        "deletions": 452
    }
}