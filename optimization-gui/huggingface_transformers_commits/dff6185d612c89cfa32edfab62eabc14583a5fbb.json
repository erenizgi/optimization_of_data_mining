{
    "author": "nevertmr",
    "message": "docs: fix typo in 'quantization-aware training' (#39904)",
    "sha": "dff6185d612c89cfa32edfab62eabc14583a5fbb",
    "files": [
        {
            "sha": "0177cb0555aecad7a0986f331c20f771814310b7",
            "filename": "docs/source/en/quantization/fp_quant.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dff6185d612c89cfa32edfab62eabc14583a5fbb/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dff6185d612c89cfa32edfab62eabc14583a5fbb/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md?ref=dff6185d612c89cfa32edfab62eabc14583a5fbb",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # FP-Quant\n \n-[FP-Quant](https://github.com/IST-DASLab/FP-Quant) is a family of quantization algorithms tailored for the Blackwell generation of Nvidia GPUs. The goal is to allow for efficient post-training quantization (PTQ) and quantization-aware trainin (QAT) of LLMs in the [MXFP4 and NVFP4 data-types](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).\n+[FP-Quant](https://github.com/IST-DASLab/FP-Quant) is a family of quantization algorithms tailored for the Blackwell generation of Nvidia GPUs. The goal is to allow for efficient post-training quantization (PTQ) and quantization-aware training (QAT) of LLMs in the [MXFP4 and NVFP4 data-types](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf).\n \n Currently, only PTQ with MXFP4 is supported. Models can either be quantized on the fly with `quantization_config=FPQuantConfig()`:\n \n@@ -63,4 +63,4 @@ model.forward = torch.compile(model.forward, mode=\"max-autotune\", fullgraph=True\n \n FP-Quant currently performs best for very large batch size processing.\n \n-See [QuTLASS README](https://github.com/IST-DASLab/qutlass/blob/main/README.md) for speedups.\n\\ No newline at end of file\n+See [QuTLASS README](https://github.com/IST-DASLab/qutlass/blob/main/README.md) for speedups."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}