{
    "author": "gante",
    "message": "[tests] Test all cache implementations (#37873)",
    "sha": "1b222903c3e1cfd9492d75e4b2548aa8bd458674",
    "files": [
        {
            "sha": "85a09f03de22d3d29c9873fd123d08ebfeb73439",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -2,6 +2,7 @@\n import importlib.metadata\n import json\n import os\n+import warnings\n from dataclasses import dataclass\n from typing import Any, Dict, Iterable, List, Optional, Tuple, Union\n \n@@ -950,6 +951,8 @@ def _dequantize(self, qtensor):\n \n class SinkCache(Cache):\n     \"\"\"\n+    Deprecated.\n+\n     A cache that as described in the [Attention Sinks paper](https://arxiv.org/abs/2309.17453). It allows the model to\n     generate beyond the length of its context window, without losing fluency in the conversation. As it discards past\n     tokens, the model will lose the ability to generate tokens that depend on the context that was discarded.\n@@ -994,6 +997,13 @@ def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n         self._sin_cache = None\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n \n+        warnings.warn(\n+            \"`SinkCache` is deprecated and will be removed in v4.53.0. You can achieve similar functionality by \"\n+            \"using a model with a sliding window attention mechanism, or by expanding RoPE and optionally using an \"\n+            \"offloaded cache implementation.\",\n+            FutureWarning,\n+        )\n+\n     @staticmethod\n     def _rotate_half(x):\n         x1 = x[..., : x.shape[-1] // 2]\n@@ -1404,7 +1414,7 @@ def update(\n \n         slicing = torch.ones(self.max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n         cache_position = cache_position.clamp(0, self.max_cache_len - 1)\n-        to_shift = cache_position >= self.max_cache_len - 1\n+        to_shift = cache_position > self.max_cache_len - 1\n         indices = (slicing + to_shift[-1].int() - 1) % self.max_cache_len\n \n         k_out = k_out[:, :, indices]\n@@ -1673,6 +1683,7 @@ def __init__(\n                 \"config and it's not set to None.\"\n             )\n         self.max_cache_len = max_cache_len\n+        self._sliding_window_max_len = min(config.sliding_window, max_cache_len)\n         self.max_batch_size = max_batch_size\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         self.head_dim = (\n@@ -1694,7 +1705,7 @@ def __init__(\n         sliding_cache_shape = (\n             self.max_batch_size,\n             self.num_key_value_heads,\n-            min(config.sliding_window, max_cache_len),\n+            self._sliding_window_max_len,\n             self.head_dim,\n         )\n         device = torch.device(device) if device is not None else None\n@@ -1726,7 +1737,7 @@ def _sliding_update(self, cache_position, layer_idx, key_states, value_states, k\n \n         slicing = torch.ones(max_cache_len, dtype=torch.long, device=value_states.device).cumsum(0)\n         cache_position = cache_position.clamp(0, max_cache_len - 1)\n-        to_shift = cache_position >= max_cache_len - 1\n+        to_shift = cache_position > max_cache_len - 1\n         indices = (slicing + to_shift[-1].int() - 1) % max_cache_len\n         k_out = k_out[:, :, indices]\n         v_out = v_out[:, :, indices]\n@@ -1873,6 +1884,7 @@ def __init__(\n         else:\n             self.sliding_window = config.sliding_window\n         self.max_cache_len = max_cache_len\n+        self._sliding_window_max_len = min(self.sliding_window, max_cache_len)\n         self.max_batch_size = max_batch_size\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self._dtype = dtype\n@@ -1894,12 +1906,7 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         num_key_value_heads = key_states.shape[1]\n         device = key_states.device\n         global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n-        sliding_cache_shape = (\n-            self.max_batch_size,\n-            num_key_value_heads,\n-            self.sliding_window,\n-            self.head_dim,\n-        )\n+        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self._sliding_window_max_len, self.head_dim)\n         # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n         # breaks when updating the cache.\n         cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape\n@@ -2039,12 +2046,7 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         device = key_states.device if self.is_sliding[layer_idx] else self.offload_device\n         pin_memory = not self.is_sliding[layer_idx]\n         global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n-        sliding_cache_shape = (\n-            self.max_batch_size,\n-            num_key_value_heads,\n-            self.sliding_window,\n-            self.head_dim,\n-        )\n+        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self._sliding_window_max_len, self.head_dim)\n         # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n         # breaks when updating the cache.\n         cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape"
        },
        {
            "sha": "5daf34a80a96a91bbc374cfcde79dff0b2aac9e9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -37,7 +37,6 @@\n     OffloadedCache,\n     OffloadedHybridCache,\n     QuantizedCacheConfig,\n-    StaticCache,\n )\n from ..configuration_utils import PretrainedConfig\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n@@ -553,8 +552,14 @@ def prepare_inputs_for_generation(\n                     model_input = model_input.clone(memory_format=torch.contiguous_format)\n                 model_inputs[model_input_name] = model_input\n \n-        # 6. Create 4D attention mask is we are using a `StaticCache` (important for performant compiled forward pass)\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n+        # 6. Create 4D attention mask is we are using a compilable cache (important for performant compiled forward\n+        # pass)\n+        if (\n+            isinstance(past_key_values, Cache)\n+            and past_key_values.is_compileable\n+            and attention_mask is not None\n+            and attention_mask.ndim == 2\n+        ):\n             if model_inputs[\"inputs_embeds\"] is not None:\n                 batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n             else:"
        },
        {
            "sha": "f6a1861d308b6c9f5736603a54ce5badc07f0adc",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Callable, List, Optional, Tuple, Union\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -991,10 +991,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1005,7 +1005,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "64d1f24f8b6ef0967877d571e67a191082e8977f",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -602,10 +602,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -616,7 +616,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "1f21db9649303066b9dcab6bf1a5fcfb721cb15f",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -761,10 +761,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -775,7 +775,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "e9eca929c3b9083426d9240f04c5b78db6d7ca43",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -1394,10 +1394,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1408,7 +1408,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "5dcbf09abeb770b34b2337e212cbe1485038a1b3",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -21,7 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -607,10 +607,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -621,7 +621,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "b5a3fc61009dd70822229963317a6067f340969a",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -33,7 +33,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -640,10 +640,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -654,7 +654,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "ccde31d341215d672f2ae6805cf6a07b34baffa3",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -1132,10 +1132,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1146,7 +1146,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "b54ff78302d00fc53a622f73e02a9bbd04b4759d",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -12,7 +12,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -785,10 +785,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -799,7 +799,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "f1393c694daff2627f66e0d743c75031d0a969f8",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -888,10 +888,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -902,7 +902,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "67a643b273e6bfc9a501ab90508dcce74b2863e4",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -29,7 +29,7 @@\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -1474,10 +1474,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1488,7 +1488,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "555d8e22fc38d5d73ec5a1a8366ae7e689f7fcc9",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -608,10 +608,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -622,7 +622,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "38e0282264d30ac752edeaab7a7fd08d45941357",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -623,10 +623,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -637,7 +637,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "0d7ac7c769ec0041228d317b2385789d020e2d8f",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -631,10 +631,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -645,7 +645,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "c343dd7f270d76c42b82ebaeecc06a7a61621664",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -808,10 +808,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -822,7 +822,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "0cb34d04e4954754a87b54187002852f0662b47a",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -10,7 +10,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -622,10 +622,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -636,7 +636,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "3497374f764ab3aff2f17b01b95b549ecf045b95",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -22,7 +22,7 @@\n from torch import Tensor, nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...file_utils import add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -659,10 +659,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -673,7 +673,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "5dafef1d99ae23893fdf870869af7473e0e7382f",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -909,10 +909,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -923,7 +923,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "e909c8ae0293e1acab81c8137679af3ef1b51cc5",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -623,10 +623,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -637,7 +637,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "c7c48c7470d24d07119526016ae2cbaa54d02ca5",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -21,7 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n@@ -1111,10 +1111,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1125,7 +1125,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "251962c979e4c917d533d2631bd81d9716dbbc55",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -26,7 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n@@ -1056,10 +1056,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1070,7 +1070,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "7d3e6cca23e25c7e9bdbe6e1b95f76d7ec7aa593",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -26,7 +26,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -608,10 +608,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -622,7 +622,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "eae379be3811f00c3e8a10b414200313669586e1",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -29,7 +29,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import ModelOutput\n@@ -1404,10 +1404,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1418,7 +1418,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "ca8d55f63064f73a91a2b98de6a11a47d97faa47",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -1111,10 +1111,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1125,7 +1125,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "cad5502e3fb313b916f88eb970dda8da1256cd44",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -613,10 +613,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -627,7 +627,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "ea9a046a88fd9f7106d3284d7eab7dc7304d813f",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1619,10 +1619,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1633,7 +1633,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "d842bd7c13134d8731913b1cb0e137ed2855fad0",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n@@ -1080,10 +1080,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1094,7 +1094,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "475b24d907886938c79fb221e88e421530601836",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -956,10 +956,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -970,7 +970,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "855e967c5fdeaac8314eca78f387883c18d4129d",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1210,10 +1210,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1224,7 +1224,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "aaca0943c96b80373c761875610700f4805d3877",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -870,10 +870,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -884,7 +884,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "2284f949f0894058a7b1e395731d958482d656a9",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -11,7 +11,7 @@\n import torch.nn.functional as F\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -583,10 +583,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -597,7 +597,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "978c80dc5d31ed2c87779ac08d1498da8915be55",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -10,7 +10,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n@@ -589,10 +589,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -603,7 +603,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "90d8418db37aff4bb86a500d126af7bd9fc5ede2",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -22,7 +22,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n@@ -660,10 +660,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -674,7 +674,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "f273ce1705703dce559ce87f12f155dc911c91a8",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -27,7 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -670,10 +670,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -684,7 +684,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "f505bfd0e178cb9701a75d357d3dbd359e8a3842",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -11,7 +11,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -594,10 +594,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -608,7 +608,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "ac5c8b48771173391e4a2b55e6b2ba5eb64152c4",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1606,10 +1606,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1620,7 +1620,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "a99f52747f82f75778e3cd35ea3cf044d4719bcf",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n from transformers.generation import GenerationConfig\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1019,10 +1019,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1033,7 +1033,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "7350cc1cc87bc3f0fb793d070e37ffd96ff6b799",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -27,7 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n@@ -924,10 +924,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -938,7 +938,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "041bd244c6fac51a17120588cb03380ed05bf23e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -24,7 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1153,10 +1153,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1167,7 +1167,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "ec520e8791ce3a98ee2e10b50f9796c4b366aab9",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -25,7 +25,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -1224,10 +1224,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1238,7 +1238,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "c5af44a95251ebcb8e91bd5f267de276b336c837",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -34,7 +34,7 @@\n )\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_utils import PreTrainedModel\n@@ -1556,10 +1556,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1570,7 +1570,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "9af07e345a26949184bf2f88f702870ebce00e76",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -23,7 +23,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import (\n@@ -867,10 +867,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -881,7 +881,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "5ef38be037a3177bb6472d4a1365533dfc81f8b5",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -1394,10 +1394,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1408,7 +1408,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "980f57aa342e76d9d97f7d1a80b44f57d334cd50",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 151,
            "deletions": 258,
            "changes": 409,
            "blob_url": "https://github.com/huggingface/transformers/blob/1b222903c3e1cfd9492d75e4b2548aa8bd458674/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1b222903c3e1cfd9492d75e4b2548aa8bd458674/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=1b222903c3e1cfd9492d75e4b2548aa8bd458674",
            "patch": "@@ -18,20 +18,20 @@\n from parameterized import parameterized\n \n from transformers import set_seed\n+from transformers.generation.configuration_utils import ALL_CACHE_IMPLEMENTATIONS\n from transformers.testing_utils import (\n     CaptureStderr,\n     cleanup,\n     get_gpu_count,\n     is_torch_available,\n-    require_gptq,\n-    require_non_xpu,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n     torch_device,\n )\n+from transformers.utils import is_optimum_quanto_available, is_torch_greater_or_equal\n \n \n if is_torch_available():\n@@ -40,15 +40,24 @@\n     from transformers import (\n         AutoModelForCausalLM,\n         AutoTokenizer,\n+        Cache,\n         ClvpForCausalLM,\n         DynamicCache,\n         GenerationConfig,\n         LlamaConfig,\n-        SinkCache,\n         StaticCache,\n         convert_and_export_with_cache,\n     )\n-    from transformers.utils import is_torch_greater_or_equal\n+\n+\n+TEST_CACHE_IMPLEMENTATIONS = [\n+    cache_name\n+    for cache_name in ALL_CACHE_IMPLEMENTATIONS\n+    # TODO (joao): Mamba is not compatible with most models, remove from `ALL_CACHE_IMPLEMENTATIONS`?\n+    if cache_name != \"mamba\"\n+    # TODO (joao): offloaded_hybrid == offloaded_hybrid_chunked, deprecate one of them\n+    if cache_name != \"offloaded_hybrid\"\n+]\n \n \n @require_torch\n@@ -176,9 +185,121 @@ def _random_kvs(config):\n         self.assertTrue(cached_values.shape == (1, 1, 10, 128))\n \n \n-@require_torch_accelerator\n class CacheIntegrationTest(unittest.TestCase):\n-    \"\"\"Cache tests that require loading models\"\"\"\n+    \"\"\"Fast cache integration tests that share the same small model\"\"\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        # Load once and reuse across tests\n+        cls.tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M-Instruct\", padding_side=\"left\")\n+        cls.model = AutoModelForCausalLM.from_pretrained(\n+            \"HuggingFaceTB/SmolLM2-135M-Instruct\", device_map=\"auto\", torch_dtype=torch.float16\n+        )\n+        cls.model.config.sliding_window = 256  # hack to enable the use of caches with sliding windows\n+\n+    def _skip_on_uninstalled_cache_dependencies(self, cache_implementation):\n+        \"\"\"Function to skip tests on missing cache dependencies, given a cache implementation\"\"\"\n+        if cache_implementation == \"quantized\" and not is_optimum_quanto_available():\n+            self.skipTest(\"Quanto is not available\")\n+        if \"offloaded\" in cache_implementation:\n+            has_accelerator = torch_device is not None and torch_device != \"cpu\"\n+            if not has_accelerator:\n+                self.skipTest(\"Offloaded caches require an accelerator\")\n+\n+    @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n+    def test_cache_batched(self, cache_implementation):\n+        \"\"\"Sanity check: caches' `.update` function expects batched inputs\"\"\"\n+        self._skip_on_uninstalled_cache_dependencies(cache_implementation)\n+\n+        EXPECTED_GENERATION = [\"A sequence: 1, 2, 3, 4, 5, 6, 7, 8,\", \"A sequence: A, B, C, D, E, F, G, H\"]\n+\n+        inputs = self.tokenizer(\n+            [\"A sequence: 1, 2, 3, 4, 5\", \"A sequence: A, B, C\"], padding=True, return_tensors=\"pt\"\n+        )\n+        inputs = inputs.to(self.model.device)\n+\n+        gen_out = self.model.generate(\n+            **inputs,\n+            do_sample=False,\n+            max_new_tokens=10,\n+            return_dict_in_generate=True,\n+            cache_implementation=cache_implementation,\n+            disable_compile=True,\n+        )\n+        # Sanity check: a cache was used\n+        self.assertIsInstance(gen_out.past_key_values, Cache)\n+        # Confirm that the output matches expectations\n+        decoded = self.tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n+        self.assertListEqual(decoded, EXPECTED_GENERATION)\n+\n+    @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n+    def test_cache_beam_search(self, cache_implementation):\n+        \"\"\"\n+        Sanity check: caches' `reorder_cache` is operational. We can confirm this by looking at the beam indices\n+        (an output sequence contains multiple beam indices).\n+        \"\"\"\n+        self._skip_on_uninstalled_cache_dependencies(cache_implementation)\n+        if cache_implementation == \"offloaded_hybrid_chunked\":\n+            # TODO (joao, cyril): something is off with `offloaded_hybrid_chunked` aka `OffloadedHybridCache`: the\n+            # output sequence (and the corresponding beam scores, if we add `output_scores=True`) are significantly\n+            # different from the other caches.\n+            self.skipTest(\"`offloaded_hybrid_chunked` fails this test\")\n+\n+        EXPECTED_GENERATION = [\n+            \"Blue is the color of the sky, and the color of\",\n+            \"Blue is the color of the sky, and the second is\",\n+        ]\n+\n+        inputs = self.tokenizer([\"Blue is\"], return_tensors=\"pt\").to(self.model.device)\n+        gen_out = self.model.generate(\n+            **inputs,\n+            do_sample=False,\n+            max_new_tokens=10,\n+            num_beams=2,\n+            num_return_sequences=2,\n+            cache_implementation=cache_implementation,\n+            disable_compile=True,\n+            return_dict_in_generate=True,\n+        )\n+        # Sanity check: a cache was used\n+        self.assertIsInstance(gen_out.past_key_values, Cache)\n+        # At least one of the sequences requires multiple beam indices -> `reorder_cache` had to shift things around\n+        self.assertTrue(any(len(set(beams_in_sequence)) > 1 for beams_in_sequence in gen_out.beam_indices))\n+        # Confirm that the output matches expectations\n+        decoded = self.tokenizer.batch_decode(gen_out.sequences, skip_special_tokens=True)\n+        self.assertListEqual(decoded, EXPECTED_GENERATION)\n+\n+    @parameterized.expand(TEST_CACHE_IMPLEMENTATIONS)\n+    def test_cache_extra_left_padding(self, cache_implementation):\n+        \"\"\"Tests that adding extra left-padding does not affect the generation with the cache\"\"\"\n+        self._skip_on_uninstalled_cache_dependencies(cache_implementation)\n+\n+        EXPECTED_GENERATION = [\"The cat's whiskers are also a sign of anxiety.\"]\n+\n+        inputs = self.tokenizer([\"The cat\"], padding=True, return_tensors=\"pt\").to(self.model.device)\n+        generation_kwargs = {\n+            \"do_sample\": False,\n+            \"max_new_tokens\": 10,\n+            \"cache_implementation\": cache_implementation,\n+            \"disable_compile\": True,\n+        }\n+\n+        gen_out = self.model.generate(**inputs, **generation_kwargs)\n+        decoded = self.tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n+        self.assertListEqual(decoded, EXPECTED_GENERATION)\n+\n+        # Now with extra left-padding\n+        inputs_expanded = self.tokenizer([\"The cat\"], padding=True, return_tensors=\"pt\", pad_to_multiple_of=32)\n+        inputs_expanded = inputs_expanded.to(self.model.device)\n+        self.assertTrue(inputs.input_ids.shape[1] < inputs_expanded.input_ids.shape[1])\n+        gen_out = self.model.generate(**inputs_expanded, **generation_kwargs)\n+        decoded = self.tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n+        self.assertListEqual(decoded, EXPECTED_GENERATION)\n+\n+\n+@require_torch_accelerator\n+class CacheHardIntegrationTest(unittest.TestCase):\n+    \"\"\"Hard cache integration tests that require loading different models\"\"\"\n \n     def tearDown(self):\n         # Some tests use large models, which might result in suboptimal torch re-allocation if we run multiple tests\n@@ -187,18 +308,15 @@ def tearDown(self):\n \n     @slow\n     def test_dynamic_cache_hard(self):\n+        \"\"\"Hard test for base cache implementation -- minor numerical fluctuations will cause this test to fail\"\"\"\n         tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n         model = AutoModelForCausalLM.from_pretrained(\n             \"meta-llama/Llama-2-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16\n         )\n         inputs = tokenizer([\"Here's everything I know about cats. Cats\"], return_tensors=\"pt\").to(model.device)\n \n-        # DynamicCache and the legacy cache format should be equivalent\n-        set_seed(0)\n-        gen_out_legacy = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n         set_seed(0)\n-        gen_out = model.generate(**inputs, do_sample=True, max_new_tokens=256, past_key_values=DynamicCache())\n-        self.assertListEqual(gen_out_legacy.tolist(), gen_out.tolist())\n+        gen_out = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n \n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         expected_text = (\n@@ -215,138 +333,11 @@ def test_dynamic_cache_hard(self):\n         )\n         self.assertEqual(decoded[0], expected_text)\n \n-    @slow\n-    def test_dynamic_cache_batched(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n-        tokenizer.pad_token = tokenizer.eos_token\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n-        inputs = tokenizer([\"A sequence: 1, 2, 3, 4, 5\", \"A sequence: A, B, C\"], padding=True, return_tensors=\"pt\").to(\n-            model.device\n-        )\n-\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10, past_key_values=DynamicCache())\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        expected_text = [\"A sequence: 1, 2, 3, 4, 5, 6, 7, 8,\", \"A sequence: A, B, C, D, E, F, G, H\"]\n-        self.assertListEqual(decoded, expected_text)\n-\n-    @slow\n-    def test_dynamic_cache_beam_search(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", padding_side=\"left\")\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"meta-llama/Llama-2-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n-\n-        inputs = tokenizer([\"The best color is\"], return_tensors=\"pt\").to(model.device)\n-        gen_out = model.generate(\n-            **inputs,\n-            do_sample=False,\n-            max_new_tokens=20,\n-            num_beams=2,\n-            num_return_sequences=2,\n-        )\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        expected_text = [\n-            \"The best color is the one that makes you feel good.\\nThe best color is the one that makes you feel good\",\n-            \"The best color is the one that suits you.\\nThe best color is the one that suits you. The\",\n-        ]\n-        self.assertListEqual(decoded, expected_text)\n-\n-    @slow\n-    def test_hybrid_cache_n_sequences(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2-9b\")\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"google/gemma-2-9b\",\n-            device_map=\"auto\",\n-            torch_dtype=torch.bfloat16,\n-            attn_implementation=\"eager\",\n-        )\n-\n-        inputs = tokenizer([\"Hello I am doing\"], return_tensors=\"pt\").to(model.device)\n-\n-        gen_out = model.generate(\n-            **inputs,\n-            do_sample=False,\n-            max_new_tokens=20,\n-            num_return_sequences=2,\n-            num_beams=2,\n-        )\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        expected_text = [\n-            \"Hello I am doing a project for my school and I am trying to make a program that will allow me to input a\",\n-            \"Hello I am doing a project for my school and I am trying to make a program that will allow me to use a\",\n-        ]\n-        self.assertListEqual(decoded, expected_text)\n-\n-    @require_non_xpu\n-    @require_gptq\n-    @slow\n-    def test_sink_cache_hard(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/LLaMa-7B-GPTQ\")\n-        model = AutoModelForCausalLM.from_pretrained(\"TheBloke/LLaMa-7B-GPTQ\", device_map=\"auto\")\n-\n-        inputs = tokenizer([\"Vaswani et al. (2017) introduced the Transformers\"], return_tensors=\"pt\").to(model.device)\n-\n-        # Set up the SinkCache. Using a small window length to contain computational complexity. If this example is run\n-        # without a SinkCache, the last few tokens are gibberish (ends in \"of the of the of a of a of\")\n-        cache = SinkCache(window_length=508, num_sink_tokens=4)\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=3000, past_key_values=cache)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        self.assertTrue(decoded[0].endswith(\"to perform a variety of tasks. The Transformer is a neural network\"))\n-\n-    @slow\n-    def test_sink_cache_iterative_prompts(self):\n-        \"\"\"Tests that SinkCache supports more than one new token at once, when shifting the cache\"\"\"\n-        tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"HuggingFaceH4/zephyr-7b-beta\", device_map=\"auto\", torch_dtype=torch.float16\n-        )\n-        prompt = (\n-            \"Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences \"\n-            \"and must-see attractions.\"\n-        )\n-\n-        # Prepare generation settings\n-        cache = SinkCache(window_length=256, num_sink_tokens=4)\n-        input_ids = torch.tensor([], device=model.device, dtype=torch.int)\n-        for _ in range(3):\n-            # Tokenize the prompt with the correct chat template\n-            chat = [{\"role\": \"user\", \"content\": prompt}]\n-            tokenized_chat = tokenizer.apply_chat_template(chat, return_tensors=\"pt\", add_generation_prompt=True).to(\n-                model.device\n-            )\n-            input_ids = torch.cat((input_ids, tokenized_chat), dim=1)\n-\n-            # Perform the generation\n-            gen_out = model.generate(\n-                input_ids, do_sample=False, max_new_tokens=100, past_key_values=cache, use_cache=True\n-            )\n-            input_ids = gen_out\n-\n-        # We went well beyond the cache length\n-        self.assertTrue(input_ids.shape[1] > cache.get_max_cache_shape() * 1.5)\n-\n-        # And it still produces a coherent english\n-        decoded = tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n-        last_output = (\n-            \"<|assistant|>\\nAs the sun began to set over the Pacific Ocean, I found myself standing on the shores of \"\n-            \"Waikiki Beach, my heart filled with awe and wonder. I had just returned from a two-week journey to the \"\n-            \"beautiful island of Hawaii, and it had been an unforgettable experience filled with cultural experiences \"\n-            \"and must-see attractions that left me breathless.\\n\\nOne of the most memorable experiences of my trip \"\n-            \"was visiting the historic district of Honolulu. Here,\"\n-        )\n-        self.assertTrue(decoded[0].endswith(last_output))\n-\n-    @parameterized.expand(\n-        [\n-            (\"eager\", \"static\"),\n-            (\"sdpa\", \"static\"),\n-        ]\n-    )\n+    @parameterized.expand([(\"eager\"), (\"sdpa\")])\n     @require_torch_gpu\n     @slow\n-    def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_implementation):\n+    def test_static_cache_greedy_decoding_pad_left(self, attn_implementation):\n+        \"\"\"Tests that different cache implementations work well with eager and SDPA inference\"\"\"\n         EXPECTED_GENERATION = [\n             \"The best color is the one that complements the skin tone of the\",\n             \"We should not undermind the issues at hand.\\nWe should not undermind the issues\",\n@@ -371,124 +362,19 @@ def test_static_cache_greedy_decoding_pad_left(self, attn_implementation, cache_\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n         set_seed(0)\n-        model.generation_config.cache_implementation = cache_implementation\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n+        gen_out = model.generate(\n+            **inputs, do_sample=False, max_new_tokens=10, cache_implementation=\"static\", disable_compile=True\n+        )\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         with self.subTest(f\"{attn_implementation}, static, eager\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n         set_seed(0)\n-        model.forward = torch.compile(model.forward)\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n+        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10, cache_implementation=\"static\")\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         with self.subTest(f\"{attn_implementation}, static, compiled\"):\n             self.assertListEqual(decoded, EXPECTED_GENERATION)\n \n-    @slow\n-    def test_dynamic_cache_extra_left_padding(self):\n-        \"\"\"Tests that adding extra left-padding does not affect the generation with the dynamic cache\"\"\"\n-        EXPECTED_GENERATION = [\n-            \"The best color is the one that complements the skin tone of the\",\n-            \"We should not undermind the issues at hand.\\nWe should not undermind the issues\",\n-        ]\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\", padding_side=\"left\", pad_token=\"<s>\"\n-        )\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\",\n-            torch_dtype=torch.bfloat16,\n-        ).to(torch_device)\n-        inputs = tokenizer(\n-            [\"The best color is\", \"We should not undermind the issues at hand\"], padding=True, return_tensors=\"pt\"\n-        ).to(model.device)\n-\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n-        # Now with extra left-padding\n-        inputs_expanded = tokenizer(\n-            [\"The best color is\", \"We should not undermind the issues at hand\"],\n-            padding=True,\n-            return_tensors=\"pt\",\n-            pad_to_multiple_of=32,\n-        ).to(model.device)\n-        self.assertTrue(inputs.input_ids.shape[1] < inputs_expanded.input_ids.shape[1])\n-        gen_out = model.generate(**inputs_expanded, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n-    @slow\n-    def test_static_cache_extra_left_padding(self):\n-        \"\"\"Tests that adding extra left-padding does not affect the generation with the static cache\"\"\"\n-        EXPECTED_GENERATION = [\n-            \"The best color is the one that complements the skin tone of the\",\n-            \"We should not undermind the issues at hand.\\nWe should not undermind the issues\",\n-        ]\n-\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\", padding_side=\"left\", pad_token=\"<s>\"\n-        )\n-        model = AutoModelForCausalLM.from_pretrained(\n-            \"NousResearch/Llama-2-7b-chat-hf\",\n-            torch_dtype=torch.bfloat16,\n-        ).to(torch_device)\n-        inputs = tokenizer(\n-            [\"The best color is\", \"We should not undermind the issues at hand\"], padding=True, return_tensors=\"pt\"\n-        ).to(model.device)\n-\n-        model.generation_config.cache_implementation = \"static\"\n-\n-        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n-        # Now with extra left-padding\n-        inputs_expanded = tokenizer(\n-            [\"The best color is\", \"We should not undermind the issues at hand\"],\n-            padding=True,\n-            return_tensors=\"pt\",\n-            pad_to_multiple_of=32,\n-        ).to(model.device)\n-        self.assertTrue(inputs.input_ids.shape[1] < inputs_expanded.input_ids.shape[1])\n-        gen_out = model.generate(**inputs_expanded, do_sample=False, max_new_tokens=10)\n-        decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n-        self.assertListEqual(decoded, EXPECTED_GENERATION)\n-\n-    @unittest.skip(reason=\"TODO @gante static cache's does not support beam search yet\")\n-    def test_static_cache_beam_search(self):\n-        pass\n-\n-    @require_torch_accelerator\n-    @slow\n-    def test_offloaded_cache_equivalent_to_dynamic_cache(self):\n-        \"\"\"Tests that OffloadedCache produces the same result as the default DynamicCache\"\"\"\n-        model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n-        tokenizer = AutoTokenizer.from_pretrained(model_name)\n-        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n-        device = model.device\n-\n-        if not is_torch_greater_or_equal(\"2.7\", accept_dev=True) and device.type == \"xpu\":\n-            self.skipTest(reason=\"This test requires torch >= 2.7 to run on xpu.\")\n-\n-        input_text = \"Fun fact:\"\n-        inputs = tokenizer(input_text, return_tensors=\"pt\").to(device)\n-        common = {\n-            \"num_beams\": 4,\n-            \"num_beam_groups\": 2,\n-            \"num_return_sequences\": 4,\n-            \"diversity_penalty\": 1.0,\n-            \"max_new_tokens\": 20,\n-            \"early_stopping\": True,\n-        }\n-        original = GenerationConfig(**common)\n-        offloaded = GenerationConfig(cache_implementation=\"offloaded\", **common)\n-        original_outputs = model.generate(generation_config=original, **inputs)\n-        offloaded_outputs = model.generate(generation_config=offloaded, **inputs)\n-        for original_output, offloaded_output in zip(original_outputs, offloaded_outputs):\n-            assert torch.all(original_output == offloaded_output).item()\n-\n     @require_torch_accelerator\n     @slow\n     def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n@@ -526,12 +412,14 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         torch_accelerator_module.reset_peak_memory_stats(device)\n         model.generate(generation_config=offloaded, **inputs)\n         offloaded_peak_memory = torch_accelerator_module.max_memory_allocated(device)\n-        print(f\"original_peak_memory: {original_peak_memory}, offloaded_peak_memory: {offloaded_peak_memory}\")\n-        assert offloaded_peak_memory < original_peak_memory\n+        self.assertTrue(offloaded_peak_memory < original_peak_memory)\n \n     @require_torch_gpu\n     @slow\n     def test_cache_copy(self):\n+        \"\"\"Tests that we can manually set a cache, copy, and reuse it for generation\"\"\"\n+        # TODO (joao): test for all cache implementations in `CacheIntegrationTest` after standardizing the\n+        # lazy init of cache layers\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n         model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n@@ -542,7 +430,7 @@ def test_cache_copy(self):\n \n         INITIAL_PROMPT = \"You are a helpful assistant. \"\n         inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n-        # This is the common prompt cached, we need to run forward without grad to be abel to copy\n+        # This is the common prompt cached, we need to run forward without grad to be able to copy\n         with torch.no_grad():\n             prompt_cache = model(**inputs_initial_prompt, past_key_values=prompt_cache).past_key_values\n \n@@ -551,14 +439,19 @@ def test_cache_copy(self):\n         for prompt in prompts:\n             new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n             past_key_values = copy.deepcopy(prompt_cache)\n-            outputs = model.generate(**new_inputs, past_key_values=past_key_values, max_new_tokens=40)\n+            outputs = model.generate(\n+                **new_inputs, past_key_values=past_key_values, max_new_tokens=40, disable_compile=True\n+            )\n             response = tokenizer.batch_decode(outputs)[0]\n             responses.append(response)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is an enriching experience that broadens our horizons and exposes us to new cultures, landscapes, and people. Whether it's a week\",\n-            'You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital of France.\\n\\n\\n\\n\\n\\n## Query:\\n\\nIn a detailed analysis, compare the economic impacts of the introduction of the'\n-        ]  # fmt: skip\n+            \"You are a helpful assistant. Help me to write a blogpost about travelling.\\n\\nTraveling is a wonderful \"\n+            \"way to explore new places, cultures, and experiences. Whether you are a seasoned traveler or a \"\n+            \"first-time adventurer, there is always something\",\n+            \"You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital \"\n+            \"of France.\\n\\n\\n\\n\\n\\n\\n<|endoftext|>\",\n+        ]\n         self.assertEqual(responses, EXPECTED_DECODED_TEXT)\n \n     @require_torch_multi_gpu\n@@ -609,7 +502,7 @@ def test_static_cache_no_cuda_graph_skips(self):\n         # on `main`, prior to #36543, this would send stderr messages about cuda graphs being skipped.\n         with CaptureStderr() as cap:\n             model.generate(**inputs, max_new_tokens=2, cache_implementation=\"static\")\n-        self.assertEqual(cap.err, \"\")\n+        self.assertNotIn(\"cuda\", cap.err.lower())\n \n     @require_torch_multi_gpu\n     @slow"
        }
    ],
    "stats": {
        "total": 776,
        "additions": 338,
        "deletions": 438
    }
}