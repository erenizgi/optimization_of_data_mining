{
    "author": "wizyoung",
    "message": "Fix the key name for _load_rng_state under torch.cuda (#36138)\n\nfix load key name for _load_rng_state under torch.cuda\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "12962fe84b32fa6a716f8b196c0d45db37b7d7d5",
    "files": [
        {
            "sha": "f970885314f1b0302e9a971b477f151cec4e9b9e",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12962fe84b32fa6a716f8b196c0d45db37b7d7d5/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12962fe84b32fa6a716f8b196c0d45db37b7d7d5/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=12962fe84b32fa6a716f8b196c0d45db37b7d7d5",
            "patch": "@@ -3125,7 +3125,7 @@ def _load_rng_state(self, checkpoint):\n \n         is_distributed = self.args.parallel_mode == ParallelMode.DISTRIBUTED\n         if torch.cuda.is_available():\n-            set_rng_state_for_device(\"GPU\", torch.cuda, checkpoint_rng_state, is_distributed)\n+            set_rng_state_for_device(\"CUDA\", torch.cuda, checkpoint_rng_state, is_distributed)\n         if is_torch_npu_available():\n             set_rng_state_for_device(\"NPU\", torch.npu, checkpoint_rng_state, is_distributed)\n         if is_torch_mlu_available():"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}