{
    "author": "bfineran",
    "message": "HFQuantizer implementation for compressed-tensors library (#31704)\n\n* Add compressed-tensors HFQuantizer implementation\r\n\r\n* flag serializable as False\r\n\r\n* run\r\n\r\n* revive lines deleted by ruff\r\n\r\n* fixes to load+save from sparseml, edit config to quantization_config, and load back\r\n\r\n* address satrat comment\r\n\r\n* compressed_tensors to compressed-tensors and revert back is_serializable\r\n\r\n* rename quant_method from sparseml to compressed-tensors\r\n\r\n* tests\r\n\r\n* edit tests\r\n\r\n* clean up tests\r\n\r\n* make style\r\n\r\n* cleanup\r\n\r\n* cleanup\r\n\r\n* add test skip for when compressed tensors is not installed\r\n\r\n* remove pydantic import + style\r\n\r\n* delay torch import in test\r\n\r\n* initial docs\r\n\r\n* update main init for compressed tensors config\r\n\r\n* make fix-copies\r\n\r\n* docstring\r\n\r\n* remove fill_docstring\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* review comments\r\n\r\n* review comments\r\n\r\n* comments - suppress warnings on state dict load, tests, fixes\r\n\r\n* bug-fix - remove unnecessary call to apply quant lifecycle\r\n\r\n* run_compressed compatability\r\n\r\n* revert changes not needed for compression\r\n\r\n* no longer need unexpected keys fn\r\n\r\n* unexpected keys not needed either\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* add to_diff_dict\r\n\r\n* update docs and expand testing\r\n\r\n* Update _toctree.yml with compressed-tensors\r\n\r\n* Update src/transformers/utils/quantization_config.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* update doc\r\n\r\n* add note about saving a loaded model\r\n\r\n---------\r\n\r\nCo-authored-by: George Ohashi <george@neuralmagic.com>\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\nCo-authored-by: Sara Adkins <sara@neuralmagic.com>\r\nCo-authored-by: Sara Adkins <sara.adkins65@gmail.com>\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\nCo-authored-by: Dipika Sikka <ds3822@columbia.edu>\r\nCo-authored-by: Dipika <dipikasikka1@gmail.com>",
    "sha": "574a9e12bbe787a4f6b2c170962be84e17c87d0b",
    "files": [
        {
            "sha": "e7f2a9d67567371e07e2ad758a30b86763a0c041",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -177,6 +177,8 @@\n     title: Optimum\n   - local: quantization/torchao\n     title: TorchAO\n+  - local: quantization/compressed_tensors\n+    title: compressed-tensors\n   - local: quantization/contribute\n     title: Contribute new quantization method\n   title: Quantization Methods"
        },
        {
            "sha": "a2f831f65976ecaaf1ee3a703ecb196c311e4936",
            "filename": "docs/source/en/main_classes/quantization.md",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fquantization.md?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -61,7 +61,10 @@ Learn how to quantize models in the [Quantization](../quantization) guide.\n \n [[autodoc]] FbgemmFp8Config\n \n+## CompressedTensorsConfig\n+\n+[[autodoc]] CompressedTensorsConfig\n+\n ## TorchAoConfig\n \n [[autodoc]] TorchAoConfig\n-"
        },
        {
            "sha": "f385aae965f662680906a3ad6de0a7d3d821b944",
            "filename": "docs/source/en/quantization/compressed_tensors.md",
            "status": "added",
            "additions": 230,
            "deletions": 0,
            "changes": 230,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fcompressed_tensors.md?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -0,0 +1,230 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+# Compressed Tensors\n+\n+The [`compressed-tensors`](https://github.com/neuralmagic/compressed-tensors) library provides a versatile and efficient way to store and manage compressed model checkpoints. This library supports various quantization and sparsity schemes, making it a unified format for handling different model optimizations like GPTQ, AWQ, SmoothQuant, INT8, FP8, SparseGPT, and more.\n+\n+Some of the supported formats include:\n+1. `dense`\n+2. `int-quantized`: INT8 quantized models\n+    - sample [model/config](https://huggingface.co/nm-testing/tinyllama-w8a8-compressed-hf-quantizer)\n+3. `float-quantized`: FP8 quantized models; currently support E4M3\n+    - sample [model/config](https://huggingface.co/nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat/tree/main)\n+4. `pack-quantized`: INT4 or INT8 weight-quantized models, packed into INT32. For INT4, the weights have an INT4 range but are stored as INT8 and   then packed into INT32.\n+    - sample [model/config](nm-testing/tinyllama-w4a16-compressed-hf-quantizer)\n+\n+Compressed models can be easily created using [llm-compressor](https://github.com/vllm-project/llm-compressor).\n+Alternatively models can be created indepedenty and serialized with a compressed tensors config.\n+\n+To find existing models on the Hugging Face Model Hub, search for the [`compressed-tensors` tag](https://huggingface.co/models?other=compressed-tensors).\n+\n+#### Features:\n+ - Weight and activation precisions: FP8, INT4, INT8 (for Q/DQ arbitrary precision is allowed for INT)\n+ - Quantization scales and zero-points strategies: [tensor, channel, group, block, token](https://github.com/neuralmagic/compressed-tensors/blob/83b2e7a969d70606421a76b9a3d112646077c8de/src/compressed_tensors/quantization/quant_args.py#L43-L52)\n+ - Dynamic per-token activation quantization (or any static strategy)\n+ - Sparsity can be \n+ - Supports quantization of arbitrary modules, not just Linear modules\n+ - Targeted support or ignoring of modules by name or class\n+\n+## Installation\n+\n+It is recommended to install stable releases of compressed-tensors from [PyPI](https://pypi.org/project/compressed-tensors):\n+```bash\n+pip install compressed-tensors\n+```\n+\n+Developers who want to experiment with the latest features can also install the package from source:\n+```bash\n+git clone https://github.com/neuralmagic/compressed-tensors\n+cd compressed-tensors\n+pip install -e .\n+```\n+\n+## Quickstart Model Load\n+Quantized models can be easily loaded for inference as shown below. Only models that have already been quantized can be loaded at the moment. To quantize a model into the compressed-tensors format see [llm-compressor](https://github.com/vllm-project/llm-compressor).\n+\n+```python\n+from transformers import AutoModelForCausalLM\n+\n+# Load the model in compressed-tensors format\n+ct_model = AutoModelForCausalLM.from_pretrained(\"nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf\")\n+\n+# Measure memory usage\n+mem_params = sum([param.nelement()*param.element_size() for param in ct_model.parameters()])\n+print(f\"{mem/2**30:.4f} GB\")\n+# 8.4575 GB\n+```\n+\n+We can see just above that the compressed-tensors FP8 checkpoint of Llama 3.1 8B is able to be loaded for inference using half of the memory of the unquantized reference checkpoint.\n+\n+## Sample Use Cases - Load and run an FP8 model\n+\n+```python\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+prompt = [\n+    \"Hello, my name is\",\n+    \"The capital of France is\",\n+    \"The future of AI is\"\n+]\n+\n+model_name = \"nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat\"\n+\n+quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+\n+inputs = tokenizer(prompt, return_tensors=\"pt\")\n+generated_ids = quantized_model.generate(**inputs, max_length=50, do_sample=False)\n+outputs = tokenizer.batch_decode(generated_ids)\n+\n+print(outputs)\n+\n+\"\"\"\n+['<|begin_of_text|>Hello, my name is [Name]. I am a [Your Profession/Student] and I am here to learn about the [Course/Program] at [University/Institution]. I am excited to be here and I am looking forward to', '<|begin_of_text|>The capital of France is Paris, which is located in the north-central part of the country. Paris is the most populous city in France and is known for its stunning architecture, art museums, fashion, and romantic atmosphere. The city is home to', \"<|begin_of_text|>The future of AI is here, and it's already changing the way we live and work. From virtual assistants to self-driving cars, AI is transforming industries and revolutionizing the way we interact with technology. But what does the future of AI hold\"]\n+\"\"\"\n+\n+```\n+\n+The above shows a quick example for running generation using a `compressed-tensors`\n+model. Currently, once loaded the model cannot be saved.\n+\n+## Deep dive into a compressed-tensors model checkpoint\n+\n+In this example we will examine how the compressed-tensors model nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf is defined through its configuration entry and see how this translates to the loaded model representation. \n+\n+First, let us look at the [`quantization_config` of the model](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf/blob/main/config.json). At a glance it looks overwhelming with the number of entries but this is because compressed-tensors is a format that allows for flexible expression both during and after model compression.\n+\n+In practice for checkpoint loading and inference the configuration can be simplified to not include all the default or empty entries, so we will do that here to focus on what compression is actually represented.\n+\n+```yaml\n+\"quantization_config\": {\n+  \"config_groups\": {\n+    \"group_0\": {\n+      \"input_activations\": {\n+        \"num_bits\": 8,\n+        \"strategy\": \"tensor\",\n+        \"type\": \"float\"\n+      },\n+      \"targets\": [\"Linear\"],\n+      \"weights\": {\n+        \"num_bits\": 8,\n+        \"strategy\": \"tensor\",\n+        \"type\": \"float\"\n+      }\n+    }\n+  },\n+  \"format\": \"naive-quantized\",\n+  \"ignore\": [\"lm_head\"],\n+  \"quant_method\": \"compressed-tensors\",\n+  \"quantization_status\": \"frozen\"\n+},\n+```\n+\n+We can see from the above configuration that it is specifying one config group that includes weight and activation quantization to FP8 with a static per-tensor strategy. It is also worth noting that in the `ignore` list there is an entry to skip quantization of the `lm_head` module, so that module should be untouched in the checkpoint.\n+\n+To see the result of the configuration in practice, we can simply use the [safetensors viewer](https://huggingface.co/nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf?show_file_info=model.safetensors.index.json) on the model card to see the quantized weights, input_scale, and weight_scale for all of the Linear modules in the first model layer (and so on for the rest of the layers).\n+\n+| Tensors | Shape |\tPrecision |\n+| ------- | ----- | --------- |\n+model.layers.0.input_layernorm.weight\t| [4â€¯096]\t| BF16 \n+model.layers.0.mlp.down_proj.input_scale\t| [1]\t| BF16 \n+model.layers.0.mlp.down_proj.weight\t| [4â€¯096, 14â€¯336] |\tF8_E4M3 \n+model.layers.0.mlp.down_proj.weight_scale |\t[1]\t| BF16 \n+model.layers.0.mlp.gate_proj.input_scale |\t[1]\t| BF16 \n+model.layers.0.mlp.gate_proj.weight\t| [14â€¯336, 4â€¯096]\t| F8_E4M3 \n+model.layers.0.mlp.gate_proj.weight_scale\t| [1] |\tBF16 \n+model.layers.0.mlp.up_proj.input_scale|\t[1]\t|BF16 \n+model.layers.0.mlp.up_proj.weight |\t[14â€¯336, 4â€¯096]\t| F8_E4M3 \n+model.layers.0.mlp.up_proj.weight_scale | [1]\t| BF16 \n+model.layers.0.post_attention_layernorm.weight |\t[4â€¯096]\t|BF16 \n+model.layers.0.self_attn.k_proj.input_scale |\t[1]\t|  BF16\n+model.layers.0.self_attn.k_proj.weight |\t[1â€¯024, 4â€¯096]|\tF8_E4M3\n+model.layers.0.self_attn.k_proj.weight_scale |[1]\t| BF16 \n+model.layers.0.self_attn.o_proj.input_scale\t| [1]\t| BF16\n+model.layers.0.self_attn.o_proj.weight | [4â€¯096, 4â€¯096]\t| F8_E4M3 \n+model.layers.0.self_attn.o_proj.weight_scale | [1]\t| BF16 \n+model.layers.0.self_attn.q_proj.input_scale\t| [1]\t| BF16 \n+model.layers.0.self_attn.q_proj.weight | [4â€¯096, 4â€¯096]\t| F8_E4M3 \n+model.layers.0.self_attn.q_proj.weight_scale |\t[1] | BF16 \n+model.layers.0.self_attn.v_proj.input_scale\t| [1] | BF16 \n+model.layers.0.self_attn.v_proj.weight |\t[1â€¯024, 4â€¯096]\t| F8_E4M3 \n+model.layers.0.self_attn.v_proj.weight_scale |\t[1] |\tBF16 \n+\n+When we load the model with the compressed-tensors HFQuantizer integration, we can see that all of the Linear modules that are specified within the quantization configuration have been replaced by `CompressedLinear` modules that manage the compressed weights and forward pass for inference. Note that the `lm_head` mentioned before in the ignore list is still kept as an unquantized Linear module.\n+\n+```python\n+from transformers import AutoModelForCausalLM\n+\n+ct_model = AutoModelForCausalLM.from_pretrained(\"nm-testing/Meta-Llama-3.1-8B-Instruct-FP8-hf\")\n+print(ct_model)\n+\"\"\"\n+LlamaForCausalLM(\n+  (model): LlamaModel(\n+    (embed_tokens): Embedding(128256, 4096)\n+    (layers): ModuleList(\n+      (0-31): 32 x LlamaDecoderLayer(\n+        (self_attn): LlamaSdpaAttention(\n+          (q_proj): CompressedLinear(\n+            in_features=4096, out_features=4096, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (k_proj): CompressedLinear(\n+            in_features=4096, out_features=1024, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (v_proj): CompressedLinear(\n+            in_features=4096, out_features=1024, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (o_proj): CompressedLinear(\n+            in_features=4096, out_features=4096, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (rotary_emb): LlamaRotaryEmbedding()\n+        )\n+        (mlp): LlamaMLP(\n+          (gate_proj): CompressedLinear(\n+            in_features=4096, out_features=14336, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (up_proj): CompressedLinear(\n+            in_features=4096, out_features=14336, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (down_proj): CompressedLinear(\n+            in_features=14336, out_features=4096, bias=False\n+            (input_observer): MovingAverageMinMaxObserver()\n+            (weight_observer): MovingAverageMinMaxObserver()\n+          )\n+          (act_fn): SiLU()\n+        )\n+        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n+        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n+      )\n+    )\n+    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n+    (rotary_emb): LlamaRotaryEmbedding()\n+  )\n+  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n+)\n+\"\"\"\n+```"
        },
        {
            "sha": "ef8ed444d9d49b5086b84a8483f1002b1f30c3e2",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -50,6 +50,7 @@ Use the table below to help you decide which quantization method to use.\n | [AQLM](./aqlm)                                | ðŸ”´                       |  ðŸŸ¢   |     ðŸŸ¢     | ðŸ”´              | ðŸ”´                     | ðŸŸ¢                      | 1 / 2          | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/Vahe1994/AQLM            |\n | [AWQ](./awq) | ðŸ”´                       | ðŸ”´   | ðŸŸ¢        | ðŸŸ¢              | ðŸ”´                     | ?                       | 4              | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/casper-hansen/AutoAWQ    |\n | [bitsandbytes](./bitsandbytes)     | ðŸŸ¢            | ðŸŸ¡ *   |     ðŸŸ¢     | ðŸŸ¡ *            | ðŸ”´ **    | ðŸ”´    (soon!)          | 4 / 8          | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n+| [compressed-tensors](./compressed_tensors)                        | ðŸ”´                       | ðŸŸ¢   |     ðŸŸ¢     | ðŸŸ¢              | ðŸ”´                     | ðŸ”´                       | 1 - 8          | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/neuralmagic/compressed-tensors |\n | [EETQ](./eetq)                                | ðŸŸ¢                       | ðŸ”´   | ðŸŸ¢        | ðŸ”´              | ðŸ”´                     | ?                       | 8              | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/NetEase-FuXi/EETQ        |\n | GGUF / GGML (llama.cpp)             | ðŸŸ¢                       | ðŸŸ¢   | ðŸŸ¢        | ðŸ”´              | ðŸŸ¢                     | ðŸ”´                       | 1 - 8          | ðŸ”´                                   | [See GGUF section](../gguf)                | [See GGUF section](../gguf)                      | https://github.com/ggerganov/llama.cpp      |\n | [GPTQ](./gptq)                                | ðŸ”´                       | ðŸ”´   | ðŸŸ¢        | ðŸŸ¢              | ðŸ”´                     | ðŸ”´                       | 2 - 3 - 4 - 8          | ðŸŸ¢                                   | ðŸŸ¢            | ðŸŸ¢                      | https://github.com/AutoGPTQ/AutoGPTQ        |"
        },
        {
            "sha": "b9d5a386f9c18aa3c1fa5132ce3baa79761b6b59",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -958,6 +958,7 @@\n         \"AqlmConfig\",\n         \"AwqConfig\",\n         \"BitsAndBytesConfig\",\n+        \"CompressedTensorsConfig\",\n         \"EetqConfig\",\n         \"FbgemmFp8Config\",\n         \"GPTQConfig\",\n@@ -5802,6 +5803,7 @@\n         AqlmConfig,\n         AwqConfig,\n         BitsAndBytesConfig,\n+        CompressedTensorsConfig,\n         EetqConfig,\n         FbgemmFp8Config,\n         GPTQConfig,"
        },
        {
            "sha": "1dcd87c993a2e6e6f5525537234bfe284a9123e9",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -19,6 +19,7 @@\n     AqlmConfig,\n     AwqConfig,\n     BitsAndBytesConfig,\n+    CompressedTensorsConfig,\n     EetqConfig,\n     FbgemmFp8Config,\n     GPTQConfig,\n@@ -32,6 +33,7 @@\n from .quantizer_awq import AwqQuantizer\n from .quantizer_bnb_4bit import Bnb4BitHfQuantizer\n from .quantizer_bnb_8bit import Bnb8BitHfQuantizer\n+from .quantizer_compressed_tensors import CompressedTensorsHfQuantizer\n from .quantizer_eetq import EetqHfQuantizer\n from .quantizer_fbgemm_fp8 import FbgemmFp8HfQuantizer\n from .quantizer_gptq import GptqHfQuantizer\n@@ -49,6 +51,7 @@\n     \"quanto\": QuantoHfQuantizer,\n     \"eetq\": EetqHfQuantizer,\n     \"hqq\": HqqHfQuantizer,\n+    \"compressed-tensors\": CompressedTensorsHfQuantizer,\n     \"fbgemm_fp8\": FbgemmFp8HfQuantizer,\n     \"torchao\": TorchAoHfQuantizer,\n }\n@@ -62,6 +65,7 @@\n     \"aqlm\": AqlmConfig,\n     \"quanto\": QuantoConfig,\n     \"hqq\": HqqConfig,\n+    \"compressed-tensors\": CompressedTensorsConfig,\n     \"fbgemm_fp8\": FbgemmFp8Config,\n     \"torchao\": TorchAoConfig,\n }"
        },
        {
            "sha": "5531838e568afbff1fbc5265c678e8bd8bef063c",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "added",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -0,0 +1,77 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ..utils import is_compressed_tensors_available, is_torch_available, logging\n+from ..utils.quantization_config import QuantizationConfigMixin\n+from .base import HfQuantizer\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class CompressedTensorsHfQuantizer(HfQuantizer):\n+    \"\"\"\n+    Quantizer for the compressed_tensors package.  Loads and restores models to\n+    quantized state with compressed_tensors\n+    \"\"\"\n+\n+    requires_calibration = True\n+    required_packages = [\"compressed_tensors\"]\n+\n+    def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n+        super().__init__(quantization_config, **kwargs)\n+\n+        from compressed_tensors.compressors import ModelCompressor\n+\n+        self.compressor = ModelCompressor.from_compression_config(quantization_config)\n+\n+    def validate_environment(self, *args, **kwargs):\n+        if not is_compressed_tensors_available():\n+            raise ImportError(\n+                \"Using `compressed_tensors` quantized models requires the compressed-tensors library: \"\n+                \"`pip install compressed-tensors`\"\n+            )\n+        if not is_torch_available():\n+            # torch already should be installed as part of compressed tensors\n+            raise ImportError(\"torch is required for using compressed-tensors quantization\")\n+\n+    def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n+        if torch_dtype is None:\n+            logger.info(\"Loading model using torch.float16 for compressed-tensors quantization\")\n+            torch_dtype = torch.float16\n+        elif torch_dtype != torch.float16:\n+            logger.info(\n+                \"We suggest you to set `torch_dtype=torch.float16` for better efficiency with compressed_tensors.\"\n+            )\n+        return torch_dtype\n+\n+    def _process_model_before_weight_loading(self, model, **kwargs):\n+        from compressed_tensors.quantization import apply_quantization_config\n+\n+        ct_quantization_config = self.compressor.quantization_config\n+        apply_quantization_config(model, ct_quantization_config, run_compressed=True)\n+\n+    def _process_model_after_weight_loading(self, model, **kwargs):\n+        pass\n+\n+    @property\n+    def is_trainable(self):\n+        return False\n+\n+    @property\n+    def is_serializable(self):\n+        return False"
        },
        {
            "sha": "a5f257c6534ed1fc5cebcfa06a9cd6a26e47d3a0",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -63,6 +63,7 @@\n     is_bitsandbytes_available,\n     is_bitsandbytes_multi_backend_available,\n     is_bs4_available,\n+    is_compressed_tensors_available,\n     is_cv2_available,\n     is_cython_available,\n     is_decord_available,\n@@ -1199,6 +1200,13 @@ def require_quanto(test_case):\n     return unittest.skipUnless(is_quanto_available(), \"test requires quanto\")(test_case)\n \n \n+def require_compressed_tensors(test_case):\n+    \"\"\"\n+    Decorator for compressed_tensors dependency\n+    \"\"\"\n+    return unittest.skipUnless(is_compressed_tensors_available(), \"test requires compressed_tensors\")(test_case)\n+\n+\n def require_fbgemm_gpu(test_case):\n     \"\"\"\n     Decorator for fbgemm_gpu dependency"
        },
        {
            "sha": "134da3474becc033cda66805d4bdc46084415dff",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -124,6 +124,7 @@\n     is_bitsandbytes_multi_backend_available,\n     is_bs4_available,\n     is_coloredlogs_available,\n+    is_compressed_tensors_available,\n     is_cv2_available,\n     is_cython_available,\n     is_datasets_available,"
        },
        {
            "sha": "169d3491053e572b496e2085e641de1c1b6c6285",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -142,6 +142,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n # `importlib.metadata.version` doesn't work with `awq`\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n _quanto_available = _is_package_available(\"quanto\")\n+_compressed_tensors_available = _is_package_available(\"compressed_tensors\")\n _pandas_available = _is_package_available(\"pandas\")\n _peft_available = _is_package_available(\"peft\")\n _phonemizer_available = _is_package_available(\"phonemizer\")\n@@ -963,6 +964,10 @@ def is_quanto_available():\n     return _quanto_available\n \n \n+def is_compressed_tensors_available():\n+    return _compressed_tensors_available\n+\n+\n def is_auto_gptq_available():\n     return _auto_gptq_available\n "
        },
        {
            "sha": "23a983af7420790fe0e7e454a59500619a859a9c",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 125,
            "deletions": 0,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -42,6 +42,7 @@ class QuantizationMethod(str, Enum):\n     QUANTO = \"quanto\"\n     EETQ = \"eetq\"\n     HQQ = \"hqq\"\n+    COMPRESSED_TENSORS = \"compressed-tensors\"\n     FBGEMM_FP8 = \"fbgemm_fp8\"\n     TORCHAO = \"torchao\"\n \n@@ -1051,6 +1052,130 @@ def post_init(self):\n             raise ValueError(f\"Only support weights in {accepted_weights} but found {self.weights}\")\n \n \n+class CompressedTensorsConfig(QuantizationConfigMixin):\n+    \"\"\"\n+    This is a wrapper class that handles compressed-tensors quantization config options.\n+    It is a wrapper around `compressed_tensors.QuantizationConfig`\n+    Args:\n+        config_groups (`typing.Dict[str, typing.Union[ForwardRef('QuantizationScheme'), typing.List[str]]]`, *optional*):\n+            dictionary mapping group name to a quantization scheme definition\n+        format (`str`, *optional*, defaults to `\"dense\"`):\n+            format the model is represented as\n+        quantization_status (`QuantizationStatus`, *optional*, defaults to `\"initialized\"`):\n+            status of model in the quantization lifecycle, ie 'initialized', 'calibration', 'frozen'\n+        kv_cache_scheme (`typing.Union[QuantizationArgs, NoneType]`, *optional*):\n+            specifies quantization of the kv cache. If None, kv cache is not quantized.\n+        global_compression_ratio (`typing.Union[float, NoneType]`, *optional*):\n+            0-1 float percentage of model compression\n+        ignore (`typing.Union[typing.List[str], NoneType]`, *optional*):\n+            layer names or types to not quantize, supports regex prefixed by 're:'\n+        sparsity_config (`typing.Dict[str, typing.Any]`, *optional*):\n+            configuration for sparsity compression\n+        quant_method (`str`, *optional*, defaults to `\"compressed-tensors\"`):\n+            do not override, should be compressed-tensors\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        config_groups: Dict[str, Union[\"QuantizationScheme\", List[str]]] = None,  # noqa: F821\n+        format: str = \"dense\",\n+        quantization_status: \"QuantizationStatus\" = \"initialized\",  # noqa: F821\n+        kv_cache_scheme: Optional[\"QuantizationArgs\"] = None,  # noqa: F821\n+        global_compression_ratio: Optional[float] = None,\n+        ignore: Optional[List[str]] = None,\n+        sparsity_config: Dict[str, Any] = None,\n+        quant_method: str = \"compressed-tensors\",\n+        **kwargs,\n+    ):\n+        from compressed_tensors import QuantizationConfig\n+        from compressed_tensors.config import SparsityCompressionConfig\n+\n+        self.quantization_config = None\n+        self.sparsity_config = None\n+\n+        # parse from dict to load nested QuantizationScheme objects\n+        if config_groups:\n+            self.quantization_config = QuantizationConfig.parse_obj(\n+                {\n+                    \"config_groups\": config_groups,\n+                    \"quant_method\": quant_method,\n+                    \"format\": format,\n+                    \"quantization_status\": quantization_status,\n+                    \"kv_cache_scheme\": kv_cache_scheme,\n+                    \"global_compression_ratio\": global_compression_ratio,\n+                    \"ignore\": ignore,\n+                    **kwargs,\n+                }\n+            )\n+\n+        if sparsity_config:\n+            self.sparsity_config = SparsityCompressionConfig.load_from_registry(\n+                sparsity_config.get(\"format\"), **sparsity_config\n+            )\n+\n+        super().__init__(quant_method=QuantizationMethod.COMPRESSED_TENSORS)\n+\n+    @classmethod\n+    def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n+        \"\"\"\n+        Instantiates a [`CompressedTensorsConfig`] from a Python dictionary of parameters.\n+        Optionally unwraps any args from the nested quantization_config\n+\n+        Args:\n+            config_dict (`Dict[str, Any]`):\n+                Dictionary that will be used to instantiate the configuration object.\n+            return_unused_kwargs (`bool`,*optional*, defaults to `False`):\n+                Whether or not to return a list of unused keyword arguments. Used for `from_pretrained` method in\n+                `PreTrainedModel`.\n+            kwargs (`Dict[str, Any]`):\n+                Additional parameters from which to initialize the configuration object.\n+\n+        Returns:\n+            [`QuantizationConfigMixin`]: The configuration object instantiated from those parameters.\n+        \"\"\"\n+        if \"quantization_config\" in config_dict:\n+            config_dict = dict(\n+                sparsity_config=config_dict.get(\"sparsity_config\"),\n+                **config_dict[\"quantization_config\"],\n+            )\n+\n+        return super().from_dict(config_dict, return_unused_kwargs=return_unused_kwargs, **kwargs)\n+\n+    def to_dict(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. Returns:\n+            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance.\n+        \"\"\"\n+        quantization_config = self.quantization_config.dict() if self.quantization_config is not None else None\n+        sparsity_config = self.sparsity_config.dict() if self.sparsity_config is not None else None\n+\n+        return {\n+            \"quantization_config\": quantization_config,\n+            \"sparsity_config\": sparsity_config,\n+        }\n+\n+    def to_diff_dict(self) -> Dict[str, Any]:\n+        \"\"\"\n+        Removes all attributes from config which correspond to the default config attributes for better readability and\n+        serializes to a Python dictionary.\n+        Returns:\n+            `Dict[str, Any]`: Dictionary of all the attributes that make up this configuration instance,\n+        \"\"\"\n+        config_dict = self.to_dict()\n+\n+        # get the default config dict\n+        default_config_dict = CompressedTensorsConfig().to_dict()\n+\n+        serializable_config_dict = {}\n+\n+        # only serialize values that differ from the default config\n+        for key, value in config_dict.items():\n+            if value != default_config_dict[key]:\n+                serializable_config_dict[key] = value\n+\n+        return serializable_config_dict\n+\n+\n @dataclass\n class FbgemmFp8Config(QuantizationConfigMixin):\n     \"\"\""
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/quantization/compressed_tensor/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/tests%2Fquantization%2Fcompressed_tensor%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/tests%2Fquantization%2Fcompressed_tensor%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2F__init__.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b"
        },
        {
            "sha": "cbcf492f7c97533a6b6ed5294f2fcf3aaa8ede28",
            "filename": "tests/quantization/compressed_tensor/test_compressed_tensors.py",
            "status": "added",
            "additions": 87,
            "deletions": 0,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/574a9e12bbe787a4f6b2c170962be84e17c87d0b/tests%2Fquantization%2Fcompressed_tensor%2Ftest_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/574a9e12bbe787a4f6b2c170962be84e17c87d0b/tests%2Fquantization%2Fcompressed_tensor%2Ftest_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fcompressed_tensor%2Ftest_compressed_tensors.py?ref=574a9e12bbe787a4f6b2c170962be84e17c87d0b",
            "patch": "@@ -0,0 +1,87 @@\n+import gc\n+import unittest\n+\n+from transformers import AutoModelForCausalLM, AutoTokenizer, CompressedTensorsConfig\n+from transformers.testing_utils import require_compressed_tensors, require_torch\n+from transformers.utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@require_compressed_tensors\n+@require_torch\n+class CompressedTensorsTest(unittest.TestCase):\n+    tinyllama_w8a16 = \"nm-testing/tinyllama-w8a16-dense-hf-quantizer\"\n+    tinyllama_w4a16 = \"nm-testing/tinyllama-w4a16-compressed-hf-quantizer\"\n+    tinyllama_w8a8 = \"nm-testing/tinyllama-w8a8-compressed-hf-quantizer\"\n+    llama3_8b_fp8 = \"nm-testing/Meta-Llama-3-8B-Instruct-fp8-hf_compat\"\n+\n+    prompt = \"Paris is the capital of which country?\"\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+        gc.collect()\n+\n+    def test_config_args(self):\n+        with self.assertRaises(ValueError):\n+            # passing quant scheme directly is not allowed\n+            CompressedTensorsConfig(config_groups={\"weights\": {\"num_bits\": 8}})\n+        CompressedTensorsConfig(\n+            config_groups={\"FP8\": [\"Linear\"]},\n+            ignore=[\"lm_head\"],\n+            quantization_status=\"frozen\",\n+            sparsity_config={\"format\": \"dense\"},\n+        )\n+\n+    def test_config_to_from_dict(self):\n+        config = CompressedTensorsConfig(config_groups={\"FP8\": [\"Linear\"]}, sparsity_config={\"format\": \"dense\"})\n+        config_dict = config.to_dict()\n+        config_from_dict = CompressedTensorsConfig.from_dict(config_dict)\n+\n+        from compressed_tensors import QuantizationConfig, SparsityCompressionConfig\n+\n+        self.assertIsInstance(config_from_dict.quantization_config, QuantizationConfig)\n+        self.assertIsInstance(config_from_dict.sparsity_config, SparsityCompressionConfig)\n+\n+    def test_tinyllama_w8a8(self):\n+        expected_out = \"<s> Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country?\\n\\n**A) Paris**\\n\\n**Q** ** Paris is the capital of which country\"\n+        self._test_quantized_model(self.tinyllama_w8a8, expected_out)\n+\n+    def test_tinyllama_w4a16(self):\n+        expected_out = \"<s> Paris is the capital of which country?\\nAnswer: Paris is the capital of France.\\nQuestion: Which country is the capital of which city?\\nAnswer: The capital of the city of New York is New York.\\nQuestion: Which\"\n+        self._test_quantized_model(self.tinyllama_w4a16, expected_out)\n+\n+    def test_tinyllama_w8a16(self):\n+        expected_out = \"<s> Paris is the capital of which country?\\nA. France\\nB. Germany\\nC. Spain\\nD. Italy\\nE. Switzerland\\nQ10. Which of the following is not a country in the European Union?\\nA.\"\n+        self._test_quantized_model(self.tinyllama_w8a16, expected_out)\n+\n+    def test_llama_8b_fp8(self):\n+        expected_out = \"<|begin_of_text|>Paris is the capital of which country? France\\nWhat is the name of the famous art museum in Paris? The Louvre\\nWhat is the name of the famous opera house in Paris? Palais Garnier\\nWhat is the name of the\"\n+        self._test_quantized_model(self.llama3_8b_fp8, expected_out)\n+\n+    def _test_quantized_model(self, model_name: str, expected_output: str):\n+        \"\"\"Carry out generation\"\"\"\n+        quantized_model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n+        tokenizer = AutoTokenizer.from_pretrained(model_name)\n+        device = quantized_model.device\n+        self.assertIsNotNone(\n+            quantized_model.config.quantization_config,\n+            \"quantization_config should not be None\",\n+        )\n+        self.assertTrue(\n+            any(\n+                key\n+                for key, tensor in quantized_model.state_dict().items()\n+                if \"scale\" in key and not torch.all(tensor == 1.0)\n+            ),\n+            \"quantized model should load a non-trivial scale into the state dict\",\n+        )\n+        inputs = tokenizer(self.prompt, return_tensors=\"pt\").to(device)\n+        generated_ids = quantized_model.generate(**inputs, max_length=50, do_sample=False)\n+        outputs = tokenizer.batch_decode(generated_ids)\n+\n+        self.assertIsNotNone(outputs)\n+        self.assertEqual(outputs[0], expected_output)"
        }
    ],
    "stats": {
        "total": 547,
        "additions": 546,
        "deletions": 1
    }
}